

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="https://cdn.jsdelivr.net/gh/Enderfga/Enderfga/Backup/favicon.png">
  <link rel="icon" href="https://cdn.jsdelivr.net/gh/Enderfga/Enderfga/Backup/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="机器学习期中大作业，神经网络回归与分类">
  <meta name="author" content="Enderfga">
  <meta name="keywords" content="">
  <meta name="description" content="机器学习期中大作业，神经网络回归与分类">
<meta property="og:type" content="article">
<meta property="og:title" content="神经网络回归与分类（波士顿房价与红酒分类）">
<meta property="og:url" content="http://example.com/2021/11/16/middle/index.html">
<meta property="og:site_name" content="Enderfga&#39;Blog">
<meta property="og:description" content="机器学习期中大作业，神经网络回归与分类">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211114180429830.png">
<meta property="og:image" content="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211114180032126.png">
<meta property="og:image" content="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211113213541281.png">
<meta property="og:image" content="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211113213557078.png">
<meta property="og:image" content="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211114180235544.png">
<meta property="og:image" content="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211113213650281.png">
<meta property="og:image" content="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211113213743041.png">
<meta property="og:image" content="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211113213753140.png">
<meta property="og:image" content="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211113192037294.png">
<meta property="og:image" content="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211113192136112.png">
<meta property="og:image" content="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211114180628211.png">
<meta property="og:image" content="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211112080119294.png">
<meta property="og:image" content="https://gitee.com/sysu_20354027/pic/raw/master/img/482493cc74974f85ba09b50697801c27.jpeg">
<meta property="og:image" content="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211114180715729.png">
<meta property="og:image" content="https://gitee.com/sysu_20354027/pic/raw/master/img/998084-20180726204924171-1721363009.png">
<meta property="og:image" content="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211114102651780.png">
<meta property="og:image" content="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211114171851292.png">
<meta property="og:image" content="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211114171902074.png">
<meta property="og:image" content="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211114222524771.png">
<meta property="og:image" content="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211114221915446.png">
<meta property="og:image" content="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211114221934910.png">
<meta property="og:image" content="https://gitee.com/sysu_20354027/pic/raw/master/img/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy92STluWWU5NGZzRmJubFJEeHdxZVozWG9YQ3dZR1JnVHdDRHFLaWFBZGRWZlp5elJKNnREekE3ejJCUlFpY0RzUlJhWGVpYVd6aWFoaWNldlFyRFIyVU1RQm9nLzY0MA">
<meta property="article:published_time" content="2021-11-16T11:57:45.000Z">
<meta property="article:modified_time" content="2021-11-16T12:03:17.704Z">
<meta property="article:author" content="Enderfga">
<meta property="article:tag" content="course">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211114180429830.png">
  
  <title>神经网络回归与分类（波士顿房价与红酒分类） - Enderfga&#39;Blog</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.4.0/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.8.12","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname"}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Enderfga</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/">
                <i class="iconfont icon-link-fill"></i>
                友链
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('https://cdn.jsdelivr.net/gh/Enderfga/Enderfga/Backup/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="神经网络回归与分类（波士顿房价与红酒分类）">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2021-11-16 19:57" pubdate>
        2021年11月16日 晚上
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      13k 字
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      40 分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">神经网络回归与分类（波士顿房价与红酒分类）</h1>
            
              <p class="note note-info">
                
                  本文最后更新于：2021年11月16日 晚上
                
              </p>
            
            <div class="markdown-body">
              <p>机器学习期中大作业，神经网络回归与分类</p>
<span id="more"></span>
<h1 id="一、描述所使用的神经网络模型"><a href="#一、描述所使用的神经网络模型" class="headerlink" title="一、描述所使用的神经网络模型"></a>一、描述所使用的神经网络模型</h1><h2 id="1-1-神经元模型"><a href="#1-1-神经元模型" class="headerlink" title="1.1 神经元模型"></a>1.1 神经元模型</h2><h3 id="1-1-1-神经元模型的定义"><a href="#1-1-1-神经元模型的定义" class="headerlink" title="1.1.1 神经元模型的定义"></a>1.1.1 神经元模型的定义</h3><p><strong>神经网络是由具有适应性的简单单元组成的广泛并行互联的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互反应。神经网络中最基本的成分是神经元模型，即上述的“简单单元”。</strong></p>
<h3 id="1-1-2-M-P神经元模型"><a href="#1-1-2-M-P神经元模型" class="headerlink" title="1.1.2 M-P神经元模型"></a>1.1.2 M-P神经元模型</h3><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211114180429830.png" srcset="/img/loading.gif" lazyload alt="image-20211114180429830"></p>
<p><strong>输入</strong>：来自其他n个神经元传递过来的输入信号</p>
<p><strong>处理</strong>：输入信号通过带权重的连接进行传递, 神经元接受到总输入值将其与神经元的阈值进行比较</p>
<p><strong>输出</strong>：通过激活函数的处理以得到输出</p>
<h3 id="1-1-3-激活函数"><a href="#1-1-3-激活函数" class="headerlink" title="1.1.3 激活函数"></a>1.1.3 激活函数</h3><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211114180032126.png" srcset="/img/loading.gif" lazyload alt="image-20211114180032126"></p>
<h4 id="1-1-3-1-激活函数的介绍"><a href="#1-1-3-1-激活函数的介绍" class="headerlink" title="1.1.3.1 激活函数的介绍"></a>1.1.3.1 激活函数的介绍</h4><p><strong>如下图所示，神经网络中的每个神经元节点接受上一层神经元的输出值作为本神经元的输入值，并将输入值传递给下一层，输入层神经元节点会将输入属性值直接传递给下一层（隐层或输出层）。在多层神经网络中，上层节点的输出和下层节点的输入之间具有一个函数关系，这个函数称为激活函数（又称激励函数）。</strong></p>
<p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211113213541281.png" srcset="/img/loading.gif" lazyload alt="image-20211113213541281"></p>
<h4 id="1-1-3-2-激活函数的用途"><a href="#1-1-3-2-激活函数的用途" class="headerlink" title="1.1.3.2 激活函数的用途"></a>1.1.3.2 激活函数的用途</h4><p><strong>如果不用激励函数（相当于激励函数是f(x) = x），每一层节点的输入都是上层输出的线性函数，这样无论神经网络有多少层，输出都是输入的线性组合，这种情况就是最原始的感知机，网络的逼近能力就相当有限。当我们引入非线性函数作为激励函数，深层神经网络表达能力就更加强大，不再是输入的线性组合，几乎可以逼近任意函数。</strong></p>
<h4 id="1-1-3-3-一些常见的激活函数及其性质"><a href="#1-1-3-3-一些常见的激活函数及其性质" class="headerlink" title="1.1.3.3 一些常见的激活函数及其性质"></a>1.1.3.3 一些常见的激活函数及其性质</h4><h5 id="1-1-3-3-1-Relu激活函数"><a href="#1-1-3-3-1-Relu激活函数" class="headerlink" title="1.1.3.3.1 Relu激活函数"></a>1.1.3.3.1 Relu激活函数</h5><p><strong>Relu函数的解析式：</strong></p>
<script type="math/tex; mode=display">
f_{Relu}(x)=max(0,x)</script><p><strong>Relu函数及其导数的图像如下图所示：</strong></p>
<p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211113213557078.png" srcset="/img/loading.gif" lazyload alt="image-20211113213557078"></p>
<p><strong>优点：</strong></p>
<p><strong>①计算效率较高</strong></p>
<p><strong>②兼具线性和非线性特性</strong></p>
<p><strong>缺点：</strong></p>
<p><strong>梯度消失问题：在x&lt;0时，神经元保持非激活状态，且在反向传导（backward pass）中梯度为零</strong></p>
<h5 id="1-1-3-3-2-Sigmoid激活函数"><a href="#1-1-3-3-2-Sigmoid激活函数" class="headerlink" title="1.1.3.3.2 Sigmoid激活函数"></a>1.1.3.3.2 Sigmoid激活函数</h5><p><strong>Sigmoid 函数的解析式：</strong></p>
<script type="math/tex; mode=display">
f_{Sigmoid}(x)=\frac{1}{1+e^{-x}}</script><p><strong>Sigmoid函数及其导数的图像如下图所示：</strong></p>
<p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211114180235544.png" srcset="/img/loading.gif" lazyload alt="image-20211114180235544"></p>
<p><strong>优点：</strong></p>
<p><strong>①梯度的“平滑性”</strong></p>
<p><strong>②输出在“0-1区间”</strong></p>
<p><strong>缺点：</strong></p>
<p><strong>①梯度消失问题：神经网络使用 Sigmoid 激活函数进行反向传播时，输出接近0或1的神经元其梯度趋近于0 </strong></p>
<p><strong>②计算成本问题：涉及指数计算</strong></p>
<p><strong>③不以零为中心：Sigmoid 输出不以零为中心</strong></p>
<h5 id="1-1-3-3-3-tanh-激活函数"><a href="#1-1-3-3-3-tanh-激活函数" class="headerlink" title="1.1.3.3.3  tanh 激活函数"></a>1.1.3.3.3  tanh 激活函数</h5><p><strong>tanh函数的解析式：</strong></p>
<script type="math/tex; mode=display">
f_{tanh}(x)=\frac{1-e^{-2x}}{1+e^{-2x}}</script><p><strong>tanh函数及其导数的图像如下图所示：</strong></p>
<p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211113213650281.png" srcset="/img/loading.gif" lazyload alt="image-20211113213650281"></p>
<p><strong>优点：</strong></p>
<p><strong>①梯度的“平滑性”</strong></p>
<p><strong>②输出以零为中心</strong></p>
<p><strong>缺点：</strong></p>
<p><strong>①梯度消失问题：神经网络使用 tanh 激活函数进行反向传播时，输出接近-1或1的神经元其梯度趋近于0</strong></p>
<p><strong>②计算成本问题：涉及指数计算</strong></p>
<h2 id="1-2-神经网络模型"><a href="#1-2-神经网络模型" class="headerlink" title="1.2 神经网络模型"></a>1.2 神经网络模型</h2><h3 id="1-2-1-神经网络模型的定义"><a href="#1-2-1-神经网络模型的定义" class="headerlink" title="1.2.1 神经网络模型的定义"></a>1.2.1 神经网络模型的定义</h3><p><strong>将若干神经元按一定的层次结构连接起来就得到了神经网络，可将神经网络视为包含了若干参数的数学模型，这个模型是由若干个函数相互（嵌套）代入而得。</strong></p>
<p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211113213743041.png" srcset="/img/loading.gif" lazyload alt="image-20211113213743041"></p>
<h3 id="1-2-2-多层前馈神经网络"><a href="#1-2-2-多层前馈神经网络" class="headerlink" title="1.2.2 多层前馈神经网络"></a>1.2.2 多层前馈神经网络</h3><h4 id="1-2-2-1-定义"><a href="#1-2-2-1-定义" class="headerlink" title="1.2.2.1 定义"></a>1.2.2.1 定义</h4><p><strong>每层神经元与下一层神经元全互联，神经元之间不存在同层连接也不存在跨层连接。输入层接受外界输入，隐含层与输出层神经元对信号进行加工，最终结果由输出层神经元输出。根据训练数据来调整神经元之间的“连接权”以及每个功能神经元的“阈值”。</strong></p>
<p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211113213753140.png" srcset="/img/loading.gif" lazyload alt="image-20211113213753140"></p>
<h4 id="1-2-2-2-模型训练"><a href="#1-2-2-2-模型训练" class="headerlink" title="1.2.2.2 模型训练"></a>1.2.2.2 模型训练</h4><p><strong>数据</strong>：</p>
<script type="math/tex; mode=display">
D=\{(x_1,y_1),(x_2,y_2),……,(x_m,y_m)\},x_i∈R^d,y_i∈R^l</script><p><strong>模型</strong>：若干神经元按一定的层次结构连接起来，每层神经元与下一层神经元全互联，神经元之间不存在同层连接也不存在跨层连接，所形成的神经网络模型。</p>
<p><strong>策略</strong>：</p>
<p><strong>①平方损失（回归问题）</strong></p>
<script type="math/tex; mode=display">
L(y_i,f(x_i))=(y_i-f(x_i)^2),ERM</script><p><strong> ②交叉熵损失（二分类问题）</strong></p>
<hr>
<script type="math/tex; mode=display">
L(y_i,f(x_i))=-y_ilog(p(y_i=1|x_i))-(1-y_i)log(p(y_i=0|x_i))</script><p><strong>算法</strong>：误差逆传播算法（error Back Propagation）</p>
<h1 id="二、描述训练模型所使用的算法"><a href="#二、描述训练模型所使用的算法" class="headerlink" title="二、描述训练模型所使用的算法"></a>二、描述训练模型所使用的算法</h1><h1 id="2-1-误差逆传播算法"><a href="#2-1-误差逆传播算法" class="headerlink" title="2.1 误差逆传播算法"></a>2.1 误差逆传播算法</h1><h3 id="2-1-1-应用领域"><a href="#2-1-1-应用领域" class="headerlink" title="2.1.1 应用领域"></a>2.1.1 应用领域</h3><p><strong>反向传播算法应用较为广泛，从字面意思理解，与前向传播相互对应。在简单的神经网络中，反向传播算法，可以理解为最优化损失函数过程，求解每个参与运算的参数的梯度的方法。在前馈神经网中，反向传播从求解损失函数偏导过程中，步步向前求解每一层的参数梯度。在卷积神经网络中，反向传播可以求解全连接层的参数梯度。在循环神经网络中，反向传播算法可以求解每一个时刻t或者状态t的参数梯度（在RNN\LSTM\GRU中，反向传播更多是BPTT）。如今对于BP的理解，认为是在优化损失函数或者目标函数过程中，求解参与运算的参数的梯度方法，是一种比较普遍的说法。</strong></p>
<h3 id="2-1-2-网络结构"><a href="#2-1-2-网络结构" class="headerlink" title="2.1.2 网络结构"></a>2.1.2 网络结构</h3><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211113192037294.png" srcset="/img/loading.gif" lazyload alt="image-20211113192037294"></p>
<ol>
<li><strong>正向传播求损失，反向传播回传误差</strong></li>
<li><strong>根据误差信号修正每层的权重</strong></li>
<li><strong>f是激活函数；f(netj)是隐层的输出； f(netk）是输出层的输出O; d是target。</strong></li>
</ol>
<h3 id="2-1-2-基本参数结构"><a href="#2-1-2-基本参数结构" class="headerlink" title="2.1.2 基本参数结构"></a>2.1.2 基本参数结构</h3><p><strong>为了方便讨论，我们以一个隐层的神经网络结构进行推导。多隐层的神经网络推导思想与此类似，可推广。如下图为一个神经网络结构。</strong></p>
<p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211113192136112.png" srcset="/img/loading.gif" lazyload alt="image-20211113192136112"></p>
<h4 id="2-1-2-1参数简述"><a href="#2-1-2-1参数简述" class="headerlink" title="2.1.2.1参数简述"></a>2.1.2.1参数简述</h4><script type="math/tex; mode=display">
\begin{align}
&输入参数：x_1^k,\dots ,x_i^k,\dots,x_d^k\\
&输入层到第一隐层第h个神经元的权重：v_{1h},\dots,v_{ih},\dots,v_{dh}\\
&第一层第h个神经元输入：\alpha _{h}= \sum_{i=1}^{d}v_{ih}x_i^k\\
&第一隐层阙值：\gamma _{1},\dots,\gamma _{h},\dots,\gamma _{q}\\
&第一隐层第h个输出：b_h=f_{sigmoid}(\alpha _h-\gamma _h) \\
&第一隐层到第j个输出神经元的权重：w_{1j},\dots,w_{hj},\dots,w_{qj}\\
&第j个输出神经元的输入：\beta _j=\sum_{h=1}^{q}w_{hj}b_h\\
&输出层阙值：\theta _{1},\dots,\theta _{j},\dots,\theta_{l}\\
&输出值：y_j^k=f_{sigmoid}(\alpha _h-\gamma _h)
\end{align}</script><p><strong>所以前向传播计算误差为：</strong></p>
<script type="math/tex; mode=display">
E_{k}=\frac{1}{2} \sum_{j=1}^{l}\left(y_{j}^{k}-\hat{y}_{j}^{k}\right)^{2}</script><h3 id="2-1-3-参数调整策略"><a href="#2-1-3-参数调整策略" class="headerlink" title="2.1.3 参数调整策略"></a>2.1.3 参数调整策略</h3><p><strong>BP算法的核心思想：使用梯度下降来搜索可能的权向量的假设空间，以找到最佳的拟合样例的权向量。具体而言，即利用损失函数，每次向损失函数负梯度方向移动，直到损失函数取得最小值。或者说，反向传播算法，是根据损失函数，求出损失函数关于每一层的权值及偏置项的偏导数，也称为梯度，用该值更新初始的权值和偏置项，一直更新到损失函数取得最小值或是设置的迭代次数完成为止。以此来计算神经网络中的最佳的参数。</strong></p>
<script type="math/tex; mode=display">
损失函数：E_{k}=\frac{1}{2} \sum_{j=1}^{l}\left(y_{j}^{k}-\hat{y}_{j}^{k}\right)^{2}</script><h4 id="2-1-3-1-计算准备"><a href="#2-1-3-1-计算准备" class="headerlink" title="2.1.3.1 计算准备"></a>2.1.3.1 计算准备</h4><script type="math/tex; mode=display">
\frac{\partial E_k}{\partial \hat{y_j^k} }=(y_j^k-\hat{y_j^k})(-1)\\
f_{sigmoid}(x)^{(1)}=f_{sigmoid}(x)(1-f_{sigmoid}(x))\\
\eta:学习率</script><p><strong>下面，我们讲分别讨论每个参数的更新：</strong></p>
<h4 id="2-1-3-2-w更新"><a href="#2-1-3-2-w更新" class="headerlink" title="2.1.3.2 w更新"></a>2.1.3.2 <strong>w</strong>更新</h4><script type="math/tex; mode=display">
更新公式：w_{hj}=w_{hj}+\bigtriangleup w_{hj}</script><p><strong>下面对</strong>\bigtriangleup w_{hj}<strong>进行讨论:</strong></p>
<script type="math/tex; mode=display">
\begin{align}
\bigtriangleup w_{hj}=&-\eta  \frac{\partial E_k}{\partial w_{hj}}\\
=&-\eta (\frac{\partial E_k}{\partial \hat{y_j^k} }\cdot\frac{\partial \hat{y_j^k}}{\partial \beta _j}  \cdot \frac{\partial \beta _j}{\partial w_{hj}}  )\\
=&-\eta (y_j^k-\hat{y_j^k})(-1)\cdot \hat{y_j^k} (1-\hat{y_j^k})\cdot b_h\\
令g_j&=(y_j^k-\hat{y_j^k})\cdot \hat{y_j^k} (1-\hat{y_j^k})\\
\\
最终可得&\bigtriangleup w_{hj}=\eta g_jb_h
\end{align}</script><h4 id="2-1-3-3-theta更新"><a href="#2-1-3-3-theta更新" class="headerlink" title="2.1.3.3 \theta更新"></a>2.1.3.3 <strong>\theta</strong>更新</h4><script type="math/tex; mode=display">
更新公式：\theta_{j}=\theta_{j}+\bigtriangleup \theta_{j}</script><p><strong>下面对</strong>\bigtriangleup \theta_{j}<strong>进行讨论：</strong></p>
<script type="math/tex; mode=display">
\begin{align}
\bigtriangleup \theta_j=&-\eta  \frac{\partial E_k}{\partial \theta_{j}}\\
=&-\eta  (\frac{\partial E_k}{\partial \hat{y_j^k} }\cdot\frac{\partial \hat{y_j^k}}{\partial \theta _j}    )\\
=&-\eta (y_j^k-\hat{y_j^k})(-1)\cdot \hat{y_j^k} (1-\hat{y_j^k})\cdot(-1)\\
=&-\eta g_j
\end{align}</script><h4 id="2-1-3-4-v更新"><a href="#2-1-3-4-v更新" class="headerlink" title="2.1.3.4 v更新"></a>2.1.3.4 <strong>v</strong>更新</h4><script type="math/tex; mode=display">
更新公式：v_{ih}=v_{ih}+\bigtriangleup v_{ih}</script><p><strong>下面对</strong>\bigtriangleup v_{ih}<strong>进行讨论：</strong></p>
<script type="math/tex; mode=display">
\begin{align}
\bigtriangleup v_{ih}=&-\eta \frac{\partial E_k}{\partial v_{ih}}\\ 
=&-\eta\frac{\partial E_k}{\partial b_h}\cdot  \frac{\partial b_h}{\partial \alpha _h}\cdot  \frac{\partial \alpha _h}{\partial v_{ih}} \\
=&-\eta(\sum_{j=1}^{l}\frac{\partial E_k}{\partial \hat{y_j^k} }\cdot\frac{\partial \hat{y_j^k}}{\partial \beta _j}  \cdot \frac{\partial \beta _j}{\partial b_{h}})\cdot b_h(1-b_h)\cdot x_i^k\\
=&\eta \sum_{j=1}^{l}g_jw_{hj}b_h(1-b_h)x_i^k 
\end{align}</script><h4 id="2-1-3-5-gamma更新"><a href="#2-1-3-5-gamma更新" class="headerlink" title="2.1.3.5 \gamma更新"></a>2.1.3.5 <strong>\gamma</strong>更新</h4><script type="math/tex; mode=display">
更新公式：\gamma_{h}=\gamma_{h}+\bigtriangleup \gamma_{h}</script><p><strong>下面对</strong>\bigtriangleup \gamma_{h}<strong>进行讨论：</strong></p>
<script type="math/tex; mode=display">
\begin{align}
\bigtriangleup \gamma_{h}=&-\eta \frac{\partial E_k}{\partial \gamma_{h}}\\ 
=&-\eta\frac{\partial E_k}{\partial b_h}\cdot  \frac{\partial b_h}{\partial \gamma _h} \\
=&-\eta(\sum_{j=1}^{l}\frac{\partial E_k}{\partial \hat{y_j^k} }\cdot\frac{\partial \hat{y_j^k}}{\partial \beta _j}  \cdot \frac{\partial \beta _j}{\partial b_{h}})\cdot b_h(1-b_h)\cdot (-1)\\
=&\eta \sum_{j=1}^{l}g_jw_{hj}b_h(1-b_h)(-1)
\end{align}</script><h3 id="2-1-3-算法伪代码"><a href="#2-1-3-算法伪代码" class="headerlink" title="2.1.3 算法伪代码"></a>2.1.3 算法伪代码</h3><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211114180628211.png" srcset="/img/loading.gif" lazyload alt="image-20211114180628211"></p>
<h1 id="三、描述模型超参数确定的过程，分析模型训练结果"><a href="#三、描述模型超参数确定的过程，分析模型训练结果" class="headerlink" title="三、描述模型超参数确定的过程，分析模型训练结果"></a>三、描述模型超参数确定的过程，分析模型训练结果</h1><h2 id="3-1-超参数的概念"><a href="#3-1-超参数的概念" class="headerlink" title="3.1 超参数的概念"></a>3.1 超参数的概念</h2><p><strong>大部分机器学习算法都需要花费大量时间去训练，而在训练之前需要提前配置一些变量。这些变量对训练结果影响很大，但没有对任何数据集都适用的一组变量，需要根据具体应用具体配置，这些需要配置的变量称之为超参数（hyperparameters）。区分超参数和模型参数最大的一点就是是否通过数据来进行调整，模型参数通常是有数据来驱动调整，超参数则不需要数据来驱动，而是在训练前或者训练中人为的进行调整的参数。例如卷积核的具体核参数就是指模型参数，这是由数据驱动的。而学习率则是人为来进行调整的超参数。这里需要注意的是，通常情况下卷积核数量、卷积核尺寸这些也是超参数，注意与卷积核的核参数区分。</strong></p>
<h2 id="3-2-神经网络包含的超参数"><a href="#3-2-神经网络包含的超参数" class="headerlink" title="3.2 神经网络包含的超参数"></a>3.2 神经网络包含的超参数</h2><h3 id="3-2-1-超参数种类"><a href="#3-2-1-超参数种类" class="headerlink" title="3.2.1 超参数种类"></a>3.2.1 超参数种类</h3><p><strong>通常可以将超参数分为三类：网络参数、优化参数、正则化参数。</strong></p>
<p><strong> 网络参数：可指网络层与层之间的交互方式（相加、相乘或者串接等）、卷积核数量和卷积核尺寸、网络层数（也称深度）和激活函数等。</strong></p>
<p><strong> 优化参数：一般指学习率（learning rate）、批样本数量（batch size）、不同优化器的参数以及部分损失函数的可调参数。</strong></p>
<p><strong> 正则化：权重衰减系数，丢弃法比率（dropout）</strong></p>
<p><strong>神经网络包含的超参数具体为以下十一个：</strong></p>
<ol>
<li><strong>学习率 η</strong></li>
<li><strong>正则化参数 λ</strong></li>
<li><strong>神经网络的层数 L</strong></li>
<li><strong>每一个隐层中神经元的个数 j</strong></li>
<li><strong>学习的回合数Epoch</strong></li>
<li><strong>小批量数据 minibatch 的大小</strong></li>
<li><strong>输出神经元的编码方式</strong></li>
<li><strong>代价函数的选择</strong></li>
<li><strong>权重初始化的方法</strong></li>
<li><strong>神经元激活函数的种类</strong></li>
<li><strong>参加训练模型数据的规模</strong></li>
</ol>
<p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211112080119294.png" srcset="/img/loading.gif" lazyload alt="image-20211112080119294"></p>
<p><strong>在上图中可以看到超参数 2，3，4， 7 主要影响的时神经网络的分类正确率；9 主要影响代价函数曲线下降速度，同时有时也会影响正确率；1，8，10 主要影响学习速度，这点主要体现在训练数据代价函数曲线的下降速度上；5，6，11 主要影响模型分类正确率和训练用总体时间。这上面所提到的时某个超参数对于神经网络想到的首要影响，并不代表着该超参数只影响学习速度或者正确率。</strong></p>
<h3 id="3-2-2-超参数重要性顺序"><a href="#3-2-2-超参数重要性顺序" class="headerlink" title="3.2.2 超参数重要性顺序"></a>3.2.2 超参数重要性顺序</h3><ul>
<li><strong>首先， 学习率，损失函数上的可调参数。在网络参数、优化参数、正则化参数中最重要的超参数可能就是学习率了。学习率直接控制着训练中网络梯度更新的量级，直接影响着模型的有效容限能力；损失函数上的可调参数，这些参数通常情况下需要结合实际的损失函数来调整，大部分情况下这些参数也能很直接的影响到模型的的有效容限能力。这些损失一般可分成三类，第一类辅助损失结合常见的损失函数，起到辅助优化特征表达的作用。例如度量学习中的Center loss，通常结合交叉熵损失伴随一个权重完成一些特定的任务。这种情况下一般建议辅助损失值不高于或者不低于交叉熵损失值的两个数量级；第二类，多任务模型的多个损失函数，每个损失函数之间或独立或相关，用于各自任务，这种情况取决于任务之间本身的相关性，目前笔者并没有一个普适的经验由于提供参考；第三类，独立损失函数，这类损失通常会在特定的任务有显著性的效果。例如RetinaNet中的focal loss，其中的参数γ，α，对最终的效果会产生较大的影响。这类损失通常论文中会给出特定的建议值。</strong></li>
<li><strong>其次，批样本数量，动量优化器（Gradient Descent with Momentum）的动量参数β。批样本决定了数量梯度下降的方向。过小的批数量，极端情况下，例如batch size为1，即每个样本都去修正一次梯度方向，样本之间的差异越大越难以收敛。若网络中存在批归一化（batchnorm），batch size过小则更难以收敛，甚至垮掉。这是因为数据样本越少，统计量越不具有代表性，噪声也相应的增加。而过大的batch size，会使得梯度方向基本稳定，容易陷入局部最优解，降低精度。一般参考范围会取在[1:1024]之间，当然这个不是绝对的，需要结合具体场景和样本情况；动量衰减参数β是计算梯度的指数加权平均数，并利用该值来更新参数，设置为 0.9 是一个常见且效果不错的选择；</strong></li>
<li><strong>最后，Adam优化器的超参数、权重衰减系数、丢弃法比率（dropout）和网络参数。在这里说明下，这些参数重要性放在最后并不等价于这些参数不重要。而是表示这些参数在大部分实践中不建议过多尝试，例如Adam优化器中的β1，β2，ϵ，常设为 0.9、0.999、10−8就会有不错的表现。权重衰减系数通常会有个建议值，例如0.0005 ，使用建议值即可，不必过多尝试。dropout通常会在全连接层之间使用防止过拟合，建议比率控制在[0.2,0.5]之间。使用dropout时需要特别注意两点：一、在RNN中，如果直接放在memory cell中,循环会放大噪声，扰乱学习。一般会建议放在输入和输出层；二、不建议dropout后直接跟上batchnorm，dropout很可能影响batchnorm计算统计量，导致方差偏移，这种情况下会使得推理阶段出现模型完全垮掉的极端情况；网络参数通常也属于超参数的范围内，通常情况下增加网络层数能增加模型的容限能力，但模型真正有效的容限能力还和样本数量和质量、层之间的关系等有关，所以一般情况下会选择先固定网络层数，调优到一定阶段或者有大量的硬件资源支持可以在网络深度上进行进一步调整。</strong></li>
</ul>
<h2 id="3-3-模型超参数确定"><a href="#3-3-模型超参数确定" class="headerlink" title="3.3 模型超参数确定"></a>3.3 模型超参数确定</h2><h3 id="3-3-1-超参数调优的原因"><a href="#3-3-1-超参数调优的原因" class="headerlink" title="3.3.1 超参数调优的原因"></a>3.3.1 超参数调优的原因</h3><p><strong>本质上，这是模型优化寻找最优解和正则项之间的关系。网络模型优化调整的目的是为了寻找到全局最优解（或者相比更好的局部最优解），而正则项又希望模型尽量拟合到最优。两者通常情况下，存在一定的对立，但两者的目标是一致的，即最小化期望风险。模型优化希望最小化经验风险，而容易陷入过拟合，正则项用来约束模型复杂度。所以如何平衡两者之间的关系，得到最优或者较优的解就是超参数调整优化的目的。</strong></p>
<h3 id="3-3-2-模型超参数的确定"><a href="#3-3-2-模型超参数的确定" class="headerlink" title="3.3.2 模型超参数的确定"></a>3.3.2 模型超参数的确定</h3><p><strong>四种主流超参数调优技术：</strong></p>
<ol>
<li><strong>传统或手动调参</strong></li>
<li><strong>网格搜索</strong></li>
<li><strong>随机搜索</strong></li>
<li><strong>贝叶斯搜索</strong></li>
</ol>
<p><strong>在传统的调优中，我们通过手动检查随机超参数集来训练算法，并选择最适合我们目标的参数集。但这种方法不能保证得到最佳的参数组合，反复试验会消耗更多的时间。</strong></p>
<h4 id="3-3-2-1-网格搜索"><a href="#3-3-2-1-网格搜索" class="headerlink" title="3.3.2.1 网格搜索"></a>3.3.2.1 网格搜索</h4><ul>
<li><strong>网格搜索是一种基本的超参数调整技术。它类似于手动调优，为网格中指定的所有给定超参数值的每个排列建立模型，并评估和选择最佳模型。由于它尝试每一种超参数组合，并根据交叉验证分数选择最佳组合，这使得 GridsearchCV 极其缓慢。</strong></li>
<li><strong>这种启发式的搜索算法对超参数搜索算法，被称之为网格搜索。(如果人工处理所有可能的超参数组合，通常的办法是，根据超参数的维度，列成相应的表格，比如说k的取值有[2，3，4，5，6，7，8]，另一个系数比如λ取值有[0.01,0.03,0.1,0.3]等，这样就可以列出一个二维表格，组合出7*4种可能性的超参数组合，再对每一个格子中具体的超参数组合，通过交叉验证的方式进行模型性能的评估，然后通过验证性能的比较，最终筛选出最佳的超参数数据组合)</strong></li>
<li><strong>网格搜索采用交叉验证的方法，来寻找更好的超参数组合的过程非常耗时，由于各个新模型在执行交叉验证的过程中是相互独立的，那么我们可以充分利用多核处理器甚至是分布式的计算资源来从事并行搜索，从而成倍的节省运算时间。</strong><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/482493cc74974f85ba09b50697801c27.jpeg" srcset="/img/loading.gif" lazyload alt="img"></li>
</ul>
<h4 id="3-3-2-2-随机搜索"><a href="#3-3-2-2-随机搜索" class="headerlink" title="3.3.2.2 随机搜索"></a>3.3.2.2 随机搜索</h4><p><strong>使用随机搜索代替网格搜索的动机是，在许多情况下，所有的超参数可能并非同等重要。随机搜索从超参数空间中随机选择参数组合，参数按 n_iter 给定的迭代次数进行选择。随机搜索已经被实践证明比网格搜索得到的结果更好，但随机搜索的问题是它不能保证给出最佳的参数组合。</strong></p>
<p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211114180715729.png" srcset="/img/loading.gif" lazyload alt="image-20211114180715729"></p>
<h4 id="3-3-2-3-贝叶斯优化"><a href="#3-3-2-3-贝叶斯优化" class="headerlink" title="3.3.2.3 贝叶斯优化"></a>3.3.2.3 贝叶斯优化</h4><p><strong>贝叶斯优化属于一类被称为*</strong>sequential model-based optimization*(SMBO)的优化算法。这些算法使用先前对损失 f 的观测，来确定下一个(最佳)点来取样 f。该算法大致可以概括如下：</p>
<ol>
<li><strong>使用先前计算过的点 X1: n，计算损失 f 的后验期望值。</strong></li>
<li><strong>在一个新的点 Xnew取样损失 f ，它最大化了 f 的期望的某些效用函数。该函数指定 f 域的哪些区域是最适合采样的。</strong></li>
</ol>
<p><strong>重复这些步骤，直到达到某种收敛准则。</strong></p>
<h5 id="3-3-2-3-1-高斯过程"><a href="#3-3-2-3-1-高斯过程" class="headerlink" title="3.3.2.3.1 高斯过程"></a>3.3.2.3.1 高斯过程</h5><p><strong>在贝叶斯调参过程中，假设一组超参数组合是X=x1,x2,…,xn(xn表示某一个超参数的值)，而这组超参数与最后我们需要优化的损失函数存在一个函数关系，最终的评估结果为Y，通过什么样的X可以取得最优的Y，我们假设是f(X)， Y=F(X)</strong></p>
<p><strong>而目前机器学习其实是一个黑盒子(black box),即我们只知道input和output，所以上面的函数f(x)很难确定。所以我们需要将注意力转移到一个我们可以解决的函数上去.</strong></p>
<p><strong>于是可以假设这个寻找最优化参数的过程是一个高斯过程。高斯过程有个特点，就是当随机遍历一定的数据点并拿到结果之后，可以大致绘制出整个数据的分布曲线。</strong></p>
<p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/998084-20180726204924171-1721363009.png" srcset="/img/loading.gif" lazyload alt="img"></p>
<h5 id="3-3-2-3-2-贝叶斯优化理论"><a href="#3-3-2-3-2-贝叶斯优化理论" class="headerlink" title="3.3.2.3.2 贝叶斯优化理论"></a>3.3.2.3.2 贝叶斯优化理论</h5><p><strong>还是这张图，把横轴看作是参数组合X，纵轴看作是这个参数的结果Y。可以通过已经构建的曲线，找到曲线上升的方向，从而在这个方向上继续探索，这样就可以大概率拿到更好的结果。在生活的轨迹上，如果找到一条明确通往幸福的路，可以继续向前探索，因为大概率可以成功，但也许也有会错过更好的机会，陷入局部最优解。请看上图中的五角星，如果我们处于它的位置，继续向上走会迎来一个高峰，但是如果后退，在下降一段时间之后可能会迎来更高的波峰，你该如何选择。</strong></p>
<p><strong>于是，在参数的探索中要掌握一个平衡：</strong></p>
<p><strong>开发：在明确的曲线上扬方向继续走，大概率获得更好的结果，但是容易陷入局部最优。</strong></p>
<p><strong>探索：除了在曲线上扬的方向，在其它的区域也不忘寻找</strong></p>
<p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211114102651780.png" srcset="/img/loading.gif" lazyload alt="image-20211114102651780"></p>
<h2 id="3-4-结果分析"><a href="#3-4-结果分析" class="headerlink" title="3.4 结果分析"></a>3.4 结果分析</h2><h3 id="3-4-1-MAPE"><a href="#3-4-1-MAPE" class="headerlink" title="3.4.1 MAPE"></a>3.4.1 MAPE</h3><p><strong>平均绝对百分比误差（Mean Absolute Percentage Error）</strong></p>
<script type="math/tex; mode=display">
M A P E=\frac{100 \%}{n} \sum_{i=1}^{n}\left|\frac{\hat{y}_{i}-y_{i}}{y_{i}}\right|</script><p><strong>范围[0,+∞)，MAPE 为0%表示完美模型，MAPE 大于 100 %则表示劣质模型。</strong></p>
<p><strong>注意：当真实值有数据等于0时，存在分母0除问题，该公式不可用！</strong></p>
<h3 id="3-4-2-调参前结果及分析"><a href="#3-4-2-调参前结果及分析" class="headerlink" title="3.4.2 调参前结果及分析"></a>3.4.2 调参前结果及分析</h3><h4 id="3-4-2-1-代码"><a href="#3-4-2-1-代码" class="headerlink" title="3.4.2.1 代码"></a>3.4.2.1 代码</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs routeros">mlp = MLPRegressor(hidden_layer_sizes=(100), <span class="hljs-attribute">activation</span>=<span class="hljs-string">&#x27;relu&#x27;</span>, <span class="hljs-attribute">solver</span>=<span class="hljs-string">&#x27;adam&#x27;</span>, <span class="hljs-attribute">alpha</span>=0.0001, <span class="hljs-attribute">batch_size</span>=<span class="hljs-string">&#x27;auto&#x27;</span>, <span class="hljs-attribute">learning_rate</span>=<span class="hljs-string">&#x27;constant&#x27;</span>, <span class="hljs-attribute">learning_rate_init</span>=0.001, <span class="hljs-attribute">power_t</span>=0.5, <span class="hljs-attribute">max_iter</span>=200, <span class="hljs-attribute">shuffle</span>=<span class="hljs-literal">True</span>, <span class="hljs-attribute">random_state</span>=None, <span class="hljs-attribute">tol</span>=0.0001, <span class="hljs-attribute">verbose</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">warm_start</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">momentum</span>=0.9, <span class="hljs-attribute">nesterovs_momentum</span>=<span class="hljs-literal">True</span>, <span class="hljs-attribute">early_stopping</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">validation_fraction</span>=0.1, <span class="hljs-attribute">beta_1</span>=0.9, <span class="hljs-attribute">beta_2</span>=0.999, <span class="hljs-attribute">epsilon</span>=1e-08, <span class="hljs-attribute">n_iter_no_change</span>=10, <span class="hljs-attribute">max_fun</span>=15000) #所有参数默认<br>mlp.fit(X_std, Y)<br>MAPE = -1<span class="hljs-number">*c</span>ross_val_score(mlp, X_std, Y, <span class="hljs-attribute">cv</span>=rkf,scoring=&#x27;neg_mean_absolute_percentage_error&#x27;).mean()<br><span class="hljs-builtin-name">print</span>(<span class="hljs-string">&#x27;MAPE:&#x27;</span>,MAPE)<br></code></pre></td></tr></table></figure>
<h4 id="3-4-2-2-结果及分析"><a href="#3-4-2-2-结果及分析" class="headerlink" title="3.4.2.2 结果及分析"></a>3.4.2.2 结果及分析</h4><p><strong>首先我们使用</strong><a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.neural_network"><code>sklearn.neural_network</code></a>.<a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html?highlight=mlp#sklearn.neural_network.MLPRegressor">MLPRegressor</a>中的所有默认参数设置来训练模型，五次五折交叉验证的平均MAPE为：0.2036462204885882</p>
<p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211114171851292.png" srcset="/img/loading.gif" lazyload alt="image-20211114171851292"></p>
<p><strong>                                                                     **</strong>Fig.1**  调参前残差图</p>
<p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211114171902074.png" srcset="/img/loading.gif" lazyload alt="image-20211114171902074"></p>
<p><strong>                                                                    **</strong>Fig.2**  调参前预测误差图</p>
<p><strong>残差是因变量未被自变量解释的部分，线性模型要求残差服从独立同分布，且分布类型为正态分布。通过一系列方法判断残差是否符合这一要求，可以达到检验模型是否符合相应假设的目的。从上图可以看出，我们的训练集和测试集的R</strong>^2^在0.75左右，说明我们的模型训练结果具有一定的可信度，但并不理想。下面我们进行调参，尝试提高准确率。</p>
<h3 id="3-4-3-网格搜索调参"><a href="#3-4-3-网格搜索调参" class="headerlink" title="3.4.3 网格搜索调参"></a>3.4.3 网格搜索调参</h3><p><strong>接下来我们手动调试模型，将各个超参数逐一修改并查看MAPE的变化结果，最终得出’**</strong>hidden_layer_sizes<strong>‘，’</strong>activation<strong>‘，’</strong>solver<strong>‘，’</strong>alpha<strong>‘，’</strong>learning_rate**’</p>
<p><strong>这五个对结果影响较大的参数。</strong></p>
<p><strong>最后我们利用GridSearchCV结合一些“经验结论”来搜索出最优的超参数。</strong></p>
<p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211114222524771.png" srcset="/img/loading.gif" lazyload alt="image-20211114222524771"></p>
<h4 id="3-4-3-1-代码"><a href="#3-4-3-1-代码" class="headerlink" title="3.4.3.1 代码"></a>3.4.3.1 代码</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># 超参数调优</span><br><span class="hljs-keyword">from</span> sklearn.model_selection import GridSearchCV<br>parameters = &#123;<span class="hljs-string">&#x27;hidden_layer_sizes&#x27;</span>: [(10,10,10,10,10),(20,20,20,20,20),(30,30,30,30,30),(40,40,40,40,40),(50,50,50,50,50),(60,60,60,60,60),(70,70,70,70,70),(80,80,80,80,80),(90,90,90,90,90),(100,100,100,100,100)],<br>                <span class="hljs-string">&#x27;activation&#x27;</span>: [<span class="hljs-string">&#x27;identity&#x27;</span>, <span class="hljs-string">&#x27;logistic&#x27;</span>,<span class="hljs-string">&#x27;tanh&#x27;</span>, <span class="hljs-string">&#x27;relu&#x27;</span>],<br>                <span class="hljs-string">&#x27;solver&#x27;</span>: [<span class="hljs-string">&#x27;adam&#x27;</span>,<span class="hljs-string">&#x27;lbgfs&#x27;</span>,<span class="hljs-string">&#x27;sgd&#x27;</span>],<br>                <span class="hljs-string">&#x27;alpha&#x27;</span>: [0.0001, 0.001, 0.01, 0.1, 1,10,100],<br>                <span class="hljs-string">&#x27;learning_rate&#x27;</span>: [<span class="hljs-string">&#x27;constant&#x27;</span>, <span class="hljs-string">&#x27;invscaling&#x27;</span>, <span class="hljs-string">&#x27;adaptive&#x27;</span>]&#125;<br>grid = GridSearchCV(mlp, parameters, <span class="hljs-attribute">cv</span>=rkf, <span class="hljs-attribute">scoring</span>=<span class="hljs-string">&#x27;neg_mean_absolute_percentage_error&#x27;</span>,n_jobs=-1)<br>grid.fit(X_std, Y)<br><span class="hljs-builtin-name">print</span>(<span class="hljs-string">&#x27;最优参数：&#x27;</span>,grid.best_params_)<br><span class="hljs-builtin-name">print</span>(<span class="hljs-string">&#x27;最优模型得分：&#x27;</span>,grid.best_score_)<br></code></pre></td></tr></table></figure>
<h4 id="3-4-3-2-结果及分析"><a href="#3-4-3-2-结果及分析" class="headerlink" title="3.4.3.2 结果及分析"></a>3.4.3.2 结果及分析</h4><ul>
<li><strong>最优参数： {‘activation’: ‘relu’, ‘alpha’: 1, ‘hidden_layer_sizes’: (100, 100, 100, 100, 100), ‘learning_rate’: ‘adaptive’, ‘solver’: ‘sgd’} </strong></li>
<li><strong>最优模型得分： 0.10669120458358475</strong></li>
</ul>
<p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211114221915446.png" srcset="/img/loading.gif" lazyload alt="image-20211114221915446"></p>
<p><strong>                                                                   **</strong>Fig.3**  调参后残差图</p>
<p><strong>                                                                                      </strong><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211114221934910.png" srcset="/img/loading.gif" lazyload alt="image-20211114221934910"></p>
<p><strong>                                                                  **</strong>Fig.4**  调参后预测误差图</p>
<p><strong>经过网格搜索最优参数后，我们的模型得到大幅度提升。从上可见，我们的训练和测试的R</strong>^2^均在0.98以上，说明模型对训练集的拟合效果和泛化能力都很强。</p>
<h1 id="四、总结模型训练过程中的收获"><a href="#四、总结模型训练过程中的收获" class="headerlink" title="四、总结模型训练过程中的收获"></a>四、总结模型训练过程中的收获</h1><h2 id="4-1-神经网络的训练过程"><a href="#4-1-神经网络的训练过程" class="headerlink" title="4.1 神经网络的训练过程"></a>4.1 神经网络的训练过程</h2><p><strong>简单的神经网络的训练过程包括以下几个步骤：</strong></p>
<ol>
<li><strong>定义一个包含多个可学习参数（权重）的神经网络；</strong></li>
<li><strong>对输入的数据集进行迭代计算；</strong></li>
<li><strong>通过多层网络结构来处理输入数据；</strong></li>
<li><strong>计算损失值（输出值与目标值的差值）；</strong></li>
<li><strong>反向传播梯度到神经网络的参数中；</strong></li>
<li><strong>根据更新规则来更新网络中的权重值。</strong></li>
</ol>
<h2 id="4-2-确定超参数"><a href="#4-2-确定超参数" class="headerlink" title="4.2 确定超参数"></a>4.2 确定超参数</h2><p><strong>其中，如何定义一个包含多个可学习参数的神经网络（即如何确定模型的超参数）是重点，会影响神经网络学习速度和最后结果。我们确定超参数的步骤如下：</strong></p>
<p><strong>①我们根据经验结论手动调试模型，将各个超参数逐一修改并查看MAPE的变化结果，最终得出’**</strong>hidden_layer_sizes<strong>‘，’</strong>activation<strong>‘，’</strong>solver<strong>‘，’</strong>alpha**’，</p>
<p><strong>‘**</strong>learning_rate**’这五个对结果影响较大的参数。</p>
<p><strong>②搜集“经验总结”的资料后，我们用网格搜索法对下列超参数进行排列组合，得到10<em>*</em></strong>4<strong>*</strong>3<strong>*</strong>7<strong>*</strong>3=2520种超参数的排列组合方式。**</p>
<figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs gcode">parameters = &#123;<span class="hljs-string">&#x27;hidden_layer_sizes&#x27;</span>: [<span class="hljs-comment">(10,10,10,10,10)</span>,<span class="hljs-comment">(20,20,20,20,20)</span>,<span class="hljs-comment">(30,30,30,30,30)</span>,<span class="hljs-comment">(40,40,40,40,40)</span>,<span class="hljs-comment">(50,50,50,50,50)</span>,<span class="hljs-comment">(60,60,60,60,60)</span>,<span class="hljs-comment">(70,70,70,70,70)</span>,<span class="hljs-comment">(80,80,80,80,80)</span>,<span class="hljs-comment">(90,90,90,90,90)</span>,<span class="hljs-comment">(100,100,100,100,100)</span>],<br>                <span class="hljs-string">&#x27;activation&#x27;</span>: [<span class="hljs-string">&#x27;identity&#x27;</span>, <span class="hljs-string">&#x27;logistic&#x27;</span>,<span class="hljs-string">&#x27;tanh&#x27;</span>, <span class="hljs-string">&#x27;relu&#x27;</span>],<br>                <span class="hljs-string">&#x27;solver&#x27;</span>: [<span class="hljs-string">&#x27;adam&#x27;</span>,<span class="hljs-string">&#x27;lbgfs&#x27;</span>,<span class="hljs-string">&#x27;sgd&#x27;</span>],<br>                <span class="hljs-string">&#x27;alpha&#x27;</span>: [<span class="hljs-number">0.0001</span>, <span class="hljs-number">0.001</span>, <span class="hljs-number">0.01</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">1</span>,<span class="hljs-number">10</span>,<span class="hljs-number">100</span>],<br>                <span class="hljs-string">&#x27;learning_rate&#x27;</span>: [<span class="hljs-string">&#x27;constant&#x27;</span>, <span class="hljs-string">&#x27;invscaling&#x27;</span>, <span class="hljs-string">&#x27;adaptive&#x27;</span>]&#125;<br></code></pre></td></tr></table></figure>
<p><strong>③使用五次五折将数据划分为25份，把上述2520种超参数的组合都跑一遍数据（计算神经网络中的最佳的参数用的是误差逆传播算法），每一个组合都会得到25个MAPE值，取平均；之后2520份MAPE中的最小值对应的超参数组合即为我们选定的最优超参数组合。</strong></p>
<h2 id="4-3-防止过拟合的方法"><a href="#4-3-防止过拟合的方法" class="headerlink" title="4.3 防止过拟合的方法"></a>4.3 防止过拟合的方法</h2><p><strong>在机器学习模型（特别是深度学习模型）的训练过程中，模型是非常容易过拟合的。深度学习模型在不断的训练过程中训练误差会逐渐降低，但测试误差的走势则不一定。</strong></p>
<p><strong>①正则化方法。正则化方法包括L0正则、L1正则和L2正则，而正则一般是在目标函数之后加上对于的范数。但是在机器学习中一般使用L2正则。</strong></p>
<p><strong>②数据增强（Data augmentation），增大数据的训练量；还有一个原因就是我们用于训练的数据量太小导致的，训练数据占总数据的比例过小。</strong></p>
<p><strong>③重新清洗数据，导致过拟合的一个原因也有可能是数据不纯导致的，如果出现了过拟合就需要我们重新清洗数据。</strong></p>
<p><strong>④提前终止法（Early stopping），对模型进行训练的过程即是对模型的参数进行学习更新的过程，这个参数学习的过程往往会用到一些迭代方法，如梯度下降（Gradient descent）学习算法。提前终止法便是一种迭代次数截断的方法来防止过拟合的方法，即在模型对训练数据集迭代收敛之前停止迭代来防止过拟合。</strong></p>
<p><strong>⑤丢弃法（Dropout）。这个方法在神经网络里面很常用。丢弃法是ImageNet中提出的一种方法，通俗一点讲就是丢弃法在训练的时候让神经元以一定的概率不工作。具体看下图：</strong><br><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy92STluWWU5NGZzRmJubFJEeHdxZVozWG9YQ3dZR1JnVHdDRHFLaWFBZGRWZlp5elJKNnREekE3ejJCUlFpY0RzUlJhWGVpYVd6aWFoaWNldlFyRFIyVU1RQm9nLzY0MA" srcset="/img/loading.gif" lazyload alt="img"></p>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/course/">course</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
            </div>

            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <p id="jinrishici-sentence">愿我如星君如月，夜夜流光相皎洁</p> <a href="https://beian.miit.gov.cn/" target="_blank" rel="nofollow noopener"><span> 粤ICP备2021112653号-1</span></a> 
  </div>
  

  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/js/local-search.js" ></script>



  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4.12.0/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.0/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.6/dist/clipboard.min.js" ></script>
  






    <script  src="https://cdn.jsdelivr.net/npm/typed.js@2.0.11/lib/typed.min.js" ></script>
    <script>
      (function (window, document) {
        var typing = Fluid.plugins.typing;
        var title = document.getElementById('subtitle').title;
        
        typing(title)
        
      })(window, document);
    </script>
  



  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        loader: {
          
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-svg.js" ></script>

  











<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>

  <script src="https://sdk.jinrishici.com/v2/browser/jinrishici.js" charset="utf-8" defer></script>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"https://cdn.jsdelivr.net/gh/Enderfga/live2d-widget-models/packages/live2d-widget-model-hijiki/assets/hijiki.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"react":{"opacity":0.7},"log":false});</script></body>
</html>