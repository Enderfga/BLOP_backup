

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="https://img.enderfga.cn/img/favicon.png">
  <link rel="icon" href="https://img.enderfga.cn/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Enderfga">
  <meta name="keywords" content="">
  
    <meta name="description" content="Fine-Grained Human Feedback Gives Better Rewards for Language Model Training  We propose Fine-Grained RLHF, a framework that enables training and learning from reward functions that are fine-grai">
<meta property="og:type" content="article">
<meta property="og:title" content="Fine-Grained Human Feedback Gives Better Rewards for Language Model Training">
<meta property="og:url" content="http://enderfga.cn/2023/06/20/Fine-Grained/index.html">
<meta property="og:site_name" content="Enderfga&#39;Blog">
<meta property="og:description" content="Fine-Grained Human Feedback Gives Better Rewards for Language Model Training  We propose Fine-Grained RLHF, a framework that enables training and learning from reward functions that are fine-grai">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img.enderfga.cn/img/teaser-smaller.jpg">
<meta property="article:published_time" content="2023-06-20T05:21:59.000Z">
<meta property="article:modified_time" content="2023-06-20T05:23:23.510Z">
<meta property="article:author" content="Enderfga">
<meta property="article:tag" content="自然语言处理">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://img.enderfga.cn/img/teaser-smaller.jpg">
  
  
  
  <title>Fine-Grained Human Feedback Gives Better Rewards for Language Model Training - Enderfga&#39;Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"enderfga.cn","root":"/","version":"1.9.4","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":"oG62pOOQWEHEoerO","google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":"8yKzDhPjGo7c9GbBxy5UimY5-gzGzoHsz","app_key":"2ghy7HB3wrWXUD1VLNd92jeC","server_url":"https://8ykzdhpj.lc-cn-n1-shared.com","path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  
    <!-- Baidu Analytics -->
    <script async>
      if (!Fluid.ctx.dnt) {
        var _hmt = _hmt || [];
        (function() {
          var hm = document.createElement("script");
          hm.src = "https://hm.baidu.com/hm.js?oG62pOOQWEHEoerO";
          var s = document.getElementsByTagName("script")[0];
          s.parentNode.insertBefore(hm, s);
        })();
      }
    </script>
  

  

  

  

  

  

  
    
  



  
<meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="Enderfga'Blog" type="application/atom+xml">
</head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Enderfga</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="https://enderfga.cn/phd">
                <i class="iconfont icon-switch-fill"></i>
                <span>游戏</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" target="_blank" rel="noopener" href="https://pan.enderfga.workers.dev/">
                <i class="iconfont icon-briefcase"></i>
                <span>网盘</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/">
                <i class="iconfont icon-link-fill"></i>
                <span>友链</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('https://img.enderfga.cn/img/wallhaven-vqq3m5.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="Fine-Grained Human Feedback Gives Better Rewards for Language Model Training"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-06-20 13:21" pubdate>
          2023年6月20日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          <!-- compatible with older versions-->
          6k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          <!-- compatible with older versions-->
          51 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">Fine-Grained Human Feedback Gives Better Rewards for Language Model Training</h1>
            
              <p class="note note-info">
                
                  
                    <!-- compatible with older versions-->
                    本文最后更新于：2023年6月20日 下午
                  
                
              </p>
            
            
              <div class="markdown-body">
                
                <h1
id="fine-grained-human-feedback-gives-better-rewards-for-language-model-training">Fine-Grained
Human Feedback Gives Better Rewards for Language Model Training</h1>
<p><img src="https://img.enderfga.cn/img/teaser-smaller.jpg" srcset="/img/loading.gif" lazyload /></p>
<p>We propose <strong>Fine-Grained RLHF</strong>, a framework that
enables training and learning from reward functions that are
fine-grained in two respects:</p>
<ol type="1">
<li>density, providing a reward after every segment (e.g., a sentence)
is generated</li>
<li>incorporating multiple reward models associated with different
feedback types (e.g., factual incorrectness, irrelevance, and
information incompleteness).</li>
</ol>
<h2 id="what-are-fine-grained-rewards">What are Fine-Grained
Rewards?</h2>
<p>Prior work in RLHF focused on collecting human preferences regarding
the overall quality of language model (LM) outputs. However, this type
of holistic feedback offers limited information. In our paper, we
introduce <strong>fine-grained human feedback</strong> (e.g., which
sub-sentence is irrelevant, which sentence is not truthful, which
sentence is toxic) as an explicit training signal. Our rewards are
fine-grained in two aspects:</p>
<p><strong>(a) Density</strong>: We provide a reward after each segment
(e.g., a sentence) is generated, similar to OpenAI's "step-by-step
process reward". We found that this approach is more informative than
holistic feedback and, thus, more effective for RL.</p>
<p><strong>(b) Multiple reward models associated with different feedback
types</strong>: We employ multiple reward models to capture different
types of feedback (e.g., factual inaccuracy, irrelevance, and
information incompleteness). Interestingly, we observed that these
reward models both complement and compete with each other. By adjusting
the weights of the reward models, we can control the balance between the
different types of feedback and <strong>tailor the LM for different
tasks</strong> according to specific needs.</p>
<h2 id="abstract">Abstract</h2>
<p>Language models (LMs) often exhibit undesirable text generation
behaviors, including generating false, toxic, or irrelevant outputs.
Reinforcement learning from human feedback (RLHF)—where human preference
judgments on LM outputs are transformed into a learning signal—has
recently shown promise in addressing these issues. However, such
holistic feedback conveys limited information on long text outputs; it
does not indicate which aspects of the outputs influenced user
preference; e.g., which parts contain what type(s) of errors. In this
paper, we use fine-grained human feedback (e.g., which sentence is
false, which sub-sentence is irrelevant) as an explicit training signal.
We introduce FINE-GRAINED RLHF, a framework that enables training and
learning from reward functions that are fine-grained in two respects:
(1) density, providing a reward after every segment (e.g., a sentence)
is generated; and (2) incorporating multiple reward models associated
with different feedback types (e.g., factual incorrectness, irrelevance,
and information incompleteness). We conduct experiments on
detoxification and long-form question answering to illustrate how
learning with such reward functions leads to improved performance,
supported by both automatic and human evaluation. Additionally, we show
that LM behaviors can be customized using different combinations of
fine-grained reward models.</p>
<h2 id="task-1-detoxification">Task 1: Detoxification</h2>
<p>The task of detoxification aims to reduce the toxicity in the model
generation. We use Perspective API to measure toxicity. It returns a
toxicity value between 0 (not toxic) and 1 (toxic).</p>
<p><img src="https://img.enderfga.cn/img/toxicity_demo.png" srcset="/img/loading.gif" lazyload /></p>
<p>We compare two kinds of rewards:</p>
<ol type="a">
<li><p><strong>Holistic Rewards for (non-)Toxicity</strong>: We use
1-Perspective(y) as the reward</p></li>
<li><p><strong>Sentence-level (Fine-Grained) Rewards for
(non-)Toxicity</strong>: We query the API after the model generates each
sentence instead of generating the full sequence. For each generated
sentence, we use -Δ(Perspective(y)) as the reward for the sentence (i.e.
how much toxicity is changed from generating the current
sentence).</p></li>
</ol>
<p><img src="https://img.enderfga.cn/img/toxicity_results.png" srcset="/img/loading.gif" lazyload /></p>
<p>Table 1 shows that Our Fine-Grained RLHF with sentence-level
fine-grained reward attains the lowest toxicity and perplexity among all
methods, while maintaining a similar level of diversity. Figure 2 shows
that learning from denser fine-grained reward is more <strong>sample
efficient</strong> than holistic reward. One explanation is that
fine-grained reward locates where the toxic content is, which is a
stronger training signal compared with a scalar reward for the whole
text.</p>
<h2 id="task-2-long-form-question-answering">Task 2: Long-Form Question
Answering</h2>
<p>We collect <strong>QA-Feeback</strong>, a dataset of long-form
question answering, with human preferences and fine-grained feedback.
QA-Feedback is based on ASQA, a dataset that focuses on answering
ambiguous factoid questions.</p>
<p>There are three types of fine-grained human feedback, and we train a
fine-grained reward model for each of them:</p>
<p><span class="math inline">\(\color{blue}{\text{C1: irrelevance,
repetition, and incoherence (rel.)}}\)</span>; The reward model has the
density level of sub-sentences; i.e., returns a score for each
sub-sentence. If the sub-sentence is irrelevant, repetitive, or
incoherent, the reward is -1; otherwise, the reward is +1.</p>
<p><span class="math inline">\(\color{goldenrod}{\text{C2: incorrect or
unverifiable facts (fact.)}}\)</span>; The reward model has the density
level of sentences; i.e., returns a score for each sentence. If the
sentence has any factual error, the reward is -1; otherwise, the reward
is +1.</p>
<p><span class="math inline">\(\color{green}{\text{C3: incomplete
information (comp.)}}\)</span>; The reward model checks if the response
is complete and covers all the information in the reference passages
that are related to the question. This reward model gives one reward for
the whole response.</p>
<h2 id="fine-grained-human-evaluation">Fine-Grained Human
Evaluation</h2>
<p>We compare our <strong>Fine-Grained RLHF</strong> with the following
baselines:</p>
<p><strong>SFT</strong>: The supervised finetuning model (trained on 1K
training examples) that is used as the initial policy for our RLHF
experiments.</p>
<p><strong>Pref. RLHF</strong>: The baseline RLHF model that uses
holistic reward.</p>
<p><strong>SFT-Full</strong>: We finetune LM with human-written
responses (provided by ASQA) of all training examples and denote this
model as SFT-Full. Notice that each gold response takes 15 min to
annotate (according to ASQA), which takes much longer time than our
feedback annotation (6 min).</p>
<p><img src="https://img.enderfga.cn/img/human_eval.png" srcset="/img/loading.gif" lazyload /></p>
<p>Human evaluation shows that our <strong>Fine-Grained RLHF</strong>
outperforms SFT and Preference RLHF on all error types. Also, RLHF (both
preference-based and fine-grained) are particularly effective in
reducing factual errors.</p>
<h2 id="customize-lm-behaviors">Customize LM behaviors</h2>
<p><img src="https://img.enderfga.cn/img/customization.png" srcset="/img/loading.gif" lazyload /></p>
<p>By changing the weight of the <span
class="math inline">\(\color{blue}{\text{Relevance reward
model}}\)</span>, and keeping the weight of the other two reward models
fixed, we can customize how detailed and lengthy the LM responses would
be. Here we compare the outputs of three LMs, trained with different
reward model combinations.</p>
<h2
id="fine-grained-reward-models-both-complement-and-compete-with-each-other">Fine-Grained
reward models both complement and compete with each other</h2>
<p><img src="https://img.enderfga.cn/img/analysis.png" srcset="/img/loading.gif" lazyload /></p>
<p>We find that there is a trade-off between the three reward models.
<span class="math inline">\(\color{blue}{\text{Relevance RM}}\)</span>
prefers shorter and more concise responses, while <span
class="math inline">\(\color{green}{\text{Info Completeness
RM}}\)</span> prefers longer and more informative responses. Thus, these
two rewards compete against each other during training and eventually
reach a balance. Meanwhile, <span
class="math inline">\(\color{goldenrod}{\text{Factuality RM}}\)</span>
continuously improves the factual correctness of the response. Finally,
removing any one of the reward models will degrade the performance.</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">#自然语言处理</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Fine-Grained Human Feedback Gives Better Rewards for Language Model Training</div>
      <div>http://enderfga.cn/2023/06/20/Fine-Grained/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Enderfga</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2023年6月20日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2023/06/27/GILL/" title="Generating Images with Multimodal Language Models">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Generating Images with Multimodal Language Models</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/06/14/model/" title="Generation Model">
                        <span class="hidden-mobile">Generation Model</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <script type="text/javascript">
    Fluid.utils.loadComments('#comments', function() {
      var light = 'github-light';
      var dark = 'github-dark';
      var schema = document.documentElement.getAttribute('data-user-color-scheme');
      if (schema === 'dark') {
        schema = dark;
      } else {
        schema = light;
      }
      window.UtterancesThemeLight = light;
      window.UtterancesThemeDark = dark;
      var s = document.createElement('script');
      s.setAttribute('src', 'https://utteranc.es/client.js');
      s.setAttribute('repo', 'Enderfga/commit-utterances');
      s.setAttribute('issue-term', 'title');
      
      s.setAttribute('label', 'utterances');
      
      s.setAttribute('theme', schema);
      s.setAttribute('crossorigin', 'anonymous');
      document.getElementById('comments').appendChild(s);
    })
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  





  <script>
  Fluid.utils.createScript('https://lib.baomitu.com/mermaid/8.13.10/mermaid.min.js', function() {
    mermaid.initialize({"theme":"default"});

    Fluid.events.registerRefreshCallback(function() {
      if ('mermaid' in window) {
        mermaid.init();
      }
    });
  });
</script>





  <!-- Custom -->
  <div class="col-lg-7 mx-auto nopadding-x-md">
    <div class="container custom post-custom mx-auto">
      <img src="https://img.enderfga.cn/img/20220420162434.png" srcset="/img/loading.gif" lazyload class="rounded mx-auto d-block mt-5" style="width:150px; height:150px;">
    </div>
  </div>


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <p id="jinrishici-sentence">愿我如星君如月，夜夜流光相皎洁</p> <a href="https://beian.miit.gov.cn/" target="_blank" rel="nofollow noopener"><span> 粤ICP备2021112653号-1</span></a> <script src="https://sdk.jinrishici.com/v2/browser/jinrishici.js" charset="utf-8"></script> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="leancloud-site-pv-container" style="display: none">
        总访问量 
        <span id="leancloud-site-pv"></span>
         次
      </span>
    
    
      <span id="leancloud-site-uv-container" style="display: none">
        总访客数 
        <span id="leancloud-site-uv"></span>
         人
      </span>
    
    

  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.0/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script defer src="/js/leancloud.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg-full.js">
</script>
</body>
</html>
