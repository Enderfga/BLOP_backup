

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="https://img.enderfga.cn/img/favicon.png">
  <link rel="icon" href="https://img.enderfga.cn/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Enderfga">
  <meta name="keywords" content="">
  
    <meta name="description" content="From Twitter of Yam Peleg.">
<meta property="og:type" content="article">
<meta property="og:title" content="GPT4泄露的技术细节">
<meta property="og:url" content="http://enderfga.cn/2023/07/11/gpt4/index.html">
<meta property="og:site_name" content="Enderfga&#39;Blog">
<meta property="og:description" content="From Twitter of Yam Peleg.">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img.enderfga.cn/img/20230711100816.png">
<meta property="article:published_time" content="2023-07-11T01:12:24.000Z">
<meta property="article:modified_time" content="2023-07-11T02:08:49.747Z">
<meta property="article:author" content="Enderfga">
<meta property="article:tag" content="自然语言处理">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://img.enderfga.cn/img/20230711100816.png">
  
  
  
  <title>GPT4泄露的技术细节 - Enderfga&#39;Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"enderfga.cn","root":"/","version":"1.9.5","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":false,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":"oG62pOOQWEHEoerO","google":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":"8yKzDhPjGo7c9GbBxy5UimY5-gzGzoHsz","app_key":"2ghy7HB3wrWXUD1VLNd92jeC","server_url":"https://8ykzdhpj.lc-cn-n1-shared.com","path":"window.location.pathname","ignore_local":false},"gtag":null},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  
    <!-- Baidu Analytics -->
    <script async>
      if (!Fluid.ctx.dnt) {
        var _hmt = _hmt || [];
        (function() {
          var hm = document.createElement("script");
          hm.src = "https://hm.baidu.com/hm.js?oG62pOOQWEHEoerO";
          var s = document.getElementsByTagName("script")[0];
          s.parentNode.insertBefore(hm, s);
        })();
      }
    </script>
  

  

  

  

  

  
    
  



  
<meta name="generator" content="Hexo 5.4.2"><link rel="alternate" href="/atom.xml" title="Enderfga'Blog" type="application/atom+xml">
</head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Enderfga</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="https://enderfga.cn/phd">
                <i class="iconfont icon-switch-fill"></i>
                <span>游戏</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" target="_blank" rel="noopener" href="https://pan.enderfga.workers.dev/">
                <i class="iconfont icon-briefcase"></i>
                <span>网盘</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/">
                <i class="iconfont icon-link-fill"></i>
                <span>友链</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('https://img.enderfga.cn/img/crown.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="GPT4泄露的技术细节"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-07-11 09:12" pubdate>
          2023年7月11日 上午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          <!-- compatible with older versions-->
          8.7k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          <!-- compatible with older versions-->
          73 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">GPT4泄露的技术细节</h1>
            
              <p class="note note-info">
                
                  
                    <!-- compatible with older versions-->
                    本文最后更新于：2023年7月11日 上午
                  
                
              </p>
            
            
              <div class="markdown-body">
                
                <p>From Twitter of Yam Peleg.</p>
<p><img src="https://img.enderfga.cn/img/20230711100816.png" srcset="/img/loading.gif" lazyload /></p>
<span id="more"></span>
<h1 id="gpt-4s-details-are-leaked.">GPT-4's details are leaked.</h1>
<p>It is over. Everything is here:</p>
<h2 id="parameters-count">Parameters count</h2>
<p>GPT-4 is more than 10x the size of GPT-3. We believe it has a total
of ~1.8 trillion parameters across 120 layers.</p>
<p>GPT-4的规模是GPT-3的10倍以上。我们认为它拥有约1,800亿个参数，分布在120层。</p>
<h2 id="mixture-of-experts---confirmed">Mixture Of Experts -
Confirmed</h2>
<p>OpenAl was able to keep costs reasonable by utilizing a mixture of
experts(MoE) model.</p>
<p>They utilizes 16 experts within their model, each is about ~111B
parameters for MLP 2 of these experts are routed to per forward
pass.</p>
<p>OpenAI通过使用专家混合（MoE）模型，能够将成本控制在合理的范围内。</p>
<p>他们的模型利用了16个专家，每个专家大约拥有1110亿个参数，其中每次前向传递（forward
pass）使用其中的2个专家。</p>
<h2 id="moe-routing">MoE Routing</h2>
<p>While the literature talks a lot about advanced routing algorithms
for choosing which experts to route each token to, OpenAl's is allegedly
quite simple, for the current GPT-4 model.</p>
<p>There roughly ~55B shared parameters for attention.</p>
<p>尽管文献中谈到了很多关于选择将每个标记（token）路由到哪些专家的先进路由算法，但据称OpenAI在当前的GPT-4模型中采用的路由方法相当简单。</p>
<p>在注意力机制中，大约有550亿个共享参数。</p>
<h2 id="inference">Inference</h2>
<p>Each forward pass inference (generation of 1 token) only utilizes
~280B parameters and ~560 TFLOPs. This contrasts with the ~1.8 trillion
parameters and ~3,700 TFLOP that would be required per forward pass of a
purely dense model.</p>
<p>每次前向传递的推理（生成一个标记）只使用了大约280亿个参数和560
TFLOPS的计算量。这与纯密集模型每次前向传递所需的约1,800亿个参数和3,700
TFLOPS形成了对比。</p>
<h2 id="dataset">Dataset</h2>
<p>GPT-4 is trained on ~13T tokens. These are not unique tokens, they
count the epochs as more tokens as well.</p>
<p>Epoch number: 2 epochs for text-based data and 4 for code-based
data.</p>
<p>There is millions of rows of instruction fine-tuning data from
ScaleAl &amp; internally.</p>
<p>GPT-4的训练使用了大约130万亿个标记（tokens）。这些标记不是唯一的标记，还计算了多个时期（epochs）的标记数量。</p>
<p>对于基于文本的数据，使用了2个时期，对于基于代码的数据，使用了4个时期。</p>
<p>此外，还有来自ScaleAl和内部的数百万行指令微调数据。</p>
<h2 id="gpt-4-32k">GPT-4 32K</h2>
<p>There was an 8k context length (seq len) for the pre-training phase.
The 32k seq len version of GPT-4 is based on fine-tuning of the 8k after
the pre-training.</p>
<p>在预训练阶段，GPT-4使用了8,000个上下文长度（序列长度）。而32,000个序列长度的GPT-4版本是在预训练后对8,000个序列长度进行微调得到的。</p>
<h2 id="batch-size">Batch Size</h2>
<p>The batch size was gradually ramped up over a number of days on the
cluster, but by the end, OpenAl was using a batch size of 60 million!
This, of course, is “only” a batch size of 75 million tokens per expert
due to not every expert seeing all tokens.</p>
<p>在集群上的几天时间里，批量大小逐渐增加，但最终OpenAI使用了6000万的批量大小！当然，由于并非每个专家都能看到所有的标记，因此每个专家实际上只处理了7500万个标记的批量大小。</p>
<h3 id="for-the-real-batch-size">For the real batch size</h3>
<p>Divide this number by the seq len to get the real batch size. Just
stop with this misleading numbers already.</p>
<p>将这个数字除以序列长度以获取实际的批量大小。请不要再提供这种误导性的数字了。</p>
<h2 id="parallelism-strategies">Parallelism Strategies</h2>
<p>To parallelize across all their A100s GPUs</p>
<p>They utilized 8-way tensor parallelism as that is the limit for
NVLink. Beyond that, they are using 15-way pipeline parallelism.</p>
<p>(likely used ZeRo Stage 1.lt is possiblethey used block-level
FSDP)</p>
<p>为了在所有A100 GPU上实现并行计算，他们采用了8路张量并行（tensor
parallelism），因为这是NVLink的限制。此外，他们还使用了15路管道并行（pipeline
parallelism）。</p>
<p>很可能他们使用了ZeRo阶段1，同时也可能使用了块级FSDP（Fully Sharded
Data Parallelism）。</p>
<h2 id="training-cost">Training Cost</h2>
<p>OpenAl's training FLOPS for GPT-4 is ~2.15e25,</p>
<p>on~25,000 A100s for 90 to 100 days at about 32% to 36% MFU.</p>
<p>Part of this extremely low utilization is due to an absurd number of
failures requiring checkpoints that needed to be restarted from.</p>
<p>If their cost in the cloud was about $1 per A100 hour, the training
costs for this run alone would be about $63 million</p>
<p>(Today, the pre-training could be done with ~8,192 H100 in ~55 days
for $21.5 million at $2 per H100 hour.)</p>
<p>OpenAI的GPT-4训练的浮点运算速度（FLOPS）约为2.15e25次。</p>
<p>在大约25,000个A100
GPU上进行训练，持续时间为90至100天，利用率约为32%至36%。</p>
<p>这种极低的利用率部分是由于大量的故障导致需要重新启动检查点。</p>
<p>如果他们在云端的成本约为每个A100每小时1美元，单单这次训练的成本将约为6300万美元。</p>
<p>（现在，使用大约8,192个H100
GPU进行预训练，需要大约55天时间，成本为2150万美元，每个H100每小时2美元。）</p>
<h2 id="mixture-of-expert-tradeoffs">Mixture of Expert Tradeoffs</h2>
<p>There are multiple MoE tradeoffs taken:</p>
<p>For example, MoE is incredibly difficult to deal with on inference
because not every part of the model is utilized on every token
generation.</p>
<p>This means parts may sit dormant when other parts are being used.
When serving users, this really hurts utilization rates.</p>
<p>Researchers have shown that using 64 to 128 experts achieves better
loss than 16 experts, but that's purely research.</p>
<p>There are multiple reasons to go with fewer experts. One reason for
OpenAI choosing 16 experts is because more experts are difficult to
generalize at many tasks. More experts can also be more difficult to
achieve convergence with.</p>
<p>With such a large training run, OpenAI Instead chose to be more
conservative onthe number of experts.</p>
<p>有多个专家混合（MoE）的权衡被考虑：</p>
<p>例如，在推理过程中，处理MoE非常困难，因为并非每个模型的部分在每个标记生成过程中都被利用。</p>
<p>这意味着某些部分可能处于闲置状态，而其他部分正在被使用。在为用户提供服务时，这会极大地降低利用率。</p>
<p>研究人员已经证明，使用64到128个专家比使用16个专家可以获得更好的损失结果，但这仅仅是研究结果。</p>
<p>选择较少的专家有多个原因。OpenAI选择使用16个专家之一的原因是因为更多的专家在许多任务上很难进行泛化。更多的专家也可能更难实现收敛。</p>
<p>在如此庞大的训练过程中，OpenAI选择在专家数量上更加保守。</p>
<h2 id="gpt-4-inference-cost">GPT-4 Inference Cost</h2>
<p>GPT-4 costs 3x that of the 175B parameter Davinchi.</p>
<p>This is largely due to the larger clusters required for GPT-4 and
much lower utilization achieved.</p>
<p>AN estimate of it's costs is $0.0049 cents per 1k tokens for 128
A100s to inference GPT-4 8k seq len and $0.0021cents per 1k tokens for
128 H100's to inference GPT-4 8k seq len. It should be noted, we assume
decent high utilization,and keeping batch sizes high.</p>
<p>GPT-4的成本是1750亿参数的Davinci模型的3倍。</p>
<p>这主要是由于GPT-4需要更大规模的集群，并且实现了更低的利用率。</p>
<p>据估计，使用128个A100 GPU进行GPT-4
8,000个序列长度的推理，每1,000个标记的成本约为0.0049美分；使用128个H100
GPU进行GPT-4
8,000个序列长度的推理，每1,000个标记的成本约为0.0021美分。需要注意的是，我们假设了良好的高利用率，并保持了较高的批量大小。</p>
<h2 id="multi-query-attention">Multi-Query Attention</h2>
<p>OpenAl are using MQA just like everybody else.</p>
<p>Because of that only 1 head is needed and memory capacity can be
significantly reduced for the KV cache. Even then, the 32k seq len GPT-4
definitely cannot run on 40GB A10Os, and the 8k is capped onmax bsz.</p>
<p>OpenAI也像其他人一样使用了MQA（Multi-QKV Attention）。</p>
<p>由于使用了MQA，只需要一个注意力头（head），并且可以显著降低KV缓存的内存需求。即便如此，32,000个序列长度的GPT-4肯定无法在40GB的A10O上运行，而8,000个序列长度则有最大批量大小的限制。</p>
<h2 id="continuous-batching">Continuous batching</h2>
<p>OpenAl implements both variable batch sizes and continuous batching.
This is so as to allow some level of maximum latency as well optimizing
the inference costs.</p>
<p>OpenAI实现了可变批量大小和连续批处理。这样做可以在一定程度上允许最大延迟，并优化推理成本。</p>
<h2 id="vision-multi-modal">Vision Multi-Modal</h2>
<p>It is a separate vision encoder from the text encoder, with
cross-attention. The architecture is similar to Flamingo. This adds more
parameters on top of the 1.8T of GPT-4. lt is fine-tuned with another ~2
trillion tokens, after the text only pre-training.</p>
<p>On the vision model, OpenAl wanted to train it from scratch, but it
wasn't mature enough, so they wanted to derisk it by starting with
text.</p>
<p>One of the primary purposes of this vision capability is for
autonomous agents able to read web pages and transcribe what's in images
and video.</p>
<p>Some of the data they train on is joint data (rendered LaTeX/text),
screen shots of web page, youtube videos: samplingframes, and run
Whisper around it to get transcript.</p>
<p>GPT-4引入了一个与文本编码器分离的独立视觉编码器，并具有交叉注意力机制。其架构类似于Flamingo模型。这使得GPT-4的参数量在1,800亿的基础上增加了更多。在仅进行文本预训练之后，还对该视觉模型进行了约2万亿个额外的微调标记。</p>
<p>在视觉模型方面，OpenAI原本希望从头开始训练，但该模型尚未足够成熟，所以他们选择通过从文本开始来减轻风险。</p>
<p>该视觉功能的主要目的之一是使自主代理能够阅读网页并转录图像和视频中的内容。</p>
<p>他们训练的一部分数据是联合数据，包括渲染的LaTeX/文本、网页截图、YouTube视频（采样帧），并使用Whisper技术对其进行转录。</p>
<h2 id="speculative-decoding">Speculative Decoding</h2>
<p>OpenAl might be using speculative decoding on GPT-4's inference. (not
sure100%)</p>
<p>The idea is to use a smaller faster model to decode several tokens in
advance, and then feeds them into a large oracle model as a single
batch.</p>
<p>lf the small model was right about its predictions-the larger model
agrees and we can decode several tokens in a single batch.</p>
<p>But if the larger model rejects the tokens predicted by the draft
model then the rest of the batch is discarded. And wecontinue with the
larger model.</p>
<p>The conspiracy theory that the new GPT-4 quality had been
deteriorated might be simply because they are letting the oracle model
accept lower probability sequences from the speculative decoding
model.</p>
<p>OpenAI可能正在使用GPT-4推理中的推测解码（speculative
decoding），但无法确定。</p>
<p>这种方法是使用一个更小更快的模型提前解码多个标记，然后将它们作为一个批次输入到大型的预测模型中。</p>
<p>如果小型模型的预测正确，大型模型会同意并可以一次性解码多个标记。</p>
<p>但是，如果大型模型拒绝了草稿模型预测的标记，那么批次中的剩余部分将被丢弃，并继续使用大型模型。</p>
<p>有一种阴谋论认为，GPT-4的质量下降是因为他们让预测模型接受了由推测解码模型生成的低概率序列。</p>
<p>请注意，以上内容仅为推测，实际情况可能与之有所不同。</p>
<h2 id="inference-architecture">Inference Architecture</h2>
<p>The inference runs on a cluster of 128 GPUs.</p>
<p>There are multiple of these clusters in multiple datacenters in
different locations.</p>
<p>It is done in 8-way tensor parallelism and 16-way pipeline
parallelism.</p>
<p>Each node of 8 GPUs has only ~130B parameters, or less than 30GB per
GPU at FP16 and less than 15GB at FP8/int8.</p>
<p>The model has 120, so it fits in 15 different nodes. [Possibly the
there are less layers on the first node since it needs to also compute
the embeddings]</p>
<p>According to these numbers: OpenAl should have trained on 2x the
tokens if they were trying to go by chinchilla'soptimal.</p>
<p>[let alone surpass it like we do]</p>
<p>This goes to show that they are strugglingto get high quality
data.</p>
<p>推理过程在由128个GPU组成的集群上运行。</p>
<p>这些集群分布在不同地点的多个数据中心中。</p>
<p>推理过程采用8路张量并行和16路管道并行。</p>
<p>每个包含8个GPU的节点只有约1,300亿个参数，即每个GPU约30GB（FP16）或15GB（FP8/INT8）。</p>
<p>模型有120层，因此需要15个不同的节点来容纳模型。（可能第一个节点的层数较少，因为它还需要计算嵌入层）</p>
<p>根据这些数字，如果OpenAI试图按照chinchilla的最优设置进行训练，他们应该训练2倍的标记。</p>
<p>这表明他们在获取高质量数据方面遇到了困难。</p>
<h2 id="why-no-fsdp">Why no FSDP?</h2>
<p>A possible reason for this could be that some of the hardware infra
they secured is of an older generation.</p>
<p>This is pretty common at local compute clusters as the
organisationusually upgrade the infra in several "waves" to avoid a
complete pause ofoperation.</p>
<p>With such a high amount of pipeline parallelism it is very likely
that just like the rest of us they suffer from the "batch bubble":
slight idle timebetween batches.</p>
<p>Again: There is no magic.</p>
<p>They know what they are doing but it is not magic.</p>
<p>一个可能的原因是，他们所使用的硬件基础设施中有一部分是较旧一代的。</p>
<p>这在本地计算集群中非常常见，通常组织会分几个"波次"进行基础设施的升级，以避免完全停止运营。</p>
<p>由于存在如此高的管道并行性，他们很可能像其他人一样遭受"批处理泡沫"的影响：批次之间存在轻微的空闲时间。</p>
<p>再次强调：没有什么魔法。</p>
<p>他们知道自己在做什么，但这并非魔法。</p>
<hr />
<p>OpenAI是将GPT-4的架构保密，这并不是因为对人类存在着某种潜在风险，而是因为他们所构建的东西是可复制的。实际上，我们预计在不久的将来，谷歌、Meta、Anthropic、Inflection、Character、腾讯、字节跳动、百度等公司都将拥有与GPT-4同等甚至更强大的模型能力。</p>
<p>不要误会，OpenAI的工程能力非常出色，他们所构建的东西令人难以置信，但是他们所采取的解决方案并非神奇。这是一个优雅的解决方案，需要考虑许多复杂的权衡。扩大规模只是战斗的一部分。OpenAI最持久的优势在于他们在实际应用中拥有最多的用户、领先的工程人才，并且可以通过未来的模型继续超越其他竞争对手。</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/" class="print-no-link">#自然语言处理</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>GPT4泄露的技术细节</div>
      <div>http://enderfga.cn/2023/07/11/gpt4/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Enderfga</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2023年7月11日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2023/10/10/last/" title="Embrace Mistakes, Avoid Comparisons">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Embrace Mistakes, Avoid Comparisons</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/06/27/GILL/" title="Generating Images with Multimodal Language Models">
                        <span class="hidden-mobile">Generating Images with Multimodal Language Models</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <script type="text/javascript">
    Fluid.utils.loadComments('#comments', function() {
      var light = 'github-light';
      var dark = 'github-dark';
      var schema = document.documentElement.getAttribute('data-user-color-scheme');
      if (schema === 'dark') {
        schema = dark;
      } else {
        schema = light;
      }
      window.UtterancesThemeLight = light;
      window.UtterancesThemeDark = dark;
      var s = document.createElement('script');
      s.setAttribute('src', 'https://utteranc.es/client.js');
      s.setAttribute('repo', 'Enderfga/commit-utterances');
      s.setAttribute('issue-term', 'title');
      
      s.setAttribute('label', 'utterances');
      
      s.setAttribute('theme', schema);
      s.setAttribute('crossorigin', 'anonymous');
      document.getElementById('comments').appendChild(s);
    })
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  





  <script>
  Fluid.utils.createScript('https://lib.baomitu.com/mermaid/8.13.10/mermaid.min.js', function() {
    mermaid.initialize({"theme":"default"});

    Fluid.utils.listenDOMLoaded(function() {
      Fluid.events.registerRefreshCallback(function() {
        if ('mermaid' in window) {
          mermaid.init();
        }
      });
    });
  });
</script>





  <!-- Custom -->
  <div class="col-lg-7 mx-auto nopadding-x-md">
    <div class="container custom post-custom mx-auto">
      <img src="https://img.enderfga.cn/img/20220420162434.png" srcset="/img/loading.gif" lazyload class="rounded mx-auto d-block mt-5" style="width:150px; height:150px;">
    </div>
  </div>


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <p id="jinrishici-sentence">愿我如星君如月，夜夜流光相皎洁</p> <a href="https://beian.miit.gov.cn/" target="_blank" rel="nofollow noopener"><span> 粤ICP备2021112653号-1</span></a> <script src="https://sdk.jinrishici.com/v2/browser/jinrishici.js" charset="utf-8"></script> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="leancloud-site-pv-container" style="display: none">
        总访问量 
        <span id="leancloud-site-pv"></span>
         次
      </span>
    
    
      <span id="leancloud-site-uv-container" style="display: none">
        总访客数 
        <span id="leancloud-site-uv"></span>
         人
      </span>
    
    

  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  

<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.0/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script defer src="/js/leancloud.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg-full.js">
</script>
</body>
</html>
