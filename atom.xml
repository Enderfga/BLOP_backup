<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Enderfga&#39;Blog</title>
  
  
  <link href="http://enderfga.cn/atom.xml" rel="self"/>
  
  <link href="http://enderfga.cn/"/>
  <updated>2023-06-14T06:44:11.672Z</updated>
  <id>http://enderfga.cn/</id>
  
  <author>
    <name>Enderfga</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Generation Model</title>
    <link href="http://enderfga.cn/2023/06/14/model/"/>
    <id>http://enderfga.cn/2023/06/14/model/</id>
    <published>2023-06-14T06:41:21.000Z</published>
    <updated>2023-06-14T06:44:11.672Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>对 diffusion 结构的改进</p><span id="more"></span><h1 id="Generation-Model"><a href="#Generation-Model" class="headerlink" title="Generation Model"></a>Generation Model</h1><h2 id="RAPHAEL-Text-to-Image-Generation-via-Large-Mixture-of-Diffusion-Paths"><a href="#RAPHAEL-Text-to-Image-Generation-via-Large-Mixture-of-Diffusion-Paths" class="headerlink" title="RAPHAEL: Text-to-Image Generation via Large Mixture of Diffusion Paths"></a>RAPHAEL: Text-to-Image Generation via Large Mixture of Diffusion Paths</h2><p>香港大学，商汤科技，23.5.29，<a href="https://raphael-painter.github.io/">https://raphael-painter.github.io/</a></p><ol><li><strong>Research Problem:</strong> The research problem addressed in this paper is the generation of <strong>highly artistic images that accurately portray text prompts</strong>. Existing models often fail to adequately preserve textual concepts within the generated images due to the reliance on a classic cross-attention mechanism for integrating text descriptions into visual representations.</li><li><strong>Motivation:</strong> The motivation behind this research is to improve the precision of <strong>alignment between text and image</strong> in the generation process. This is aimed at <strong>enhancing the quality and aesthetic</strong> appeal of the generated images, and enabling them to <strong>accurately represent the associated text prompt</strong>.</li><li><p><strong>Solution:</strong> The authors propose a text-conditional image diffusion model, RAPHAEL, which stacks tens of mixture-of-experts (MoE) layers, including both space-MoE and time-MoE layers. This configuration leads to billions of diffusion paths from the network input to the output. Each path can act as a “painter” responsible for rendering a particular concept to an image region at a specific timestep. The authors also propose an edge-supervised learning module to further enhance the image quality and aesthetic appeal of the generated images.</p><p><img src="https://img.enderfga.cn/img/image-20230613125221600.png" alt=""></p></li><li><strong>What’s New:</strong> The novelty of this research lies in the introduction of RAPHAEL, which outperforms recent cutting-edge models in terms of both image quality and aesthetic appeal. RAPHAEL exhibits superior performance in switching images across diverse styles, such as Japanese comics, realism, cyberpunk, and ink illustration. It also establishes a new <strong>state-of-the-art</strong> with a zero-shot FID-30k score of 6.61 on the COCO dataset. Furthermore, RAPHAEL is capable of generating images with resolutions up to 4096 × 6144 with rich image contents and details, when combined with a tailor-made SR-GAN model.</li></ol><h3 id="Mixture-of-experts？"><a href="#Mixture-of-experts？" class="headerlink" title="Mixture of experts？"></a>Mixture of experts？</h3><p><img src="https://img.enderfga.cn/img/image-20230613131954667.png" alt=""></p><p>一种集成学习(Ensemble Learning) 技术，一个系统中包含多个分开的网络，每个网络去处理全部训练样本的一个子集。这种方式可以看做是把多层网络进行了模块化的转换。</p><p>假设我们已经知道数据集中存在一些天然的子集（比如来自不同的domain，不同的topic），那么用单个模型去学习，就会受到很多干扰（interference），导致学习很慢、泛化困难。这时，我们可以使用多个模型（即专家，expert）去学习，使用一个门网络（gating network）来决定每个数据应该被哪个模型去训练，这样就可以减轻不同类型样本之间的干扰。</p><p>MoE 将预测建模任务分解为若干子任务，在每个子任务上训练一个专家模型（Expert Model），开发一个门控模型（Gating Model），该模型根据要预测的输入来学习信任哪个专家，并组合预测结果。</p><h3 id="Detail"><a href="#Detail" class="headerlink" title="Detail"></a>Detail</h3><ul><li>LAION-5B and some internal datasets</li><li>Multi-scale Training</li><li>3B</li><li>GPT3.5生成的100个常见的形容词</li></ul><h2 id="SnapFusion-Text-to-Image-Diffusion-Model-on-Mobile-Devices-within-Two-Seconds"><a href="#SnapFusion-Text-to-Image-Diffusion-Model-on-Mobile-Devices-within-Two-Seconds" class="headerlink" title="SnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds"></a>SnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds</h2><p>Snap Inc.，东北大学，23.6.1，<a href="https://snap-research.github.io/SnapFusion/">https://snap-research.github.io/SnapFusion/</a></p><ol><li><strong>研究问题：</strong>这篇论文解决的研究问题是如何在移动设备上快速运行文本到图像的扩散模型。现有的模型虽然能够生成出色的图像，但由于其复杂的网络架构和大量的去噪迭代，使得它们在计算上非常昂贵，运行速度慢。</li><li><strong>动机：</strong>这项研究的动机是改进现有模型的运行速度和计算成本。这不仅可以降低运行模型的成本，还可以避免在用户数据被发送到第三方时涉及到的隐私问题。</li><li><strong>解决方法：</strong>作者通过引入高效的网络架构和改进步骤蒸馏来解决这个问题。他们提出了一个高效的UNet，通过识别原始模型的冗余，并通过数据蒸馏减少图像解码器的计算。此外，他们通过探索训练策略和引入分类器无关指导的正则化来增强步骤蒸馏。</li><li><strong>创新之处：</strong>这项研究的创新之处在于，它是第一个能够在移动设备上运行文本到图像扩散模型的方法，运行时间少于2秒。此外，他们的模型在8个去噪步骤中比Stable Diffusion v1.5的50个步骤获得了更好的FID和CLIP分数。这项工作通过将强大的文本到图像扩散模型带到用户手中，使内容创建民主化。</li></ol><p><img src="https://img.enderfga.cn/img/image-20230613134941299.png" alt=""></p><p>为了优化 UNet 结构，我们提出一套 UNet 结构自动评估、进化流程：先对 UNet 进行鲁棒性训练（Robust Training），在训练中随机 drop 一些模块，以此来测试出每个模块对性能的真实影响，从而构建一个 “对 CLIP score 的影响 vs. latency” 的查找表；然后根据该查找表，优先去除对 CLIP score 影响不大同时又很耗时的模块。这一套流程是在线自动进行，完成之后，我们就得到了一个全新的 UNet 结构，称为 Efficient UNet。相比原版 UNet，实现 7.4x 加速且性能不降。</p><p><img src="https://img.enderfga.cn/img/image-20230613135018817.png" alt=""></p><p>这篇论文中的蒸馏过程主要包括两个部分：图像解码器的蒸馏和步骤蒸馏。</p><ol><li><strong>图像解码器的蒸馏：</strong>作者提出了一个蒸馏流程，使用合成数据来学习通过通道减少获得的高效图像解码器。他们使用文本提示从SD-v1.5的UNet中获取潜在表示，然后将其转发到他们的高效图像解码器和SD-v1.5的解码器以生成两个图像。然后，通过最小化两个图像之间的均方误差来优化解码器。</li><li><strong>步骤蒸馏：</strong>步骤蒸馏是一种减少UNet迭代去噪步骤的方法，以实现更快的速度。他们采用了步骤蒸馏的研究方向，其中通过蒸馏教师模型来减少推理步骤。例如，如果教师模型需要32步，那么学生模型可能只需要16步，这样，学生模型的速度就可以提高2倍。</li></ol><p>这篇论文还提出了一种新的蒸馏方法，称为CFG-Aware Step Distillation，它在计算损失之前对教师和学生进行分类器无关的指导。这种方法在提高CLIP分数方面表现出显著的效果。</p><h3 id="贡献："><a href="#贡献：" class="headerlink" title="贡献："></a>贡献：</h3><p>（1）通过对现有 UNet 的逐层分析，定位速度瓶颈，提出一种新的高效 UNet 结构（Efficient UNet），可以等效替换原 Stable Diffusion 中的 UNet，实现 7.4x 加速；</p><p>（2）对推理阶段的迭代步数进行优化，提出一种全新的步数蒸馏方案（CFG-aware Step Distillation），减少步数的同时可显著提升 CLIP score，实现 6.25x 加速。</p><h3 id="不足："><a href="#不足：" class="headerlink" title="不足："></a>不足：</h3><ol><li>SD 模型在多种图像生成场景中都可以使用，本文囿于时间，目前只关注了 text to image 这个核心任务，后期将跟进其他任务（如 inpainting，ControlNet 等等）。</li><li>本文主要关注速度上的提升，并未对模型存储进行优化。我们相信所提出的 Efficient UNet 仍然具备压缩的空间，结合其他的高性能优化方法（如剪枝，量化），有望缩小存储，并将时间降低到 1 秒以内，离端上实时 SD 更进一步。</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;对 diffusion 结构的改进&lt;/p&gt;</summary>
    
    
    
    
    <category term="计算机数据" scheme="http://enderfga.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion——RLHF</title>
    <link href="http://enderfga.cn/2023/05/31/diffrlhf/"/>
    <id>http://enderfga.cn/2023/05/31/diffrlhf/</id>
    <published>2023-05-31T08:20:07.000Z</published>
    <updated>2023-05-31T08:26:12.419Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>最近读的两篇关于使用RLHF微调diffusion的论文。</p><span id="more"></span><h1 id="Diffusion-RLHF"><a href="#Diffusion-RLHF" class="headerlink" title="Diffusion_RLHF"></a>Diffusion_RLHF</h1><h2 id="Training-Diffusion-Models-with-Reinforcement-Learning"><a href="#Training-Diffusion-Models-with-Reinforcement-Learning" class="headerlink" title="Training Diffusion Models with Reinforcement Learning"></a>Training Diffusion Models with Reinforcement Learning</h2><p><strong>5.23， University of California, Berkeley</strong></p><p><strong>研究问题 (Research Problem)</strong>: 论文主要解决的问题是如何优化扩散模型以实现特定目标。扩散模型是一种生成模型，通常使用对对数似然目标的近似进行训练。然而，大多数扩散模型的使用场景并不关心似然性，而是关心下游目标，如人类感知的图像质量或药物效果。</p><p><strong>动机 (Motivation)</strong>: 这项研究的动机是通过更紧密地将扩散模型与实际目标对齐，以提高其性能。这对于<em>难以通过提示表达的任务（如图像可压缩性）以及从人类反馈中得出的任务（如审美质量）</em>尤其相关。</p><p><strong>新颖性 (What’s New)</strong>: 作者提出了一种名为Denoising Diffusion Policy Optimization (DDPO)的方法，该方法将去噪视为多步决策问题。这使得可以使用比替代奖励加权似然方法更有效的策略梯度算法。此外，作者还展示了DDPO如何改进使用视觉语言模型反馈的提示图像对齐，而无需额外的数据收集或人类注释。</p><p><strong>方法的总体思想 (Overall Idea of the Method)</strong>: DDPO方法的总体思想是将去噪过程视为一个多步骤的决策问题，从而可以使用策略梯度算法进行优化。这种方法可以适应各种目标，包括那些难以通过提示表达的目标，如图像可压缩性，以及那些从人类反馈中得出的目标，如审美质量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> io<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">encode_jpeg</span>(<span class="hljs-params"> x , quality = <span class="hljs-number">95</span> </span>) :</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">x : np array of shape (H, W, 3) and dtype uint8</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>    img = Image.fromarray(x)<br>    buffer = io.BytesIO()<br>    img.save(buffer, ‘JPEG’, quality = quality)<br>    jpeg = buffer.getvalue()<br>    <span class="hljs-built_in">bytes</span> = np.frombuffer(jpeg, dtype =np.uint8)<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(<span class="hljs-built_in">bytes</span>)/<span class="hljs-number">1000</span><br></code></pre></td></tr></table></figure><p><img src="https://img.enderfga.cn/img/image-20230530135032898.png" alt=""></p><p><strong>实验主要提升的效果 (Main Improvements in Experiments)</strong>: 实验结果表明，DDPO在所有任务上都明显优于RWR，表明将去噪过程形式化为马尔可夫决策过程并直接估计策略梯度比优化奖励加权似然的下界更有效。此外，DDPO还能够有效地适应预训练模型，只需指定奖励函数，无需进行任何进一步的数据策划。</p><p><img src="https://img.enderfga.cn/img/image-20230530135637339.png" alt=""></p><p><strong>具体来说，DDPO的实现可以分为以下几个步骤：</strong></p><ol><li><strong>定义决策问题</strong>：首先，DDPO将去噪过程定义为一个马尔可夫决策过程（MDP）。在这个MDP中，每一步都包括一个状态（当前的去噪图像）和一个动作（下一步的去噪操作）。</li><li><strong>定义奖励函数</strong>：然后，DDPO定义了一个奖励函数，用于评估每一步去噪操作的效果。这个奖励函数可以基于任何与任务目标相关的度量，例如在这篇论文中，奖励可能是基于图像的压缩性、审美质量或与提示的对齐程度。</li><li><strong>优化策略</strong>：接下来，DDPO使用策略梯度算法来优化去噪策略。这个过程包括生成一组去噪轨迹，计算每个轨迹的预期奖励，然后使用这些预期奖励来更新策略的参数。</li><li><strong>迭代优化</strong>：最后，DDPO通过迭代这个过程，逐步改进去噪策略。每一轮迭代都会生成新的去噪轨迹，计算新的预期奖励，然后使用这些新的预期奖励来更新策略的参数。<script type="math/tex; mode=display">\hat{g}_{\mathrm{IS}}=\mathbb{E}\left[\sum_{t=0}^T \frac{p_\theta\left(\mathbf{x}_{t-1} \mid \mathbf{c}, t, \mathbf{x}_t\right)}{p_{\theta_{\text {old }}}\left(\mathbf{x}_{t-1} \mid \mathbf{c}, t, \mathbf{x}_t\right)} \nabla_\theta \log p_\theta\left(\mathbf{x}_{t-1} \mid \mathbf{c}, t, \mathbf{x}_t\right) r\left(\mathbf{x}_0, \mathbf{c}\right)\right]</script></li></ol><h2 id="DPOK-Reinforcement-Learning-for-Fine-tuning-Text-to-Image-Diffusion-Models"><a href="#DPOK-Reinforcement-Learning-for-Fine-tuning-Text-to-Image-Diffusion-Models" class="headerlink" title="DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models"></a>DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models</h2><p><strong>5.25，Google research</strong></p><p><strong>研究问题 (Research Problem)</strong>: 本文主要研究如何通过在线强化学习（RL）来优化和微调文本到图像的模型，以改善其性能和质量。</p><p><strong>动机 (Motivation)</strong>: 尽管文本到图像的模型已经取得了显著的进步，但它们仍然存在系统性的弱点，例如<em>有限的对象组合能力和在生成指定颜色和数量的对象时的困难</em>。此外，使用<em>人类反馈进行学习已被证明是一种有效的方法</em>来克服这些限制。然而，<em>基于奖励的监督微调往往会导致图像质量的下降</em>。因此，本文提出了一种新的在线RL微调方法来解决这些问题。</p><p><strong>新颖性 (What’s New)</strong>: 本文提出了一种名为DPOK的新方法，该方法将策略优化与KL正则化结合起来。与以往的工作不同，本文将微调任务定义为一个RL问题，并使用策略梯度来更新预训练的文本到图像扩散模型，以最大化反馈训练的奖励。</p><p><strong>方法的总体思想 (Overall Idea of the Method)</strong>: DPOK方法的核心思想是利用在线强化学习来微调文本到图像的模型。在这个过程中，模型是在新的样本上进行更新的，这些样本来自于之前训练的模型。此外，该方法还引入了Kullback-Leibler (KL) 散度作为正则化项，以确保更新后的模型不会偏离原始模型太远。</p><p><strong>作者引入了两种KL正则化方法，一种是KL-D，另一种是KL-O。KL-D基于预训练模型的数据，通过调整原始奖励中的一个移位因子来实现正则化，使得每个样本的权重更趋向于均匀分布。而KL-O则通过在奖励加权损失中引入一个额外的项来实现，这个额外的项惩罚了从预训练模型和当前模型得出的去噪方向之间的L2距离。</strong></p><script type="math/tex; mode=display">\begin{aligned}& \mathbb{E}_{p(z)}\left[\alpha \mathbb{E}_{p_\theta^*\left(x_{0: T} \mid z\right)}\left[-r\left(x_0, z\right) \sum_{t=1}^T \log p_\theta\left(x_{t-1} \mid x_t, z\right)\right]\right. \\& \left.+\beta \sum_{t=1}^T \mathbb{E}_{p_\theta^*\left(x_t \mid z\right)}\left[\operatorname{KL}\left(p_\theta\left(x_{t-1} \mid x_t, z\right) \| p_{\mathrm{pre}}\left(x_{t-1} \mid x_t, z\right)\right)\right]\right]\end{aligned}</script><p><strong>实验主要提升的效果 (Main Improvements in Experiments)</strong>: 在实验中，作者发现在线RL微调能够在保持高图像保真度的同时，实现强大的文本-图像对齐。此外，与监督微调相比，在线训练允许在（监督）训练数据集之外评估奖励模型和条件KL散度，这提供了明显的优势。在实证比较中，作者还在监督微调方法中引入了KL正则项，以进行公平的比较。</p><p><img src="https://img.enderfga.cn/img/image-20230530141846201.png" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20230530142225063.png" alt=""></p><p><strong>与上面一篇的对比：</strong></p><ol><li><strong>相似性</strong>：Black等人的工作和本文都探讨了在线强化学习微调用于改进文本到图像扩散模型。他们都展示了RL微调可以优于监督微调，这与本文的观察结果一致。</li><li><strong>差异性</strong>：在本文中，作者不仅关注奖励优化，还受到监督微调中的失败案例（如过饱和或非真实感图像）的启发，旨在找到一种带有KL正则化的RL解决方案来解决问题。此外，本文还系统地分析了监督微调和在线微调中KL正则化的理论依据，并展示了在线RL微调中KL正则化比监督微调更有效。通过采用在线KL正则化，本文的算法成功地在保持高奖励和图像质量的同时，避免了过度优化的问题。</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近读的两篇关于使用RLHF微调diffusion的论文。&lt;/p&gt;</summary>
    
    
    
    
    <category term="计算机视觉" scheme="http://enderfga.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>基于知识库的自动问答系统</title>
    <link href="http://enderfga.cn/2023/05/31/kbqa/"/>
    <id>http://enderfga.cn/2023/05/31/kbqa/</id>
    <published>2023-05-31T08:09:40.000Z</published>
    <updated>2023-05-31T08:21:52.428Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>自然语言处理期中和期末项目。</p><span id="more"></span><embed src="./kbqa1.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    <summary type="html">&lt;p&gt;自然语言处理期中和期末项目。&lt;/p&gt;</summary>
    
    
    
    
    <category term="自然语言处理" scheme="http://enderfga.cn/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>四种常见梯度算法应用</title>
    <link href="http://enderfga.cn/2023/05/31/opt/"/>
    <id>http://enderfga.cn/2023/05/31/opt/</id>
    <published>2023-05-31T07:30:34.000Z</published>
    <updated>2023-05-31T07:50:54.232Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>最优化理论与算法期中，<a href="https://enderfga.cn/markmap.html">笔记链接</a></p><p><img src="https://img.enderfga.cn/img/image-20230531154210275.png" alt=""></p><span id="more"></span><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br></pre></td><td class="code"><pre><code class="hljs matlab"><span class="hljs-comment">% 利用最速梯度下降法求解函数的极小点</span><br><br><span class="hljs-comment">% 函数定义</span><br>f = @(x) <span class="hljs-number">0.5</span>*x(<span class="hljs-number">1</span>)^<span class="hljs-number">2</span> + x(<span class="hljs-number">2</span>)^<span class="hljs-number">2</span>;<br><br><span class="hljs-comment">% 梯度定义</span><br>grad_f = @(x) [x(<span class="hljs-number">1</span>); <span class="hljs-number">2</span>*x(<span class="hljs-number">2</span>)];<br><br><span class="hljs-comment">% 初始值和收敛阈值</span><br>x0 = [<span class="hljs-number">2</span>; <span class="hljs-number">1</span>];<br>epsilon = <span class="hljs-number">1e-2</span>;<br><br><span class="hljs-comment">% 最速梯度下降法</span><br>alpha = <span class="hljs-number">0.1</span>;  <span class="hljs-comment">% 初始步长</span><br>x = x0;<br>grad = grad_f(x);<br>iter = <span class="hljs-number">0</span>;<br><br><span class="hljs-keyword">while</span> norm(grad) &gt; epsilon<br>    <span class="hljs-comment">% 计算步长</span><br>    <span class="hljs-comment">% 这里可以使用线搜索方法来选择合适的步长 alpha，如 Armijo 规则或 Wolfe 条件</span><br>  <br>    <span class="hljs-comment">% 更新变量</span><br>    x = x - alpha * grad;<br>    grad = grad_f(x);<br>  <br>    iter = iter + <span class="hljs-number">1</span>;<br><span class="hljs-keyword">end</span><br><br><span class="hljs-comment">% 输出结果</span><br>fprintf(<span class="hljs-string">&#x27;最小值点: (%f, %f)\n&#x27;</span>, x(<span class="hljs-number">1</span>), x(<span class="hljs-number">2</span>));<br>fprintf(<span class="hljs-string">&#x27;迭代次数: %d\n&#x27;</span>, iter);<br><br><span class="hljs-comment">% 利用牛顿法求解函数的极小点</span><br><br><span class="hljs-comment">% 函数定义</span><br>f = @(x) <span class="hljs-number">100</span>*(x(<span class="hljs-number">2</span>) - x(<span class="hljs-number">1</span>)^<span class="hljs-number">2</span>)^<span class="hljs-number">2</span> + (<span class="hljs-number">1</span> - x(<span class="hljs-number">1</span>))^<span class="hljs-number">2</span>;<br><br><span class="hljs-comment">% 梯度定义</span><br>grad_f = @(x) [<span class="hljs-number">-400</span>*x(<span class="hljs-number">1</span>)*(x(<span class="hljs-number">2</span>) - x(<span class="hljs-number">1</span>)^<span class="hljs-number">2</span>) - <span class="hljs-number">2</span>*(<span class="hljs-number">1</span> - x(<span class="hljs-number">1</span>));<br>                <span class="hljs-number">200</span>*(x(<span class="hljs-number">2</span>) - x(<span class="hljs-number">1</span>)^<span class="hljs-number">2</span>)];<br><br><span class="hljs-comment">% Hessian 矩阵定义</span><br>hessian = @(x) [<span class="hljs-number">1200</span>*x(<span class="hljs-number">1</span>)^<span class="hljs-number">2</span> - <span class="hljs-number">400</span>*x(<span class="hljs-number">2</span>) + <span class="hljs-number">2</span>, <span class="hljs-number">-400</span>*x(<span class="hljs-number">1</span>);<br>                <span class="hljs-number">-400</span>*x(<span class="hljs-number">1</span>), <span class="hljs-number">200</span>];<br><br><span class="hljs-comment">% 初始值和收敛阈值</span><br>x0 = [<span class="hljs-number">-2</span>; <span class="hljs-number">2</span>];<br>epsilon = <span class="hljs-number">1e-2</span>;<br><br><span class="hljs-comment">% 牛顿法</span><br>x = x0;<br>grad = grad_f(x);<br>iter = <span class="hljs-number">0</span>;<br><br><span class="hljs-keyword">while</span> norm(grad) &gt; epsilon<br>    <span class="hljs-comment">% 计算 Hessian 矩阵和其逆矩阵</span><br>    H = hessian(x);<br>    inv_H = inv(H);<br>  <br>    <span class="hljs-comment">% 更新变量</span><br>    x = x - inv_H * grad;<br>    grad = grad_f(x);<br>  <br>    iter = iter + <span class="hljs-number">1</span>;<br><span class="hljs-keyword">end</span><br><br><span class="hljs-comment">% 输出结果</span><br>fprintf(<span class="hljs-string">&#x27;最小值点: (%f, %f)\n&#x27;</span>, x(<span class="hljs-number">1</span>), x(<span class="hljs-number">2</span>));<br>fprintf(<span class="hljs-string">&#x27;迭代次数: %d\n&#x27;</span>, iter);<br><br><span class="hljs-comment">% 利用共轭梯度法求解方程组的根</span><br><br><span class="hljs-comment">% 原始方程组的系数矩阵 A</span><br>A = [<span class="hljs-number">4</span>, <span class="hljs-number">-3</span>; <span class="hljs-number">2</span>, <span class="hljs-number">1</span>];<br><br><span class="hljs-comment">% 原始方程组的右侧向量 b</span><br>b = [<span class="hljs-number">11</span>; <span class="hljs-number">13</span>];<br><br><span class="hljs-comment">% 转化为对称正定矩阵 B = A^T * A</span><br>B = A&#x27; * A;<br><br><span class="hljs-comment">% 转化后的方程组的右侧向量</span><br>b_new = A&#x27; * b;<br><br><span class="hljs-comment">% 初始值和收敛阈值</span><br>x0 = [<span class="hljs-number">0</span>; <span class="hljs-number">0</span>];<br>epsilon = <span class="hljs-number">1e-6</span>;<br><br><span class="hljs-comment">% 共轭梯度法</span><br>x = x0;<br>r = b_new - B * x;<br>p = r;<br>iter = <span class="hljs-number">0</span>;<br><br><span class="hljs-keyword">while</span> norm(r) &gt; epsilon<br>    alpha = (r&#x27; * r) / (p&#x27; * B * p);<br>    x = x + alpha * p;<br>    r_new = r - alpha * B * p;<br>    <span class="hljs-built_in">beta</span> = (r_new&#x27; * r_new) / (r&#x27; * r);<br>    p = r_new + <span class="hljs-built_in">beta</span> * p;<br>    r = r_new;<br>    iter = iter + <span class="hljs-number">1</span>;<br><span class="hljs-keyword">end</span><br><br><span class="hljs-comment">% 输出结果</span><br>fprintf(<span class="hljs-string">&#x27;方程的根: (%f, %f)\n&#x27;</span>, x(<span class="hljs-number">1</span>), x(<span class="hljs-number">2</span>));<br>fprintf(<span class="hljs-string">&#x27;迭代次数: %d\n&#x27;</span>, iter);<br><br><span class="hljs-comment">% 利用DFP算法求解函数的极小点</span><br><br><span class="hljs-comment">% 函数定义</span><br>f = @(x) x(<span class="hljs-number">1</span>)^<span class="hljs-number">2</span> + <span class="hljs-number">2</span>*x(<span class="hljs-number">2</span>)^<span class="hljs-number">2</span> - <span class="hljs-number">4</span>*x(<span class="hljs-number">1</span>) - <span class="hljs-number">2</span>*x(<span class="hljs-number">1</span>)*x(<span class="hljs-number">2</span>);<br><br><span class="hljs-comment">% 梯度定义</span><br>grad_f = @(x) [<span class="hljs-number">2</span>*x(<span class="hljs-number">1</span>) - <span class="hljs-number">4</span> - <span class="hljs-number">2</span>*x(<span class="hljs-number">2</span>); <span class="hljs-number">4</span>*x(<span class="hljs-number">2</span>) - <span class="hljs-number">2</span>*x(<span class="hljs-number">1</span>)];<br><br><span class="hljs-comment">% 初始点和初始近似Hessian矩阵</span><br>x0 = [<span class="hljs-number">1</span>; <span class="hljs-number">1</span>];<br>H0 = <span class="hljs-built_in">eye</span>(<span class="hljs-number">2</span>);<br><br><span class="hljs-comment">% 最大迭代次数和停止迭代的阈值</span><br>max_iter = <span class="hljs-number">100</span>;<br>epsilon = <span class="hljs-number">1e-6</span>;<br><br><span class="hljs-comment">% DFP算法</span><br>x = x0;<br>H = H0;<br>g = grad_f(x);<br>iter = <span class="hljs-number">0</span>;<br><br><span class="hljs-keyword">while</span> norm(g) &gt; epsilon &amp;&amp; iter &lt; max_iter<br>    d = -H * g;  <span class="hljs-comment">% 计算搜索方向</span><br>  <br>    <span class="hljs-comment">% 使用线搜索方法选择合适的步长</span><br>    alpha = <span class="hljs-number">1</span>; <span class="hljs-comment">% 这里可以使用固定步长或者其他线搜索方法</span><br>  <br>    x_new = x + alpha * d;<br>    g_new = grad_f(x_new);<br>    s = x_new - x;<br>    y = g_new - g;<br>  <br>    rho = <span class="hljs-number">1</span> / (y&#x27; * s);<br>    H = (<span class="hljs-built_in">eye</span>(<span class="hljs-number">2</span>) - rho * s * y&#x27;) * H * (<span class="hljs-built_in">eye</span>(<span class="hljs-number">2</span>) - rho * y * s&#x27;) + rho * s * s&#x27;; <span class="hljs-comment">% 更新近似Hessian矩阵</span><br>  <br>    x = x_new;<br>    g = g_new;<br>    iter = iter + <span class="hljs-number">1</span>;<br><span class="hljs-keyword">end</span><br><br><span class="hljs-comment">% 输出结果</span><br>fprintf(<span class="hljs-string">&#x27;最小值点: (%f, %f)\n&#x27;</span>, x(<span class="hljs-number">1</span>), x(<span class="hljs-number">2</span>));<br>fprintf(<span class="hljs-string">&#x27;迭代次数: %d\n&#x27;</span>, iter);<br><br><br></code></pre></td></tr></table></figure><embed src="./opt.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    <summary type="html">&lt;p&gt;最优化理论与算法期中，&lt;a href=&quot;https://enderfga.cn/markmap.html&quot;&gt;笔记链接&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img.enderfga.cn/img/image-20230531154210275.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    
    <category term="最优化理论" scheme="http://enderfga.cn/tags/%E6%9C%80%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title>3D论文PPT</title>
    <link href="http://enderfga.cn/2023/03/23/3dppt/"/>
    <id>http://enderfga.cn/2023/03/23/3dppt/</id>
    <published>2023-03-23T11:25:47.000Z</published>
    <updated>2023-05-31T08:28:21.754Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="https://img.enderfga.cn/img/image-20230323192941643.png" alt="image-20230323192941643"></p><embed src="./3d.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="计算机视觉" scheme="http://enderfga.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>RLHF</title>
    <link href="http://enderfga.cn/2023/03/01/RLHF/"/>
    <id>http://enderfga.cn/2023/03/01/RLHF/</id>
    <published>2023-03-01T03:17:42.000Z</published>
    <updated>2023-03-01T03:19:08.223Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="RLHF"><a href="#RLHF" class="headerlink" title="RLHF"></a>RLHF</h1><h2 id="Aligning-Text-to-Image-Models-using-Human-Feedback"><a href="#Aligning-Text-to-Image-Models-using-Human-Feedback" class="headerlink" title="Aligning Text-to-Image Models using Human Feedback"></a>Aligning Text-to-Image Models using Human Feedback</h2><ul><li><p>Google Research ,University of California</p></li><li><p>2023.2.23</p><span id="more"></span></li></ul><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>深度生成模型在文本到图像合成方面取得了令人印象深刻的成果，但当前的文本到图像模型往往生成与文本提示不够相符的图像。</p><p>本文的动机是改进文本到图像合成模型，使其能够更好地与文本提示对齐。</p><p>作者的方法比预训练模型更准确地生成具有指定颜色、计数和背景的对象。</p><h2 id="Proposal"><a href="#Proposal" class="headerlink" title="Proposal"></a>Proposal</h2><ul><li>提出了一种简单而有效的微调方法，用于使用人类反馈对文本到图像模型进行对齐。</li><li>使用人类反馈进行微调可以显着提高文本到图像模型的图像文本对齐，在人类评估中，我们的模型在图像文本对齐方面达到了高达47％的改善，但图像保真度略有降低。</li><li>学习的奖励函数比CLIP分数更准确地预测了人类对质量的评估。</li><li>基于作者学习的奖励函数的采样也可以显着改善图像文本对齐。</li></ul><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><ul><li>T2I models</li><li>Evaluating image-text alignment</li><li>Learning with human feedback</li></ul><p>与先前关注利用人类反馈改善语言模型和RL代理的工作相比，该工作探索了使用人类反馈来调整多模式文本到图像模型与人类偏好的方法。许多关于利用人类反馈学习的先前工作都包括学习一个奖励函数并最大化奖励加权可能性（通常被称为监督微调）。受其成功的启发，作者提出了一种利用人类反馈进行微调的方法来改善文本到图像模型。</p><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p><img src="https://img.enderfga.cn/img/image-20230228130311025.png" alt=""></p><p>包括三个阶段：</p><ol><li>首先从一系列文本提示中生成一组不同的图像，这些文本提示旨在测试文本到图像模型的各种功能。 </li><li>然后，人类评级者对这些图像提供二进制反馈。 </li><li>接下来，训练一个奖励模型，以文本提示和图像作为输入来预测人类反馈。</li><li>最后，我们使用奖励加权对数似然度来微调文本到图像模型，以改善文本图像对齐。</li></ol><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p>实验部分旨在测试人类反馈参与模型微调的有效性。实验用到的模型为 Stable Diffusion v1.5</p><p><img src="https://img.enderfga.cn/img/image-20230228132659591.png" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20230228132741226.png" alt=""></p><p>本文方法显著提高了图像 - 文本对齐，具体来说，模型生成的图像中有 50% 的样本获得至少三分之二的赞成票（投票数量为 7 票或更多赞成票），然而，微调会稍微降低图像保真度（15% 比 10%）。</p><p><img src="https://img.enderfga.cn/img/image-20230228132824327.png" alt=""></p><p>本文模型生成的图像符合 prompt 指定的颜色、计数和背景。值得注意的是，本文模型还能生成没有见过的文本 prompt 图像，并且质量非常高</p><p><img src="https://img.enderfga.cn/img/image-20230228132908052.png" alt=""></p><p>有奖励（绿色）比 CLIP 分数（红色）更符合典型的人类意图。</p><h2 id="Limitations-and-future-directions"><a href="#Limitations-and-future-directions" class="headerlink" title="Limitations and future directions"></a>Limitations and future directions</h2><ol><li><p><strong>更细致的人类反馈</strong>，存在一些较差的生成，如高饱和度的图像颜色，指示评级者寻找更多样化的失败模式（过度饱和的颜色，不切实际的动物解剖学，物理违规等）将提高这些方面的性能。</p></li><li><p><strong>多样化和大型人类数据集</strong>，为了简化问题，作者考虑了有限的文本类别（计数，颜色，背景），因此人类反馈也相对简单（好或坏）。由于这一点，人类反馈数据的多样性有限。将其扩展到更主观的文本类别（如艺术创作）和更细致的人类反馈将是未来研究的重要方向。</p></li><li><p><strong>不同的目标和算法</strong>，为了更新文本到图像模型，作者使用奖励加权的最大似然。然而，与语言领域的先前工作类似，使用RL算法将是一个有趣的方向。作者相信RLHF微调可能会产生更好的模型，因为</p><p>（a）在更新期间使用在线样本生成</p><p>（b）KL正则化可以减轻对奖励函数的过度拟合。</p></li></ol>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;RLHF&quot;&gt;&lt;a href=&quot;#RLHF&quot; class=&quot;headerlink&quot; title=&quot;RLHF&quot;&gt;&lt;/a&gt;RLHF&lt;/h1&gt;&lt;h2 id=&quot;Aligning-Text-to-Image-Models-using-Human-Feedback&quot;&gt;&lt;a href=&quot;#Aligning-Text-to-Image-Models-using-Human-Feedback&quot; class=&quot;headerlink&quot; title=&quot;Aligning Text-to-Image Models using Human Feedback&quot;&gt;&lt;/a&gt;Aligning Text-to-Image Models using Human Feedback&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Google Research ,University of California&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;2023.2.23&lt;/p&gt;</summary>
    
    
    
    
    <category term="计算机视觉" scheme="http://enderfga.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>Realfusion</title>
    <link href="http://enderfga.cn/2023/03/01/Realfusion/"/>
    <id>http://enderfga.cn/2023/03/01/Realfusion/</id>
    <published>2023-03-01T03:14:27.000Z</published>
    <updated>2023-03-01T03:16:21.212Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="RealFusion："><a href="#RealFusion：" class="headerlink" title="RealFusion："></a>RealFusion：</h1><h3 id="360°-Reconstruction-of-Any-Object-from-a-Single-Image"><a href="#360°-Reconstruction-of-Any-Object-from-a-Single-Image" class="headerlink" title="360° Reconstruction of Any Object from a Single Image"></a>360° Reconstruction of Any Object from a Single Image</h3><ul><li><p>Oxford University</p></li><li><p>2023.2.23</p><span id="more"></span><h2 id="Demo"><a href="#Demo" class="headerlink" title="Demo"></a>Demo</h2><p><a href="https://lukemelas.github.io/realfusion/">https://lukemelas.github.io/realfusion/</a></p><p><img src="https://img.enderfga.cn/img/splash-figure-v2.png" alt="Examples"></p></li></ul><h2 id="Motivation-Single-View-3D-Reconstruction"><a href="#Motivation-Single-View-3D-Reconstruction" class="headerlink" title="Motivation: Single-View 3D Reconstruction"></a>Motivation: Single-View 3D Reconstruction</h2><ul><li>Reconstructing the 3D structure of an object from a single 2D view is a fundamental challenge in computer vision.</li><li>In the case of a single view, the reconstruction problem is highly ill-posed.<br>As a result, the task requires semantic understanding obtained by learning.<br>Despite the difficulty of this task, humans are adept at using a range of monocular cues to infer the 3D structures of objects from single views.</li></ul><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><h3 id="Category-level-3D-Reconstruction"><a href="#Category-level-3D-Reconstruction" class="headerlink" title="Category-level 3D Reconstruction"></a>Category-level 3D Reconstruction</h3><ul><li>Most prior work tackles the problem of category-specific single-view 3D reconstruction by training a category-level reconstruction model.</li><li>The work: Going beyond category-level 3D reconstruction<ul><li>This work aims to go beyond category-specific images to images of arbitrary objects. This setting is highly challenging, but humans perform it effortlessly when they observe new objects.</li></ul></li></ul><h3 id="Single-View-3D-Reconstruction"><a href="#Single-View-3D-Reconstruction" class="headerlink" title="Single-View 3D Reconstruction"></a>Single-View 3D Reconstruction</h3><ul><li>Arbitrary-object 3D reconstruction has been challenging because the problem fundamentally requires the use of large-scale 3D priors over object shapes, which have not been available.</li><li>With the recent rise of large-scale pretraining, this problem has become tractable.<br>Examples include:<ul><li>Contrastive: CLIP</li><li>Autoregressive: DALL-E / Parti</li><li>Diffusion Models: DALL-E 2 / Imagen / Stable Diffusion</li></ul></li><li>These pretrained models may be used as priors for a variety of vision tasks, and we are particularly interested in 3D reconstruction.<ul><li>At a high level, you can think of these models as a tool for optimizing the realism of an input image.</li></ul></li><li>In this way, they enable an elegant approach to 3D generation and reconstruction: using these large-scale pretrained models to enforce that a differentiable scene looks realistic from random views.</li></ul><h2 id="Proposal"><a href="#Proposal" class="headerlink" title="Proposal"></a>Proposal</h2><p>(1) We propose <strong>RealFusion</strong>, a method that can extract from a single image of an object a 360◦ photographic 3D reconstruction without assumptions on the type of object imaged or 3D supervision of any kind; </p><p>(2) We do so by leveraging an existing 2D <strong>diffusion image generator</strong> via a new single image variant of textual inversion; </p><p>(3) We also introduce new regularizers and provide an efficient implementation using <strong>InstantNGP</strong>; </p><p>(4) We demonstrate <strong>state-of-the-art</strong> reconstruction results on a number of in-the-wild images and images from existing datasets when compared to alternative approaches.  </p><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><ul><li>Image-based reconstruction of appearnce and geometry</li><li>Few-view reconstruction</li><li>Single-view reconstruction  </li><li>Extracting 3D models from 2D generators  </li><li>Diffusion Models  </li></ul><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p><img src="https://img.enderfga.cn/img/image-20230227112154724.png" alt=""></p><ul><li>This approach forms the backbone of our method, RealFusion.<br>(0) [Init] We are given a single image and a function $\boldsymbol{p}_{\text {prior }}(\cdot)$ which computes the likelihood of an input image $\boldsymbol{I}$. We choose a camera view and represent our scene with a differentiably-renderable representation $\boldsymbol{x}$, for example a NeRF.<br>(1) [Reconstruction] We render $\boldsymbol{x}$ from our given view and minimize the loss with respect to the real input image $\mathbf{I}$.<br>(2) [Prior] We render images $\boldsymbol{I}_{\text {prior }}$ of $\boldsymbol{x}$ from randomly-chosen views on a hemisphere surrounding the origin, and we optimize $\boldsymbol{p}_{\text {prior }}\left(\boldsymbol{I}_{\text {priol }}\right)$ to enforce that $\boldsymbol{x}$ looks realistic from all directions.</li><li>Prior work has explored this question in the domain of 3D generation<ul><li>Dreamfields: CLIP prior</li><li>DreamFusion: Diffusion model prior</li></ul></li><li>In our work, we adopt a diffusion model prior using Stable Diffusion, a text-conditional latent diffusion model.</li><li>As currently stated, our set up combines a reconstruction objective with a latent diffusion-based prior objective, which is conditioned on a manual text prompt (e.g. “An image of a fish.”)</li><li>However, we found that these results were lacking.</li><li>In particular, the 3D shapes that are generated look like the input object from the input view, but do not look like the input object from other views.</li><li>To fix this, we need to modify the prior to place a high likelihood on our input object, rather than a generic object with the same description.</li><li>We do so by performing textual inversion.<ul><li>We optimize a text embedding $\mathbf{e}$ in the text encoder of the diffusion model to match our input image.</li><li>Usually textual inversion is performed with multiple views of an object, but we substitute these views with heavy image augmentations.</li></ul></li><li>We also add other pieces of regularization:<br>(1) A regularization on rendered normals<br>(2) A coarse-to-fine training setup</li><li>However, the key piece of the puzzle is the textual inversion.</li></ul><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p><img src="https://img.enderfga.cn/img/image-20230227165614420.png" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20230227165925713.png" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20230227165956615.png" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20230227170021757.png" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20230227170047165.png" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20230227170111552.png" alt=""></p><h2 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h2><ul><li>Requires per-image optimization<ul><li>Both the textual inversion and the 3D optimization procedure must be performed separately for each input image.</li><li>As a result, the process is relatively slow and difficult to apply to large datasets</li></ul></li><li>In some cases, reconstruction fails to produce a solid shape<ul><li>Perhaps this could be alleviated with better inductive biases or regularization terms</li></ul></li><li>In some cases, reconstruction produces two-headed objects<ul><li>This is known as the Janus Problem</li></ul></li></ul>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;RealFusion：&quot;&gt;&lt;a href=&quot;#RealFusion：&quot; class=&quot;headerlink&quot; title=&quot;RealFusion：&quot;&gt;&lt;/a&gt;RealFusion：&lt;/h1&gt;&lt;h3 id=&quot;360°-Reconstruction-of-Any-Object-from-a-Single-Image&quot;&gt;&lt;a href=&quot;#360°-Reconstruction-of-Any-Object-from-a-Single-Image&quot; class=&quot;headerlink&quot; title=&quot;360° Reconstruction of Any Object from a Single Image&quot;&gt;&lt;/a&gt;360° Reconstruction of Any Object from a Single Image&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Oxford University&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;2023.2.23&lt;/p&gt;</summary>
    
    
    
    
    <category term="计算机视觉" scheme="http://enderfga.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>Outstanding Wordle</title>
    <link href="http://enderfga.cn/2023/02/22/wordle/"/>
    <id>http://enderfga.cn/2023/02/22/wordle/</id>
    <published>2023-02-22T13:38:54.000Z</published>
    <updated>2023-02-22T13:49:05.366Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>唯一一次认真参加数学建模，虽然曾经的我确实不喜欢这类赛事。假如拿到O/F奖了就考虑写写经验或者录个视频，捞了就当我没说。</p><span id="more"></span><embed src="./wordle.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    <summary type="html">&lt;p&gt;唯一一次认真参加数学建模，虽然曾经的我确实不喜欢这类赛事。假如拿到O/F奖了就考虑写写经验或者录个视频，捞了就当我没说。&lt;/p&gt;</summary>
    
    
    
    
    <category term="数学建模" scheme="http://enderfga.cn/tags/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/"/>
    
  </entry>
  
  <entry>
    <title>亚太数模论文</title>
    <link href="http://enderfga.cn/2023/02/20/apmcm/"/>
    <id>http://enderfga.cn/2023/02/20/apmcm/</id>
    <published>2023-02-20T11:02:17.000Z</published>
    <updated>2023-02-20T11:05:59.145Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>这么烂的学术垃圾都能拿奖，这比赛确实没含金量</p><span id="more"></span><embed src="./apmcm.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    <summary type="html">&lt;p&gt;这么烂的学术垃圾都能拿奖，这比赛确实没含金量&lt;/p&gt;</summary>
    
    
    
    
    <category term="数学建模" scheme="http://enderfga.cn/tags/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/"/>
    
  </entry>
  
  <entry>
    <title>MAV3D:Text-To-4D Dynamic Scene Generation</title>
    <link href="http://enderfga.cn/2023/02/11/mav/"/>
    <id>http://enderfga.cn/2023/02/11/mav/</id>
    <published>2023-02-11T15:42:41.000Z</published>
    <updated>2023-02-11T15:51:26.196Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>论文阅读笔记</p><span id="more"></span><div style="text-align:center"><iframe width="100%" height="500" src="https://player.bilibili.com/player.html?aid=479164157&bvid=BV1aT411R77Z&cid=1003655006&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe></iframe></div><h1 id="MAV3D-Text-To-4D-Dynamic-Scene-Generation"><a href="#MAV3D-Text-To-4D-Dynamic-Scene-Generation" class="headerlink" title="MAV3D:Text-To-4D Dynamic Scene Generation"></a>MAV3D:Text-To-4D Dynamic Scene Generation</h1><ul><li><strong>Meta AI</strong></li><li><p><strong>2023.1.26</strong></p><h2 id="Demo"><a href="#Demo" class="headerlink" title="Demo"></a>Demo</h2><p><a href="https://make-a-video3d.github.io/">https://make-a-video3d.github.io/</a></p><video src="./rotating_grid.mp4"></video></li></ul><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><ol><li><strong>需要一个有效的、端到端可学习的动态三维场景表征；</strong></li><li><strong>需要一个有监督学习的数据源，因为目前并不存在大规模的（文本，4D）对的数据集可供学习；</strong></li><li><strong>需要在空间和时间维度上扩展输出的分辨率，因为4D输出需要大量的内存和计算能力；</strong></li></ol><h2 id="Proposal"><a href="#Proposal" class="headerlink" title="Proposal"></a>Proposal</h2><ol><li><strong>本文提出了MAV3D，利用了T2V模型和动态NeRFs，实现从自然语言描述生成动态三维时间表示；</strong></li><li><strong>提出了一个从静态到动态的多阶段优化方案，逐步纳入静态、时间和超分辨率模型的梯度信息。</strong></li></ol><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><h3 id="dynamic-NeRFs"><a href="#dynamic-NeRFs" class="headerlink" title="dynamic NeRFs"></a>dynamic NeRFs</h3><p><strong>适用于动态场景的NeRF变体</strong></p><h3 id="MAV"><a href="#MAV" class="headerlink" title="MAV"></a>MAV</h3><p><strong>Make A Video，通过在未标记的视频上训练，拓展了文本到图像（T2I）模型。</strong></p><h3 id="DreamFusion"><a href="#DreamFusion" class="headerlink" title="DreamFusion"></a>DreamFusion</h3><p><strong>以NeRF的形式从文本描述中学习3D表示，提出了一个基于概率密度蒸馏的loss（SDS）</strong></p><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p><img src="https://img.enderfga.cn/img/image-20230209142352394.png" alt=""></p><h3 id="4D-Scene-Representation"><a href="#4D-Scene-Representation" class="headerlink" title="4D Scene Representation"></a>4D Scene Representation</h3><p><img src="https://img.enderfga.cn/img/image-20230209141807225.png" alt=""></p><script type="math/tex; mode=display">\left(\tau, c_i\right)=f_\theta(x, y, z, t)</script><script type="math/tex; mode=display">\left[P_{x y}^{X Y R_1}+P_{z t}^{Z T R_1} ; P_{x z}^{X Z R_2}+P_{y t}^{Y T R_2} ; P_{y z}^{Y Z R_3}+P_{y z}^{X T R_3}\right]</script><h3 id="Dynamic-Scene-Optimization"><a href="#Dynamic-Scene-Optimization" class="headerlink" title="Dynamic Scene Optimization"></a>Dynamic Scene Optimization</h3><h4 id="为了监督4D场景与文本提示p匹配，引入SDS-T（temporal-Score-Distillation-Sampling-）"><a href="#为了监督4D场景与文本提示p匹配，引入SDS-T（temporal-Score-Distillation-Sampling-）" class="headerlink" title="为了监督4D场景与文本提示p匹配，引入SDS-T（temporal Score Distillation Sampling  ）"></a>为了监督4D场景与文本提示p匹配，引入SDS-T（temporal Score Distillation Sampling  ）</h4><script type="math/tex; mode=display">\nabla_\theta \mathcal{L}_{S D S-T}=E_{\sigma, \epsilon}\left[w(\sigma)\left(\hat{\epsilon}\left(V_{(\bar{\theta}, \sigma, \epsilon)} \mid y, \sigma\right)-\epsilon\right) \frac{\partial V_\theta}{\partial \theta}\right]\\</script><script type="math/tex; mode=display">\nabla_\theta \mathcal{L}_{\mathrm{SDS}}(\phi, \mathbf{x}=g(\theta)) \triangleq \mathbb{E}_{t, \epsilon}\left[w(t)\left(\hat{\epsilon}_\phi\left(\mathbf{z}_t ; y, t\right)-\epsilon\right) \frac{\partial \mathbf{x}}{\partial \theta}\right]</script><h4 id="从静态到动态的场景优化"><a href="#从静态到动态的场景优化" class="headerlink" title="从静态到动态的场景优化"></a>从静态到动态的场景优化</h4><h4 id="动态相机"><a href="#动态相机" class="headerlink" title="动态相机"></a>动态相机</h4><h4 id="FPS-采样"><a href="#FPS-采样" class="headerlink" title="FPS 采样"></a>FPS 采样</h4><h4 id="高斯退火"><a href="#高斯退火" class="headerlink" title="高斯退火"></a>高斯退火</h4><h4 id="全变分损失"><a href="#全变分损失" class="headerlink" title="全变分损失"></a>全变分损失</h4><h3 id="Super-Resolution-Fine-Tuning"><a href="#Super-Resolution-Fine-Tuning" class="headerlink" title="Super-Resolution Fine-Tuning"></a>Super-Resolution Fine-Tuning</h3><p><img src="https://img.enderfga.cn/img/image-20230209152345831.png" alt=""></p><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p><strong>Metrics</strong>：R-Precision and human preference</p><p><img src="https://img.enderfga.cn/img/image-20230209152747908.png" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20230209152759865.png" alt=""></p><h2 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h2><ul><li><strong>将动态NeRFs转换为实时应用的不连续网格序列的效率很低，如果能直接预测顶点的轨迹，就能得到改善。</strong></li><li><strong>利用超分辨率信息提高了表示的质量，但对于更高细节的纹理还需要进一步改进。</strong></li><li><strong>文本到四维动态场景生成的表示质量取决于T2V模型从不同视角生成视频的能力。</strong></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;论文阅读笔记&lt;/p&gt;</summary>
    
    
    
    
    <category term="计算机视觉" scheme="http://enderfga.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>Test to 3d随笔</title>
    <link href="http://enderfga.cn/2023/02/09/3d/"/>
    <id>http://enderfga.cn/2023/02/09/3d/</id>
    <published>2023-02-09T08:51:10.000Z</published>
    <updated>2023-02-09T08:53:02.663Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>随便记的笔记</p><span id="more"></span><h1 id="Text-to-3D"><a href="#Text-to-3D" class="headerlink" title="Text-to-3D"></a>Text-to-3D</h1><h2 id="NeRF-Representing-Scenes-as-Neural-Radiance-Fields-for-View-Synthesis"><a href="#NeRF-Representing-Scenes-as-Neural-Radiance-Fields-for-View-Synthesis" class="headerlink" title="NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis"></a>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</h2><p><img src="https://img.enderfga.cn/img/ed4df06e919cae0e638015fa78d935eb_1_Figure_1.png" alt=""></p><p><strong>输入为连续的5维坐标（xyz坐标，以及视野角度theta和phi）；输出是空间位置的体密度以及该位置的发射射线（这里射线是根据视角变化的）。</strong></p><ol><li><strong>用 network 存体素信息: </strong>(x, y, z, \theta, \phi) \rightarrow(\mathbf{c}, \sigma)</li><li><strong>然后用体素渲染方程获得生成视角图片：光线采样+积分</strong><script type="math/tex; mode=display">C(\mathbf{r})=\int_{t_n}^{t_f} T(t) \sigma(\mathbf{r}(t)) \mathbf{c}(\mathbf{r}(t), \mathbf{d}) d t, \text { where } T(t)=\exp \left(-\int_{t_n}^t \sigma(\mathbf{r}(s)) d s\right)</script></li><li><strong>最后与原视角图片计算损失更新网络</strong></li></ol><h2 id="DreamFusion-Text-to-3D-using-2D-Diffusion"><a href="#DreamFusion-Text-to-3D-using-2D-Diffusion" class="headerlink" title="DreamFusion: Text-to-3D using 2D Diffusion"></a>DreamFusion: Text-to-3D using 2D Diffusion</h2><p><img src="https://img.enderfga.cn/img/image-20230202213651008.png" alt=""></p><p><strong>三维合成并不存在大规模的标注数据，也没有一个高效的模型架构对3D数据进行降噪</strong></p><p><strong>使用NERF的格式，使用预训练的text to 2d，加上他们提出的一个基于概率密度蒸馏的loss，证明了预训练图像扩散模型作为先验模型的有效性</strong></p><h2 id="Magic3D-High-Resolution-Text-to-3D-Content-Creation"><a href="#Magic3D-High-Resolution-Text-to-3D-Content-Creation" class="headerlink" title="Magic3D: High-Resolution Text-to-3D Content Creation"></a>Magic3D: High-Resolution Text-to-3D Content Creation</h2><p><img src="https://img.enderfga.cn/img/f3fcff88aa23d692c243bda5b3dd5467_3_Figure_2_1114637308.png" alt=""></p><p><strong>用一个两阶段的优化框架来提高速度和分辨率：利用低分辨率的扩散先验获得一个粗略的模型，并以稀疏的三维哈希网格结构加速。使用粗略表示作为初始化，进一步优化纹理三维网格模型，用高效的可微分渲染器与高分辨率的stable diffusion模型交互。</strong></p><h2 id="Point-E-A-System-for-Generating-3D-Point-Clouds-from-Complex-Prompts"><a href="#Point-E-A-System-for-Generating-3D-Point-Clouds-from-Complex-Prompts" class="headerlink" title="Point-E: A System for Generating 3D Point Clouds from Complex Prompts"></a>Point-E: A System for Generating 3D Point Clouds from Complex Prompts</h2><p><img src="https://img.enderfga.cn/img/v2-b0ca9d9f44550ec6ab34dfe21797ea7e_720w.webp" alt=""></p><p><strong>不输出传统意义上的 3D 图像，它会生成点云，或空间中代表 3D 形状的离散数据点集</strong></p><p><strong>点云更容易合成，但它们无法捕获对象的细粒度形状或纹理，训练了一个额外的人工智能系统来将 Point-E 的点云转换为网格</strong></p><p><strong>算力和时间需求小 但质量差</strong></p><h2 id="Dream3D-Zero-Shot-Text-to-3D-Synthesis-Using-3D-Shape-Prior-and-Text-to-Image-Diffusion-Models"><a href="#Dream3D-Zero-Shot-Text-to-3D-Synthesis-Using-3D-Shape-Prior-and-Text-to-Image-Diffusion-Models" class="headerlink" title="Dream3D: Zero-Shot Text-to-3D Synthesis Using 3D Shape Prior and Text-to-Image Diffusion Models"></a>Dream3D: Zero-Shot Text-to-3D Synthesis Using 3D Shape Prior and Text-to-Image Diffusion Models</h2><p><img src="https://img.enderfga.cn/img/image-20230202234404950.png" alt=""></p><p><strong>引入一个显式3D先验形状，来优化CLIP引导的3D优化任务。具体的讲，首先在文本到形状转换时，使用输入文本生成了一个质量的3D形状来作为先验知识。然后使用它来初始化神经辐射场，并使用完整prompt进行优化</strong></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;随便记的笔记&lt;/p&gt;</summary>
    
    
    
    
    <category term="笔记" scheme="http://enderfga.cn/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>三维手势姿态估计算法研究</title>
    <link href="http://enderfga.cn/2023/01/13/nyu/"/>
    <id>http://enderfga.cn/2023/01/13/nyu/</id>
    <published>2023-01-13T14:08:50.000Z</published>
    <updated>2023-01-13T14:19:04.839Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>人工智能原理实验期末项目</p><span id="more"></span><embed src="./nyu.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    <summary type="html">&lt;p&gt;人工智能原理实验期末项目&lt;/p&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://enderfga.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Mind-Diffusion</title>
    <link href="http://enderfga.cn/2023/01/13/diff/"/>
    <id>http://enderfga.cn/2023/01/13/diff/</id>
    <published>2023-01-13T14:04:03.000Z</published>
    <updated>2023-01-13T14:19:04.833Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>计算机视觉之diffusion model with mindspore</p><span id="more"></span><embed src="./diff.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    <summary type="html">&lt;p&gt;计算机视觉之diffusion model with mindspore&lt;/p&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://enderfga.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>基于 JMAG 软件的电机仿真分析</title>
    <link href="http://enderfga.cn/2023/01/13/mach/"/>
    <id>http://enderfga.cn/2023/01/13/mach/</id>
    <published>2023-01-13T13:59:11.000Z</published>
    <updated>2023-01-13T14:19:04.835Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>压根没上过课，不知道这写的是啥</p><span id="more"></span><embed src="./mach.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    <summary type="html">&lt;p&gt;压根没上过课，不知道这写的是啥&lt;/p&gt;</summary>
    
    
    
    
    <category term="电机与拖动技术" scheme="http://enderfga.cn/tags/%E7%94%B5%E6%9C%BA%E4%B8%8E%E6%8B%96%E5%8A%A8%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title>智能车协同实验</title>
    <link href="http://enderfga.cn/2023/01/13/car/"/>
    <id>http://enderfga.cn/2023/01/13/car/</id>
    <published>2023-01-13T13:52:04.000Z</published>
    <updated>2023-01-13T14:19:04.830Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>多智能体集群控制技术智能车实验报告</p><span id="more"></span><embed src="./1.pdf" width="100%" height="750" type="application/pdf"><embed src="./2.pdf" width="100%" height="750" type="application/pdf"><embed src="./3.pdf" width="100%" height="750" type="application/pdf"><embed src="./4.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    <summary type="html">&lt;p&gt;多智能体集群控制技术智能车实验报告&lt;/p&gt;</summary>
    
    
    
    
    <category term="多智能体集群" scheme="http://enderfga.cn/tags/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E9%9B%86%E7%BE%A4/"/>
    
  </entry>
  
  <entry>
    <title>四旋翼集群编队</title>
    <link href="http://enderfga.cn/2023/01/13/multi/"/>
    <id>http://enderfga.cn/2023/01/13/multi/</id>
    <published>2023-01-13T13:37:11.000Z</published>
    <updated>2023-01-14T12:23:24.113Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>多智能体集群控制技术期末项目报告</p><span id="more"></span><h1 id="单积分模型SYSU编队设计"><a href="#单积分模型SYSU编队设计" class="headerlink" title="单积分模型SYSU编队设计"></a>单积分模型SYSU编队设计</h1><p><img src="https://img.enderfga.cn/img/SI.gif" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20230113213938616.png" alt=""></p><h1 id="控制四旋翼飞行器实现编队方式设计"><a href="#控制四旋翼飞行器实现编队方式设计" class="headerlink" title="控制四旋翼飞行器实现编队方式设计"></a>控制四旋翼飞行器实现编队方式设计</h1><p><img src="https://img.enderfga.cn/img/test.gif" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20230113214008450.png" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20230113214017680.png" alt=""></p><ul><li><p>Position control</p><script type="math/tex; mode=display">\begin{array}{ll}\text { PID: } & \ddot{\boldsymbol{p}}_{i, c}=\ddot{\boldsymbol{p}}_i{ }^{d e s}+K_{d, i}\left(\dot{\boldsymbol{p}}_i{ }^{\text {des }}-\dot{\boldsymbol{p}}_i\right)+K_{p, i}\left(\boldsymbol{p}_i{ }^{\text {des }}-\boldsymbol{p}_i\right) \\\text { Model: } & u_1=m\left(g+\ddot{\boldsymbol{p}}_{3, c}\right) \quad \text { (Newton Equation) } \\& \phi_c=\frac{1}{g}\left(\ddot{\boldsymbol{p}}_{1, c} \sin \Psi-\ddot{\boldsymbol{p}}_{2, c} \cos \psi \right) \quad \theta_c=\frac{1}{g}\left(\ddot{\boldsymbol{p}}_{1, c} \cos \Psi+\ddot{\boldsymbol{p}}_{2, c} \sin \Psi\right)\end{array}</script></li><li>Attitude control</li></ul><script type="math/tex; mode=display">PID: \quad\left[\begin{array}{c}\ddot{\phi}_c \\ \ddot{\theta}_c \\ \ddot{\psi}_c\end{array}\right]=\left[\begin{array}{c}K_{p, \phi}\left(\phi_c-\phi\right)+K_{d, \phi}\left(\dot{\phi}_c-\dot{\phi}\right) \\ K_{p, \theta}\left(\theta_c-\theta\right)+K_{d, \phi}\left(\dot{\theta}_c-\dot{\theta}\right) \\ K_{p, \psi}\left(\psi_c-\psi\right)+K_{d, \psi}\left(\dot{\psi}_c-\dot{\psi}\right)\end{array}\right]</script><script type="math/tex; mode=display">Model: \quad \boldsymbol{u}_2=\boldsymbol{I} \cdot\left[\begin{array}{c}\ddot{\phi}_c \\ \ddot{\theta}_c \\ \ddot{\psi}_c\end{array}\right]+\left[\begin{array}{c}\omega_x \\ \omega_y \\ \omega_z\end{array}\right] \times \boldsymbol{I} \cdot\left[\begin{array}{c}\omega_x \\ \omega_y \\ \omega_z\end{array}\right] (Euler Equation)</script><p>我使用的控制器遵循上述公式采取了PID控制，结合单积分模型的控制共同决定了结果分数。当然字母间距、运行时间等也能对误差产生一定影响。</p><p>单积分模型中控制增益kv与刚度矩阵R、距离误差z和期望速度dv相乘，起到了限制距离误差和期望速度之间的平衡作用。具体来说，当kv增加时，系统会更快地收敛到目标状态，但是可能会出现振荡。当kv减小时，系统会更缓慢地收敛到目标状态，甚至会导致无人机几乎不动的情况。</p><p>控制器的原理是输入期望控制，输出飞行器整体推力与力矩。公式整体的推导较为复杂，涉及机器人运动学与动力学，且会解欧拉牛顿方程，但对公式的直观理解可以更好理解公式；这个公式基本是外环位置，内环姿态，计算扭矩与推力，可见推力与飞行器质量与z轴加速度有关，通过计算期望角度计算扭矩。</p><p>我使用了Ziegler-Nichols整定方法来调节PID参数，首先将积分和微分增益设置为0，然后比例增益从零开始逐渐增加，直到到达极限增益<em>KU</em>，此时控制器输出值以恒定值振荡。<em>KU</em>和振荡周期<em>TU</em>根据不同的类型，按下表中的方式来设置比例、积分和微分增益。</p><script type="math/tex; mode=display">\begin{array}{|c|c|c|c|}\hline \text { Controller } & K_p & K_d & K_i \\\hline \text { P } & 0.5 K_u & - & - \\\hline \text { PD } & 0.8 K_u & K_p T_u / 8 & - \\\hline \text { PID } & 0.6 K_u & K_p T_u / 8 & 2 K_p / T_u \\\hline\end{array}</script><p>在位置控制中，使用了三轴PID控制器来控制x, y, z轴上的运动。使用了误差积分来消除误差；在姿态控制中，通过计算出当前的欧拉角（phi, theta, psi），并使用欧拉角的导数来控制飞行器的姿态。</p><p>Kp是比例系数，它控制着系统的稳定性和响应速度。当Kp增大时，系统的响应速度会变快，但同时也会增加系统的震荡。</p><p>Ki是积分系数，它控制着系统的累计误差。当Ki增大时，系统会更快地消除误差，但同时也会增加系统的积分饱和。</p><p>Kd是微分系数，它控制着系统的响应速度。当Kd增大时，系统的响应速度会变快，但同时也会增加系统的偏差。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;多智能体集群控制技术期末项目报告&lt;/p&gt;</summary>
    
    
    
    
    <category term="多智能体集群" scheme="http://enderfga.cn/tags/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E9%9B%86%E7%BE%A4/"/>
    
  </entry>
  
  <entry>
    <title>Tiny SSD目标检测</title>
    <link href="http://enderfga.cn/2022/12/11/ssd/"/>
    <id>http://enderfga.cn/2022/12/11/ssd/</id>
    <published>2022-12-11T13:23:46.000Z</published>
    <updated>2022-12-11T13:27:07.369Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Github: <a href="https://github.com/Enderfga/TinySSD_sysu">https://github.com/Enderfga/TinySSD_sysu</a></p><span id="more"></span><h1 id="Tiny-SSD-A-Tiny-Single-shot-Detection-Deep-Convolutional-Neural-Network-for-Real-time-Embedded-Object-Detection"><a href="#Tiny-SSD-A-Tiny-Single-shot-Detection-Deep-Convolutional-Neural-Network-for-Real-time-Embedded-Object-Detection" class="headerlink" title="Tiny SSD: A Tiny Single-shot Detection Deep Convolutional Neural Network for Real-time Embedded Object Detection"></a>Tiny SSD: A Tiny Single-shot Detection Deep Convolutional Neural Network for Real-time Embedded Object Detection</h1><p>This repo contains the code, data and trained models for the paper <a href="https://arxiv.org/pdf/1802.06488.pdf">Tiny SSD: A Tiny Single-shot Detection Deep Convolutional Neural Network for Real-time Embedded Object Detection</a>.</p><h2 id="Quick-Links"><a href="#Quick-Links" class="headerlink" title="Quick Links"></a>Quick Links</h2><ul><li><a href="#Overview"> Overview</a></li><li><a href="#Requirements"> Requirements</a></li><li><a href="#How-to-Install">How to Install</a></li><li><a href="#Description-of-Codes">Description of Codes</a></li><li><a href="#Preprocessing"> Preprocessing</a><ul><li><a href="#Preprocessed-Data">Preprocessed Data</a></li></ul></li><li><a href="#How-to-Run">How to Run</a><ul><li><a href="#Train"> Train</a><ul><li><a href="#Finetuning-from-an-existing-checkpoint">Finetuning from an existing checkpoint</a></li></ul></li><li><a href="#Evaluate"> Evaluate</a></li></ul></li><li><a href="#Results-Outputs-Checkpoints">Results, Outputs, Checkpoints</a></li></ul><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>Tiny SSD is a single-shot detection deep convolutional neural network for real-time embedded object detection.<br>It brings together the efficieny of Fire microarchitecture introduced in <strong>SqueezeNet</strong> and object detection performance of <strong>SSD (Single Shot Object Detector)</strong>.</p><p><img src="https://img.enderfga.cn/img/ssd.svg" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20221018133431973.png" alt=""></p><h2 id="Requirements"><a href="#Requirements" class="headerlink" title="Requirements"></a>Requirements</h2><ul><li>numpy</li><li>pandas</li><li>matplotlib</li><li>opencv-python</li><li>torch</li><li>torchvision</li></ul><h2 id="How-to-Install"><a href="#How-to-Install" class="headerlink" title="How to Install"></a>How to Install</h2> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">conda create -n env python=3.8 -y<br>conda activate env<br></code></pre></td></tr></table></figure> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">pip install -r requirements.txt<br></code></pre></td></tr></table></figure><h2 id="Description-of-Files"><a href="#Description-of-Files" class="headerlink" title="Description of Files"></a>Description of Files</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs stylus">│──<span class="hljs-selector-tag">main</span><span class="hljs-selector-class">.py</span>                 -&gt; Run models using different models<br>│──README.md<br>│──requirements.txt<br>│──test<span class="hljs-selector-class">.py</span>                 -&gt; Testing Model<br>│──train<span class="hljs-selector-class">.py</span>                -&gt; Training Model<br>│<br>├─data<br>│  │  dataloader<span class="hljs-selector-class">.py</span>         -&gt; dataloader and <span class="hljs-attribute">transform</span><br>│  │  __init__.py<br>│  │<br>│  └─detection<br>│      │  create_train<span class="hljs-selector-class">.py</span>   -&gt; data preprocessing<br>│      │<br>│      ├─<span class="hljs-attribute">background</span><br>│      ├─sysu_train<br>│      │  │  <span class="hljs-selector-tag">label</span>.csv<br>│      │  │<br>│      │  └─images<br>│      ├─target<br>│      │      <span class="hljs-number">0</span>.jpg<br>│      │      <span class="hljs-number">0</span>.png<br>│      │      <span class="hljs-number">1</span>.png<br>│      │<br>│      └─test<br>│              <span class="hljs-number">1</span>.jpg<br>│              <span class="hljs-number">2</span>.jpg<br>│<br>├─model<br>│  │  TinySSD<span class="hljs-selector-class">.py</span>             -&gt; Definition of the model<br>│  │  __init__.py<br>│  │<br>│  └─checkpoints             -&gt; Trained model weights<br>│     net_100.pkl<br>└─utils                      -&gt; utility functions<br>        anchor.py<br>        iou.py<br>        utils.py<br>        __init__.py<br></code></pre></td></tr></table></figure><h2 id="Preprocessing"><a href="#Preprocessing" class="headerlink" title="Preprocessing"></a>Preprocessing</h2><p>We use /data/detection/background to generate the target detection dataset for our experiments.</p><p>Since the generated data is stored in the repository, there is no need to run this step.</p><h3 id="Preprocessed-Data"><a href="#Preprocessed-Data" class="headerlink" title="Preprocessed Data"></a>Preprocessed Data</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">cd data/detection/<br>python create_train.py<br></code></pre></td></tr></table></figure><h2 id="How-to-Run"><a href="#How-to-Run" class="headerlink" title="How to Run"></a>How to Run</h2><h3 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">python main.py --mode=train --batch_size=256 --epochs=100<br></code></pre></td></tr></table></figure><p>The checkpoints will be saved in a subfolder of <code>./model/checkpoints/</code>.</p><h4 id="Finetuning-from-an-existing-checkpoint"><a href="#Finetuning-from-an-existing-checkpoint" class="headerlink" title="Finetuning from an existing checkpoint"></a>Finetuning from an existing checkpoint</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">python main.py --mode=train --batch_size=256 --epochs=100 --path=[model path]<br></code></pre></td></tr></table></figure><p>model path should be a subdirectory in the <code>./model/checkpoints/</code> directory, e.g. <code>--path=./model/checkpoints/net_100.pkl</code></p><h3 id="Evaluate"><a href="#Evaluate" class="headerlink" title="Evaluate"></a>Evaluate</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">python main.py --mode=test --threshold=<span class="hljs-number">0.3</span> --path=./model/checkpoints/net_100.pkl<br></code></pre></td></tr></table></figure><h2 id="Results-Outputs-Checkpoints"><a href="#Results-Outputs-Checkpoints" class="headerlink" title="Results, Outputs, Checkpoints"></a>Results, Outputs, Checkpoints</h2><p>the ./model/checkpoints/net_100.pkl：class err 1.54e-03, bbox mae 1.90e-03</p><p>I used the following methods to improve performance：</p><ol><li><p>HD anti-white detection object to adapt to the test image</p><p><img src="https://img.enderfga.cn/img/image-20221018160400682.png" alt=""></p></li><li><p>Flip and rotate images, etc. to improve generalization performance</p><p><img src="https://img.enderfga.cn/img/image-20221018155947834.png" alt=""></p></li><li><p>soft_nms</p><p><img src="https://img.enderfga.cn/img/42166d224f4a20a4d58841b70d795a2a730ed0e4.jpeg@f_auto" alt=""></p></li><li><p>smooth_L1</p><p><img src="https://img.enderfga.cn/img/image-20221018155842392.png" alt=""></p></li><li><p>Focal Loss</p><p><img src="https://img.enderfga.cn/img/20221025160416.png" alt=""></p></li></ol><p>If we have more classes, we can further improve the model in the following aspects:</p><ol><li>When an object is much smaller compared with the image, the model could resize the input image bigger.</li><li>There are typically a vast number of negative anchor boxes. To make the class distribution more balanced, we could downsample negative anchor boxes.</li><li>In the loss function, assign different weight hyperparameters to the class loss and the offset loss.</li><li>Use other methods to evaluate the object detection model, such as those in the single shot multibox detection paper (Liu et al., 2016).</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;Github: &lt;a href=&quot;https://github.com/Enderfga/TinySSD_sysu&quot;&gt;https://github.com/Enderfga/TinySSD_sysu&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://enderfga.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>人脸检测mtcnn复现</title>
    <link href="http://enderfga.cn/2022/12/11/mtcnn/"/>
    <id>http://enderfga.cn/2022/12/11/mtcnn/</id>
    <published>2022-12-11T12:56:31.000Z</published>
    <updated>2022-12-11T13:28:02.081Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Github: <a href="https://github.com/Enderfga/mtCNN_sysu">https://github.com/Enderfga/mtCNN_sysu</a></p><span id="more"></span><h1 id="Joint-Face-Detection-and-Alignment-using-Multi-task-Cascaded-Convolutional-Networks"><a href="#Joint-Face-Detection-and-Alignment-using-Multi-task-Cascaded-Convolutional-Networks" class="headerlink" title="Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks"></a>Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks</h1><p><strong>This repo contains the code, data and trained models for the paper </strong><a href="https://arxiv.org/ftp/arxiv/papers/1604/1604.02878.pdf">Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks</a>.<strong></strong>Try out the Gradio Web Demo: <a href="https://huggingface.co/spaces/Enderfga/mtCNN_sysu"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue" alt="Hugging Face Spaces"></a><br><img src="https://img.enderfga.cn/img/1faec03527783e6e8ee03d519e167aa.png" alt=""></p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p><strong>MTCNN is a popular algorithm for face detection that uses multiple neural networks to detect faces in images. It is capable of detecting faces under various lighting and pose conditions and can detect multiple faces in an image.</strong></p><p><strong>We have implemented MTCNN using the pytorch framework. Pytorch is a popular deep learning framework that provides tools for building and training neural networks. </strong></p><p><img src="https://img.enderfga.cn/img/image-20221208152130975.png" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20221208152231511.png" alt=""></p><h2 id="Description-of-file"><a href="#Description-of-file" class="headerlink" title="Description of file"></a>Description of file</h2><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs vim">├── README.md                      # explanatory document<br>├── get_data.<span class="hljs-keyword">py</span>                    # Generate corresponding training data depending <span class="hljs-keyword">on</span> the <span class="hljs-built_in">input</span> “--net”<br>├── img                            # mid.png <span class="hljs-keyword">is</span> used <span class="hljs-keyword">for</span> testing visualization effects,other images are the corresponding results.<br>│   ├── mid.png<br>│   ├── onet.png<br>│   ├── pnet.png<br>│   ├── rnet.png<br>│   ├── result.png<br>│   └── result.jpg<br>├── model_store                    # Our <span class="hljs-keyword">pre</span>-trained model<br>│   ├── onet_epoch_20.<span class="hljs-keyword">pt</span><br>│   ├── pnet_epoch_20.<span class="hljs-keyword">pt</span><br>│   └── rnet_epoch_20.<span class="hljs-keyword">pt</span><br>├── requirements.txt               # Environmental <span class="hljs-keyword">version</span> requirements<br>├── test.<span class="hljs-keyword">py</span>                        # Specify different <span class="hljs-string">&quot;--net&quot;</span> <span class="hljs-keyword">to</span> <span class="hljs-built_in">get</span> the corresponding visualization results<br>├── test.<span class="hljs-keyword">sh</span>                        # Used <span class="hljs-keyword">to</span> test mid.png, which will test the output visualization of three networks<br>├── train.out                      # Our <span class="hljs-built_in">complete</span> training <span class="hljs-built_in">log</span> <span class="hljs-keyword">for</span> this experiment<br>├── train.<span class="hljs-keyword">py</span>                       # Specify different <span class="hljs-string">&quot;--net&quot;</span> <span class="hljs-keyword">for</span> the training of the corresponding network<br>├── train.<span class="hljs-keyword">sh</span>                       # Generate data from start <span class="hljs-keyword">to</span> <span class="hljs-keyword">finish</span> <span class="hljs-built_in">and</span> train<br>└── utils                          # Some common tool functions <span class="hljs-built_in">and</span> modules<br>    ├── config.<span class="hljs-keyword">py</span><br>    ├── dataloader.<span class="hljs-keyword">py</span><br>    ├── detect.<span class="hljs-keyword">py</span><br>    ├── models.<span class="hljs-keyword">py</span><br>    ├── tool.<span class="hljs-keyword">py</span><br>    └── vision.<span class="hljs-keyword">py</span><br></code></pre></td></tr></table></figure><h2 id="Requirements"><a href="#Requirements" class="headerlink" title="Requirements"></a>Requirements</h2><ul><li><strong>numpy==1.21.4</strong></li><li><strong>matplotlib==3.5.0</strong></li><li><strong>opencv-python==4.4.0.42</strong></li><li><strong>torch==1.13.0+cu116</strong></li></ul><h2 id="How-to-Install"><a href="#How-to-Install" class="headerlink" title="How to Install"></a>How to Install</h2> <figure class="highlight mel"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs mel">conda create -n <span class="hljs-keyword">env</span> <span class="hljs-keyword">python</span>=<span class="hljs-number">3.8</span> -y<br>conda activate <span class="hljs-keyword">env</span><br></code></pre></td></tr></table></figure> <figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">pip <span class="hljs-keyword">install</span> -r requirements.txt<br></code></pre></td></tr></table></figure><h2 id="Preprocessing"><a href="#Preprocessing" class="headerlink" title="Preprocessing"></a>Preprocessing</h2><ul><li><strong>download </strong><a href="http://shuoyang1213.me/WIDERFACE/">WIDER_FACE</a> face detection data then store it into ./data_set/face_detection</li><li><strong>download </strong><a href="http://mmlab.ie.cuhk.edu.hk/archive/CNN_FacePoint.htm">CNN_FacePoint</a> face detection and landmark data then store it into ./data_set/face_landmark</li></ul><h3 id="Preprocessed-Data"><a href="#Preprocessed-Data" class="headerlink" title="Preprocessed Data"></a>Preprocessed Data</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># Before training Pnet</span><br>python get_data.py <span class="hljs-attribute">--net</span>=pnet<br><span class="hljs-comment"># Before training Rnet, please use your trained model path</span><br>python get_data.py <span class="hljs-attribute">--net</span>=rnet <span class="hljs-attribute">--pnet_path</span>=./model_store/pnet_epoch_20.pt<br><span class="hljs-comment"># Before training Onet, please use your trained model path</span><br>python get_data.py <span class="hljs-attribute">--net</span>=onet <span class="hljs-attribute">--pnet_path</span>=./model_store/pnet_epoch_20.pt <span class="hljs-attribute">--rnet_path</span>=./model_store/rnet_epoch_20.pt<br></code></pre></td></tr></table></figure><h2 id="How-to-Run"><a href="#How-to-Run" class="headerlink" title="How to Run"></a>How to Run</h2><h3 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h3><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs vim"><span class="hljs-keyword">python</span> train.<span class="hljs-keyword">py</span> --net=pnet/rnet/onet #Specify the corresponding network <span class="hljs-keyword">to</span> start training<br>bash train.<span class="hljs-keyword">sh</span>                        #Alternatively, use the <span class="hljs-keyword">sh</span> <span class="hljs-keyword">file</span> <span class="hljs-keyword">to</span> train in order<br></code></pre></td></tr></table></figure><p><strong>The checkpoints will be saved in a subfolder of </strong><code>./model_store/*</code>.</p><h4 id="Finetuning-from-an-existing-checkpoint"><a href="#Finetuning-from-an-existing-checkpoint" class="headerlink" title="Finetuning from an existing checkpoint"></a>Finetuning from an existing checkpoint</h4><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">python train.py --net=pnet<span class="hljs-regexp">/rnet/</span>onet --load=[model path]<br></code></pre></td></tr></table></figure><p><strong>model path should be a subdirectory in the </strong><code>./model_store/</code> directory, e.g. <code>--load=./model_store/pnet_epoch_20.pt</code></p><h3 id="Evaluate"><a href="#Evaluate" class="headerlink" title="Evaluate"></a>Evaluate</h3><h4 id="Use-the-sh-file-to-test-in-order"><a href="#Use-the-sh-file-to-test-in-order" class="headerlink" title="Use the sh file to test in order"></a>Use the sh file to test in order</h4><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stata">bash <span class="hljs-keyword">test</span>.<span class="hljs-keyword">sh</span><br></code></pre></td></tr></table></figure><h4 id="To-detect-a-single-image"><a href="#To-detect-a-single-image" class="headerlink" title="To detect a single image"></a>To detect a single image</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">python test.py <span class="hljs-attribute">--net</span>=pnet/rnet/onet  <span class="hljs-attribute">--path</span>=test.jpg<br></code></pre></td></tr></table></figure><h4 id="To-detect-a-video-stream-from-a-camera"><a href="#To-detect-a-video-stream-from-a-camera" class="headerlink" title="To detect a video stream from a camera"></a>To detect a video stream from a camera</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">python test.py <span class="hljs-attribute">--input_mode</span>=0<br></code></pre></td></tr></table></figure><h4 id="The-result-of-“—net-pnet”"><a href="#The-result-of-“—net-pnet”" class="headerlink" title="The result of  “—net=pnet”"></a>The result of  “—net=pnet”</h4><p><img src="https://img.enderfga.cn/img/20221208160900.png" alt=""></p><h4 id="The-result-of-“—net-rnet”"><a href="#The-result-of-“—net-rnet”" class="headerlink" title="The result of  “—net=rnet”"></a>The result of  “—net=rnet”</h4><p><img src="https://img.enderfga.cn/img/image-20221208155022083.png" alt=""></p><h4 id="The-result-of-“—net-onet”"><a href="#The-result-of-“—net-onet”" class="headerlink" title="The result of  “—net=onet”"></a>The result of  “—net=onet”</h4><p><img src="https://img.enderfga.cn/img/image-20221208155044451.png" alt=""></p><embed src="./mtcnn.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    <summary type="html">&lt;p&gt;Github: &lt;a href=&quot;https://github.com/Enderfga/mtCNN_sysu&quot;&gt;https://github.com/Enderfga/mtCNN_sysu&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://enderfga.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>无人机飞越冰湖</title>
    <link href="http://enderfga.cn/2022/11/22/RL/"/>
    <id>http://enderfga.cn/2022/11/22/RL/</id>
    <published>2022-11-22T07:42:38.000Z</published>
    <updated>2022-12-11T13:05:23.626Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><embed src="./RL.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="强化学习" scheme="http://enderfga.cn/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Plant Pathology 2021 with MindSpore</title>
    <link href="http://enderfga.cn/2022/11/07/plant/"/>
    <id>http://enderfga.cn/2022/11/07/plant/</id>
    <published>2022-11-07T12:45:52.000Z</published>
    <updated>2022-11-07T12:49:00.716Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><embed src="./plant.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="深度学习" scheme="http://enderfga.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
