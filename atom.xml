<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Enderfga&#39;Blog</title>
  
  
  <link href="http://enderfga.cn/atom.xml" rel="self"/>
  
  <link href="http://enderfga.cn/"/>
  <updated>2023-10-10T17:42:49.584Z</updated>
  <id>http://enderfga.cn/</id>
  
  <author>
    <name>Enderfga</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Embrace Mistakes, Avoid Comparisons</title>
    <link href="http://enderfga.cn/2023/10/10/last/"/>
    <id>http://enderfga.cn/2023/10/10/last/</id>
    <published>2023-10-09T17:15:25.000Z</published>
    <updated>2023-10-10T17:42:49.584Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>大学本科快结束了。</p><span id="more"></span><p>You are not here to be perfect, nor you are here to compete or compare. You are here to learn. So don’t be afraid of mistakes and failures, and don’t instinctively reflect on some random passer-bys in your life.</p><p>我一点也没做好二十岁的准备，挺纳闷的，就像谁从背后推给我一样。”我以为我二十多岁，会去看山河大海，落日余晖。可事实上是，我还在找寻我自己。</p><p><img src="https://img.enderfga.cn/img/image-20231010011731583.png" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20231011014219469.png" alt=""></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;大学本科快结束了。&lt;/p&gt;</summary>
    
    
    
    
    <category term="闲谈" scheme="http://enderfga.cn/tags/%E9%97%B2%E8%B0%88/"/>
    
  </entry>
  
  <entry>
    <title>GPT4泄露的技术细节</title>
    <link href="http://enderfga.cn/2023/07/11/gpt4/"/>
    <id>http://enderfga.cn/2023/07/11/gpt4/</id>
    <published>2023-07-11T01:12:24.000Z</published>
    <updated>2023-07-11T02:08:49.747Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>From Twitter of Yam Peleg.</p><p><img src="https://img.enderfga.cn/img/20230711100816.png" alt=""></p><span id="more"></span><h1 id="GPT-4’s-details-are-leaked"><a href="#GPT-4’s-details-are-leaked" class="headerlink" title="GPT-4’s details are leaked."></a>GPT-4’s details are leaked.</h1><p>It is over.<br>Everything is here:</p><h2 id="Parameters-count"><a href="#Parameters-count" class="headerlink" title="Parameters count"></a>Parameters count</h2><p>GPT-4 is more than 10x the size of GPT-3. We believe it has a total of ~1.8 trillion parameters across 120 layers.</p><p>GPT-4的规模是GPT-3的10倍以上。我们认为它拥有约1,800亿个参数，分布在120层。</p><h2 id="Mixture-Of-Experts-Confirmed"><a href="#Mixture-Of-Experts-Confirmed" class="headerlink" title="Mixture Of Experts - Confirmed"></a>Mixture Of Experts - Confirmed</h2><p>OpenAl was able to keep costs reasonable by utilizing a mixture of experts(MoE) model.</p><p>They utilizes 16 experts within their model, each is about ~111B parameters for MLP 2 of these experts are routed to per forward pass.</p><p>OpenAI通过使用专家混合（MoE）模型，能够将成本控制在合理的范围内。</p><p>他们的模型利用了16个专家，每个专家大约拥有1110亿个参数，其中每次前向传递（forward pass）使用其中的2个专家。</p><h2 id="MoE-Routing"><a href="#MoE-Routing" class="headerlink" title="MoE Routing"></a>MoE Routing</h2><p>While the literature talks a lot about advanced routing algorithms for choosing which experts to route each token to, OpenAl’s is allegedly quite simple, for the current GPT-4 model.</p><p>There roughly ~55B shared parameters for attention.</p><p>尽管文献中谈到了很多关于选择将每个标记（token）路由到哪些专家的先进路由算法，但据称OpenAI在当前的GPT-4模型中采用的路由方法相当简单。</p><p>在注意力机制中，大约有550亿个共享参数。</p><h2 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h2><p>Each forward pass inference (generation of 1 token) only utilizes ~280B parameters and ~560 TFLOPs. This<br>contrasts with the ~1.8 trillion parameters and ~3,700 TFLOP that would be required per forward pass of a purely dense model.</p><p>每次前向传递的推理（生成一个标记）只使用了大约280亿个参数和560 TFLOPS的计算量。这与纯密集模型每次前向传递所需的约1,800亿个参数和3,700 TFLOPS形成了对比。</p><h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><p>GPT-4 is trained on ~13T tokens.<br>These are not unique tokens, they count the epochs as more tokens as well.</p><p>Epoch number: 2 epochs for text-based data and 4 for code-based data.</p><p>There is millions of rows of instruction fine-tuning data from ScaleAl &amp; internally.</p><p>GPT-4的训练使用了大约130万亿个标记（tokens）。这些标记不是唯一的标记，还计算了多个时期（epochs）的标记数量。</p><p>对于基于文本的数据，使用了2个时期，对于基于代码的数据，使用了4个时期。</p><p>此外，还有来自ScaleAl和内部的数百万行指令微调数据。</p><h2 id="GPT-4-32K"><a href="#GPT-4-32K" class="headerlink" title="GPT-4 32K"></a>GPT-4 32K</h2><p>There was an 8k context length (seq len) for the pre-training phase. The 32k seq len version of GPT-4 is based on fine-tuning of the 8k after the pre-training.</p><p>在预训练阶段，GPT-4使用了8,000个上下文长度（序列长度）。而32,000个序列长度的GPT-4版本是在预训练后对8,000个序列长度进行微调得到的。</p><h2 id="Batch-Size"><a href="#Batch-Size" class="headerlink" title="Batch Size"></a>Batch Size</h2><p>The batch size was gradually ramped up over a number of days on the cluster, but by the end, OpenAl was using a batch size of 60 million! This, of course, is “only” a batch size of 75 million tokens per expert due to not every expert seeing all tokens.</p><p>在集群上的几天时间里，批量大小逐渐增加，但最终OpenAI使用了6000万的批量大小！当然，由于并非每个专家都能看到所有的标记，因此每个专家实际上只处理了7500万个标记的批量大小。</p><h3 id="For-the-real-batch-size"><a href="#For-the-real-batch-size" class="headerlink" title="For the real batch size"></a>For the real batch size</h3><p>Divide this number by the seq len to get the real batch size. Just stop with this misleading numbers already.</p><p>将这个数字除以序列长度以获取实际的批量大小。请不要再提供这种误导性的数字了。</p><h2 id="Parallelism-Strategies"><a href="#Parallelism-Strategies" class="headerlink" title="Parallelism Strategies"></a>Parallelism Strategies</h2><p>To parallelize across all their A100s GPUs</p><p>They utilized 8-way tensor parallelism as that is the limit for NVLink.<br>Beyond that, they are using 15-way pipeline parallelism.</p><p>(likely used ZeRo Stage 1.lt is possiblethey used block-level FSDP)</p><p>为了在所有A100 GPU上实现并行计算，他们采用了8路张量并行（tensor parallelism），因为这是NVLink的限制。此外，他们还使用了15路管道并行（pipeline parallelism）。</p><p>很可能他们使用了ZeRo阶段1，同时也可能使用了块级FSDP（Fully Sharded Data Parallelism）。</p><h2 id="Training-Cost"><a href="#Training-Cost" class="headerlink" title="Training Cost"></a>Training Cost</h2><p>OpenAl’s training FLOPS for GPT-4 is ~2.15e25,</p><p>on~25,000 A100s for 90 to 100 days at about 32% to 36% MFU.</p><p>Part of this extremely low utilization is due to an absurd number of failures requiring checkpoints that needed to be restarted from.</p><p>If their cost in the cloud was about \$1 per A100 hour, the training costs for this run alone would be about \$63 million</p><p>(Today, the pre-training could be done<br>with ~8,192 H100 in ~55 days for \$21.5 million at \$2 per H100 hour.)</p><p>OpenAI的GPT-4训练的浮点运算速度（FLOPS）约为2.15e25次。</p><p>在大约25,000个A100 GPU上进行训练，持续时间为90至100天，利用率约为32%至36%。</p><p>这种极低的利用率部分是由于大量的故障导致需要重新启动检查点。</p><p>如果他们在云端的成本约为每个A100每小时1美元，单单这次训练的成本将约为6300万美元。</p><p>（现在，使用大约8,192个H100 GPU进行预训练，需要大约55天时间，成本为2150万美元，每个H100每小时2美元。）</p><h2 id="Mixture-of-Expert-Tradeoffs"><a href="#Mixture-of-Expert-Tradeoffs" class="headerlink" title="Mixture of Expert Tradeoffs"></a>Mixture of Expert Tradeoffs</h2><p>There are multiple MoE tradeoffs taken:</p><p>For example, MoE is incredibly difficult to deal with on inference because not every<br>part of the model is utilized on every token generation.</p><p>This means parts may sit dormant when other parts are being used. When serving<br>users, this really hurts utilization rates.</p><p>Researchers have shown that using 64 to 128 experts achieves better loss than 16<br>experts, but that’s purely research.</p><p>There are multiple reasons to go with fewer experts. One reason for OpenAI choosing 16 experts is because more experts are difficult to generalize at many tasks. More experts can also be more<br>difficult to achieve convergence with.</p><p>With such a large training run, OpenAI Instead chose to be more conservative onthe number of experts.</p><p>有多个专家混合（MoE）的权衡被考虑：</p><p>例如，在推理过程中，处理MoE非常困难，因为并非每个模型的部分在每个标记生成过程中都被利用。</p><p>这意味着某些部分可能处于闲置状态，而其他部分正在被使用。在为用户提供服务时，这会极大地降低利用率。</p><p>研究人员已经证明，使用64到128个专家比使用16个专家可以获得更好的损失结果，但这仅仅是研究结果。</p><p>选择较少的专家有多个原因。OpenAI选择使用16个专家之一的原因是因为更多的专家在许多任务上很难进行泛化。更多的专家也可能更难实现收敛。</p><p>在如此庞大的训练过程中，OpenAI选择在专家数量上更加保守。</p><h2 id="GPT-4-Inference-Cost"><a href="#GPT-4-Inference-Cost" class="headerlink" title="GPT-4 Inference Cost"></a>GPT-4 Inference Cost</h2><p>GPT-4 costs 3x that of the 175B parameter Davinchi.</p><p>This is largely due to the larger clusters required for GPT-4 and much lower utilization achieved.</p><p>AN estimate of it’s costs is \$0.0049 cents per 1k tokens for 128 A100s to inference GPT-4 8k seq len and \$0.0021cents per 1k tokens for 128 H100’s to inference GPT-4 8k seq len. It should be noted, we assume decent high utilization,and keeping batch sizes high.</p><p>GPT-4的成本是1750亿参数的Davinci模型的3倍。</p><p>这主要是由于GPT-4需要更大规模的集群，并且实现了更低的利用率。</p><p>据估计，使用128个A100 GPU进行GPT-4 8,000个序列长度的推理，每1,000个标记的成本约为0.0049美分；使用128个H100 GPU进行GPT-4 8,000个序列长度的推理，每1,000个标记的成本约为0.0021美分。需要注意的是，我们假设了良好的高利用率，并保持了较高的批量大小。</p><h2 id="Multi-Query-Attention"><a href="#Multi-Query-Attention" class="headerlink" title="Multi-Query Attention"></a>Multi-Query Attention</h2><p>OpenAl are using MQA just like everybody else.</p><p>Because of that only 1 head is needed and memory capacity can be significantly reduced for the KV cache. Even then, the 32k seq len GPT-4 definitely cannot run on 40GB A10Os, and the 8k is capped onmax bsz.</p><p>OpenAI也像其他人一样使用了MQA（Multi-QKV Attention）。</p><p>由于使用了MQA，只需要一个注意力头（head），并且可以显著降低KV缓存的内存需求。即便如此，32,000个序列长度的GPT-4肯定无法在40GB的A10O上运行，而8,000个序列长度则有最大批量大小的限制。</p><h2 id="Continuous-batching"><a href="#Continuous-batching" class="headerlink" title="Continuous batching"></a>Continuous batching</h2><p>OpenAl implements both variable batch sizes and continuous batching. This is so as to allow some level of maximum latency as well optimizing the inference costs.</p><p>OpenAI实现了可变批量大小和连续批处理。这样做可以在一定程度上允许最大延迟，并优化推理成本。</p><h2 id="Vision-Multi-Modal"><a href="#Vision-Multi-Modal" class="headerlink" title="Vision Multi-Modal"></a>Vision Multi-Modal</h2><p>It is a separate vision encoder from the text encoder, with cross-attention. The architecture is similar to Flamingo. This adds more parameters on top of the 1.8T of GPT-4. lt is fine-tuned with another ~2 trillion tokens, after the text only pre-training.</p><p>On the vision model, OpenAl wanted to train it from scratch, but it wasn’t mature enough, so they wanted to derisk it by starting with text.</p><p>One of the primary purposes of this vision capability is for autonomous agents able to read web pages and transcribe what’s in images and video.</p><p>Some of the data they train on is joint data (rendered LaTeX/text), screen shots of web page, youtube videos: samplingframes, and run Whisper around it to get transcript.</p><p>GPT-4引入了一个与文本编码器分离的独立视觉编码器，并具有交叉注意力机制。其架构类似于Flamingo模型。这使得GPT-4的参数量在1,800亿的基础上增加了更多。在仅进行文本预训练之后，还对该视觉模型进行了约2万亿个额外的微调标记。</p><p>在视觉模型方面，OpenAI原本希望从头开始训练，但该模型尚未足够成熟，所以他们选择通过从文本开始来减轻风险。</p><p>该视觉功能的主要目的之一是使自主代理能够阅读网页并转录图像和视频中的内容。</p><p>他们训练的一部分数据是联合数据，包括渲染的LaTeX/文本、网页截图、YouTube视频（采样帧），并使用Whisper技术对其进行转录。</p><h2 id="Speculative-Decoding"><a href="#Speculative-Decoding" class="headerlink" title="Speculative Decoding"></a>Speculative Decoding</h2><p>OpenAl might be using speculative decoding on GPT-4’s inference. (not sure100%)</p><p>The idea is to use a smaller faster model to decode several tokens in advance, and then feeds them into a large oracle model as a single batch.</p><p>lf the small model was right about its predictions-the larger model agrees and we can decode several tokens in a single batch.</p><p>But if the larger model rejects the tokens predicted by the draft model then the rest of the batch is discarded. And wecontinue with the larger model.</p><p>The conspiracy theory that the new GPT-4 quality had been deteriorated might be simply because they are letting the oracle model accept lower probability sequences from the speculative decoding model.</p><p>OpenAI可能正在使用GPT-4推理中的推测解码（speculative decoding），但无法确定。</p><p>这种方法是使用一个更小更快的模型提前解码多个标记，然后将它们作为一个批次输入到大型的预测模型中。</p><p>如果小型模型的预测正确，大型模型会同意并可以一次性解码多个标记。</p><p>但是，如果大型模型拒绝了草稿模型预测的标记，那么批次中的剩余部分将被丢弃，并继续使用大型模型。</p><p>有一种阴谋论认为，GPT-4的质量下降是因为他们让预测模型接受了由推测解码模型生成的低概率序列。</p><p>请注意，以上内容仅为推测，实际情况可能与之有所不同。</p><h2 id="Inference-Architecture"><a href="#Inference-Architecture" class="headerlink" title="Inference Architecture"></a>Inference Architecture</h2><p>The inference runs on a cluster of 128 GPUs.</p><p>There are multiple of these clusters in multiple datacenters in different locations.</p><p>It is done in 8-way tensor parallelism and 16-way pipeline parallelism.</p><p>Each node of 8 GPUs has only ~130B parameters, or less than 30GB per GPU at FP16 and less than 15GB at FP8/int8.</p><p>The model has 120, so it fits in 15 different nodes. [Possibly the there are less layers on the first node since it needs to also compute the embeddings]</p><p>According to these numbers: OpenAl should have trained on 2x the tokens if they were trying to go by chinchilla’soptimal.</p><p>[let alone surpass it like we do]</p><p>This goes to show that they are strugglingto get high quality data.</p><p>推理过程在由128个GPU组成的集群上运行。</p><p>这些集群分布在不同地点的多个数据中心中。</p><p>推理过程采用8路张量并行和16路管道并行。</p><p>每个包含8个GPU的节点只有约1,300亿个参数，即每个GPU约30GB（FP16）或15GB（FP8/INT8）。</p><p>模型有120层，因此需要15个不同的节点来容纳模型。（可能第一个节点的层数较少，因为它还需要计算嵌入层）</p><p>根据这些数字，如果OpenAI试图按照chinchilla的最优设置进行训练，他们应该训练2倍的标记。</p><p>这表明他们在获取高质量数据方面遇到了困难。</p><h2 id="Why-no-FSDP"><a href="#Why-no-FSDP" class="headerlink" title="Why no FSDP?"></a>Why no FSDP?</h2><p>A possible reason for this could be that some of the hardware infra they secured is of an older generation.</p><p>This is pretty common at local compute clusters as the organisationusually upgrade the infra in several “waves” to avoid a complete pause ofoperation.</p><p>With such a high amount of pipeline parallelism it is very likely that just like the rest of us they suffer from the “batch bubble”: slight idle timebetween batches.</p><p>Again: There is no magic.</p><p>They know what they are doing but it is not magic.</p><p>一个可能的原因是，他们所使用的硬件基础设施中有一部分是较旧一代的。</p><p>这在本地计算集群中非常常见，通常组织会分几个”波次”进行基础设施的升级，以避免完全停止运营。</p><p>由于存在如此高的管道并行性，他们很可能像其他人一样遭受”批处理泡沫”的影响：批次之间存在轻微的空闲时间。</p><p>再次强调：没有什么魔法。</p><p>他们知道自己在做什么，但这并非魔法。</p><hr><p>OpenAI是将GPT-4的架构保密，这并不是因为对人类存在着某种潜在风险，而是因为他们所构建的东西是可复制的。实际上，我们预计在不久的将来，谷歌、Meta、Anthropic、Inflection、Character、腾讯、字节跳动、百度等公司都将拥有与GPT-4同等甚至更强大的模型能力。</p><p>不要误会，OpenAI的工程能力非常出色，他们所构建的东西令人难以置信，但是他们所采取的解决方案并非神奇。这是一个优雅的解决方案，需要考虑许多复杂的权衡。扩大规模只是战斗的一部分。OpenAI最持久的优势在于他们在实际应用中拥有最多的用户、领先的工程人才，并且可以通过未来的模型继续超越其他竞争对手。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;From Twitter of Yam Peleg.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img.enderfga.cn/img/20230711100816.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    
    <category term="自然语言处理" scheme="http://enderfga.cn/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>Generating Images with Multimodal Language Models</title>
    <link href="http://enderfga.cn/2023/06/27/GILL/"/>
    <id>http://enderfga.cn/2023/06/27/GILL/</id>
    <published>2023-06-27T02:00:49.000Z</published>
    <updated>2023-08-11T05:28:16.003Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>科研笔记</p><span id="more"></span><h1 id="Generating-Images-with-Multimodal-Language-Models"><a href="#Generating-Images-with-Multimodal-Language-Models" class="headerlink" title="Generating Images with Multimodal Language Models"></a>Generating Images with Multimodal Language Models</h1><p><img src="https://img.enderfga.cn/img/teaser.gif" alt="Example of GILL generating multimodal dialogue."></p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by <strong>mapping between their embedding spaces</strong>. Our model demonstrates a wide suite of multimodal capabilities: <strong>image retrieval, novel image generation, and multimodal dialogue</strong>. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text — outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence.</p><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p><img src="https://img.enderfga.cn/img/architecture.png" alt="Model architecture of GILL."></p><p>GILL (Generating Images with Large Language Models) is capable of processing arbitrarily interleaved image-and-text inputs to generate text, retrieve images, and generate novel images.</p><p>Our findings show that it is possible to efficiently map the output embedding space of a frozen text-only LLM to the embedding space of a frozen text-to-image generation model (in this work, Stable Diffusion) despite both models using entirely different text encoders. We achieve this by finetuning a small number of parameters on image-caption pairs, in contrast to other methods which require interleaved image-text training data. Our approach is computationally efficient and does not require running the image generation model at training time.</p><p>During inference, the model takes in arbitrarily interleaved image and text inputs, and produces text interleaved with image embeddings. After deciding whether to retrieve or generate for a particular set of tokens, it returns the appropriate image outputs (either retrieved or generated).</p><h2 id="Capabilities"><a href="#Capabilities" class="headerlink" title="Capabilities"></a>Capabilities</h2><p>One of the more compelling applications of GILL is perhaps its ability to generalize to many different tasks, due to the LLM pretraining and freezing. We showcase several of these capabilities below. More qualitative results are provided in our <a href="https://arxiv.org/abs/2305.17216">paper</a>.</p><h3 id="Multimodal-Dialogue-Generation"><a href="#Multimodal-Dialogue-Generation" class="headerlink" title="Multimodal Dialogue Generation"></a>Multimodal Dialogue Generation</h3><p><img src="https://img.enderfga.cn/img/dialogue.png" alt="Multimodal dialogue example"></p><p>GILL can be prompted to generate dialogue-like text, producing multimodal dialogue by interleaving generated text, retrieved images, and generated images. (View as <a href="https://jykoh.com/gill/dialogue.gif">GIF</a> or <a href="https://jykoh.com/gill/dialogue.mp4">MP4</a>)</p><h3 id="Generating-from-Visual-Stories"><a href="#Generating-from-Visual-Stories" class="headerlink" title="Generating from Visual Stories"></a>Generating from Visual Stories</h3><p><img src="https://img.enderfga.cn/img/visual_stories.png" alt="Generating from Visual Stories"></p><p>GILL can condition on interleaved image-and-text inputs to generate more relevant images compared to non-LLM based text-to-image generation models.</p><h3 id="Comparison-Against-Stable-Diffusion"><a href="#Comparison-Against-Stable-Diffusion" class="headerlink" title="Comparison Against Stable Diffusion"></a>Comparison Against Stable Diffusion</h3><p><img src="https://img.enderfga.cn/img/compare_sd.png" alt="Comparison against Stable Diffusion"></p><p>The GILLMapper module we introduce allows our model to map effectively to the SD image generation backbone, outperforming or matching SD for many examples from <a href="https://sites.research.google/parti/">PartiPrompts</a>.</p><h3 id="Comparison-Against-FROMAGe"><a href="#Comparison-Against-FROMAGe" class="headerlink" title="Comparison Against FROMAGe"></a>Comparison Against FROMAGe</h3><p><img src="https://img.enderfga.cn/img/fromage_comparison.png" alt="Comparison Against FROMAGe"></p><p>Our model composites multimodal information to produce relevant image and text outputs. It can outperform baseline models that are limited to image retrieval.</p><h2 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h2><p>While GILL introduces many exciting capabilities, it is an early research prototype and has several limitations. GILL relies on an LLM backbone for many of its capabilities. As such, it also inherits many of the limitations that are typical of LLMs. More details and discussion are provided in our <a href="https://arxiv.org/abs/2305.17216">paper and appendix</a>:</p><ul><li>GILL does not always produce images when prompted, or when it is (evidently) useful for the dialogue.</li><li>A limitation of GILL is in its limited visual processing. At the moment, we use only 4 visual vectors to represent each input image (due to computational constraints), which may not capture all the relevant visual information needed for downstream tasks.</li><li>Our model inherits some of the unintended behaviors of LLMs, such as the potential for hallucinations, where it generates content that is false or not relevant to the input data. It also sometimes generates repetitive text, and does not always generate coherent dialogue text.</li></ul><p>One of the advantages of our model is that <strong>it is modular, and can benefit from stronger visual and language models released in the future</strong>. It is likely that it will also benefit from stronger text-to-image generation backbones, or through finetuning the generation backbone rather than just the GILLMapper module. We leave such scaling explorations for future work.</p><ul><li><strong>Research Problem:</strong> The research problem addressed in this paper is the fusion of large language models (LLMs) with pre-trained image encoder and decoder models. The goal is to create a model that can handle multimodal capabilities, such as image retrieval, novel image generation, and multimodal dialogue. The model should be able to condition on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs.</li><li><strong>Motivation:</strong> The motivation behind this research is to <strong>leverage the strong text representations of LLMs for visual outputs</strong>. The authors aim to outperform baseline generation models on tasks with <em>longer and more complex language</em>. They also aim to create a model that can decide whether to retrieve or generate at inference time, and can process image-and-text inputs, and produce retrieved images, generated images, and generated text.</li><li><strong>Solution:</strong> The authors propose a method called Generating Images with Large Language Models (GILL). This method efficiently maps the output embedding space of a frozen text-only LLM to that of a frozen generation model. They achieve this by finetuning a small number of parameters on image-caption pairs. They also propose efficient architectural changes to learn the LLM-to-generation mapping effectively with the GILLMapper module. To decide whether to produce a retrieved image or a generated one at inference time, they train a decision model that outputs a decision conditioned on the LM hidden representations.</li><li><strong>Novelty:</strong> The novelty of this research lies in the proposed method’s ability to <strong>process arbitrarily interleaved image-and-text inputs to generate text, retrieve images, and generate novel images</strong>. It is the first model capable of outputting retrieved images, novel images, and text, interleaving these for coherent multimodal dialogue generation. The authors also propose efficient architectural changes to learn the LLM-to-generation mapping effectively with the GILLMapper module, a lightweight Transformer conditioned on special learnt text tokens.</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;科研笔记&lt;/p&gt;</summary>
    
    
    
    
    <category term="计算机视觉" scheme="http://enderfga.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>Fine-Grained Human Feedback Gives Better Rewards for Language Model Training</title>
    <link href="http://enderfga.cn/2023/06/20/Fine-Grained/"/>
    <id>http://enderfga.cn/2023/06/20/Fine-Grained/</id>
    <published>2023-06-20T05:21:59.000Z</published>
    <updated>2023-08-11T05:28:41.404Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>科研笔记</p><span id="more"></span><h1 id="Fine-Grained-Human-Feedback-Gives-Better-Rewards-for-Language-Model-Training"><a href="#Fine-Grained-Human-Feedback-Gives-Better-Rewards-for-Language-Model-Training" class="headerlink" title="Fine-Grained Human Feedback Gives Better Rewards for Language Model Training"></a>Fine-Grained Human Feedback Gives Better Rewards for Language Model Training</h1><p><img src="https://img.enderfga.cn/img/teaser-smaller.jpg" alt=""></p><p>We propose <strong>Fine-Grained RLHF</strong>, a framework that enables training and learning from reward functions that are fine-grained in two respects:</p><ol><li>density, providing a reward after every segment (e.g., a sentence) is generated</li><li>incorporating multiple reward models associated with different feedback types (e.g., factual incorrectness, irrelevance, and information incompleteness).</li></ol><h2 id="What-are-Fine-Grained-Rewards"><a href="#What-are-Fine-Grained-Rewards" class="headerlink" title="What are Fine-Grained Rewards?"></a>What are Fine-Grained Rewards?</h2><p>Prior work in RLHF focused on collecting human preferences regarding the overall quality of language model (LM) outputs. However, this type of holistic feedback offers limited information. In our paper, we introduce <strong>fine-grained human feedback</strong> (e.g., which sub-sentence is irrelevant, which sentence is not truthful, which sentence is toxic) as an explicit training signal. Our rewards are fine-grained in two aspects:</p><p><strong>(a) Density</strong>: We provide a reward after each segment (e.g., a sentence) is generated, similar to OpenAI’s “step-by-step process reward”. We found that this approach is more informative than holistic feedback and, thus, more effective for RL.</p><p><strong>(b) Multiple reward models associated with different feedback types</strong>: We employ multiple reward models to capture different types of feedback (e.g., factual inaccuracy, irrelevance, and information incompleteness). Interestingly, we observed that these reward models both complement and compete with each other. By adjusting the weights of the reward models, we can control the balance between the different types of feedback and <strong>tailor the LM for different tasks</strong> according to specific needs.</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Language models (LMs) often exhibit undesirable text generation behaviors, including generating false, toxic, or irrelevant outputs. Reinforcement learning from human feedback (RLHF)—where human preference judgments on LM outputs are transformed into a learning signal—has recently shown promise in addressing these issues. However, such holistic feedback conveys limited information on long text outputs; it does not indicate which aspects of the outputs influenced user preference; e.g., which parts contain what type(s) of errors. In this paper, we use fine-grained human feedback (e.g., which sentence is false, which sub-sentence is irrelevant) as an explicit training signal. We introduce FINE-GRAINED RLHF, a framework that enables training and learning from reward functions that are fine-grained in two respects: (1) density, providing a reward after every segment (e.g., a sentence) is generated; and (2) incorporating multiple reward models associated with different feedback types (e.g., factual incorrectness, irrelevance, and information incompleteness). We conduct experiments on detoxification and long-form question answering to illustrate how learning with such reward functions leads to improved performance, supported by both automatic and human evaluation. Additionally, we show that LM behaviors can be customized using different combinations of fine-grained reward models.</p><h2 id="Task-1-Detoxification"><a href="#Task-1-Detoxification" class="headerlink" title="Task 1: Detoxification"></a>Task 1: Detoxification</h2><p>The task of detoxification aims to reduce the toxicity in the model generation. We use Perspective API to measure toxicity. It returns a toxicity value between 0 (not toxic) and 1 (toxic).</p><p><img src="https://img.enderfga.cn/img/toxicity_demo.png" alt=""></p><p>We compare two kinds of rewards:</p><p>(a) <strong>Holistic Rewards for (non-)Toxicity</strong>: We use 1-Perspective(y) as the reward</p><p>(b) <strong>Sentence-level (Fine-Grained) Rewards for (non-)Toxicity</strong>: We query the API after the model generates each sentence instead of generating the full sequence. For each generated sentence, we use -Δ(Perspective(y)) as the reward for the sentence (i.e. how much toxicity is changed from generating the current sentence).</p><p><img src="https://img.enderfga.cn/img/toxicity_results.png" alt=""></p><p>Table 1 shows that Our Fine-Grained RLHF with sentence-level fine-grained reward attains the lowest toxicity and perplexity among all methods, while maintaining a similar level of diversity. Figure 2 shows that learning from denser fine-grained reward is more <strong>sample efficient</strong> than holistic reward. One explanation is that fine-grained reward locates where the toxic content is, which is a stronger training signal compared with a scalar reward for the whole text.</p><h2 id="Task-2-Long-Form-Question-Answering"><a href="#Task-2-Long-Form-Question-Answering" class="headerlink" title="Task 2: Long-Form Question Answering"></a>Task 2: Long-Form Question Answering</h2><p>We collect <strong>QA-Feeback</strong>, a dataset of long-form question answering, with human preferences and fine-grained feedback. QA-Feedback is based on ASQA, a dataset that focuses on answering ambiguous factoid questions.</p><p>There are three types of fine-grained human feedback, and we train a fine-grained reward model for each of them:</p><p>$\color{blue}{\text{C1: irrelevance, repetition, and incoherence (rel.)}}$; The reward model has the density level of sub-sentences; i.e., returns a score for each sub-sentence. If the sub-sentence is irrelevant, repetitive, or incoherent, the reward is -1; otherwise, the reward is +1.</p><p>$\color{goldenrod}{\text{C2: incorrect or unverifiable facts (fact.)}}$; The reward model has the density level of sentences; i.e., returns a score for each sentence. If the sentence has any factual error, the reward is -1; otherwise, the reward is +1.</p><p>$\color{green}{\text{C3: incomplete information (comp.)}}$; The reward model checks if the response is complete and covers all the information in the reference passages that are related to the question. This reward model gives one reward for the whole response.</p><h2 id="Fine-Grained-Human-Evaluation"><a href="#Fine-Grained-Human-Evaluation" class="headerlink" title="Fine-Grained Human Evaluation"></a>Fine-Grained Human Evaluation</h2><p>We compare our <strong>Fine-Grained RLHF</strong> with the following baselines:</p><p><strong>SFT</strong>: The supervised finetuning model (trained on 1K training examples) that is used as the initial policy for our RLHF experiments.</p><p><strong>Pref. RLHF</strong>: The baseline RLHF model that uses holistic reward.</p><p><strong>SFT-Full</strong>: We finetune LM with human-written responses (provided by ASQA) of all training examples and denote this model as SFT-Full. Notice that each gold response takes 15 min to annotate (according to ASQA), which takes much longer time than our feedback annotation (6 min).</p><p><img src="https://img.enderfga.cn/img/human_eval.png" alt=""></p><p>Human evaluation shows that our <strong>Fine-Grained RLHF</strong> outperforms SFT and Preference RLHF on all error types. Also, RLHF (both preference-based and fine-grained) are particularly effective in reducing factual errors.</p><h2 id="Customize-LM-behaviors"><a href="#Customize-LM-behaviors" class="headerlink" title="Customize LM behaviors"></a>Customize LM behaviors</h2><p><img src="https://img.enderfga.cn/img/customization.png" alt=""></p><p>By changing the weight of the $\color{blue}{\text{Relevance reward model}}$, and keeping the weight of the other two reward models fixed, we can customize how detailed and lengthy the LM responses would be. Here we compare the outputs of three LMs, trained with different reward model combinations.</p><h2 id="Fine-Grained-reward-models-both-complement-and-compete-with-each-other"><a href="#Fine-Grained-reward-models-both-complement-and-compete-with-each-other" class="headerlink" title="Fine-Grained reward models both complement and compete with each other"></a>Fine-Grained reward models both complement and compete with each other</h2><p><img src="https://img.enderfga.cn/img/analysis.png" alt=""></p><p>We find that there is a trade-off between the three reward models. $\color{blue}{\text{Relevance RM}}$ prefers shorter and more concise responses, while $\color{green}{\text{Info Completeness RM}}$ prefers longer and more informative responses. Thus, these two rewards compete against each other during training and eventually reach a balance. Meanwhile, $\color{goldenrod}{\text{Factuality RM}}$ continuously improves the factual correctness of the response. Finally, removing any one of the reward models will degrade the performance.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;科研笔记&lt;/p&gt;</summary>
    
    
    
    
    <category term="自然语言处理" scheme="http://enderfga.cn/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>Generation Model</title>
    <link href="http://enderfga.cn/2023/06/14/model/"/>
    <id>http://enderfga.cn/2023/06/14/model/</id>
    <published>2023-06-14T06:41:21.000Z</published>
    <updated>2023-06-27T02:06:42.385Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>对 diffusion 结构的改进</p><span id="more"></span><h1 id="Generation-Model"><a href="#Generation-Model" class="headerlink" title="Generation Model"></a>Generation Model</h1><h2 id="RAPHAEL-Text-to-Image-Generation-via-Large-Mixture-of-Diffusion-Paths"><a href="#RAPHAEL-Text-to-Image-Generation-via-Large-Mixture-of-Diffusion-Paths" class="headerlink" title="RAPHAEL: Text-to-Image Generation via Large Mixture of Diffusion Paths"></a>RAPHAEL: Text-to-Image Generation via Large Mixture of Diffusion Paths</h2><p>香港大学，商汤科技，23.5.29，<a href="https://raphael-painter.github.io/">https://raphael-painter.github.io/</a></p><ol><li><strong>Research Problem:</strong> The research problem addressed in this paper is the generation of <strong>highly artistic images that accurately portray text prompts</strong>. Existing models often fail to adequately preserve textual concepts within the generated images due to the reliance on a classic cross-attention mechanism for integrating text descriptions into visual representations.</li><li><strong>Motivation:</strong> The motivation behind this research is to improve the precision of <strong>alignment between text and image</strong> in the generation process. This is aimed at <strong>enhancing the quality and aesthetic</strong> appeal of the generated images, and enabling them to <strong>accurately represent the associated text prompt</strong>.</li><li><p><strong>Solution:</strong> The authors propose a text-conditional image diffusion model, RAPHAEL, which stacks tens of mixture-of-experts (MoE) layers, including both space-MoE and time-MoE layers. This configuration leads to billions of diffusion paths from the network input to the output. Each path can act as a “painter” responsible for rendering a particular concept to an image region at a specific timestep. The authors also propose an edge-supervised learning module to further enhance the image quality and aesthetic appeal of the generated images.</p><p><img src="https://img.enderfga.cn/img/image-20230613125221600.png" alt=""></p></li><li><strong>What’s New:</strong> The novelty of this research lies in the introduction of RAPHAEL, which outperforms recent cutting-edge models in terms of both image quality and aesthetic appeal. RAPHAEL exhibits superior performance in switching images across diverse styles, such as Japanese comics, realism, cyberpunk, and ink illustration. It also establishes a new <strong>state-of-the-art</strong> with a zero-shot FID-30k score of 6.61 on the COCO dataset. Furthermore, RAPHAEL is capable of generating images with resolutions up to 4096 × 6144 with rich image contents and details, when combined with a tailor-made SR-GAN model.</li></ol><h3 id="Mixture-of-experts？"><a href="#Mixture-of-experts？" class="headerlink" title="Mixture of experts？"></a>Mixture of experts？</h3><p><img src="https://img.enderfga.cn/img/image-20230613131954667.png" alt=""></p><p>一种集成学习(Ensemble Learning) 技术，一个系统中包含多个分开的网络，每个网络去处理全部训练样本的一个子集。这种方式可以看做是把多层网络进行了模块化的转换。</p><p>假设我们已经知道数据集中存在一些天然的子集（比如来自不同的domain，不同的topic），那么用单个模型去学习，就会受到很多干扰（interference），导致学习很慢、泛化困难。这时，我们可以使用多个模型（即专家，expert）去学习，使用一个门网络（gating network）来决定每个数据应该被哪个模型去训练，这样就可以减轻不同类型样本之间的干扰。</p><p>MoE 将预测建模任务分解为若干子任务，在每个子任务上训练一个专家模型（Expert Model），开发一个门控模型（Gating Model），该模型根据要预测的输入来学习信任哪个专家，并组合预测结果。</p><h3 id="Detail"><a href="#Detail" class="headerlink" title="Detail"></a>Detail</h3><ul><li>LAION-5B and some internal datasets</li><li>Multi-scale Training</li><li>3B</li><li>GPT3.5生成的100个常见的形容词</li></ul><h2 id="SnapFusion-Text-to-Image-Diffusion-Model-on-Mobile-Devices-within-Two-Seconds"><a href="#SnapFusion-Text-to-Image-Diffusion-Model-on-Mobile-Devices-within-Two-Seconds" class="headerlink" title="SnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds"></a>SnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds</h2><p>Snap Inc.，东北大学，23.6.1，<a href="https://snap-research.github.io/SnapFusion/">https://snap-research.github.io/SnapFusion/</a></p><ol><li><strong>研究问题：</strong>这篇论文解决的研究问题是如何在移动设备上快速运行文本到图像的扩散模型。现有的模型虽然能够生成出色的图像，但由于其复杂的网络架构和大量的去噪迭代，使得它们在计算上非常昂贵，运行速度慢。</li><li><strong>动机：</strong>这项研究的动机是改进现有模型的运行速度和计算成本。这不仅可以降低运行模型的成本，还可以避免在用户数据被发送到第三方时涉及到的隐私问题。</li><li><strong>解决方法：</strong>作者通过引入高效的网络架构和改进步骤蒸馏来解决这个问题。他们提出了一个高效的UNet，通过识别原始模型的冗余，并通过数据蒸馏减少图像解码器的计算。此外，他们通过探索训练策略和引入分类器无关指导的正则化来增强步骤蒸馏。</li><li><strong>创新之处：</strong>这项研究的创新之处在于，它是第一个能够在移动设备上运行文本到图像扩散模型的方法，运行时间少于2秒。此外，他们的模型在8个去噪步骤中比Stable Diffusion v1.5的50个步骤获得了更好的FID和CLIP分数。这项工作通过将强大的文本到图像扩散模型带到用户手中，使内容创建民主化。</li></ol><p><img src="https://img.enderfga.cn/img/image-20230613134941299.png" alt=""></p><p>为了优化 UNet 结构，我们提出一套 UNet 结构自动评估、进化流程：先对 UNet 进行鲁棒性训练（Robust Training），在训练中随机 drop 一些模块，以此来测试出每个模块对性能的真实影响，从而构建一个 “对 CLIP score 的影响 vs. latency” 的查找表；然后根据该查找表，优先去除对 CLIP score 影响不大同时又很耗时的模块。这一套流程是在线自动进行，完成之后，我们就得到了一个全新的 UNet 结构，称为 Efficient UNet。相比原版 UNet，实现 7.4x 加速且性能不降。</p><p><img src="https://img.enderfga.cn/img/image-20230613135018817.png" alt=""></p><p>这篇论文中的蒸馏过程主要包括两个部分：图像解码器的蒸馏和步骤蒸馏。</p><ol><li><strong>图像解码器的蒸馏：</strong>作者提出了一个蒸馏流程，使用合成数据来学习通过通道减少获得的高效图像解码器。他们使用文本提示从SD-v1.5的UNet中获取潜在表示，然后将其转发到他们的高效图像解码器和SD-v1.5的解码器以生成两个图像。然后，通过最小化两个图像之间的均方误差来优化解码器。</li><li><strong>步骤蒸馏：</strong>步骤蒸馏是一种减少UNet迭代去噪步骤的方法，以实现更快的速度。他们采用了步骤蒸馏的研究方向，其中通过蒸馏教师模型来减少推理步骤。例如，如果教师模型需要32步，那么学生模型可能只需要16步，这样，学生模型的速度就可以提高2倍。</li></ol><p>这篇论文还提出了一种新的蒸馏方法，称为CFG-Aware Step Distillation，它在计算损失之前对教师和学生进行分类器无关的指导。这种方法在提高CLIP分数方面表现出显著的效果。</p><h3 id="贡献："><a href="#贡献：" class="headerlink" title="贡献："></a>贡献：</h3><p>（1）通过对现有 UNet 的逐层分析，定位速度瓶颈，提出一种新的高效 UNet 结构（Efficient UNet），可以等效替换原 Stable Diffusion 中的 UNet，实现 7.4x 加速；</p><p>（2）对推理阶段的迭代步数进行优化，提出一种全新的步数蒸馏方案（CFG-aware Step Distillation），减少步数的同时可显著提升 CLIP score，实现 6.25x 加速。</p><h3 id="不足："><a href="#不足：" class="headerlink" title="不足："></a>不足：</h3><ol><li>SD 模型在多种图像生成场景中都可以使用，本文囿于时间，目前只关注了 text to image 这个核心任务，后期将跟进其他任务（如 inpainting，ControlNet 等等）。</li><li>本文主要关注速度上的提升，并未对模型存储进行优化。我们相信所提出的 Efficient UNet 仍然具备压缩的空间，结合其他的高性能优化方法（如剪枝，量化），有望缩小存储，并将时间降低到 1 秒以内，离端上实时 SD 更进一步。</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;对 diffusion 结构的改进&lt;/p&gt;</summary>
    
    
    
    
    <category term="计算机视觉" scheme="http://enderfga.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion——RLHF</title>
    <link href="http://enderfga.cn/2023/05/31/diffrlhf/"/>
    <id>http://enderfga.cn/2023/05/31/diffrlhf/</id>
    <published>2023-05-31T08:20:07.000Z</published>
    <updated>2023-05-31T08:26:12.419Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>最近读的两篇关于使用RLHF微调diffusion的论文。</p><span id="more"></span><h1 id="Diffusion-RLHF"><a href="#Diffusion-RLHF" class="headerlink" title="Diffusion_RLHF"></a>Diffusion_RLHF</h1><h2 id="Training-Diffusion-Models-with-Reinforcement-Learning"><a href="#Training-Diffusion-Models-with-Reinforcement-Learning" class="headerlink" title="Training Diffusion Models with Reinforcement Learning"></a>Training Diffusion Models with Reinforcement Learning</h2><p><strong>5.23， University of California, Berkeley</strong></p><p><strong>研究问题 (Research Problem)</strong>: 论文主要解决的问题是如何优化扩散模型以实现特定目标。扩散模型是一种生成模型，通常使用对对数似然目标的近似进行训练。然而，大多数扩散模型的使用场景并不关心似然性，而是关心下游目标，如人类感知的图像质量或药物效果。</p><p><strong>动机 (Motivation)</strong>: 这项研究的动机是通过更紧密地将扩散模型与实际目标对齐，以提高其性能。这对于<em>难以通过提示表达的任务（如图像可压缩性）以及从人类反馈中得出的任务（如审美质量）</em>尤其相关。</p><p><strong>新颖性 (What’s New)</strong>: 作者提出了一种名为Denoising Diffusion Policy Optimization (DDPO)的方法，该方法将去噪视为多步决策问题。这使得可以使用比替代奖励加权似然方法更有效的策略梯度算法。此外，作者还展示了DDPO如何改进使用视觉语言模型反馈的提示图像对齐，而无需额外的数据收集或人类注释。</p><p><strong>方法的总体思想 (Overall Idea of the Method)</strong>: DDPO方法的总体思想是将去噪过程视为一个多步骤的决策问题，从而可以使用策略梯度算法进行优化。这种方法可以适应各种目标，包括那些难以通过提示表达的目标，如图像可压缩性，以及那些从人类反馈中得出的目标，如审美质量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> io<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">encode_jpeg</span>(<span class="hljs-params"> x , quality = <span class="hljs-number">95</span> </span>) :</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">x : np array of shape (H, W, 3) and dtype uint8</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>    img = Image.fromarray(x)<br>    buffer = io.BytesIO()<br>    img.save(buffer, ‘JPEG’, quality = quality)<br>    jpeg = buffer.getvalue()<br>    <span class="hljs-built_in">bytes</span> = np.frombuffer(jpeg, dtype =np.uint8)<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(<span class="hljs-built_in">bytes</span>)/<span class="hljs-number">1000</span><br></code></pre></td></tr></table></figure><p><img src="https://img.enderfga.cn/img/image-20230530135032898.png" alt=""></p><p><strong>实验主要提升的效果 (Main Improvements in Experiments)</strong>: 实验结果表明，DDPO在所有任务上都明显优于RWR，表明将去噪过程形式化为马尔可夫决策过程并直接估计策略梯度比优化奖励加权似然的下界更有效。此外，DDPO还能够有效地适应预训练模型，只需指定奖励函数，无需进行任何进一步的数据策划。</p><p><img src="https://img.enderfga.cn/img/image-20230530135637339.png" alt=""></p><p><strong>具体来说，DDPO的实现可以分为以下几个步骤：</strong></p><ol><li><strong>定义决策问题</strong>：首先，DDPO将去噪过程定义为一个马尔可夫决策过程（MDP）。在这个MDP中，每一步都包括一个状态（当前的去噪图像）和一个动作（下一步的去噪操作）。</li><li><strong>定义奖励函数</strong>：然后，DDPO定义了一个奖励函数，用于评估每一步去噪操作的效果。这个奖励函数可以基于任何与任务目标相关的度量，例如在这篇论文中，奖励可能是基于图像的压缩性、审美质量或与提示的对齐程度。</li><li><strong>优化策略</strong>：接下来，DDPO使用策略梯度算法来优化去噪策略。这个过程包括生成一组去噪轨迹，计算每个轨迹的预期奖励，然后使用这些预期奖励来更新策略的参数。</li><li><strong>迭代优化</strong>：最后，DDPO通过迭代这个过程，逐步改进去噪策略。每一轮迭代都会生成新的去噪轨迹，计算新的预期奖励，然后使用这些新的预期奖励来更新策略的参数。<script type="math/tex; mode=display">\hat{g}_{\mathrm{IS}}=\mathbb{E}\left[\sum_{t=0}^T \frac{p_\theta\left(\mathbf{x}_{t-1} \mid \mathbf{c}, t, \mathbf{x}_t\right)}{p_{\theta_{\text {old }}}\left(\mathbf{x}_{t-1} \mid \mathbf{c}, t, \mathbf{x}_t\right)} \nabla_\theta \log p_\theta\left(\mathbf{x}_{t-1} \mid \mathbf{c}, t, \mathbf{x}_t\right) r\left(\mathbf{x}_0, \mathbf{c}\right)\right]</script></li></ol><h2 id="DPOK-Reinforcement-Learning-for-Fine-tuning-Text-to-Image-Diffusion-Models"><a href="#DPOK-Reinforcement-Learning-for-Fine-tuning-Text-to-Image-Diffusion-Models" class="headerlink" title="DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models"></a>DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models</h2><p><strong>5.25，Google research</strong></p><p><strong>研究问题 (Research Problem)</strong>: 本文主要研究如何通过在线强化学习（RL）来优化和微调文本到图像的模型，以改善其性能和质量。</p><p><strong>动机 (Motivation)</strong>: 尽管文本到图像的模型已经取得了显著的进步，但它们仍然存在系统性的弱点，例如<em>有限的对象组合能力和在生成指定颜色和数量的对象时的困难</em>。此外，使用<em>人类反馈进行学习已被证明是一种有效的方法</em>来克服这些限制。然而，<em>基于奖励的监督微调往往会导致图像质量的下降</em>。因此，本文提出了一种新的在线RL微调方法来解决这些问题。</p><p><strong>新颖性 (What’s New)</strong>: 本文提出了一种名为DPOK的新方法，该方法将策略优化与KL正则化结合起来。与以往的工作不同，本文将微调任务定义为一个RL问题，并使用策略梯度来更新预训练的文本到图像扩散模型，以最大化反馈训练的奖励。</p><p><strong>方法的总体思想 (Overall Idea of the Method)</strong>: DPOK方法的核心思想是利用在线强化学习来微调文本到图像的模型。在这个过程中，模型是在新的样本上进行更新的，这些样本来自于之前训练的模型。此外，该方法还引入了Kullback-Leibler (KL) 散度作为正则化项，以确保更新后的模型不会偏离原始模型太远。</p><p><strong>作者引入了两种KL正则化方法，一种是KL-D，另一种是KL-O。KL-D基于预训练模型的数据，通过调整原始奖励中的一个移位因子来实现正则化，使得每个样本的权重更趋向于均匀分布。而KL-O则通过在奖励加权损失中引入一个额外的项来实现，这个额外的项惩罚了从预训练模型和当前模型得出的去噪方向之间的L2距离。</strong></p><script type="math/tex; mode=display">\begin{aligned}& \mathbb{E}_{p(z)}\left[\alpha \mathbb{E}_{p_\theta^*\left(x_{0: T} \mid z\right)}\left[-r\left(x_0, z\right) \sum_{t=1}^T \log p_\theta\left(x_{t-1} \mid x_t, z\right)\right]\right. \\& \left.+\beta \sum_{t=1}^T \mathbb{E}_{p_\theta^*\left(x_t \mid z\right)}\left[\operatorname{KL}\left(p_\theta\left(x_{t-1} \mid x_t, z\right) \| p_{\mathrm{pre}}\left(x_{t-1} \mid x_t, z\right)\right)\right]\right]\end{aligned}</script><p><strong>实验主要提升的效果 (Main Improvements in Experiments)</strong>: 在实验中，作者发现在线RL微调能够在保持高图像保真度的同时，实现强大的文本-图像对齐。此外，与监督微调相比，在线训练允许在（监督）训练数据集之外评估奖励模型和条件KL散度，这提供了明显的优势。在实证比较中，作者还在监督微调方法中引入了KL正则项，以进行公平的比较。</p><p><img src="https://img.enderfga.cn/img/image-20230530141846201.png" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20230530142225063.png" alt=""></p><p><strong>与上面一篇的对比：</strong></p><ol><li><strong>相似性</strong>：Black等人的工作和本文都探讨了在线强化学习微调用于改进文本到图像扩散模型。他们都展示了RL微调可以优于监督微调，这与本文的观察结果一致。</li><li><strong>差异性</strong>：在本文中，作者不仅关注奖励优化，还受到监督微调中的失败案例（如过饱和或非真实感图像）的启发，旨在找到一种带有KL正则化的RL解决方案来解决问题。此外，本文还系统地分析了监督微调和在线微调中KL正则化的理论依据，并展示了在线RL微调中KL正则化比监督微调更有效。通过采用在线KL正则化，本文的算法成功地在保持高奖励和图像质量的同时，避免了过度优化的问题。</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近读的两篇关于使用RLHF微调diffusion的论文。&lt;/p&gt;</summary>
    
    
    
    
    <category term="计算机视觉" scheme="http://enderfga.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>基于知识库的自动问答系统</title>
    <link href="http://enderfga.cn/2023/05/31/kbqa/"/>
    <id>http://enderfga.cn/2023/05/31/kbqa/</id>
    <published>2023-05-31T08:09:40.000Z</published>
    <updated>2023-07-08T10:44:34.657Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>自然语言处理期中和期末项目。</p><span id="more"></span><embed src="./kbqa1.pdf" width="100%" height="750" type="application/pdf"><embed src="./kbqa2.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    <summary type="html">&lt;p&gt;自然语言处理期中和期末项目。&lt;/p&gt;</summary>
    
    
    
    
    <category term="自然语言处理" scheme="http://enderfga.cn/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>最优化理论与算法</title>
    <link href="http://enderfga.cn/2023/05/31/opt/"/>
    <id>http://enderfga.cn/2023/05/31/opt/</id>
    <published>2023-05-31T07:30:34.000Z</published>
    <updated>2023-07-08T10:47:09.976Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>期中与期末，<a href="https://enderfga.cn/markmap.html">笔记1链接</a>；<a href="https://enderfga.cn/markmap2.html">笔记2链接</a>。</p><p><img src="https://img.enderfga.cn/img/image-20230531154210275.png" alt=""></p><span id="more"></span><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br></pre></td><td class="code"><pre><code class="hljs matlab"><span class="hljs-comment">% 利用最速梯度下降法求解函数的极小点</span><br><br><span class="hljs-comment">% 函数定义</span><br>f = @(x) <span class="hljs-number">0.5</span>*x(<span class="hljs-number">1</span>)^<span class="hljs-number">2</span> + x(<span class="hljs-number">2</span>)^<span class="hljs-number">2</span>;<br><br><span class="hljs-comment">% 梯度定义</span><br>grad_f = @(x) [x(<span class="hljs-number">1</span>); <span class="hljs-number">2</span>*x(<span class="hljs-number">2</span>)];<br><br><span class="hljs-comment">% 初始值和收敛阈值</span><br>x0 = [<span class="hljs-number">2</span>; <span class="hljs-number">1</span>];<br>epsilon = <span class="hljs-number">1e-2</span>;<br><br><span class="hljs-comment">% 最速梯度下降法</span><br>alpha = <span class="hljs-number">0.1</span>;  <span class="hljs-comment">% 初始步长</span><br>x = x0;<br>grad = grad_f(x);<br>iter = <span class="hljs-number">0</span>;<br><br><span class="hljs-keyword">while</span> norm(grad) &gt; epsilon<br>    <span class="hljs-comment">% 计算步长</span><br>    <span class="hljs-comment">% 这里可以使用线搜索方法来选择合适的步长 alpha，如 Armijo 规则或 Wolfe 条件</span><br>  <br>    <span class="hljs-comment">% 更新变量</span><br>    x = x - alpha * grad;<br>    grad = grad_f(x);<br>  <br>    iter = iter + <span class="hljs-number">1</span>;<br><span class="hljs-keyword">end</span><br><br><span class="hljs-comment">% 输出结果</span><br>fprintf(<span class="hljs-string">&#x27;最小值点: (%f, %f)\n&#x27;</span>, x(<span class="hljs-number">1</span>), x(<span class="hljs-number">2</span>));<br>fprintf(<span class="hljs-string">&#x27;迭代次数: %d\n&#x27;</span>, iter);<br><br><span class="hljs-comment">% 利用牛顿法求解函数的极小点</span><br><br><span class="hljs-comment">% 函数定义</span><br>f = @(x) <span class="hljs-number">100</span>*(x(<span class="hljs-number">2</span>) - x(<span class="hljs-number">1</span>)^<span class="hljs-number">2</span>)^<span class="hljs-number">2</span> + (<span class="hljs-number">1</span> - x(<span class="hljs-number">1</span>))^<span class="hljs-number">2</span>;<br><br><span class="hljs-comment">% 梯度定义</span><br>grad_f = @(x) [<span class="hljs-number">-400</span>*x(<span class="hljs-number">1</span>)*(x(<span class="hljs-number">2</span>) - x(<span class="hljs-number">1</span>)^<span class="hljs-number">2</span>) - <span class="hljs-number">2</span>*(<span class="hljs-number">1</span> - x(<span class="hljs-number">1</span>));<br>                <span class="hljs-number">200</span>*(x(<span class="hljs-number">2</span>) - x(<span class="hljs-number">1</span>)^<span class="hljs-number">2</span>)];<br><br><span class="hljs-comment">% Hessian 矩阵定义</span><br>hessian = @(x) [<span class="hljs-number">1200</span>*x(<span class="hljs-number">1</span>)^<span class="hljs-number">2</span> - <span class="hljs-number">400</span>*x(<span class="hljs-number">2</span>) + <span class="hljs-number">2</span>, <span class="hljs-number">-400</span>*x(<span class="hljs-number">1</span>);<br>                <span class="hljs-number">-400</span>*x(<span class="hljs-number">1</span>), <span class="hljs-number">200</span>];<br><br><span class="hljs-comment">% 初始值和收敛阈值</span><br>x0 = [<span class="hljs-number">-2</span>; <span class="hljs-number">2</span>];<br>epsilon = <span class="hljs-number">1e-2</span>;<br><br><span class="hljs-comment">% 牛顿法</span><br>x = x0;<br>grad = grad_f(x);<br>iter = <span class="hljs-number">0</span>;<br><br><span class="hljs-keyword">while</span> norm(grad) &gt; epsilon<br>    <span class="hljs-comment">% 计算 Hessian 矩阵和其逆矩阵</span><br>    H = hessian(x);<br>    inv_H = inv(H);<br>  <br>    <span class="hljs-comment">% 更新变量</span><br>    x = x - inv_H * grad;<br>    grad = grad_f(x);<br>  <br>    iter = iter + <span class="hljs-number">1</span>;<br><span class="hljs-keyword">end</span><br><br><span class="hljs-comment">% 输出结果</span><br>fprintf(<span class="hljs-string">&#x27;最小值点: (%f, %f)\n&#x27;</span>, x(<span class="hljs-number">1</span>), x(<span class="hljs-number">2</span>));<br>fprintf(<span class="hljs-string">&#x27;迭代次数: %d\n&#x27;</span>, iter);<br><br><span class="hljs-comment">% 利用共轭梯度法求解方程组的根</span><br><br><span class="hljs-comment">% 原始方程组的系数矩阵 A</span><br>A = [<span class="hljs-number">4</span>, <span class="hljs-number">-3</span>; <span class="hljs-number">2</span>, <span class="hljs-number">1</span>];<br><br><span class="hljs-comment">% 原始方程组的右侧向量 b</span><br>b = [<span class="hljs-number">11</span>; <span class="hljs-number">13</span>];<br><br><span class="hljs-comment">% 转化为对称正定矩阵 B = A^T * A</span><br>B = A&#x27; * A;<br><br><span class="hljs-comment">% 转化后的方程组的右侧向量</span><br>b_new = A&#x27; * b;<br><br><span class="hljs-comment">% 初始值和收敛阈值</span><br>x0 = [<span class="hljs-number">0</span>; <span class="hljs-number">0</span>];<br>epsilon = <span class="hljs-number">1e-6</span>;<br><br><span class="hljs-comment">% 共轭梯度法</span><br>x = x0;<br>r = b_new - B * x;<br>p = r;<br>iter = <span class="hljs-number">0</span>;<br><br><span class="hljs-keyword">while</span> norm(r) &gt; epsilon<br>    alpha = (r&#x27; * r) / (p&#x27; * B * p);<br>    x = x + alpha * p;<br>    r_new = r - alpha * B * p;<br>    <span class="hljs-built_in">beta</span> = (r_new&#x27; * r_new) / (r&#x27; * r);<br>    p = r_new + <span class="hljs-built_in">beta</span> * p;<br>    r = r_new;<br>    iter = iter + <span class="hljs-number">1</span>;<br><span class="hljs-keyword">end</span><br><br><span class="hljs-comment">% 输出结果</span><br>fprintf(<span class="hljs-string">&#x27;方程的根: (%f, %f)\n&#x27;</span>, x(<span class="hljs-number">1</span>), x(<span class="hljs-number">2</span>));<br>fprintf(<span class="hljs-string">&#x27;迭代次数: %d\n&#x27;</span>, iter);<br><br><span class="hljs-comment">% 利用DFP算法求解函数的极小点</span><br><br><span class="hljs-comment">% 函数定义</span><br>f = @(x) x(<span class="hljs-number">1</span>)^<span class="hljs-number">2</span> + <span class="hljs-number">2</span>*x(<span class="hljs-number">2</span>)^<span class="hljs-number">2</span> - <span class="hljs-number">4</span>*x(<span class="hljs-number">1</span>) - <span class="hljs-number">2</span>*x(<span class="hljs-number">1</span>)*x(<span class="hljs-number">2</span>);<br><br><span class="hljs-comment">% 梯度定义</span><br>grad_f = @(x) [<span class="hljs-number">2</span>*x(<span class="hljs-number">1</span>) - <span class="hljs-number">4</span> - <span class="hljs-number">2</span>*x(<span class="hljs-number">2</span>); <span class="hljs-number">4</span>*x(<span class="hljs-number">2</span>) - <span class="hljs-number">2</span>*x(<span class="hljs-number">1</span>)];<br><br><span class="hljs-comment">% 初始点和初始近似Hessian矩阵</span><br>x0 = [<span class="hljs-number">1</span>; <span class="hljs-number">1</span>];<br>H0 = <span class="hljs-built_in">eye</span>(<span class="hljs-number">2</span>);<br><br><span class="hljs-comment">% 最大迭代次数和停止迭代的阈值</span><br>max_iter = <span class="hljs-number">100</span>;<br>epsilon = <span class="hljs-number">1e-6</span>;<br><br><span class="hljs-comment">% DFP算法</span><br>x = x0;<br>H = H0;<br>g = grad_f(x);<br>iter = <span class="hljs-number">0</span>;<br><br><span class="hljs-keyword">while</span> norm(g) &gt; epsilon &amp;&amp; iter &lt; max_iter<br>    d = -H * g;  <span class="hljs-comment">% 计算搜索方向</span><br>  <br>    <span class="hljs-comment">% 使用线搜索方法选择合适的步长</span><br>    alpha = <span class="hljs-number">1</span>; <span class="hljs-comment">% 这里可以使用固定步长或者其他线搜索方法</span><br>  <br>    x_new = x + alpha * d;<br>    g_new = grad_f(x_new);<br>    s = x_new - x;<br>    y = g_new - g;<br>  <br>    rho = <span class="hljs-number">1</span> / (y&#x27; * s);<br>    H = (<span class="hljs-built_in">eye</span>(<span class="hljs-number">2</span>) - rho * s * y&#x27;) * H * (<span class="hljs-built_in">eye</span>(<span class="hljs-number">2</span>) - rho * y * s&#x27;) + rho * s * s&#x27;; <span class="hljs-comment">% 更新近似Hessian矩阵</span><br>  <br>    x = x_new;<br>    g = g_new;<br>    iter = iter + <span class="hljs-number">1</span>;<br><span class="hljs-keyword">end</span><br><br><span class="hljs-comment">% 输出结果</span><br>fprintf(<span class="hljs-string">&#x27;最小值点: (%f, %f)\n&#x27;</span>, x(<span class="hljs-number">1</span>), x(<span class="hljs-number">2</span>));<br>fprintf(<span class="hljs-string">&#x27;迭代次数: %d\n&#x27;</span>, iter);<br><br><br></code></pre></td></tr></table></figure><embed src="./opt.pdf" width="100%" height="750" type="application/pdf"><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br></pre></td><td class="code"><pre><code class="hljs matlab"><span class="hljs-comment">%% 第一题</span><br><span class="hljs-comment">% 定义目标函数</span><br>fun = @(x) x(<span class="hljs-number">1</span>)^<span class="hljs-number">2</span> + x(<span class="hljs-number">2</span>)^<span class="hljs-number">2</span>;<br><br><span class="hljs-comment">% 定义约束条件</span><br>A = []; b = []; <span class="hljs-comment">% 无线性不等式约束</span><br>Aeq = [<span class="hljs-number">1</span> <span class="hljs-number">1</span>]; beq = <span class="hljs-number">1</span>; <span class="hljs-comment">% 线性等式约束 x1 + x2 = 1</span><br>lb = [-Inf, -Inf]; ub = [Inf, <span class="hljs-number">1</span>/<span class="hljs-number">4</span>]; <span class="hljs-comment">% x2 &lt;= 1/4</span><br><br><span class="hljs-comment">% 定义初始值</span><br>x0 = [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>];<br><br><span class="hljs-comment">% 调用 fmincon 函数求解</span><br>options = optimoptions(<span class="hljs-string">&#x27;fmincon&#x27;</span>,<span class="hljs-string">&#x27;Display&#x27;</span>,<span class="hljs-string">&#x27;iter&#x27;</span>,<span class="hljs-string">&#x27;Algorithm&#x27;</span>,<span class="hljs-string">&#x27;interior-point&#x27;</span>);<br>[x,fval] = fmincon(fun, x0, A, b, Aeq, beq, lb, ub, [], options);<br><br><span class="hljs-comment">% 输出结果</span><br><span class="hljs-built_in">disp</span>(<span class="hljs-string">&#x27;The solution is:&#x27;</span>), <span class="hljs-built_in">disp</span>(x)<br><span class="hljs-built_in">disp</span>(<span class="hljs-string">&#x27;The minimum value of the objective function is:&#x27;</span>), <span class="hljs-built_in">disp</span>(fval)<br><br><span class="hljs-comment">%% 第二题</span><br><span class="hljs-comment">% 定义目标函数和罚函数</span><br>fun = @(x) x(<span class="hljs-number">1</span>)^<span class="hljs-number">2</span> + x(<span class="hljs-number">2</span>)^<span class="hljs-number">2</span> + x(<span class="hljs-number">3</span>)^<span class="hljs-number">2</span>;<br>g = @(x) <span class="hljs-built_in">abs</span>(x(<span class="hljs-number">1</span>) + <span class="hljs-number">2</span>*x(<span class="hljs-number">2</span>) - x(<span class="hljs-number">3</span>) - <span class="hljs-number">4</span>) + <span class="hljs-built_in">abs</span>(x(<span class="hljs-number">1</span>) - x(<span class="hljs-number">2</span>) + x(<span class="hljs-number">3</span>) + <span class="hljs-number">2</span>);<br>P = @(x, r) fun(x) + r * g(x);<br><br><span class="hljs-comment">% 定义初始值和罚项权重</span><br>x0 = [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>];<br>r = <span class="hljs-number">1</span>;<br><br><span class="hljs-comment">% 使用 fminunc 函数求解</span><br>options = optimoptions(<span class="hljs-string">&#x27;fminunc&#x27;</span>,<span class="hljs-string">&#x27;Display&#x27;</span>,<span class="hljs-string">&#x27;iter&#x27;</span>,<span class="hljs-string">&#x27;Algorithm&#x27;</span>,<span class="hljs-string">&#x27;quasi-newton&#x27;</span>);<br>[x,fval] = fminunc(@(x) P(x, r), x0, options);<br><br><span class="hljs-comment">% 输出结果</span><br><span class="hljs-built_in">disp</span>(<span class="hljs-string">&#x27;The solution is:&#x27;</span>), <span class="hljs-built_in">disp</span>(x)<br><span class="hljs-built_in">disp</span>(<span class="hljs-string">&#x27;The minimum value of the objective function is:&#x27;</span>), <span class="hljs-built_in">disp</span>(fval)<br><br><span class="hljs-comment">%% 第三题</span><br><span class="hljs-comment">% 定义目标函数</span><br>fun = @(x) -(x + <span class="hljs-number">5</span>*<span class="hljs-built_in">sin</span>(<span class="hljs-number">5</span>*x) + <span class="hljs-number">10</span>*<span class="hljs-built_in">cos</span>(<span class="hljs-number">4</span>*x)); <span class="hljs-comment">% 注意我们取负值，因为MATLAB的遗传算法默认是求最小值</span><br><br><span class="hljs-comment">% 定义遗传算法的参数</span><br>numberOfVariables = <span class="hljs-number">1</span>; <br><br><span class="hljs-comment">% 定义约束上下界</span><br>lb = <span class="hljs-number">0</span>; <br>ub = <span class="hljs-number">10</span>;<br><br><span class="hljs-comment">% 使用 ga 函数求解</span><br>[x,fval] = ga(fun,numberOfVariables,[],[],[],[],lb,ub);<br><br><span class="hljs-comment">% 输出结果</span><br><span class="hljs-built_in">disp</span>(<span class="hljs-string">&#x27;The solution is:&#x27;</span>), <span class="hljs-built_in">disp</span>(x)<br><span class="hljs-built_in">disp</span>(<span class="hljs-string">&#x27;The maximum value of the objective function is:&#x27;</span>), <span class="hljs-built_in">disp</span>(-fval)<br><br><span class="hljs-comment">%% 第四题</span><br><span class="hljs-comment">% 定义目标函数和罚函数</span><br>fun = @(x) x(<span class="hljs-number">1</span>)^<span class="hljs-number">2</span> + x(<span class="hljs-number">2</span>)^<span class="hljs-number">2</span>;<br>g = @(x) x(<span class="hljs-number">1</span>) - <span class="hljs-number">2</span>;<br>P = @(x, r) fun(x) - r * <span class="hljs-built_in">log</span>(g(x));<br><br><span class="hljs-comment">% 定义初始值和罚项权重</span><br>x0 = [<span class="hljs-number">3</span>, <span class="hljs-number">0</span>]; <span class="hljs-comment">% 初始值需要满足约束条件</span><br>r = <span class="hljs-number">0.0001</span>;<br><br><span class="hljs-comment">% 使用 fminunc 函数求解</span><br>options = optimoptions(<span class="hljs-string">&#x27;fminunc&#x27;</span>,<span class="hljs-string">&#x27;Display&#x27;</span>,<span class="hljs-string">&#x27;iter&#x27;</span>,<span class="hljs-string">&#x27;Algorithm&#x27;</span>,<span class="hljs-string">&#x27;quasi-newton&#x27;</span>);<br>[x,fval] = fminunc(@(x) P(x, r), x0, options);<br><br><span class="hljs-comment">% 输出结果</span><br><span class="hljs-built_in">disp</span>(<span class="hljs-string">&#x27;The solution is:&#x27;</span>), <span class="hljs-built_in">disp</span>(x)<br><span class="hljs-built_in">disp</span>(<span class="hljs-string">&#x27;The minimum value of the objective function is:&#x27;</span>), <span class="hljs-built_in">disp</span>(fval)<br><br><span class="hljs-comment">%% 第五题</span><br><span class="hljs-comment">% 定义目标函数</span><br>fun = @(x) sum(x.^<span class="hljs-number">2</span>);<br><br><span class="hljs-comment">% 定义参数</span><br>alphas = [<span class="hljs-number">0.001</span>,<span class="hljs-number">0.1</span>,<span class="hljs-number">0.5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">5</span>]; <span class="hljs-comment">% 邻域大小</span><br>T = <span class="hljs-number">100</span>; <span class="hljs-comment">% 初始温度</span><br>T_min = <span class="hljs-number">1e-3</span>; <span class="hljs-comment">% 最小温度</span><br>cooling_rate = <span class="hljs-number">0.95</span>; <span class="hljs-comment">% 冷却率</span><br>max_iter = <span class="hljs-number">100</span>; <span class="hljs-comment">% 每个温度下的最大迭代次数</span><br><br><span class="hljs-comment">% 定义约束条件</span><br>lb = <span class="hljs-number">-15</span> * <span class="hljs-built_in">ones</span>(<span class="hljs-number">1</span>, <span class="hljs-number">10</span>); <span class="hljs-comment">% 下界</span><br>ub = <span class="hljs-number">15</span> * <span class="hljs-built_in">ones</span>(<span class="hljs-number">1</span>, <span class="hljs-number">10</span>); <span class="hljs-comment">% 上界</span><br><br>results = <span class="hljs-built_in">zeros</span>(<span class="hljs-built_in">length</span>(alphas), <span class="hljs-number">2</span>);<br><br><span class="hljs-keyword">for</span> a = <span class="hljs-number">1</span>:<span class="hljs-built_in">length</span>(alphas)<br>    <span class="hljs-comment">% 初始化解</span><br>    x = lb + (ub - lb) .* <span class="hljs-built_in">rand</span>(<span class="hljs-number">1</span>, <span class="hljs-number">10</span>);<br>    alpha = alphas(a);<br><br>    T_current = T;<br>    <span class="hljs-keyword">while</span> T_current &gt; T_min<br>        <span class="hljs-keyword">for</span> <span class="hljs-built_in">i</span> = <span class="hljs-number">1</span>:max_iter<br>            <span class="hljs-comment">% 在邻域中随机生成新解</span><br>            x_new = x + alpha * (<span class="hljs-number">2</span>*<span class="hljs-built_in">rand</span>(<span class="hljs-number">1</span>, <span class="hljs-number">10</span>) - <span class="hljs-number">1</span>);<br>            x_new = <span class="hljs-built_in">max</span>(<span class="hljs-built_in">min</span>(x_new, ub), lb); <span class="hljs-comment">% 确保新解满足约束条件</span><br><br>            <span class="hljs-comment">% 计算目标函数的改变量</span><br>            delta_f = fun(x_new) - fun(x);<br><br>            <span class="hljs-comment">% 如果新解更好，或者满足 Metropolis 准则，则接受新解</span><br>            <span class="hljs-keyword">if</span> delta_f &lt; <span class="hljs-number">0</span> || <span class="hljs-built_in">rand</span>() &lt; <span class="hljs-built_in">exp</span>(-delta_f / T_current)<br>                x = x_new;<br>            <span class="hljs-keyword">end</span><br>        <span class="hljs-keyword">end</span><br><br>        <span class="hljs-comment">% 降低温度</span><br>        T_current = cooling_rate * T_current;<br>    <span class="hljs-keyword">end</span><br><br>    <span class="hljs-comment">% 记录结果</span><br>    results(a, :) = [alpha, fun(x)];<br><br>    <span class="hljs-comment">% 输出结果</span><br>    fprintf(<span class="hljs-string">&#x27;For alpha = %f:\n&#x27;</span>, alpha);<br>    <span class="hljs-built_in">disp</span>(<span class="hljs-string">&#x27;The solution is:&#x27;</span>), <span class="hljs-built_in">disp</span>(x)<br>    <span class="hljs-built_in">disp</span>(<span class="hljs-string">&#x27;The minimum value of the objective function is:&#x27;</span>), <span class="hljs-built_in">disp</span>(fun(x))<br><span class="hljs-keyword">end</span><br><br><span class="hljs-comment">% 可视化结果</span><br><span class="hljs-built_in">figure</span>;<br>semilogy(results(:, <span class="hljs-number">1</span>), results(:, <span class="hljs-number">2</span>), <span class="hljs-string">&#x27;-o&#x27;</span>, <span class="hljs-string">&#x27;Color&#x27;</span>, [<span class="hljs-number">0.2</span> <span class="hljs-number">0.4</span> <span class="hljs-number">0.6</span>], <span class="hljs-string">&#x27;LineWidth&#x27;</span>, <span class="hljs-number">2</span>, <span class="hljs-string">&#x27;MarkerSize&#x27;</span>, <span class="hljs-number">8</span>);<br>xlabel(<span class="hljs-string">&#x27;Alpha&#x27;</span>);<br>ylabel(<span class="hljs-string">&#x27;Minimum Value of Objective Function&#x27;</span>);<br>title(<span class="hljs-string">&#x27;Impact of Alpha on the Solution&#x27;</span>);<br><br><br><br></code></pre></td></tr></table></figure><embed src="./opt2.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    <summary type="html">&lt;p&gt;期中与期末，&lt;a href=&quot;https://enderfga.cn/markmap.html&quot;&gt;笔记1链接&lt;/a&gt;；&lt;a href=&quot;https://enderfga.cn/markmap2.html&quot;&gt;笔记2链接&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img.enderfga.cn/img/image-20230531154210275.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    
    <category term="最优化理论" scheme="http://enderfga.cn/tags/%E6%9C%80%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title>3D论文PPT</title>
    <link href="http://enderfga.cn/2023/03/23/3dppt/"/>
    <id>http://enderfga.cn/2023/03/23/3dppt/</id>
    <published>2023-03-23T11:25:47.000Z</published>
    <updated>2023-08-11T05:28:58.017Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>科研笔记</p><span id="more"></span><p><img src="https://img.enderfga.cn/img/image-20230323192941643.png" alt="image-20230323192941643"></p><embed src="./3d.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    <summary type="html">&lt;p&gt;科研笔记&lt;/p&gt;</summary>
    
    
    
    
    <category term="计算机视觉" scheme="http://enderfga.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>RLHF</title>
    <link href="http://enderfga.cn/2023/03/01/RLHF/"/>
    <id>http://enderfga.cn/2023/03/01/RLHF/</id>
    <published>2023-03-01T03:17:42.000Z</published>
    <updated>2023-03-01T03:19:08.223Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="RLHF"><a href="#RLHF" class="headerlink" title="RLHF"></a>RLHF</h1><h2 id="Aligning-Text-to-Image-Models-using-Human-Feedback"><a href="#Aligning-Text-to-Image-Models-using-Human-Feedback" class="headerlink" title="Aligning Text-to-Image Models using Human Feedback"></a>Aligning Text-to-Image Models using Human Feedback</h2><ul><li><p>Google Research ,University of California</p></li><li><p>2023.2.23</p><span id="more"></span></li></ul><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>深度生成模型在文本到图像合成方面取得了令人印象深刻的成果，但当前的文本到图像模型往往生成与文本提示不够相符的图像。</p><p>本文的动机是改进文本到图像合成模型，使其能够更好地与文本提示对齐。</p><p>作者的方法比预训练模型更准确地生成具有指定颜色、计数和背景的对象。</p><h2 id="Proposal"><a href="#Proposal" class="headerlink" title="Proposal"></a>Proposal</h2><ul><li>提出了一种简单而有效的微调方法，用于使用人类反馈对文本到图像模型进行对齐。</li><li>使用人类反馈进行微调可以显着提高文本到图像模型的图像文本对齐，在人类评估中，我们的模型在图像文本对齐方面达到了高达47％的改善，但图像保真度略有降低。</li><li>学习的奖励函数比CLIP分数更准确地预测了人类对质量的评估。</li><li>基于作者学习的奖励函数的采样也可以显着改善图像文本对齐。</li></ul><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><ul><li>T2I models</li><li>Evaluating image-text alignment</li><li>Learning with human feedback</li></ul><p>与先前关注利用人类反馈改善语言模型和RL代理的工作相比，该工作探索了使用人类反馈来调整多模式文本到图像模型与人类偏好的方法。许多关于利用人类反馈学习的先前工作都包括学习一个奖励函数并最大化奖励加权可能性（通常被称为监督微调）。受其成功的启发，作者提出了一种利用人类反馈进行微调的方法来改善文本到图像模型。</p><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p><img src="https://img.enderfga.cn/img/image-20230228130311025.png" alt=""></p><p>包括三个阶段：</p><ol><li>首先从一系列文本提示中生成一组不同的图像，这些文本提示旨在测试文本到图像模型的各种功能。 </li><li>然后，人类评级者对这些图像提供二进制反馈。 </li><li>接下来，训练一个奖励模型，以文本提示和图像作为输入来预测人类反馈。</li><li>最后，我们使用奖励加权对数似然度来微调文本到图像模型，以改善文本图像对齐。</li></ol><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p>实验部分旨在测试人类反馈参与模型微调的有效性。实验用到的模型为 Stable Diffusion v1.5</p><p><img src="https://img.enderfga.cn/img/image-20230228132659591.png" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20230228132741226.png" alt=""></p><p>本文方法显著提高了图像 - 文本对齐，具体来说，模型生成的图像中有 50% 的样本获得至少三分之二的赞成票（投票数量为 7 票或更多赞成票），然而，微调会稍微降低图像保真度（15% 比 10%）。</p><p><img src="https://img.enderfga.cn/img/image-20230228132824327.png" alt=""></p><p>本文模型生成的图像符合 prompt 指定的颜色、计数和背景。值得注意的是，本文模型还能生成没有见过的文本 prompt 图像，并且质量非常高</p><p><img src="https://img.enderfga.cn/img/image-20230228132908052.png" alt=""></p><p>有奖励（绿色）比 CLIP 分数（红色）更符合典型的人类意图。</p><h2 id="Limitations-and-future-directions"><a href="#Limitations-and-future-directions" class="headerlink" title="Limitations and future directions"></a>Limitations and future directions</h2><ol><li><p><strong>更细致的人类反馈</strong>，存在一些较差的生成，如高饱和度的图像颜色，指示评级者寻找更多样化的失败模式（过度饱和的颜色，不切实际的动物解剖学，物理违规等）将提高这些方面的性能。</p></li><li><p><strong>多样化和大型人类数据集</strong>，为了简化问题，作者考虑了有限的文本类别（计数，颜色，背景），因此人类反馈也相对简单（好或坏）。由于这一点，人类反馈数据的多样性有限。将其扩展到更主观的文本类别（如艺术创作）和更细致的人类反馈将是未来研究的重要方向。</p></li><li><p><strong>不同的目标和算法</strong>，为了更新文本到图像模型，作者使用奖励加权的最大似然。然而，与语言领域的先前工作类似，使用RL算法将是一个有趣的方向。作者相信RLHF微调可能会产生更好的模型，因为</p><p>（a）在更新期间使用在线样本生成</p><p>（b）KL正则化可以减轻对奖励函数的过度拟合。</p></li></ol>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;RLHF&quot;&gt;&lt;a href=&quot;#RLHF&quot; class=&quot;headerlink&quot; title=&quot;RLHF&quot;&gt;&lt;/a&gt;RLHF&lt;/h1&gt;&lt;h2 id=&quot;Aligning-Text-to-Image-Models-using-Human-Feedback&quot;&gt;&lt;a href=&quot;#Aligning-Text-to-Image-Models-using-Human-Feedback&quot; class=&quot;headerlink&quot; title=&quot;Aligning Text-to-Image Models using Human Feedback&quot;&gt;&lt;/a&gt;Aligning Text-to-Image Models using Human Feedback&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Google Research ,University of California&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;2023.2.23&lt;/p&gt;</summary>
    
    
    
    
    <category term="计算机视觉" scheme="http://enderfga.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>Realfusion</title>
    <link href="http://enderfga.cn/2023/03/01/Realfusion/"/>
    <id>http://enderfga.cn/2023/03/01/Realfusion/</id>
    <published>2023-03-01T03:14:27.000Z</published>
    <updated>2023-03-01T03:16:21.212Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="RealFusion："><a href="#RealFusion：" class="headerlink" title="RealFusion："></a>RealFusion：</h1><h3 id="360°-Reconstruction-of-Any-Object-from-a-Single-Image"><a href="#360°-Reconstruction-of-Any-Object-from-a-Single-Image" class="headerlink" title="360° Reconstruction of Any Object from a Single Image"></a>360° Reconstruction of Any Object from a Single Image</h3><ul><li><p>Oxford University</p></li><li><p>2023.2.23</p><span id="more"></span><h2 id="Demo"><a href="#Demo" class="headerlink" title="Demo"></a>Demo</h2><p><a href="https://lukemelas.github.io/realfusion/">https://lukemelas.github.io/realfusion/</a></p><p><img src="https://img.enderfga.cn/img/splash-figure-v2.png" alt="Examples"></p></li></ul><h2 id="Motivation-Single-View-3D-Reconstruction"><a href="#Motivation-Single-View-3D-Reconstruction" class="headerlink" title="Motivation: Single-View 3D Reconstruction"></a>Motivation: Single-View 3D Reconstruction</h2><ul><li>Reconstructing the 3D structure of an object from a single 2D view is a fundamental challenge in computer vision.</li><li>In the case of a single view, the reconstruction problem is highly ill-posed.<br>As a result, the task requires semantic understanding obtained by learning.<br>Despite the difficulty of this task, humans are adept at using a range of monocular cues to infer the 3D structures of objects from single views.</li></ul><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><h3 id="Category-level-3D-Reconstruction"><a href="#Category-level-3D-Reconstruction" class="headerlink" title="Category-level 3D Reconstruction"></a>Category-level 3D Reconstruction</h3><ul><li>Most prior work tackles the problem of category-specific single-view 3D reconstruction by training a category-level reconstruction model.</li><li>The work: Going beyond category-level 3D reconstruction<ul><li>This work aims to go beyond category-specific images to images of arbitrary objects. This setting is highly challenging, but humans perform it effortlessly when they observe new objects.</li></ul></li></ul><h3 id="Single-View-3D-Reconstruction"><a href="#Single-View-3D-Reconstruction" class="headerlink" title="Single-View 3D Reconstruction"></a>Single-View 3D Reconstruction</h3><ul><li>Arbitrary-object 3D reconstruction has been challenging because the problem fundamentally requires the use of large-scale 3D priors over object shapes, which have not been available.</li><li>With the recent rise of large-scale pretraining, this problem has become tractable.<br>Examples include:<ul><li>Contrastive: CLIP</li><li>Autoregressive: DALL-E / Parti</li><li>Diffusion Models: DALL-E 2 / Imagen / Stable Diffusion</li></ul></li><li>These pretrained models may be used as priors for a variety of vision tasks, and we are particularly interested in 3D reconstruction.<ul><li>At a high level, you can think of these models as a tool for optimizing the realism of an input image.</li></ul></li><li>In this way, they enable an elegant approach to 3D generation and reconstruction: using these large-scale pretrained models to enforce that a differentiable scene looks realistic from random views.</li></ul><h2 id="Proposal"><a href="#Proposal" class="headerlink" title="Proposal"></a>Proposal</h2><p>(1) We propose <strong>RealFusion</strong>, a method that can extract from a single image of an object a 360◦ photographic 3D reconstruction without assumptions on the type of object imaged or 3D supervision of any kind; </p><p>(2) We do so by leveraging an existing 2D <strong>diffusion image generator</strong> via a new single image variant of textual inversion; </p><p>(3) We also introduce new regularizers and provide an efficient implementation using <strong>InstantNGP</strong>; </p><p>(4) We demonstrate <strong>state-of-the-art</strong> reconstruction results on a number of in-the-wild images and images from existing datasets when compared to alternative approaches.  </p><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><ul><li>Image-based reconstruction of appearnce and geometry</li><li>Few-view reconstruction</li><li>Single-view reconstruction  </li><li>Extracting 3D models from 2D generators  </li><li>Diffusion Models  </li></ul><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p><img src="https://img.enderfga.cn/img/image-20230227112154724.png" alt=""></p><ul><li>This approach forms the backbone of our method, RealFusion.<br>(0) [Init] We are given a single image and a function $\boldsymbol{p}_{\text {prior }}(\cdot)$ which computes the likelihood of an input image $\boldsymbol{I}$. We choose a camera view and represent our scene with a differentiably-renderable representation $\boldsymbol{x}$, for example a NeRF.<br>(1) [Reconstruction] We render $\boldsymbol{x}$ from our given view and minimize the loss with respect to the real input image $\mathbf{I}$.<br>(2) [Prior] We render images $\boldsymbol{I}_{\text {prior }}$ of $\boldsymbol{x}$ from randomly-chosen views on a hemisphere surrounding the origin, and we optimize $\boldsymbol{p}_{\text {prior }}\left(\boldsymbol{I}_{\text {priol }}\right)$ to enforce that $\boldsymbol{x}$ looks realistic from all directions.</li><li>Prior work has explored this question in the domain of 3D generation<ul><li>Dreamfields: CLIP prior</li><li>DreamFusion: Diffusion model prior</li></ul></li><li>In our work, we adopt a diffusion model prior using Stable Diffusion, a text-conditional latent diffusion model.</li><li>As currently stated, our set up combines a reconstruction objective with a latent diffusion-based prior objective, which is conditioned on a manual text prompt (e.g. “An image of a fish.”)</li><li>However, we found that these results were lacking.</li><li>In particular, the 3D shapes that are generated look like the input object from the input view, but do not look like the input object from other views.</li><li>To fix this, we need to modify the prior to place a high likelihood on our input object, rather than a generic object with the same description.</li><li>We do so by performing textual inversion.<ul><li>We optimize a text embedding $\mathbf{e}$ in the text encoder of the diffusion model to match our input image.</li><li>Usually textual inversion is performed with multiple views of an object, but we substitute these views with heavy image augmentations.</li></ul></li><li>We also add other pieces of regularization:<br>(1) A regularization on rendered normals<br>(2) A coarse-to-fine training setup</li><li>However, the key piece of the puzzle is the textual inversion.</li></ul><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p><img src="https://img.enderfga.cn/img/image-20230227165614420.png" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20230227165925713.png" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20230227165956615.png" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20230227170021757.png" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20230227170047165.png" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20230227170111552.png" alt=""></p><h2 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h2><ul><li>Requires per-image optimization<ul><li>Both the textual inversion and the 3D optimization procedure must be performed separately for each input image.</li><li>As a result, the process is relatively slow and difficult to apply to large datasets</li></ul></li><li>In some cases, reconstruction fails to produce a solid shape<ul><li>Perhaps this could be alleviated with better inductive biases or regularization terms</li></ul></li><li>In some cases, reconstruction produces two-headed objects<ul><li>This is known as the Janus Problem</li></ul></li></ul>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;RealFusion：&quot;&gt;&lt;a href=&quot;#RealFusion：&quot; class=&quot;headerlink&quot; title=&quot;RealFusion：&quot;&gt;&lt;/a&gt;RealFusion：&lt;/h1&gt;&lt;h3 id=&quot;360°-Reconstruction-of-Any-Object-from-a-Single-Image&quot;&gt;&lt;a href=&quot;#360°-Reconstruction-of-Any-Object-from-a-Single-Image&quot; class=&quot;headerlink&quot; title=&quot;360° Reconstruction of Any Object from a Single Image&quot;&gt;&lt;/a&gt;360° Reconstruction of Any Object from a Single Image&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Oxford University&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;2023.2.23&lt;/p&gt;</summary>
    
    
    
    
    <category term="计算机视觉" scheme="http://enderfga.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>Outstanding Wordle</title>
    <link href="http://enderfga.cn/2023/02/22/wordle/"/>
    <id>http://enderfga.cn/2023/02/22/wordle/</id>
    <published>2023-02-22T13:38:54.000Z</published>
    <updated>2023-02-22T13:49:05.366Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>唯一一次认真参加数学建模，虽然曾经的我确实不喜欢这类赛事。假如拿到O/F奖了就考虑写写经验或者录个视频，捞了就当我没说。</p><span id="more"></span><embed src="./wordle.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    <summary type="html">&lt;p&gt;唯一一次认真参加数学建模，虽然曾经的我确实不喜欢这类赛事。假如拿到O/F奖了就考虑写写经验或者录个视频，捞了就当我没说。&lt;/p&gt;</summary>
    
    
    
    
    <category term="数学建模" scheme="http://enderfga.cn/tags/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/"/>
    
  </entry>
  
  <entry>
    <title>亚太数模论文</title>
    <link href="http://enderfga.cn/2023/02/20/apmcm/"/>
    <id>http://enderfga.cn/2023/02/20/apmcm/</id>
    <published>2023-02-20T11:02:17.000Z</published>
    <updated>2023-02-20T11:05:59.145Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>这么烂的学术垃圾都能拿奖，这比赛确实没含金量</p><span id="more"></span><embed src="./apmcm.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    <summary type="html">&lt;p&gt;这么烂的学术垃圾都能拿奖，这比赛确实没含金量&lt;/p&gt;</summary>
    
    
    
    
    <category term="数学建模" scheme="http://enderfga.cn/tags/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/"/>
    
  </entry>
  
  <entry>
    <title>MAV3D:Text-To-4D Dynamic Scene Generation</title>
    <link href="http://enderfga.cn/2023/02/11/mav/"/>
    <id>http://enderfga.cn/2023/02/11/mav/</id>
    <published>2023-02-11T15:42:41.000Z</published>
    <updated>2023-02-11T15:51:26.196Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>论文阅读笔记</p><span id="more"></span><div style="text-align:center"><iframe width="100%" height="500" src="https://player.bilibili.com/player.html?aid=479164157&bvid=BV1aT411R77Z&cid=1003655006&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe></iframe></div><h1 id="MAV3D-Text-To-4D-Dynamic-Scene-Generation"><a href="#MAV3D-Text-To-4D-Dynamic-Scene-Generation" class="headerlink" title="MAV3D:Text-To-4D Dynamic Scene Generation"></a>MAV3D:Text-To-4D Dynamic Scene Generation</h1><ul><li><strong>Meta AI</strong></li><li><p><strong>2023.1.26</strong></p><h2 id="Demo"><a href="#Demo" class="headerlink" title="Demo"></a>Demo</h2><p><a href="https://make-a-video3d.github.io/">https://make-a-video3d.github.io/</a></p><video src="./rotating_grid.mp4"></video></li></ul><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><ol><li><strong>需要一个有效的、端到端可学习的动态三维场景表征；</strong></li><li><strong>需要一个有监督学习的数据源，因为目前并不存在大规模的（文本，4D）对的数据集可供学习；</strong></li><li><strong>需要在空间和时间维度上扩展输出的分辨率，因为4D输出需要大量的内存和计算能力；</strong></li></ol><h2 id="Proposal"><a href="#Proposal" class="headerlink" title="Proposal"></a>Proposal</h2><ol><li><strong>本文提出了MAV3D，利用了T2V模型和动态NeRFs，实现从自然语言描述生成动态三维时间表示；</strong></li><li><strong>提出了一个从静态到动态的多阶段优化方案，逐步纳入静态、时间和超分辨率模型的梯度信息。</strong></li></ol><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><h3 id="dynamic-NeRFs"><a href="#dynamic-NeRFs" class="headerlink" title="dynamic NeRFs"></a>dynamic NeRFs</h3><p><strong>适用于动态场景的NeRF变体</strong></p><h3 id="MAV"><a href="#MAV" class="headerlink" title="MAV"></a>MAV</h3><p><strong>Make A Video，通过在未标记的视频上训练，拓展了文本到图像（T2I）模型。</strong></p><h3 id="DreamFusion"><a href="#DreamFusion" class="headerlink" title="DreamFusion"></a>DreamFusion</h3><p><strong>以NeRF的形式从文本描述中学习3D表示，提出了一个基于概率密度蒸馏的loss（SDS）</strong></p><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p><img src="https://img.enderfga.cn/img/image-20230209142352394.png" alt=""></p><h3 id="4D-Scene-Representation"><a href="#4D-Scene-Representation" class="headerlink" title="4D Scene Representation"></a>4D Scene Representation</h3><p><img src="https://img.enderfga.cn/img/image-20230209141807225.png" alt=""></p><script type="math/tex; mode=display">\left(\tau, c_i\right)=f_\theta(x, y, z, t)</script><script type="math/tex; mode=display">\left[P_{x y}^{X Y R_1}+P_{z t}^{Z T R_1} ; P_{x z}^{X Z R_2}+P_{y t}^{Y T R_2} ; P_{y z}^{Y Z R_3}+P_{y z}^{X T R_3}\right]</script><h3 id="Dynamic-Scene-Optimization"><a href="#Dynamic-Scene-Optimization" class="headerlink" title="Dynamic Scene Optimization"></a>Dynamic Scene Optimization</h3><h4 id="为了监督4D场景与文本提示p匹配，引入SDS-T（temporal-Score-Distillation-Sampling-）"><a href="#为了监督4D场景与文本提示p匹配，引入SDS-T（temporal-Score-Distillation-Sampling-）" class="headerlink" title="为了监督4D场景与文本提示p匹配，引入SDS-T（temporal Score Distillation Sampling  ）"></a>为了监督4D场景与文本提示p匹配，引入SDS-T（temporal Score Distillation Sampling  ）</h4><script type="math/tex; mode=display">\nabla_\theta \mathcal{L}_{S D S-T}=E_{\sigma, \epsilon}\left[w(\sigma)\left(\hat{\epsilon}\left(V_{(\bar{\theta}, \sigma, \epsilon)} \mid y, \sigma\right)-\epsilon\right) \frac{\partial V_\theta}{\partial \theta}\right]\\</script><script type="math/tex; mode=display">\nabla_\theta \mathcal{L}_{\mathrm{SDS}}(\phi, \mathbf{x}=g(\theta)) \triangleq \mathbb{E}_{t, \epsilon}\left[w(t)\left(\hat{\epsilon}_\phi\left(\mathbf{z}_t ; y, t\right)-\epsilon\right) \frac{\partial \mathbf{x}}{\partial \theta}\right]</script><h4 id="从静态到动态的场景优化"><a href="#从静态到动态的场景优化" class="headerlink" title="从静态到动态的场景优化"></a>从静态到动态的场景优化</h4><h4 id="动态相机"><a href="#动态相机" class="headerlink" title="动态相机"></a>动态相机</h4><h4 id="FPS-采样"><a href="#FPS-采样" class="headerlink" title="FPS 采样"></a>FPS 采样</h4><h4 id="高斯退火"><a href="#高斯退火" class="headerlink" title="高斯退火"></a>高斯退火</h4><h4 id="全变分损失"><a href="#全变分损失" class="headerlink" title="全变分损失"></a>全变分损失</h4><h3 id="Super-Resolution-Fine-Tuning"><a href="#Super-Resolution-Fine-Tuning" class="headerlink" title="Super-Resolution Fine-Tuning"></a>Super-Resolution Fine-Tuning</h3><p><img src="https://img.enderfga.cn/img/image-20230209152345831.png" alt=""></p><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p><strong>Metrics</strong>：R-Precision and human preference</p><p><img src="https://img.enderfga.cn/img/image-20230209152747908.png" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20230209152759865.png" alt=""></p><h2 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h2><ul><li><strong>将动态NeRFs转换为实时应用的不连续网格序列的效率很低，如果能直接预测顶点的轨迹，就能得到改善。</strong></li><li><strong>利用超分辨率信息提高了表示的质量，但对于更高细节的纹理还需要进一步改进。</strong></li><li><strong>文本到四维动态场景生成的表示质量取决于T2V模型从不同视角生成视频的能力。</strong></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;论文阅读笔记&lt;/p&gt;</summary>
    
    
    
    
    <category term="计算机视觉" scheme="http://enderfga.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>Test to 3d随笔</title>
    <link href="http://enderfga.cn/2023/02/09/3d/"/>
    <id>http://enderfga.cn/2023/02/09/3d/</id>
    <published>2023-02-09T08:51:10.000Z</published>
    <updated>2023-02-09T08:53:02.663Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>随便记的笔记</p><span id="more"></span><h1 id="Text-to-3D"><a href="#Text-to-3D" class="headerlink" title="Text-to-3D"></a>Text-to-3D</h1><h2 id="NeRF-Representing-Scenes-as-Neural-Radiance-Fields-for-View-Synthesis"><a href="#NeRF-Representing-Scenes-as-Neural-Radiance-Fields-for-View-Synthesis" class="headerlink" title="NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis"></a>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</h2><p><img src="https://img.enderfga.cn/img/ed4df06e919cae0e638015fa78d935eb_1_Figure_1.png" alt=""></p><p><strong>输入为连续的5维坐标（xyz坐标，以及视野角度theta和phi）；输出是空间位置的体密度以及该位置的发射射线（这里射线是根据视角变化的）。</strong></p><ol><li><strong>用 network 存体素信息: </strong>(x, y, z, \theta, \phi) \rightarrow(\mathbf{c}, \sigma)</li><li><strong>然后用体素渲染方程获得生成视角图片：光线采样+积分</strong><script type="math/tex; mode=display">C(\mathbf{r})=\int_{t_n}^{t_f} T(t) \sigma(\mathbf{r}(t)) \mathbf{c}(\mathbf{r}(t), \mathbf{d}) d t, \text { where } T(t)=\exp \left(-\int_{t_n}^t \sigma(\mathbf{r}(s)) d s\right)</script></li><li><strong>最后与原视角图片计算损失更新网络</strong></li></ol><h2 id="DreamFusion-Text-to-3D-using-2D-Diffusion"><a href="#DreamFusion-Text-to-3D-using-2D-Diffusion" class="headerlink" title="DreamFusion: Text-to-3D using 2D Diffusion"></a>DreamFusion: Text-to-3D using 2D Diffusion</h2><p><img src="https://img.enderfga.cn/img/image-20230202213651008.png" alt=""></p><p><strong>三维合成并不存在大规模的标注数据，也没有一个高效的模型架构对3D数据进行降噪</strong></p><p><strong>使用NERF的格式，使用预训练的text to 2d，加上他们提出的一个基于概率密度蒸馏的loss，证明了预训练图像扩散模型作为先验模型的有效性</strong></p><h2 id="Magic3D-High-Resolution-Text-to-3D-Content-Creation"><a href="#Magic3D-High-Resolution-Text-to-3D-Content-Creation" class="headerlink" title="Magic3D: High-Resolution Text-to-3D Content Creation"></a>Magic3D: High-Resolution Text-to-3D Content Creation</h2><p><img src="https://img.enderfga.cn/img/f3fcff88aa23d692c243bda5b3dd5467_3_Figure_2_1114637308.png" alt=""></p><p><strong>用一个两阶段的优化框架来提高速度和分辨率：利用低分辨率的扩散先验获得一个粗略的模型，并以稀疏的三维哈希网格结构加速。使用粗略表示作为初始化，进一步优化纹理三维网格模型，用高效的可微分渲染器与高分辨率的stable diffusion模型交互。</strong></p><h2 id="Point-E-A-System-for-Generating-3D-Point-Clouds-from-Complex-Prompts"><a href="#Point-E-A-System-for-Generating-3D-Point-Clouds-from-Complex-Prompts" class="headerlink" title="Point-E: A System for Generating 3D Point Clouds from Complex Prompts"></a>Point-E: A System for Generating 3D Point Clouds from Complex Prompts</h2><p><img src="https://img.enderfga.cn/img/v2-b0ca9d9f44550ec6ab34dfe21797ea7e_720w.webp" alt=""></p><p><strong>不输出传统意义上的 3D 图像，它会生成点云，或空间中代表 3D 形状的离散数据点集</strong></p><p><strong>点云更容易合成，但它们无法捕获对象的细粒度形状或纹理，训练了一个额外的人工智能系统来将 Point-E 的点云转换为网格</strong></p><p><strong>算力和时间需求小 但质量差</strong></p><h2 id="Dream3D-Zero-Shot-Text-to-3D-Synthesis-Using-3D-Shape-Prior-and-Text-to-Image-Diffusion-Models"><a href="#Dream3D-Zero-Shot-Text-to-3D-Synthesis-Using-3D-Shape-Prior-and-Text-to-Image-Diffusion-Models" class="headerlink" title="Dream3D: Zero-Shot Text-to-3D Synthesis Using 3D Shape Prior and Text-to-Image Diffusion Models"></a>Dream3D: Zero-Shot Text-to-3D Synthesis Using 3D Shape Prior and Text-to-Image Diffusion Models</h2><p><img src="https://img.enderfga.cn/img/image-20230202234404950.png" alt=""></p><p><strong>引入一个显式3D先验形状，来优化CLIP引导的3D优化任务。具体的讲，首先在文本到形状转换时，使用输入文本生成了一个质量的3D形状来作为先验知识。然后使用它来初始化神经辐射场，并使用完整prompt进行优化</strong></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;随便记的笔记&lt;/p&gt;</summary>
    
    
    
    
    <category term="笔记" scheme="http://enderfga.cn/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>三维手势姿态估计算法研究</title>
    <link href="http://enderfga.cn/2023/01/13/nyu/"/>
    <id>http://enderfga.cn/2023/01/13/nyu/</id>
    <published>2023-01-13T14:08:50.000Z</published>
    <updated>2023-01-13T14:19:04.839Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>人工智能原理实验期末项目</p><span id="more"></span><embed src="./nyu.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    <summary type="html">&lt;p&gt;人工智能原理实验期末项目&lt;/p&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://enderfga.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Mind-Diffusion</title>
    <link href="http://enderfga.cn/2023/01/13/diff/"/>
    <id>http://enderfga.cn/2023/01/13/diff/</id>
    <published>2023-01-13T14:04:03.000Z</published>
    <updated>2023-01-13T14:19:04.833Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>计算机视觉之diffusion model with mindspore</p><span id="more"></span><embed src="./diff.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    <summary type="html">&lt;p&gt;计算机视觉之diffusion model with mindspore&lt;/p&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://enderfga.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>基于 JMAG 软件的电机仿真分析</title>
    <link href="http://enderfga.cn/2023/01/13/mach/"/>
    <id>http://enderfga.cn/2023/01/13/mach/</id>
    <published>2023-01-13T13:59:11.000Z</published>
    <updated>2023-01-13T14:19:04.835Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>压根没上过课，不知道这写的是啥</p><span id="more"></span><embed src="./mach.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    <summary type="html">&lt;p&gt;压根没上过课，不知道这写的是啥&lt;/p&gt;</summary>
    
    
    
    
    <category term="电机与拖动技术" scheme="http://enderfga.cn/tags/%E7%94%B5%E6%9C%BA%E4%B8%8E%E6%8B%96%E5%8A%A8%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title>智能车协同实验</title>
    <link href="http://enderfga.cn/2023/01/13/car/"/>
    <id>http://enderfga.cn/2023/01/13/car/</id>
    <published>2023-01-13T13:52:04.000Z</published>
    <updated>2023-01-13T14:19:04.830Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>多智能体集群控制技术智能车实验报告</p><span id="more"></span><embed src="./1.pdf" width="100%" height="750" type="application/pdf"><embed src="./2.pdf" width="100%" height="750" type="application/pdf"><embed src="./3.pdf" width="100%" height="750" type="application/pdf"><embed src="./4.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    <summary type="html">&lt;p&gt;多智能体集群控制技术智能车实验报告&lt;/p&gt;</summary>
    
    
    
    
    <category term="多智能体集群" scheme="http://enderfga.cn/tags/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E9%9B%86%E7%BE%A4/"/>
    
  </entry>
  
  <entry>
    <title>四旋翼集群编队</title>
    <link href="http://enderfga.cn/2023/01/13/multi/"/>
    <id>http://enderfga.cn/2023/01/13/multi/</id>
    <published>2023-01-13T13:37:11.000Z</published>
    <updated>2023-01-14T12:23:24.113Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>多智能体集群控制技术期末项目报告</p><span id="more"></span><h1 id="单积分模型SYSU编队设计"><a href="#单积分模型SYSU编队设计" class="headerlink" title="单积分模型SYSU编队设计"></a>单积分模型SYSU编队设计</h1><p><img src="https://img.enderfga.cn/img/SI.gif" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20230113213938616.png" alt=""></p><h1 id="控制四旋翼飞行器实现编队方式设计"><a href="#控制四旋翼飞行器实现编队方式设计" class="headerlink" title="控制四旋翼飞行器实现编队方式设计"></a>控制四旋翼飞行器实现编队方式设计</h1><p><img src="https://img.enderfga.cn/img/test.gif" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20230113214008450.png" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20230113214017680.png" alt=""></p><ul><li><p>Position control</p><script type="math/tex; mode=display">\begin{array}{ll}\text { PID: } & \ddot{\boldsymbol{p}}_{i, c}=\ddot{\boldsymbol{p}}_i{ }^{d e s}+K_{d, i}\left(\dot{\boldsymbol{p}}_i{ }^{\text {des }}-\dot{\boldsymbol{p}}_i\right)+K_{p, i}\left(\boldsymbol{p}_i{ }^{\text {des }}-\boldsymbol{p}_i\right) \\\text { Model: } & u_1=m\left(g+\ddot{\boldsymbol{p}}_{3, c}\right) \quad \text { (Newton Equation) } \\& \phi_c=\frac{1}{g}\left(\ddot{\boldsymbol{p}}_{1, c} \sin \Psi-\ddot{\boldsymbol{p}}_{2, c} \cos \psi \right) \quad \theta_c=\frac{1}{g}\left(\ddot{\boldsymbol{p}}_{1, c} \cos \Psi+\ddot{\boldsymbol{p}}_{2, c} \sin \Psi\right)\end{array}</script></li><li>Attitude control</li></ul><script type="math/tex; mode=display">PID: \quad\left[\begin{array}{c}\ddot{\phi}_c \\ \ddot{\theta}_c \\ \ddot{\psi}_c\end{array}\right]=\left[\begin{array}{c}K_{p, \phi}\left(\phi_c-\phi\right)+K_{d, \phi}\left(\dot{\phi}_c-\dot{\phi}\right) \\ K_{p, \theta}\left(\theta_c-\theta\right)+K_{d, \phi}\left(\dot{\theta}_c-\dot{\theta}\right) \\ K_{p, \psi}\left(\psi_c-\psi\right)+K_{d, \psi}\left(\dot{\psi}_c-\dot{\psi}\right)\end{array}\right]</script><script type="math/tex; mode=display">Model: \quad \boldsymbol{u}_2=\boldsymbol{I} \cdot\left[\begin{array}{c}\ddot{\phi}_c \\ \ddot{\theta}_c \\ \ddot{\psi}_c\end{array}\right]+\left[\begin{array}{c}\omega_x \\ \omega_y \\ \omega_z\end{array}\right] \times \boldsymbol{I} \cdot\left[\begin{array}{c}\omega_x \\ \omega_y \\ \omega_z\end{array}\right] (Euler Equation)</script><p>我使用的控制器遵循上述公式采取了PID控制，结合单积分模型的控制共同决定了结果分数。当然字母间距、运行时间等也能对误差产生一定影响。</p><p>单积分模型中控制增益kv与刚度矩阵R、距离误差z和期望速度dv相乘，起到了限制距离误差和期望速度之间的平衡作用。具体来说，当kv增加时，系统会更快地收敛到目标状态，但是可能会出现振荡。当kv减小时，系统会更缓慢地收敛到目标状态，甚至会导致无人机几乎不动的情况。</p><p>控制器的原理是输入期望控制，输出飞行器整体推力与力矩。公式整体的推导较为复杂，涉及机器人运动学与动力学，且会解欧拉牛顿方程，但对公式的直观理解可以更好理解公式；这个公式基本是外环位置，内环姿态，计算扭矩与推力，可见推力与飞行器质量与z轴加速度有关，通过计算期望角度计算扭矩。</p><p>我使用了Ziegler-Nichols整定方法来调节PID参数，首先将积分和微分增益设置为0，然后比例增益从零开始逐渐增加，直到到达极限增益<em>KU</em>，此时控制器输出值以恒定值振荡。<em>KU</em>和振荡周期<em>TU</em>根据不同的类型，按下表中的方式来设置比例、积分和微分增益。</p><script type="math/tex; mode=display">\begin{array}{|c|c|c|c|}\hline \text { Controller } & K_p & K_d & K_i \\\hline \text { P } & 0.5 K_u & - & - \\\hline \text { PD } & 0.8 K_u & K_p T_u / 8 & - \\\hline \text { PID } & 0.6 K_u & K_p T_u / 8 & 2 K_p / T_u \\\hline\end{array}</script><p>在位置控制中，使用了三轴PID控制器来控制x, y, z轴上的运动。使用了误差积分来消除误差；在姿态控制中，通过计算出当前的欧拉角（phi, theta, psi），并使用欧拉角的导数来控制飞行器的姿态。</p><p>Kp是比例系数，它控制着系统的稳定性和响应速度。当Kp增大时，系统的响应速度会变快，但同时也会增加系统的震荡。</p><p>Ki是积分系数，它控制着系统的累计误差。当Ki增大时，系统会更快地消除误差，但同时也会增加系统的积分饱和。</p><p>Kd是微分系数，它控制着系统的响应速度。当Kd增大时，系统的响应速度会变快，但同时也会增加系统的偏差。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;多智能体集群控制技术期末项目报告&lt;/p&gt;</summary>
    
    
    
    
    <category term="多智能体集群" scheme="http://enderfga.cn/tags/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E9%9B%86%E7%BE%A4/"/>
    
  </entry>
  
</feed>
