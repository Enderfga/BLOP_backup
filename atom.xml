<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Enderfga&#39;Blog</title>
  
  
  <link href="http://enderfga.cn/atom.xml" rel="self"/>
  
  <link href="http://enderfga.cn/"/>
  <updated>2023-07-11T02:08:49.747Z</updated>
  <id>http://enderfga.cn/</id>
  
  <author>
    <name>Enderfga</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>GPT4泄露的技术细节</title>
    <link href="http://enderfga.cn/2023/07/11/gpt4/"/>
    <id>http://enderfga.cn/2023/07/11/gpt4/</id>
    <published>2023-07-11T01:12:24.000Z</published>
    <updated>2023-07-11T02:08:49.747Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>From Twitter of Yam Peleg.</p><p><img src="https://img.enderfga.cn/img/20230711100816.png" /></p><span id="more"></span><h1 id="gpt-4s-details-are-leaked.">GPT-4's details are leaked.</h1><p>It is over. Everything is here:</p><h2 id="parameters-count">Parameters count</h2><p>GPT-4 is more than 10x the size of GPT-3. We believe it has a totalof ~1.8 trillion parameters across 120 layers.</p><p>GPT-4的规模是GPT-3的10倍以上。我们认为它拥有约1,800亿个参数，分布在120层。</p><h2 id="mixture-of-experts---confirmed">Mixture Of Experts -Confirmed</h2><p>OpenAl was able to keep costs reasonable by utilizing a mixture ofexperts(MoE) model.</p><p>They utilizes 16 experts within their model, each is about ~111Bparameters for MLP 2 of these experts are routed to per forwardpass.</p><p>OpenAI通过使用专家混合（MoE）模型，能够将成本控制在合理的范围内。</p><p>他们的模型利用了16个专家，每个专家大约拥有1110亿个参数，其中每次前向传递（forwardpass）使用其中的2个专家。</p><h2 id="moe-routing">MoE Routing</h2><p>While the literature talks a lot about advanced routing algorithmsfor choosing which experts to route each token to, OpenAl's is allegedlyquite simple, for the current GPT-4 model.</p><p>There roughly ~55B shared parameters for attention.</p><p>尽管文献中谈到了很多关于选择将每个标记（token）路由到哪些专家的先进路由算法，但据称OpenAI在当前的GPT-4模型中采用的路由方法相当简单。</p><p>在注意力机制中，大约有550亿个共享参数。</p><h2 id="inference">Inference</h2><p>Each forward pass inference (generation of 1 token) only utilizes~280B parameters and ~560 TFLOPs. This contrasts with the ~1.8 trillionparameters and ~3,700 TFLOP that would be required per forward pass of apurely dense model.</p><p>每次前向传递的推理（生成一个标记）只使用了大约280亿个参数和560TFLOPS的计算量。这与纯密集模型每次前向传递所需的约1,800亿个参数和3,700TFLOPS形成了对比。</p><h2 id="dataset">Dataset</h2><p>GPT-4 is trained on ~13T tokens. These are not unique tokens, theycount the epochs as more tokens as well.</p><p>Epoch number: 2 epochs for text-based data and 4 for code-baseddata.</p><p>There is millions of rows of instruction fine-tuning data fromScaleAl &amp; internally.</p><p>GPT-4的训练使用了大约130万亿个标记（tokens）。这些标记不是唯一的标记，还计算了多个时期（epochs）的标记数量。</p><p>对于基于文本的数据，使用了2个时期，对于基于代码的数据，使用了4个时期。</p><p>此外，还有来自ScaleAl和内部的数百万行指令微调数据。</p><h2 id="gpt-4-32k">GPT-4 32K</h2><p>There was an 8k context length (seq len) for the pre-training phase.The 32k seq len version of GPT-4 is based on fine-tuning of the 8k afterthe pre-training.</p><p>在预训练阶段，GPT-4使用了8,000个上下文长度（序列长度）。而32,000个序列长度的GPT-4版本是在预训练后对8,000个序列长度进行微调得到的。</p><h2 id="batch-size">Batch Size</h2><p>The batch size was gradually ramped up over a number of days on thecluster, but by the end, OpenAl was using a batch size of 60 million!This, of course, is “only” a batch size of 75 million tokens per expertdue to not every expert seeing all tokens.</p><p>在集群上的几天时间里，批量大小逐渐增加，但最终OpenAI使用了6000万的批量大小！当然，由于并非每个专家都能看到所有的标记，因此每个专家实际上只处理了7500万个标记的批量大小。</p><h3 id="for-the-real-batch-size">For the real batch size</h3><p>Divide this number by the seq len to get the real batch size. Juststop with this misleading numbers already.</p><p>将这个数字除以序列长度以获取实际的批量大小。请不要再提供这种误导性的数字了。</p><h2 id="parallelism-strategies">Parallelism Strategies</h2><p>To parallelize across all their A100s GPUs</p><p>They utilized 8-way tensor parallelism as that is the limit forNVLink. Beyond that, they are using 15-way pipeline parallelism.</p><p>(likely used ZeRo Stage 1.lt is possiblethey used block-levelFSDP)</p><p>为了在所有A100 GPU上实现并行计算，他们采用了8路张量并行（tensorparallelism），因为这是NVLink的限制。此外，他们还使用了15路管道并行（pipelineparallelism）。</p><p>很可能他们使用了ZeRo阶段1，同时也可能使用了块级FSDP（Fully ShardedData Parallelism）。</p><h2 id="training-cost">Training Cost</h2><p>OpenAl's training FLOPS for GPT-4 is ~2.15e25,</p><p>on~25,000 A100s for 90 to 100 days at about 32% to 36% MFU.</p><p>Part of this extremely low utilization is due to an absurd number offailures requiring checkpoints that needed to be restarted from.</p><p>If their cost in the cloud was about $1 per A100 hour, the trainingcosts for this run alone would be about $63 million</p><p>(Today, the pre-training could be done with ~8,192 H100 in ~55 daysfor $21.5 million at $2 per H100 hour.)</p><p>OpenAI的GPT-4训练的浮点运算速度（FLOPS）约为2.15e25次。</p><p>在大约25,000个A100GPU上进行训练，持续时间为90至100天，利用率约为32%至36%。</p><p>这种极低的利用率部分是由于大量的故障导致需要重新启动检查点。</p><p>如果他们在云端的成本约为每个A100每小时1美元，单单这次训练的成本将约为6300万美元。</p><p>（现在，使用大约8,192个H100GPU进行预训练，需要大约55天时间，成本为2150万美元，每个H100每小时2美元。）</p><h2 id="mixture-of-expert-tradeoffs">Mixture of Expert Tradeoffs</h2><p>There are multiple MoE tradeoffs taken:</p><p>For example, MoE is incredibly difficult to deal with on inferencebecause not every part of the model is utilized on every tokengeneration.</p><p>This means parts may sit dormant when other parts are being used.When serving users, this really hurts utilization rates.</p><p>Researchers have shown that using 64 to 128 experts achieves betterloss than 16 experts, but that's purely research.</p><p>There are multiple reasons to go with fewer experts. One reason forOpenAI choosing 16 experts is because more experts are difficult togeneralize at many tasks. More experts can also be more difficult toachieve convergence with.</p><p>With such a large training run, OpenAI Instead chose to be moreconservative onthe number of experts.</p><p>有多个专家混合（MoE）的权衡被考虑：</p><p>例如，在推理过程中，处理MoE非常困难，因为并非每个模型的部分在每个标记生成过程中都被利用。</p><p>这意味着某些部分可能处于闲置状态，而其他部分正在被使用。在为用户提供服务时，这会极大地降低利用率。</p><p>研究人员已经证明，使用64到128个专家比使用16个专家可以获得更好的损失结果，但这仅仅是研究结果。</p><p>选择较少的专家有多个原因。OpenAI选择使用16个专家之一的原因是因为更多的专家在许多任务上很难进行泛化。更多的专家也可能更难实现收敛。</p><p>在如此庞大的训练过程中，OpenAI选择在专家数量上更加保守。</p><h2 id="gpt-4-inference-cost">GPT-4 Inference Cost</h2><p>GPT-4 costs 3x that of the 175B parameter Davinchi.</p><p>This is largely due to the larger clusters required for GPT-4 andmuch lower utilization achieved.</p><p>AN estimate of it's costs is $0.0049 cents per 1k tokens for 128A100s to inference GPT-4 8k seq len and $0.0021cents per 1k tokens for128 H100's to inference GPT-4 8k seq len. It should be noted, we assumedecent high utilization,and keeping batch sizes high.</p><p>GPT-4的成本是1750亿参数的Davinci模型的3倍。</p><p>这主要是由于GPT-4需要更大规模的集群，并且实现了更低的利用率。</p><p>据估计，使用128个A100 GPU进行GPT-48,000个序列长度的推理，每1,000个标记的成本约为0.0049美分；使用128个H100GPU进行GPT-48,000个序列长度的推理，每1,000个标记的成本约为0.0021美分。需要注意的是，我们假设了良好的高利用率，并保持了较高的批量大小。</p><h2 id="multi-query-attention">Multi-Query Attention</h2><p>OpenAl are using MQA just like everybody else.</p><p>Because of that only 1 head is needed and memory capacity can besignificantly reduced for the KV cache. Even then, the 32k seq len GPT-4definitely cannot run on 40GB A10Os, and the 8k is capped onmax bsz.</p><p>OpenAI也像其他人一样使用了MQA（Multi-QKV Attention）。</p><p>由于使用了MQA，只需要一个注意力头（head），并且可以显著降低KV缓存的内存需求。即便如此，32,000个序列长度的GPT-4肯定无法在40GB的A10O上运行，而8,000个序列长度则有最大批量大小的限制。</p><h2 id="continuous-batching">Continuous batching</h2><p>OpenAl implements both variable batch sizes and continuous batching.This is so as to allow some level of maximum latency as well optimizingthe inference costs.</p><p>OpenAI实现了可变批量大小和连续批处理。这样做可以在一定程度上允许最大延迟，并优化推理成本。</p><h2 id="vision-multi-modal">Vision Multi-Modal</h2><p>It is a separate vision encoder from the text encoder, withcross-attention. The architecture is similar to Flamingo. This adds moreparameters on top of the 1.8T of GPT-4. lt is fine-tuned with another ~2trillion tokens, after the text only pre-training.</p><p>On the vision model, OpenAl wanted to train it from scratch, but itwasn't mature enough, so they wanted to derisk it by starting withtext.</p><p>One of the primary purposes of this vision capability is forautonomous agents able to read web pages and transcribe what's in imagesand video.</p><p>Some of the data they train on is joint data (rendered LaTeX/text),screen shots of web page, youtube videos: samplingframes, and runWhisper around it to get transcript.</p><p>GPT-4引入了一个与文本编码器分离的独立视觉编码器，并具有交叉注意力机制。其架构类似于Flamingo模型。这使得GPT-4的参数量在1,800亿的基础上增加了更多。在仅进行文本预训练之后，还对该视觉模型进行了约2万亿个额外的微调标记。</p><p>在视觉模型方面，OpenAI原本希望从头开始训练，但该模型尚未足够成熟，所以他们选择通过从文本开始来减轻风险。</p><p>该视觉功能的主要目的之一是使自主代理能够阅读网页并转录图像和视频中的内容。</p><p>他们训练的一部分数据是联合数据，包括渲染的LaTeX/文本、网页截图、YouTube视频（采样帧），并使用Whisper技术对其进行转录。</p><h2 id="speculative-decoding">Speculative Decoding</h2><p>OpenAl might be using speculative decoding on GPT-4's inference. (notsure100%)</p><p>The idea is to use a smaller faster model to decode several tokens inadvance, and then feeds them into a large oracle model as a singlebatch.</p><p>lf the small model was right about its predictions-the larger modelagrees and we can decode several tokens in a single batch.</p><p>But if the larger model rejects the tokens predicted by the draftmodel then the rest of the batch is discarded. And wecontinue with thelarger model.</p><p>The conspiracy theory that the new GPT-4 quality had beendeteriorated might be simply because they are letting the oracle modelaccept lower probability sequences from the speculative decodingmodel.</p><p>OpenAI可能正在使用GPT-4推理中的推测解码（speculativedecoding），但无法确定。</p><p>这种方法是使用一个更小更快的模型提前解码多个标记，然后将它们作为一个批次输入到大型的预测模型中。</p><p>如果小型模型的预测正确，大型模型会同意并可以一次性解码多个标记。</p><p>但是，如果大型模型拒绝了草稿模型预测的标记，那么批次中的剩余部分将被丢弃，并继续使用大型模型。</p><p>有一种阴谋论认为，GPT-4的质量下降是因为他们让预测模型接受了由推测解码模型生成的低概率序列。</p><p>请注意，以上内容仅为推测，实际情况可能与之有所不同。</p><h2 id="inference-architecture">Inference Architecture</h2><p>The inference runs on a cluster of 128 GPUs.</p><p>There are multiple of these clusters in multiple datacenters indifferent locations.</p><p>It is done in 8-way tensor parallelism and 16-way pipelineparallelism.</p><p>Each node of 8 GPUs has only ~130B parameters, or less than 30GB perGPU at FP16 and less than 15GB at FP8/int8.</p><p>The model has 120, so it fits in 15 different nodes. [Possibly thethere are less layers on the first node since it needs to also computethe embeddings]</p><p>According to these numbers: OpenAl should have trained on 2x thetokens if they were trying to go by chinchilla'soptimal.</p><p>[let alone surpass it like we do]</p><p>This goes to show that they are strugglingto get high qualitydata.</p><p>推理过程在由128个GPU组成的集群上运行。</p><p>这些集群分布在不同地点的多个数据中心中。</p><p>推理过程采用8路张量并行和16路管道并行。</p><p>每个包含8个GPU的节点只有约1,300亿个参数，即每个GPU约30GB（FP16）或15GB（FP8/INT8）。</p><p>模型有120层，因此需要15个不同的节点来容纳模型。（可能第一个节点的层数较少，因为它还需要计算嵌入层）</p><p>根据这些数字，如果OpenAI试图按照chinchilla的最优设置进行训练，他们应该训练2倍的标记。</p><p>这表明他们在获取高质量数据方面遇到了困难。</p><h2 id="why-no-fsdp">Why no FSDP?</h2><p>A possible reason for this could be that some of the hardware infrathey secured is of an older generation.</p><p>This is pretty common at local compute clusters as theorganisationusually upgrade the infra in several "waves" to avoid acomplete pause ofoperation.</p><p>With such a high amount of pipeline parallelism it is very likelythat just like the rest of us they suffer from the "batch bubble":slight idle timebetween batches.</p><p>Again: There is no magic.</p><p>They know what they are doing but it is not magic.</p><p>一个可能的原因是，他们所使用的硬件基础设施中有一部分是较旧一代的。</p><p>这在本地计算集群中非常常见，通常组织会分几个"波次"进行基础设施的升级，以避免完全停止运营。</p><p>由于存在如此高的管道并行性，他们很可能像其他人一样遭受"批处理泡沫"的影响：批次之间存在轻微的空闲时间。</p><p>再次强调：没有什么魔法。</p><p>他们知道自己在做什么，但这并非魔法。</p><hr /><p>OpenAI是将GPT-4的架构保密，这并不是因为对人类存在着某种潜在风险，而是因为他们所构建的东西是可复制的。实际上，我们预计在不久的将来，谷歌、Meta、Anthropic、Inflection、Character、腾讯、字节跳动、百度等公司都将拥有与GPT-4同等甚至更强大的模型能力。</p><p>不要误会，OpenAI的工程能力非常出色，他们所构建的东西令人难以置信，但是他们所采取的解决方案并非神奇。这是一个优雅的解决方案，需要考虑许多复杂的权衡。扩大规模只是战斗的一部分。OpenAI最持久的优势在于他们在实际应用中拥有最多的用户、领先的工程人才，并且可以通过未来的模型继续超越其他竞争对手。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;From Twitter of Yam Peleg.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img.enderfga.cn/img/20230711100816.png&quot; /&gt;&lt;/p&gt;</summary>
    
    
    
    
    <category term="自然语言处理" scheme="http://enderfga.cn/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>Generating Images with Multimodal Language Models</title>
    <link href="http://enderfga.cn/2023/06/27/GILL/"/>
    <id>http://enderfga.cn/2023/06/27/GILL/</id>
    <published>2023-06-27T02:00:49.000Z</published>
    <updated>2023-06-27T02:07:00.104Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="generating-images-with-multimodal-language-models">GeneratingImages with Multimodal Language Models</h1><figure><img src="https://img.enderfga.cn/img/teaser.gif"alt="Example of GILL generating multimodal dialogue." /><figcaption aria-hidden="true">Example of GILL generating multimodaldialogue.</figcaption></figure><h2 id="abstract">Abstract</h2><p>We propose a method to fuse frozen text-only large language models(LLMs) with pre-trained image encoder and decoder models, by<strong>mapping between their embedding spaces</strong>. Our modeldemonstrates a wide suite of multimodal capabilities: <strong>imageretrieval, novel image generation, and multimodal dialogue</strong>.Ours is the first approach capable of conditioning on arbitrarilyinterleaved image and text inputs to generate coherent image (and text)outputs. To achieve strong performance on image generation, we proposean efficient mapping network to ground the LLM to an off-the-shelftext-to-image generation model. This mapping network translates hiddenrepresentations of text into the embedding space of the visual models,enabling us to leverage the strong text representations of the LLM forvisual outputs. Our approach outperforms baseline generation models ontasks with longer and more complex language. In addition to novel imagegeneration, our model is also capable of image retrieval from aprespecified dataset, and decides whether to retrieve or generate atinference time. This is done with a learnt decision module whichconditions on the hidden representations of the LLM. Our model exhibitsa wider range of capabilities compared to prior multimodal languagemodels. It can process image-and-text inputs, and produce retrievedimages, generated images, and generated text — outperforming non-LLMbased generation models across several text-to-image tasks that measurecontext dependence.</p><h2 id="model">Model</h2><figure><img src="https://img.enderfga.cn/img/architecture.png"alt="Model architecture of GILL." /><figcaption aria-hidden="true">Model architecture of GILL.</figcaption></figure><p>GILL (Generating Images with Large Language Models) is capable ofprocessing arbitrarily interleaved image-and-text inputs to generatetext, retrieve images, and generate novel images.</p><p>Our findings show that it is possible to efficiently map the outputembedding space of a frozen text-only LLM to the embedding space of afrozen text-to-image generation model (in this work, Stable Diffusion)despite both models using entirely different text encoders. We achievethis by finetuning a small number of parameters on image-caption pairs,in contrast to other methods which require interleaved image-texttraining data. Our approach is computationally efficient and does notrequire running the image generation model at training time.</p><p>During inference, the model takes in arbitrarily interleaved imageand text inputs, and produces text interleaved with image embeddings.After deciding whether to retrieve or generate for a particular set oftokens, it returns the appropriate image outputs (either retrieved orgenerated).</p><h2 id="capabilities">Capabilities</h2><p>One of the more compelling applications of GILL is perhaps itsability to generalize to many different tasks, due to the LLMpretraining and freezing. We showcase several of these capabilitiesbelow. More qualitative results are provided in our <ahref="https://arxiv.org/abs/2305.17216">paper</a>.</p><h3 id="multimodal-dialogue-generation">Multimodal DialogueGeneration</h3><figure><img src="https://img.enderfga.cn/img/dialogue.png"alt="Multimodal dialogue example" /><figcaption aria-hidden="true">Multimodal dialogue example</figcaption></figure><p>GILL can be prompted to generate dialogue-like text, producingmultimodal dialogue by interleaving generated text, retrieved images,and generated images. (View as <ahref="https://jykoh.com/gill/dialogue.gif">GIF</a> or <ahref="https://jykoh.com/gill/dialogue.mp4">MP4</a>)</p><h3 id="generating-from-visual-stories">Generating from VisualStories</h3><figure><img src="https://img.enderfga.cn/img/visual_stories.png"alt="Generating from Visual Stories" /><figcaption aria-hidden="true">Generating from VisualStories</figcaption></figure><p>GILL can condition on interleaved image-and-text inputs to generatemore relevant images compared to non-LLM based text-to-image generationmodels.</p><h3 id="comparison-against-stable-diffusion">Comparison Against StableDiffusion</h3><figure><img src="https://img.enderfga.cn/img/compare_sd.png"alt="Comparison against Stable Diffusion" /><figcaption aria-hidden="true">Comparison against StableDiffusion</figcaption></figure><p>The GILLMapper module we introduce allows our model to mapeffectively to the SD image generation backbone, outperforming ormatching SD for many examples from <ahref="https://sites.research.google/parti/">PartiPrompts</a>.</p><h3 id="comparison-against-fromage">Comparison Against FROMAGe</h3><figure><img src="https://img.enderfga.cn/img/fromage_comparison.png"alt="Comparison Against FROMAGe" /><figcaption aria-hidden="true">Comparison Against FROMAGe</figcaption></figure><p>Our model composites multimodal information to produce relevant imageand text outputs. It can outperform baseline models that are limited toimage retrieval.</p><h2 id="limitations">Limitations</h2><p>While GILL introduces many exciting capabilities, it is an earlyresearch prototype and has several limitations. GILL relies on an LLMbackbone for many of its capabilities. As such, it also inherits many ofthe limitations that are typical of LLMs. More details and discussionare provided in our <a href="https://arxiv.org/abs/2305.17216">paper andappendix</a>:</p><ul><li>GILL does not always produce images when prompted, or when it is(evidently) useful for the dialogue.</li><li>A limitation of GILL is in its limited visual processing. At themoment, we use only 4 visual vectors to represent each input image (dueto computational constraints), which may not capture all the relevantvisual information needed for downstream tasks.</li><li>Our model inherits some of the unintended behaviors of LLMs, such asthe potential for hallucinations, where it generates content that isfalse or not relevant to the input data. It also sometimes generatesrepetitive text, and does not always generate coherent dialoguetext.</li></ul><p>One of the advantages of our model is that <strong>it is modular, andcan benefit from stronger visual and language models released in thefuture</strong>. It is likely that it will also benefit from strongertext-to-image generation backbones, or through finetuning the generationbackbone rather than just the GILLMapper module. We leave such scalingexplorations for future work.</p><ul><li><strong>Research Problem:</strong> The research problem addressed inthis paper is the fusion of large language models (LLMs) withpre-trained image encoder and decoder models. The goal is to create amodel that can handle multimodal capabilities, such as image retrieval,novel image generation, and multimodal dialogue. The model should beable to condition on arbitrarily interleaved image and text inputs togenerate coherent image (and text) outputs.</li><li><strong>Motivation:</strong> The motivation behind this research isto <strong>leverage the strong text representations of LLMs for visualoutputs</strong>. The authors aim to outperform baseline generationmodels on tasks with <em>longer and more complex language</em>. Theyalso aim to create a model that can decide whether to retrieve orgenerate at inference time, and can process image-and-text inputs, andproduce retrieved images, generated images, and generated text.</li><li><strong>Solution:</strong> The authors propose a method calledGenerating Images with Large Language Models (GILL). This methodefficiently maps the output embedding space of a frozen text-only LLM tothat of a frozen generation model. They achieve this by finetuning asmall number of parameters on image-caption pairs. They also proposeefficient architectural changes to learn the LLM-to-generation mappingeffectively with the GILLMapper module. To decide whether to produce aretrieved image or a generated one at inference time, they train adecision model that outputs a decision conditioned on the LM hiddenrepresentations.</li><li><strong>Novelty:</strong> The novelty of this research lies in theproposed method's ability to <strong>process arbitrarily interleavedimage-and-text inputs to generate text, retrieve images, and generatenovel images</strong>. It is the first model capable of outputtingretrieved images, novel images, and text, interleaving these forcoherent multimodal dialogue generation. The authors also proposeefficient architectural changes to learn the LLM-to-generation mappingeffectively with the GILLMapper module, a lightweight Transformerconditioned on special learnt text tokens.</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="计算机视觉" scheme="http://enderfga.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>Fine-Grained Human Feedback Gives Better Rewards for Language Model Training</title>
    <link href="http://enderfga.cn/2023/06/20/Fine-Grained/"/>
    <id>http://enderfga.cn/2023/06/20/Fine-Grained/</id>
    <published>2023-06-20T05:21:59.000Z</published>
    <updated>2023-06-20T05:23:23.510Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1id="fine-grained-human-feedback-gives-better-rewards-for-language-model-training">Fine-GrainedHuman Feedback Gives Better Rewards for Language Model Training</h1><p><img src="https://img.enderfga.cn/img/teaser-smaller.jpg" /></p><p>We propose <strong>Fine-Grained RLHF</strong>, a framework thatenables training and learning from reward functions that arefine-grained in two respects:</p><ol type="1"><li>density, providing a reward after every segment (e.g., a sentence)is generated</li><li>incorporating multiple reward models associated with differentfeedback types (e.g., factual incorrectness, irrelevance, andinformation incompleteness).</li></ol><h2 id="what-are-fine-grained-rewards">What are Fine-GrainedRewards?</h2><p>Prior work in RLHF focused on collecting human preferences regardingthe overall quality of language model (LM) outputs. However, this typeof holistic feedback offers limited information. In our paper, weintroduce <strong>fine-grained human feedback</strong> (e.g., whichsub-sentence is irrelevant, which sentence is not truthful, whichsentence is toxic) as an explicit training signal. Our rewards arefine-grained in two aspects:</p><p><strong>(a) Density</strong>: We provide a reward after each segment(e.g., a sentence) is generated, similar to OpenAI's "step-by-stepprocess reward". We found that this approach is more informative thanholistic feedback and, thus, more effective for RL.</p><p><strong>(b) Multiple reward models associated with different feedbacktypes</strong>: We employ multiple reward models to capture differenttypes of feedback (e.g., factual inaccuracy, irrelevance, andinformation incompleteness). Interestingly, we observed that thesereward models both complement and compete with each other. By adjustingthe weights of the reward models, we can control the balance between thedifferent types of feedback and <strong>tailor the LM for differenttasks</strong> according to specific needs.</p><h2 id="abstract">Abstract</h2><p>Language models (LMs) often exhibit undesirable text generationbehaviors, including generating false, toxic, or irrelevant outputs.Reinforcement learning from human feedback (RLHF)—where human preferencejudgments on LM outputs are transformed into a learning signal—hasrecently shown promise in addressing these issues. However, suchholistic feedback conveys limited information on long text outputs; itdoes not indicate which aspects of the outputs influenced userpreference; e.g., which parts contain what type(s) of errors. In thispaper, we use fine-grained human feedback (e.g., which sentence isfalse, which sub-sentence is irrelevant) as an explicit training signal.We introduce FINE-GRAINED RLHF, a framework that enables training andlearning from reward functions that are fine-grained in two respects:(1) density, providing a reward after every segment (e.g., a sentence)is generated; and (2) incorporating multiple reward models associatedwith different feedback types (e.g., factual incorrectness, irrelevance,and information incompleteness). We conduct experiments ondetoxification and long-form question answering to illustrate howlearning with such reward functions leads to improved performance,supported by both automatic and human evaluation. Additionally, we showthat LM behaviors can be customized using different combinations offine-grained reward models.</p><h2 id="task-1-detoxification">Task 1: Detoxification</h2><p>The task of detoxification aims to reduce the toxicity in the modelgeneration. We use Perspective API to measure toxicity. It returns atoxicity value between 0 (not toxic) and 1 (toxic).</p><p><img src="https://img.enderfga.cn/img/toxicity_demo.png" /></p><p>We compare two kinds of rewards:</p><ol type="a"><li><p><strong>Holistic Rewards for (non-)Toxicity</strong>: We use1-Perspective(y) as the reward</p></li><li><p><strong>Sentence-level (Fine-Grained) Rewards for(non-)Toxicity</strong>: We query the API after the model generates eachsentence instead of generating the full sequence. For each generatedsentence, we use -Δ(Perspective(y)) as the reward for the sentence (i.e.how much toxicity is changed from generating the currentsentence).</p></li></ol><p><img src="https://img.enderfga.cn/img/toxicity_results.png" /></p><p>Table 1 shows that Our Fine-Grained RLHF with sentence-levelfine-grained reward attains the lowest toxicity and perplexity among allmethods, while maintaining a similar level of diversity. Figure 2 showsthat learning from denser fine-grained reward is more <strong>sampleefficient</strong> than holistic reward. One explanation is thatfine-grained reward locates where the toxic content is, which is astronger training signal compared with a scalar reward for the wholetext.</p><h2 id="task-2-long-form-question-answering">Task 2: Long-Form QuestionAnswering</h2><p>We collect <strong>QA-Feeback</strong>, a dataset of long-formquestion answering, with human preferences and fine-grained feedback.QA-Feedback is based on ASQA, a dataset that focuses on answeringambiguous factoid questions.</p><p>There are three types of fine-grained human feedback, and we train afine-grained reward model for each of them:</p><p><span class="math inline">\(\color{blue}{\text{C1: irrelevance,repetition, and incoherence (rel.)}}\)</span>; The reward model has thedensity level of sub-sentences; i.e., returns a score for eachsub-sentence. If the sub-sentence is irrelevant, repetitive, orincoherent, the reward is -1; otherwise, the reward is +1.</p><p><span class="math inline">\(\color{goldenrod}{\text{C2: incorrect orunverifiable facts (fact.)}}\)</span>; The reward model has the densitylevel of sentences; i.e., returns a score for each sentence. If thesentence has any factual error, the reward is -1; otherwise, the rewardis +1.</p><p><span class="math inline">\(\color{green}{\text{C3: incompleteinformation (comp.)}}\)</span>; The reward model checks if the responseis complete and covers all the information in the reference passagesthat are related to the question. This reward model gives one reward forthe whole response.</p><h2 id="fine-grained-human-evaluation">Fine-Grained HumanEvaluation</h2><p>We compare our <strong>Fine-Grained RLHF</strong> with the followingbaselines:</p><p><strong>SFT</strong>: The supervised finetuning model (trained on 1Ktraining examples) that is used as the initial policy for our RLHFexperiments.</p><p><strong>Pref. RLHF</strong>: The baseline RLHF model that usesholistic reward.</p><p><strong>SFT-Full</strong>: We finetune LM with human-writtenresponses (provided by ASQA) of all training examples and denote thismodel as SFT-Full. Notice that each gold response takes 15 min toannotate (according to ASQA), which takes much longer time than ourfeedback annotation (6 min).</p><p><img src="https://img.enderfga.cn/img/human_eval.png" /></p><p>Human evaluation shows that our <strong>Fine-Grained RLHF</strong>outperforms SFT and Preference RLHF on all error types. Also, RLHF (bothpreference-based and fine-grained) are particularly effective inreducing factual errors.</p><h2 id="customize-lm-behaviors">Customize LM behaviors</h2><p><img src="https://img.enderfga.cn/img/customization.png" /></p><p>By changing the weight of the <spanclass="math inline">\(\color{blue}{\text{Relevance rewardmodel}}\)</span>, and keeping the weight of the other two reward modelsfixed, we can customize how detailed and lengthy the LM responses wouldbe. Here we compare the outputs of three LMs, trained with differentreward model combinations.</p><h2id="fine-grained-reward-models-both-complement-and-compete-with-each-other">Fine-Grainedreward models both complement and compete with each other</h2><p><img src="https://img.enderfga.cn/img/analysis.png" /></p><p>We find that there is a trade-off between the three reward models.<span class="math inline">\(\color{blue}{\text{Relevance RM}}\)</span>prefers shorter and more concise responses, while <spanclass="math inline">\(\color{green}{\text{Info CompletenessRM}}\)</span> prefers longer and more informative responses. Thus, thesetwo rewards compete against each other during training and eventuallyreach a balance. Meanwhile, <spanclass="math inline">\(\color{goldenrod}{\text{Factuality RM}}\)</span>continuously improves the factual correctness of the response. Finally,removing any one of the reward models will degrade the performance.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="自然语言处理" scheme="http://enderfga.cn/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>Generation Model</title>
    <link href="http://enderfga.cn/2023/06/14/model/"/>
    <id>http://enderfga.cn/2023/06/14/model/</id>
    <published>2023-06-14T06:41:21.000Z</published>
    <updated>2023-06-27T02:06:42.385Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>对 diffusion 结构的改进</p><span id="more"></span><h1 id="generation-model">Generation Model</h1><h2id="raphael-text-to-image-generation-via-large-mixture-of-diffusion-paths">RAPHAEL:Text-to-Image Generation via Large Mixture of Diffusion Paths</h2><p>香港大学，商汤科技，23.5.29，https://raphael-painter.github.io/</p><ol type="1"><li><p><strong>Research Problem:</strong> The research problem addressedin this paper is the generation of <strong>highly artistic images thataccurately portray text prompts</strong>. Existing models often fail toadequately preserve textual concepts within the generated images due tothe reliance on a classic cross-attention mechanism for integrating textdescriptions into visual representations.</p></li><li><p><strong>Motivation:</strong> The motivation behind this researchis to improve the precision of <strong>alignment between text andimage</strong> in the generation process. This is aimed at<strong>enhancing the quality and aesthetic</strong> appeal of thegenerated images, and enabling them to <strong>accurately represent theassociated text prompt</strong>.</p></li><li><p><strong>Solution:</strong> The authors propose a text-conditionalimage diffusion model, RAPHAEL, which stacks tens of mixture-of-experts(MoE) layers, including both space-MoE and time-MoE layers. Thisconfiguration leads to billions of diffusion paths from the networkinput to the output. Each path can act as a "painter" responsible forrendering a particular concept to an image region at a specifictimestep. The authors also propose an edge-supervised learning module tofurther enhance the image quality and aesthetic appeal of the generatedimages.</p><p><imgsrc="https://img.enderfga.cn/img/image-20230613125221600.png" /></p></li><li><p><strong>What's New:</strong> The novelty of this research lies inthe introduction of RAPHAEL, which outperforms recent cutting-edgemodels in terms of both image quality and aesthetic appeal. RAPHAELexhibits superior performance in switching images across diverse styles,such as Japanese comics, realism, cyberpunk, and ink illustration. Italso establishes a new <strong>state-of-the-art</strong> with azero-shot FID-30k score of 6.61 on the COCO dataset. Furthermore,RAPHAEL is capable of generating images with resolutions up to 4096 ×6144 with rich image contents and details, when combined with atailor-made SR-GAN model.</p></li></ol><h3 id="mixture-of-experts">Mixture of experts？</h3><p><imgsrc="https://img.enderfga.cn/img/image-20230613131954667.png" /></p><p>一种集成学习(Ensemble Learning)技术，一个系统中包含多个分开的网络，每个网络去处理全部训练样本的一个子集。这种方式可以看做是把多层网络进行了模块化的转换。</p><p>假设我们已经知道数据集中存在一些天然的子集（比如来自不同的domain，不同的topic），那么用单个模型去学习，就会受到很多干扰（interference），导致学习很慢、泛化困难。这时，我们可以使用多个模型（即专家，expert）去学习，使用一个门网络（gatingnetwork）来决定每个数据应该被哪个模型去训练，这样就可以减轻不同类型样本之间的干扰。</p><p>MoE将预测建模任务分解为若干子任务，在每个子任务上训练一个专家模型（ExpertModel），开发一个门控模型（GatingModel），该模型根据要预测的输入来学习信任哪个专家，并组合预测结果。</p><h3 id="detail">Detail</h3><ul><li>LAION-5B and some internal datasets</li><li>Multi-scale Training</li><li>3B</li><li>GPT3.5生成的100个常见的形容词</li></ul><h2id="snapfusion-text-to-image-diffusion-model-on-mobile-devices-within-two-seconds">SnapFusion:Text-to-Image Diffusion Model on Mobile Devices within Two Seconds</h2><p>SnapInc.，东北大学，23.6.1，https://snap-research.github.io/SnapFusion/</p><ol type="1"><li><strong>研究问题：</strong>这篇论文解决的研究问题是如何在移动设备上快速运行文本到图像的扩散模型。现有的模型虽然能够生成出色的图像，但由于其复杂的网络架构和大量的去噪迭代，使得它们在计算上非常昂贵，运行速度慢。</li><li><strong>动机：</strong>这项研究的动机是改进现有模型的运行速度和计算成本。这不仅可以降低运行模型的成本，还可以避免在用户数据被发送到第三方时涉及到的隐私问题。</li><li><strong>解决方法：</strong>作者通过引入高效的网络架构和改进步骤蒸馏来解决这个问题。他们提出了一个高效的UNet，通过识别原始模型的冗余，并通过数据蒸馏减少图像解码器的计算。此外，他们通过探索训练策略和引入分类器无关指导的正则化来增强步骤蒸馏。</li><li><strong>创新之处：</strong>这项研究的创新之处在于，它是第一个能够在移动设备上运行文本到图像扩散模型的方法，运行时间少于2秒。此外，他们的模型在8个去噪步骤中比StableDiffusionv1.5的50个步骤获得了更好的FID和CLIP分数。这项工作通过将强大的文本到图像扩散模型带到用户手中，使内容创建民主化。</li></ol><p><imgsrc="https://img.enderfga.cn/img/image-20230613134941299.png" /></p><p>为了优化 UNet 结构，我们提出一套 UNet 结构自动评估、进化流程：先对UNet 进行鲁棒性训练（Robust Training），在训练中随机 drop一些模块，以此来测试出每个模块对性能的真实影响，从而构建一个 “对 CLIPscore 的影响 vs. latency” 的查找表；然后根据该查找表，优先去除对 CLIPscore影响不大同时又很耗时的模块。这一套流程是在线自动进行，完成之后，我们就得到了一个全新的UNet 结构，称为 Efficient UNet。相比原版 UNet，实现 7.4x加速且性能不降。</p><p><imgsrc="https://img.enderfga.cn/img/image-20230613135018817.png" /></p><p>这篇论文中的蒸馏过程主要包括两个部分：图像解码器的蒸馏和步骤蒸馏。</p><ol type="1"><li><strong>图像解码器的蒸馏：</strong>作者提出了一个蒸馏流程，使用合成数据来学习通过通道减少获得的高效图像解码器。他们使用文本提示从SD-v1.5的UNet中获取潜在表示，然后将其转发到他们的高效图像解码器和SD-v1.5的解码器以生成两个图像。然后，通过最小化两个图像之间的均方误差来优化解码器。</li><li><strong>步骤蒸馏：</strong>步骤蒸馏是一种减少UNet迭代去噪步骤的方法，以实现更快的速度。他们采用了步骤蒸馏的研究方向，其中通过蒸馏教师模型来减少推理步骤。例如，如果教师模型需要32步，那么学生模型可能只需要16步，这样，学生模型的速度就可以提高2倍。</li></ol><p>这篇论文还提出了一种新的蒸馏方法，称为CFG-Aware StepDistillation，它在计算损失之前对教师和学生进行分类器无关的指导。这种方法在提高CLIP分数方面表现出显著的效果。</p><h3 id="贡献">贡献：</h3><p>（1）通过对现有 UNet 的逐层分析，定位速度瓶颈，提出一种新的高效 UNet结构（Efficient UNet），可以等效替换原 Stable Diffusion 中的 UNet，实现7.4x 加速；</p><p>（2）对推理阶段的迭代步数进行优化，提出一种全新的步数蒸馏方案（CFG-awareStep Distillation），减少步数的同时可显著提升 CLIP score，实现 6.25x加速。</p><h3 id="不足">不足：</h3><ol type="1"><li>SD 模型在多种图像生成场景中都可以使用，本文囿于时间，目前只关注了text to image 这个核心任务，后期将跟进其他任务（如inpainting，ControlNet 等等）。</li><li>本文主要关注速度上的提升，并未对模型存储进行优化。我们相信所提出的Efficient UNet仍然具备压缩的空间，结合其他的高性能优化方法（如剪枝，量化），有望缩小存储，并将时间降低到1 秒以内，离端上实时 SD 更进一步。</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;对 diffusion 结构的改进&lt;/p&gt;</summary>
    
    
    
    
    <category term="计算机视觉" scheme="http://enderfga.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion——RLHF</title>
    <link href="http://enderfga.cn/2023/05/31/diffrlhf/"/>
    <id>http://enderfga.cn/2023/05/31/diffrlhf/</id>
    <published>2023-05-31T08:20:07.000Z</published>
    <updated>2023-05-31T08:26:12.419Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>最近读的两篇关于使用RLHF微调diffusion的论文。</p><span id="more"></span><h1 id="diffusion_rlhf">Diffusion_RLHF</h1><h2 id="training-diffusion-models-with-reinforcement-learning">TrainingDiffusion Models with Reinforcement Learning</h2><p><strong>5.23， University of California, Berkeley</strong></p><p><strong>研究问题 (Research Problem)</strong>:论文主要解决的问题是如何优化扩散模型以实现特定目标。扩散模型是一种生成模型，通常使用对对数似然目标的近似进行训练。然而，大多数扩散模型的使用场景并不关心似然性，而是关心下游目标，如人类感知的图像质量或药物效果。</p><p><strong>动机 (Motivation)</strong>:这项研究的动机是通过更紧密地将扩散模型与实际目标对齐，以提高其性能。这对于<em>难以通过提示表达的任务（如图像可压缩性）以及从人类反馈中得出的任务（如审美质量）</em>尤其相关。</p><p><strong>新颖性 (What's New)</strong>: 作者提出了一种名为DenoisingDiffusion Policy Optimization(DDPO)的方法，该方法将去噪视为多步决策问题。这使得可以使用比替代奖励加权似然方法更有效的策略梯度算法。此外，作者还展示了DDPO如何改进使用视觉语言模型反馈的提示图像对齐，而无需额外的数据收集或人类注释。</p><p><strong>方法的总体思想 (Overall Idea of the Method)</strong>:DDPO方法的总体思想是将去噪过程视为一个多步骤的决策问题，从而可以使用策略梯度算法进行优化。这种方法可以适应各种目标，包括那些难以通过提示表达的目标，如图像可压缩性，以及那些从人类反馈中得出的目标，如审美质量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> io<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">encode_jpeg</span>(<span class="hljs-params"> x , quality = <span class="hljs-number">95</span> </span>) :</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">x : np array of shape (H, W, 3) and dtype uint8</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>    img = Image.fromarray(x)<br>    buffer = io.BytesIO()<br>    img.save(buffer, ‘JPEG’, quality = quality)<br>    jpeg = buffer.getvalue()<br>    <span class="hljs-built_in">bytes</span> = np.frombuffer(jpeg, dtype =np.uint8)<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(<span class="hljs-built_in">bytes</span>)/<span class="hljs-number">1000</span><br></code></pre></td></tr></table></figure><p><imgsrc="https://img.enderfga.cn/img/image-20230530135032898.png" /></p><p><strong>实验主要提升的效果 (Main Improvements inExperiments)</strong>:实验结果表明，DDPO在所有任务上都明显优于RWR，表明将去噪过程形式化为马尔可夫决策过程并直接估计策略梯度比优化奖励加权似然的下界更有效。此外，DDPO还能够有效地适应预训练模型，只需指定奖励函数，无需进行任何进一步的数据策划。</p><p><imgsrc="https://img.enderfga.cn/img/image-20230530135637339.png" /></p><p><strong>具体来说，DDPO的实现可以分为以下几个步骤：</strong></p><ol type="1"><li><strong>定义决策问题</strong>：首先，DDPO将去噪过程定义为一个马尔可夫决策过程（MDP）。在这个MDP中，每一步都包括一个状态（当前的去噪图像）和一个动作（下一步的去噪操作）。</li><li><strong>定义奖励函数</strong>：然后，DDPO定义了一个奖励函数，用于评估每一步去噪操作的效果。这个奖励函数可以基于任何与任务目标相关的度量，例如在这篇论文中，奖励可能是基于图像的压缩性、审美质量或与提示的对齐程度。</li><li><strong>优化策略</strong>：接下来，DDPO使用策略梯度算法来优化去噪策略。这个过程包括生成一组去噪轨迹，计算每个轨迹的预期奖励，然后使用这些预期奖励来更新策略的参数。</li><li><strong>迭代优化</strong>：最后，DDPO通过迭代这个过程，逐步改进去噪策略。每一轮迭代都会生成新的去噪轨迹，计算新的预期奖励，然后使用这些新的预期奖励来更新策略的参数。<span class="math display">\[\hat{g}_{\mathrm{IS}}=\mathbb{E}\left[\sum_{t=0}^T\frac{p_\theta\left(\mathbf{x}_{t-1} \mid \mathbf{c}, t,\mathbf{x}_t\right)}{p_{\theta_{\text {old }}}\left(\mathbf{x}_{t-1}\mid \mathbf{c}, t, \mathbf{x}_t\right)} \nabla_\theta \logp_\theta\left(\mathbf{x}_{t-1} \mid \mathbf{c}, t, \mathbf{x}_t\right)r\left(\mathbf{x}_0, \mathbf{c}\right)\right]\]</span></li></ol><h2id="dpok-reinforcement-learning-for-fine-tuning-text-to-image-diffusion-models">DPOK:Reinforcement Learning for Fine-tuning Text-to-Image DiffusionModels</h2><p><strong>5.25，Google research</strong></p><p><strong>研究问题 (Research Problem)</strong>:本文主要研究如何通过在线强化学习（RL）来优化和微调文本到图像的模型，以改善其性能和质量。</p><p><strong>动机 (Motivation)</strong>:尽管文本到图像的模型已经取得了显著的进步，但它们仍然存在系统性的弱点，例如<em>有限的对象组合能力和在生成指定颜色和数量的对象时的困难</em>。此外，使用<em>人类反馈进行学习已被证明是一种有效的方法</em>来克服这些限制。然而，<em>基于奖励的监督微调往往会导致图像质量的下降</em>。因此，本文提出了一种新的在线RL微调方法来解决这些问题。</p><p><strong>新颖性 (What's New)</strong>:本文提出了一种名为DPOK的新方法，该方法将策略优化与KL正则化结合起来。与以往的工作不同，本文将微调任务定义为一个RL问题，并使用策略梯度来更新预训练的文本到图像扩散模型，以最大化反馈训练的奖励。</p><p><strong>方法的总体思想 (Overall Idea of the Method)</strong>:DPOK方法的核心思想是利用在线强化学习来微调文本到图像的模型。在这个过程中，模型是在新的样本上进行更新的，这些样本来自于之前训练的模型。此外，该方法还引入了Kullback-Leibler(KL) 散度作为正则化项，以确保更新后的模型不会偏离原始模型太远。</p><p><strong>作者引入了两种KL正则化方法，一种是KL-D，另一种是KL-O。KL-D基于预训练模型的数据，通过调整原始奖励中的一个移位因子来实现正则化，使得每个样本的权重更趋向于均匀分布。而KL-O则通过在奖励加权损失中引入一个额外的项来实现，这个额外的项惩罚了从预训练模型和当前模型得出的去噪方向之间的L2距离。</strong></p><p><span class="math display">\[\begin{aligned}&amp; \mathbb{E}_{p(z)}\left[\alpha \mathbb{E}_{p_\theta^*\left(x_{0: T}\mid z\right)}\left[-r\left(x_0, z\right) \sum_{t=1}^T \logp_\theta\left(x_{t-1} \mid x_t, z\right)\right]\right. \\&amp; \left.+\beta \sum_{t=1}^T \mathbb{E}_{p_\theta^*\left(x_t \midz\right)}\left[\operatorname{KL}\left(p_\theta\left(x_{t-1} \mid x_t,z\right) \| p_{\mathrm{pre}}\left(x_{t-1} \mid x_t,z\right)\right)\right]\right]\end{aligned}\]</span></p><p><strong>实验主要提升的效果 (Main Improvements inExperiments)</strong>:在实验中，作者发现在线RL微调能够在保持高图像保真度的同时，实现强大的文本-图像对齐。此外，与监督微调相比，在线训练允许在（监督）训练数据集之外评估奖励模型和条件KL散度，这提供了明显的优势。在实证比较中，作者还在监督微调方法中引入了KL正则项，以进行公平的比较。</p><p><imgsrc="https://img.enderfga.cn/img/image-20230530141846201.png" /></p><p><imgsrc="https://img.enderfga.cn/img/image-20230530142225063.png" /></p><p><strong>与上面一篇的对比：</strong></p><ol type="1"><li><strong>相似性</strong>：Black等人的工作和本文都探讨了在线强化学习微调用于改进文本到图像扩散模型。他们都展示了RL微调可以优于监督微调，这与本文的观察结果一致。</li><li><strong>差异性</strong>：在本文中，作者不仅关注奖励优化，还受到监督微调中的失败案例（如过饱和或非真实感图像）的启发，旨在找到一种带有KL正则化的RL解决方案来解决问题。此外，本文还系统地分析了监督微调和在线微调中KL正则化的理论依据，并展示了在线RL微调中KL正则化比监督微调更有效。通过采用在线KL正则化，本文的算法成功地在保持高奖励和图像质量的同时，避免了过度优化的问题。</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近读的两篇关于使用RLHF微调diffusion的论文。&lt;/p&gt;</summary>
    
    
    
    
    <category term="计算机视觉" scheme="http://enderfga.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>基于知识库的自动问答系统</title>
    <link href="http://enderfga.cn/2023/05/31/kbqa/"/>
    <id>http://enderfga.cn/2023/05/31/kbqa/</id>
    <published>2023-05-31T08:09:40.000Z</published>
    <updated>2023-07-08T10:44:34.657Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>自然语言处理期中和期末项目。</p><span id="more"></span><embed src="./kbqa1.pdf" width="100%" height="750" type="application/pdf"><embed src="./kbqa2.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    <summary type="html">&lt;p&gt;自然语言处理期中和期末项目。&lt;/p&gt;</summary>
    
    
    
    
    <category term="自然语言处理" scheme="http://enderfga.cn/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>最优化理论与算法</title>
    <link href="http://enderfga.cn/2023/05/31/opt/"/>
    <id>http://enderfga.cn/2023/05/31/opt/</id>
    <published>2023-05-31T07:30:34.000Z</published>
    <updated>2023-07-08T10:47:09.976Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>期中与期末，<ahref="https://enderfga.cn/markmap.html">笔记1链接</a>；<ahref="https://enderfga.cn/markmap2.html">笔记2链接</a>。</p><p><imgsrc="https://img.enderfga.cn/img/image-20230531154210275.png" /></p><span id="more"></span><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br></pre></td><td class="code"><pre><code class="hljs matlab"><span class="hljs-comment">% 利用最速梯度下降法求解函数的极小点</span><br><br><span class="hljs-comment">% 函数定义</span><br>f = @(x) <span class="hljs-number">0.5</span>*x(<span class="hljs-number">1</span>)^<span class="hljs-number">2</span> + x(<span class="hljs-number">2</span>)^<span class="hljs-number">2</span>;<br><br><span class="hljs-comment">% 梯度定义</span><br>grad_f = @(x) [x(<span class="hljs-number">1</span>); <span class="hljs-number">2</span>*x(<span class="hljs-number">2</span>)];<br><br><span class="hljs-comment">% 初始值和收敛阈值</span><br>x0 = [<span class="hljs-number">2</span>; <span class="hljs-number">1</span>];<br>epsilon = <span class="hljs-number">1e-2</span>;<br><br><span class="hljs-comment">% 最速梯度下降法</span><br>alpha = <span class="hljs-number">0.1</span>;  <span class="hljs-comment">% 初始步长</span><br>x = x0;<br>grad = grad_f(x);<br>iter = <span class="hljs-number">0</span>;<br><br><span class="hljs-keyword">while</span> norm(grad) &gt; epsilon<br>    <span class="hljs-comment">% 计算步长</span><br>    <span class="hljs-comment">% 这里可以使用线搜索方法来选择合适的步长 alpha，如 Armijo 规则或 Wolfe 条件</span><br>  <br>    <span class="hljs-comment">% 更新变量</span><br>    x = x - alpha * grad;<br>    grad = grad_f(x);<br>  <br>    iter = iter + <span class="hljs-number">1</span>;<br><span class="hljs-keyword">end</span><br><br><span class="hljs-comment">% 输出结果</span><br>fprintf(<span class="hljs-string">&#x27;最小值点: (%f, %f)\n&#x27;</span>, x(<span class="hljs-number">1</span>), x(<span class="hljs-number">2</span>));<br>fprintf(<span class="hljs-string">&#x27;迭代次数: %d\n&#x27;</span>, iter);<br><br><span class="hljs-comment">% 利用牛顿法求解函数的极小点</span><br><br><span class="hljs-comment">% 函数定义</span><br>f = @(x) <span class="hljs-number">100</span>*(x(<span class="hljs-number">2</span>) - x(<span class="hljs-number">1</span>)^<span class="hljs-number">2</span>)^<span class="hljs-number">2</span> + (<span class="hljs-number">1</span> - x(<span class="hljs-number">1</span>))^<span class="hljs-number">2</span>;<br><br><span class="hljs-comment">% 梯度定义</span><br>grad_f = @(x) [<span class="hljs-number">-400</span>*x(<span class="hljs-number">1</span>)*(x(<span class="hljs-number">2</span>) - x(<span class="hljs-number">1</span>)^<span class="hljs-number">2</span>) - <span class="hljs-number">2</span>*(<span class="hljs-number">1</span> - x(<span class="hljs-number">1</span>));<br>                <span class="hljs-number">200</span>*(x(<span class="hljs-number">2</span>) - x(<span class="hljs-number">1</span>)^<span class="hljs-number">2</span>)];<br><br><span class="hljs-comment">% Hessian 矩阵定义</span><br>hessian = @(x) [<span class="hljs-number">1200</span>*x(<span class="hljs-number">1</span>)^<span class="hljs-number">2</span> - <span class="hljs-number">400</span>*x(<span class="hljs-number">2</span>) + <span class="hljs-number">2</span>, <span class="hljs-number">-400</span>*x(<span class="hljs-number">1</span>);<br>                <span class="hljs-number">-400</span>*x(<span class="hljs-number">1</span>), <span class="hljs-number">200</span>];<br><br><span class="hljs-comment">% 初始值和收敛阈值</span><br>x0 = [<span class="hljs-number">-2</span>; <span class="hljs-number">2</span>];<br>epsilon = <span class="hljs-number">1e-2</span>;<br><br><span class="hljs-comment">% 牛顿法</span><br>x = x0;<br>grad = grad_f(x);<br>iter = <span class="hljs-number">0</span>;<br><br><span class="hljs-keyword">while</span> norm(grad) &gt; epsilon<br>    <span class="hljs-comment">% 计算 Hessian 矩阵和其逆矩阵</span><br>    H = hessian(x);<br>    inv_H = inv(H);<br>  <br>    <span class="hljs-comment">% 更新变量</span><br>    x = x - inv_H * grad;<br>    grad = grad_f(x);<br>  <br>    iter = iter + <span class="hljs-number">1</span>;<br><span class="hljs-keyword">end</span><br><br><span class="hljs-comment">% 输出结果</span><br>fprintf(<span class="hljs-string">&#x27;最小值点: (%f, %f)\n&#x27;</span>, x(<span class="hljs-number">1</span>), x(<span class="hljs-number">2</span>));<br>fprintf(<span class="hljs-string">&#x27;迭代次数: %d\n&#x27;</span>, iter);<br><br><span class="hljs-comment">% 利用共轭梯度法求解方程组的根</span><br><br><span class="hljs-comment">% 原始方程组的系数矩阵 A</span><br>A = [<span class="hljs-number">4</span>, <span class="hljs-number">-3</span>; <span class="hljs-number">2</span>, <span class="hljs-number">1</span>];<br><br><span class="hljs-comment">% 原始方程组的右侧向量 b</span><br>b = [<span class="hljs-number">11</span>; <span class="hljs-number">13</span>];<br><br><span class="hljs-comment">% 转化为对称正定矩阵 B = A^T * A</span><br>B = A&#x27; * A;<br><br><span class="hljs-comment">% 转化后的方程组的右侧向量</span><br>b_new = A&#x27; * b;<br><br><span class="hljs-comment">% 初始值和收敛阈值</span><br>x0 = [<span class="hljs-number">0</span>; <span class="hljs-number">0</span>];<br>epsilon = <span class="hljs-number">1e-6</span>;<br><br><span class="hljs-comment">% 共轭梯度法</span><br>x = x0;<br>r = b_new - B * x;<br>p = r;<br>iter = <span class="hljs-number">0</span>;<br><br><span class="hljs-keyword">while</span> norm(r) &gt; epsilon<br>    alpha = (r&#x27; * r) / (p&#x27; * B * p);<br>    x = x + alpha * p;<br>    r_new = r - alpha * B * p;<br>    <span class="hljs-built_in">beta</span> = (r_new&#x27; * r_new) / (r&#x27; * r);<br>    p = r_new + <span class="hljs-built_in">beta</span> * p;<br>    r = r_new;<br>    iter = iter + <span class="hljs-number">1</span>;<br><span class="hljs-keyword">end</span><br><br><span class="hljs-comment">% 输出结果</span><br>fprintf(<span class="hljs-string">&#x27;方程的根: (%f, %f)\n&#x27;</span>, x(<span class="hljs-number">1</span>), x(<span class="hljs-number">2</span>));<br>fprintf(<span class="hljs-string">&#x27;迭代次数: %d\n&#x27;</span>, iter);<br><br><span class="hljs-comment">% 利用DFP算法求解函数的极小点</span><br><br><span class="hljs-comment">% 函数定义</span><br>f = @(x) x(<span class="hljs-number">1</span>)^<span class="hljs-number">2</span> + <span class="hljs-number">2</span>*x(<span class="hljs-number">2</span>)^<span class="hljs-number">2</span> - <span class="hljs-number">4</span>*x(<span class="hljs-number">1</span>) - <span class="hljs-number">2</span>*x(<span class="hljs-number">1</span>)*x(<span class="hljs-number">2</span>);<br><br><span class="hljs-comment">% 梯度定义</span><br>grad_f = @(x) [<span class="hljs-number">2</span>*x(<span class="hljs-number">1</span>) - <span class="hljs-number">4</span> - <span class="hljs-number">2</span>*x(<span class="hljs-number">2</span>); <span class="hljs-number">4</span>*x(<span class="hljs-number">2</span>) - <span class="hljs-number">2</span>*x(<span class="hljs-number">1</span>)];<br><br><span class="hljs-comment">% 初始点和初始近似Hessian矩阵</span><br>x0 = [<span class="hljs-number">1</span>; <span class="hljs-number">1</span>];<br>H0 = <span class="hljs-built_in">eye</span>(<span class="hljs-number">2</span>);<br><br><span class="hljs-comment">% 最大迭代次数和停止迭代的阈值</span><br>max_iter = <span class="hljs-number">100</span>;<br>epsilon = <span class="hljs-number">1e-6</span>;<br><br><span class="hljs-comment">% DFP算法</span><br>x = x0;<br>H = H0;<br>g = grad_f(x);<br>iter = <span class="hljs-number">0</span>;<br><br><span class="hljs-keyword">while</span> norm(g) &gt; epsilon &amp;&amp; iter &lt; max_iter<br>    d = -H * g;  <span class="hljs-comment">% 计算搜索方向</span><br>  <br>    <span class="hljs-comment">% 使用线搜索方法选择合适的步长</span><br>    alpha = <span class="hljs-number">1</span>; <span class="hljs-comment">% 这里可以使用固定步长或者其他线搜索方法</span><br>  <br>    x_new = x + alpha * d;<br>    g_new = grad_f(x_new);<br>    s = x_new - x;<br>    y = g_new - g;<br>  <br>    rho = <span class="hljs-number">1</span> / (y&#x27; * s);<br>    H = (<span class="hljs-built_in">eye</span>(<span class="hljs-number">2</span>) - rho * s * y&#x27;) * H * (<span class="hljs-built_in">eye</span>(<span class="hljs-number">2</span>) - rho * y * s&#x27;) + rho * s * s&#x27;; <span class="hljs-comment">% 更新近似Hessian矩阵</span><br>  <br>    x = x_new;<br>    g = g_new;<br>    iter = iter + <span class="hljs-number">1</span>;<br><span class="hljs-keyword">end</span><br><br><span class="hljs-comment">% 输出结果</span><br>fprintf(<span class="hljs-string">&#x27;最小值点: (%f, %f)\n&#x27;</span>, x(<span class="hljs-number">1</span>), x(<span class="hljs-number">2</span>));<br>fprintf(<span class="hljs-string">&#x27;迭代次数: %d\n&#x27;</span>, iter);<br><br><br></code></pre></td></tr></table></figure><embed src="./opt.pdf" width="100%" height="750" type="application/pdf"><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br></pre></td><td class="code"><pre><code class="hljs matlab"><span class="hljs-comment">%% 第一题</span><br><span class="hljs-comment">% 定义目标函数</span><br>fun = @(x) x(<span class="hljs-number">1</span>)^<span class="hljs-number">2</span> + x(<span class="hljs-number">2</span>)^<span class="hljs-number">2</span>;<br><br><span class="hljs-comment">% 定义约束条件</span><br>A = []; b = []; <span class="hljs-comment">% 无线性不等式约束</span><br>Aeq = [<span class="hljs-number">1</span> <span class="hljs-number">1</span>]; beq = <span class="hljs-number">1</span>; <span class="hljs-comment">% 线性等式约束 x1 + x2 = 1</span><br>lb = [-Inf, -Inf]; ub = [Inf, <span class="hljs-number">1</span>/<span class="hljs-number">4</span>]; <span class="hljs-comment">% x2 &lt;= 1/4</span><br><br><span class="hljs-comment">% 定义初始值</span><br>x0 = [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>];<br><br><span class="hljs-comment">% 调用 fmincon 函数求解</span><br>options = optimoptions(<span class="hljs-string">&#x27;fmincon&#x27;</span>,<span class="hljs-string">&#x27;Display&#x27;</span>,<span class="hljs-string">&#x27;iter&#x27;</span>,<span class="hljs-string">&#x27;Algorithm&#x27;</span>,<span class="hljs-string">&#x27;interior-point&#x27;</span>);<br>[x,fval] = fmincon(fun, x0, A, b, Aeq, beq, lb, ub, [], options);<br><br><span class="hljs-comment">% 输出结果</span><br><span class="hljs-built_in">disp</span>(<span class="hljs-string">&#x27;The solution is:&#x27;</span>), <span class="hljs-built_in">disp</span>(x)<br><span class="hljs-built_in">disp</span>(<span class="hljs-string">&#x27;The minimum value of the objective function is:&#x27;</span>), <span class="hljs-built_in">disp</span>(fval)<br><br><span class="hljs-comment">%% 第二题</span><br><span class="hljs-comment">% 定义目标函数和罚函数</span><br>fun = @(x) x(<span class="hljs-number">1</span>)^<span class="hljs-number">2</span> + x(<span class="hljs-number">2</span>)^<span class="hljs-number">2</span> + x(<span class="hljs-number">3</span>)^<span class="hljs-number">2</span>;<br>g = @(x) <span class="hljs-built_in">abs</span>(x(<span class="hljs-number">1</span>) + <span class="hljs-number">2</span>*x(<span class="hljs-number">2</span>) - x(<span class="hljs-number">3</span>) - <span class="hljs-number">4</span>) + <span class="hljs-built_in">abs</span>(x(<span class="hljs-number">1</span>) - x(<span class="hljs-number">2</span>) + x(<span class="hljs-number">3</span>) + <span class="hljs-number">2</span>);<br>P = @(x, r) fun(x) + r * g(x);<br><br><span class="hljs-comment">% 定义初始值和罚项权重</span><br>x0 = [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>];<br>r = <span class="hljs-number">1</span>;<br><br><span class="hljs-comment">% 使用 fminunc 函数求解</span><br>options = optimoptions(<span class="hljs-string">&#x27;fminunc&#x27;</span>,<span class="hljs-string">&#x27;Display&#x27;</span>,<span class="hljs-string">&#x27;iter&#x27;</span>,<span class="hljs-string">&#x27;Algorithm&#x27;</span>,<span class="hljs-string">&#x27;quasi-newton&#x27;</span>);<br>[x,fval] = fminunc(@(x) P(x, r), x0, options);<br><br><span class="hljs-comment">% 输出结果</span><br><span class="hljs-built_in">disp</span>(<span class="hljs-string">&#x27;The solution is:&#x27;</span>), <span class="hljs-built_in">disp</span>(x)<br><span class="hljs-built_in">disp</span>(<span class="hljs-string">&#x27;The minimum value of the objective function is:&#x27;</span>), <span class="hljs-built_in">disp</span>(fval)<br><br><span class="hljs-comment">%% 第三题</span><br><span class="hljs-comment">% 定义目标函数</span><br>fun = @(x) -(x + <span class="hljs-number">5</span>*<span class="hljs-built_in">sin</span>(<span class="hljs-number">5</span>*x) + <span class="hljs-number">10</span>*<span class="hljs-built_in">cos</span>(<span class="hljs-number">4</span>*x)); <span class="hljs-comment">% 注意我们取负值，因为MATLAB的遗传算法默认是求最小值</span><br><br><span class="hljs-comment">% 定义遗传算法的参数</span><br>numberOfVariables = <span class="hljs-number">1</span>; <br><br><span class="hljs-comment">% 定义约束上下界</span><br>lb = <span class="hljs-number">0</span>; <br>ub = <span class="hljs-number">10</span>;<br><br><span class="hljs-comment">% 使用 ga 函数求解</span><br>[x,fval] = ga(fun,numberOfVariables,[],[],[],[],lb,ub);<br><br><span class="hljs-comment">% 输出结果</span><br><span class="hljs-built_in">disp</span>(<span class="hljs-string">&#x27;The solution is:&#x27;</span>), <span class="hljs-built_in">disp</span>(x)<br><span class="hljs-built_in">disp</span>(<span class="hljs-string">&#x27;The maximum value of the objective function is:&#x27;</span>), <span class="hljs-built_in">disp</span>(-fval)<br><br><span class="hljs-comment">%% 第四题</span><br><span class="hljs-comment">% 定义目标函数和罚函数</span><br>fun = @(x) x(<span class="hljs-number">1</span>)^<span class="hljs-number">2</span> + x(<span class="hljs-number">2</span>)^<span class="hljs-number">2</span>;<br>g = @(x) x(<span class="hljs-number">1</span>) - <span class="hljs-number">2</span>;<br>P = @(x, r) fun(x) - r * <span class="hljs-built_in">log</span>(g(x));<br><br><span class="hljs-comment">% 定义初始值和罚项权重</span><br>x0 = [<span class="hljs-number">3</span>, <span class="hljs-number">0</span>]; <span class="hljs-comment">% 初始值需要满足约束条件</span><br>r = <span class="hljs-number">0.0001</span>;<br><br><span class="hljs-comment">% 使用 fminunc 函数求解</span><br>options = optimoptions(<span class="hljs-string">&#x27;fminunc&#x27;</span>,<span class="hljs-string">&#x27;Display&#x27;</span>,<span class="hljs-string">&#x27;iter&#x27;</span>,<span class="hljs-string">&#x27;Algorithm&#x27;</span>,<span class="hljs-string">&#x27;quasi-newton&#x27;</span>);<br>[x,fval] = fminunc(@(x) P(x, r), x0, options);<br><br><span class="hljs-comment">% 输出结果</span><br><span class="hljs-built_in">disp</span>(<span class="hljs-string">&#x27;The solution is:&#x27;</span>), <span class="hljs-built_in">disp</span>(x)<br><span class="hljs-built_in">disp</span>(<span class="hljs-string">&#x27;The minimum value of the objective function is:&#x27;</span>), <span class="hljs-built_in">disp</span>(fval)<br><br><span class="hljs-comment">%% 第五题</span><br><span class="hljs-comment">% 定义目标函数</span><br>fun = @(x) sum(x.^<span class="hljs-number">2</span>);<br><br><span class="hljs-comment">% 定义参数</span><br>alphas = [<span class="hljs-number">0.001</span>,<span class="hljs-number">0.1</span>,<span class="hljs-number">0.5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">5</span>]; <span class="hljs-comment">% 邻域大小</span><br>T = <span class="hljs-number">100</span>; <span class="hljs-comment">% 初始温度</span><br>T_min = <span class="hljs-number">1e-3</span>; <span class="hljs-comment">% 最小温度</span><br>cooling_rate = <span class="hljs-number">0.95</span>; <span class="hljs-comment">% 冷却率</span><br>max_iter = <span class="hljs-number">100</span>; <span class="hljs-comment">% 每个温度下的最大迭代次数</span><br><br><span class="hljs-comment">% 定义约束条件</span><br>lb = <span class="hljs-number">-15</span> * <span class="hljs-built_in">ones</span>(<span class="hljs-number">1</span>, <span class="hljs-number">10</span>); <span class="hljs-comment">% 下界</span><br>ub = <span class="hljs-number">15</span> * <span class="hljs-built_in">ones</span>(<span class="hljs-number">1</span>, <span class="hljs-number">10</span>); <span class="hljs-comment">% 上界</span><br><br>results = <span class="hljs-built_in">zeros</span>(<span class="hljs-built_in">length</span>(alphas), <span class="hljs-number">2</span>);<br><br><span class="hljs-keyword">for</span> a = <span class="hljs-number">1</span>:<span class="hljs-built_in">length</span>(alphas)<br>    <span class="hljs-comment">% 初始化解</span><br>    x = lb + (ub - lb) .* <span class="hljs-built_in">rand</span>(<span class="hljs-number">1</span>, <span class="hljs-number">10</span>);<br>    alpha = alphas(a);<br><br>    T_current = T;<br>    <span class="hljs-keyword">while</span> T_current &gt; T_min<br>        <span class="hljs-keyword">for</span> <span class="hljs-built_in">i</span> = <span class="hljs-number">1</span>:max_iter<br>            <span class="hljs-comment">% 在邻域中随机生成新解</span><br>            x_new = x + alpha * (<span class="hljs-number">2</span>*<span class="hljs-built_in">rand</span>(<span class="hljs-number">1</span>, <span class="hljs-number">10</span>) - <span class="hljs-number">1</span>);<br>            x_new = <span class="hljs-built_in">max</span>(<span class="hljs-built_in">min</span>(x_new, ub), lb); <span class="hljs-comment">% 确保新解满足约束条件</span><br><br>            <span class="hljs-comment">% 计算目标函数的改变量</span><br>            delta_f = fun(x_new) - fun(x);<br><br>            <span class="hljs-comment">% 如果新解更好，或者满足 Metropolis 准则，则接受新解</span><br>            <span class="hljs-keyword">if</span> delta_f &lt; <span class="hljs-number">0</span> || <span class="hljs-built_in">rand</span>() &lt; <span class="hljs-built_in">exp</span>(-delta_f / T_current)<br>                x = x_new;<br>            <span class="hljs-keyword">end</span><br>        <span class="hljs-keyword">end</span><br><br>        <span class="hljs-comment">% 降低温度</span><br>        T_current = cooling_rate * T_current;<br>    <span class="hljs-keyword">end</span><br><br>    <span class="hljs-comment">% 记录结果</span><br>    results(a, :) = [alpha, fun(x)];<br><br>    <span class="hljs-comment">% 输出结果</span><br>    fprintf(<span class="hljs-string">&#x27;For alpha = %f:\n&#x27;</span>, alpha);<br>    <span class="hljs-built_in">disp</span>(<span class="hljs-string">&#x27;The solution is:&#x27;</span>), <span class="hljs-built_in">disp</span>(x)<br>    <span class="hljs-built_in">disp</span>(<span class="hljs-string">&#x27;The minimum value of the objective function is:&#x27;</span>), <span class="hljs-built_in">disp</span>(fun(x))<br><span class="hljs-keyword">end</span><br><br><span class="hljs-comment">% 可视化结果</span><br><span class="hljs-built_in">figure</span>;<br>semilogy(results(:, <span class="hljs-number">1</span>), results(:, <span class="hljs-number">2</span>), <span class="hljs-string">&#x27;-o&#x27;</span>, <span class="hljs-string">&#x27;Color&#x27;</span>, [<span class="hljs-number">0.2</span> <span class="hljs-number">0.4</span> <span class="hljs-number">0.6</span>], <span class="hljs-string">&#x27;LineWidth&#x27;</span>, <span class="hljs-number">2</span>, <span class="hljs-string">&#x27;MarkerSize&#x27;</span>, <span class="hljs-number">8</span>);<br>xlabel(<span class="hljs-string">&#x27;Alpha&#x27;</span>);<br>ylabel(<span class="hljs-string">&#x27;Minimum Value of Objective Function&#x27;</span>);<br>title(<span class="hljs-string">&#x27;Impact of Alpha on the Solution&#x27;</span>);<br><br><br><br></code></pre></td></tr></table></figure><embed src="./opt2.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    <summary type="html">&lt;p&gt;期中与期末，&lt;a
href=&quot;https://enderfga.cn/markmap.html&quot;&gt;笔记1链接&lt;/a&gt;；&lt;a
href=&quot;https://enderfga.cn/markmap2.html&quot;&gt;笔记2链接&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img
src=&quot;https://img.enderfga.cn/img/image-20230531154210275.png&quot; /&gt;&lt;/p&gt;</summary>
    
    
    
    
    <category term="最优化理论" scheme="http://enderfga.cn/tags/%E6%9C%80%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title>3D论文PPT</title>
    <link href="http://enderfga.cn/2023/03/23/3dppt/"/>
    <id>http://enderfga.cn/2023/03/23/3dppt/</id>
    <published>2023-03-23T11:25:47.000Z</published>
    <updated>2023-05-31T08:28:21.754Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><figure><img src="https://img.enderfga.cn/img/image-20230323192941643.png"alt="image-20230323192941643" /><figcaption aria-hidden="true">image-20230323192941643</figcaption></figure><embed src="./3d.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="计算机视觉" scheme="http://enderfga.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>RLHF</title>
    <link href="http://enderfga.cn/2023/03/01/RLHF/"/>
    <id>http://enderfga.cn/2023/03/01/RLHF/</id>
    <published>2023-03-01T03:17:42.000Z</published>
    <updated>2023-03-01T03:19:08.223Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="rlhf">RLHF</h1><h2 id="aligning-text-to-image-models-using-human-feedback">AligningText-to-Image Models using Human Feedback</h2><ul><li><p>Google Research ,University of California</p></li><li><p>2023.2.23</p><span id="more"></span></li></ul><h2 id="motivation">Motivation</h2><p>深度生成模型在文本到图像合成方面取得了令人印象深刻的成果，但当前的文本到图像模型往往生成与文本提示不够相符的图像。</p><p>本文的动机是改进文本到图像合成模型，使其能够更好地与文本提示对齐。</p><p>作者的方法比预训练模型更准确地生成具有指定颜色、计数和背景的对象。</p><h2 id="proposal">Proposal</h2><ul><li>提出了一种简单而有效的微调方法，用于使用人类反馈对文本到图像模型进行对齐。</li><li>使用人类反馈进行微调可以显着提高文本到图像模型的图像文本对齐，在人类评估中，我们的模型在图像文本对齐方面达到了高达47％的改善，但图像保真度略有降低。</li><li>学习的奖励函数比CLIP分数更准确地预测了人类对质量的评估。</li><li>基于作者学习的奖励函数的采样也可以显着改善图像文本对齐。</li></ul><h2 id="related-work">Related Work</h2><ul><li>T2I models</li><li>Evaluating image-text alignment</li><li>Learning with human feedback</li></ul><p>与先前关注利用人类反馈改善语言模型和RL代理的工作相比，该工作探索了使用人类反馈来调整多模式文本到图像模型与人类偏好的方法。许多关于利用人类反馈学习的先前工作都包括学习一个奖励函数并最大化奖励加权可能性（通常被称为监督微调）。受其成功的启发，作者提出了一种利用人类反馈进行微调的方法来改善文本到图像模型。</p><h2 id="method">Method</h2><p><imgsrc="https://img.enderfga.cn/img/image-20230228130311025.png" /></p><p>包括三个阶段：</p><ol type="1"><li>首先从一系列文本提示中生成一组不同的图像，这些文本提示旨在测试文本到图像模型的各种功能。</li><li>然后，人类评级者对这些图像提供二进制反馈。</li><li>接下来，训练一个奖励模型，以文本提示和图像作为输入来预测人类反馈。</li><li>最后，我们使用奖励加权对数似然度来微调文本到图像模型，以改善文本图像对齐。</li></ol><h2 id="experiment">Experiment</h2><p>实验部分旨在测试人类反馈参与模型微调的有效性。实验用到的模型为 StableDiffusion v1.5</p><p><imgsrc="https://img.enderfga.cn/img/image-20230228132659591.png" /></p><p><imgsrc="https://img.enderfga.cn/img/image-20230228132741226.png" /></p><p>本文方法显著提高了图像 - 文本对齐，具体来说，模型生成的图像中有 50%的样本获得至少三分之二的赞成票（投票数量为 7票或更多赞成票），然而，微调会稍微降低图像保真度（15% 比 10%）。</p><p><imgsrc="https://img.enderfga.cn/img/image-20230228132824327.png" /></p><p>本文模型生成的图像符合 prompt指定的颜色、计数和背景。值得注意的是，本文模型还能生成没有见过的文本prompt 图像，并且质量非常高</p><p><imgsrc="https://img.enderfga.cn/img/image-20230228132908052.png" /></p><p>有奖励（绿色）比 CLIP 分数（红色）更符合典型的人类意图。</p><h2 id="limitations-and-future-directions">Limitations and futuredirections</h2><ol type="1"><li><p><strong>更细致的人类反馈</strong>，存在一些较差的生成，如高饱和度的图像颜色，指示评级者寻找更多样化的失败模式（过度饱和的颜色，不切实际的动物解剖学，物理违规等）将提高这些方面的性能。</p></li><li><p><strong>多样化和大型人类数据集</strong>，为了简化问题，作者考虑了有限的文本类别（计数，颜色，背景），因此人类反馈也相对简单（好或坏）。由于这一点，人类反馈数据的多样性有限。将其扩展到更主观的文本类别（如艺术创作）和更细致的人类反馈将是未来研究的重要方向。</p></li><li><p><strong>不同的目标和算法</strong>，为了更新文本到图像模型，作者使用奖励加权的最大似然。然而，与语言领域的先前工作类似，使用RL算法将是一个有趣的方向。作者相信RLHF微调可能会产生更好的模型，因为</p><p>（a）在更新期间使用在线样本生成</p><p>（b）KL正则化可以减轻对奖励函数的过度拟合。</p></li></ol>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;rlhf&quot;&gt;RLHF&lt;/h1&gt;
&lt;h2 id=&quot;aligning-text-to-image-models-using-human-feedback&quot;&gt;Aligning
Text-to-Image Models using Human Feedback&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Google Research ,University of California&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;2023.2.23&lt;/p&gt;</summary>
    
    
    
    
    <category term="计算机视觉" scheme="http://enderfga.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>Realfusion</title>
    <link href="http://enderfga.cn/2023/03/01/Realfusion/"/>
    <id>http://enderfga.cn/2023/03/01/Realfusion/</id>
    <published>2023-03-01T03:14:27.000Z</published>
    <updated>2023-03-01T03:16:21.212Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="realfusion">RealFusion：</h1><h3 id="reconstruction-of-any-object-from-a-single-image">360°Reconstruction of Any Object from a Single Image</h3><ul><li><p>Oxford University</p></li><li><p>2023.2.23</p><span id="more"></span><h2 id="demo">Demo</h2><p>https://lukemelas.github.io/realfusion/</p><figure><img src="https://img.enderfga.cn/img/splash-figure-v2.png"alt="Examples" /><figcaption aria-hidden="true">Examples</figcaption></figure></li></ul><h2 id="motivation-single-view-3d-reconstruction">Motivation:Single-View 3D Reconstruction</h2><ul><li>Reconstructing the 3D structure of an object from a single 2D viewis a fundamental challenge in computer vision.</li><li>In the case of a single view, the reconstruction problem is highlyill-posed. As a result, the task requires semantic understandingobtained by learning. Despite the difficulty of this task, humans areadept at using a range of monocular cues to infer the 3D structures ofobjects from single views.</li></ul><h2 id="background">Background</h2><h3 id="category-level-3d-reconstruction">Category-level 3DReconstruction</h3><ul><li>Most prior work tackles the problem of category-specific single-view3D reconstruction by training a category-level reconstructionmodel.</li><li>The work: Going beyond category-level 3D reconstruction<ul><li>This work aims to go beyond category-specific images to images ofarbitrary objects. This setting is highly challenging, but humansperform it effortlessly when they observe new objects.</li></ul></li></ul><h3 id="single-view-3d-reconstruction">Single-View 3DReconstruction</h3><ul><li>Arbitrary-object 3D reconstruction has been challenging because theproblem fundamentally requires the use of large-scale 3D priors overobject shapes, which have not been available.</li><li>With the recent rise of large-scale pretraining, this problem hasbecome tractable. Examples include:<ul><li>Contrastive: CLIP</li><li>Autoregressive: DALL-E / Parti</li><li>Diffusion Models: DALL-E 2 / Imagen / Stable Diffusion</li></ul></li><li>These pretrained models may be used as priors for a variety ofvision tasks, and we are particularly interested in 3D reconstruction.<ul><li>At a high level, you can think of these models as a tool foroptimizing the realism of an input image.</li></ul></li><li>In this way, they enable an elegant approach to 3D generation andreconstruction: using these large-scale pretrained models to enforcethat a differentiable scene looks realistic from random views.</li></ul><h2 id="proposal">Proposal</h2><ol type="1"><li><p>We propose <strong>RealFusion</strong>, a method that can extractfrom a single image of an object a 360◦ photographic 3D reconstructionwithout assumptions on the type of object imaged or 3D supervision ofany kind;</p></li><li><p>We do so by leveraging an existing 2D <strong>diffusion imagegenerator</strong> via a new single image variant of textualinversion;</p></li><li><p>We also introduce new regularizers and provide an efficientimplementation using <strong>InstantNGP</strong>;</p></li><li><p>We demonstrate <strong>state-of-the-art</strong> reconstructionresults on a number of in-the-wild images and images from existingdatasets when compared to alternative approaches.</p></li></ol><h2 id="related-work">Related Work</h2><ul><li>Image-based reconstruction of appearnce and geometry</li><li>Few-view reconstruction</li><li>Single-view reconstruction<br /></li><li>Extracting 3D models from 2D generators<br /></li><li>Diffusion Models</li></ul><h2 id="method">Method</h2><p><imgsrc="https://img.enderfga.cn/img/image-20230227112154724.png" /></p><ul><li>This approach forms the backbone of our method, RealFusion.</li></ul><ol start="0" type="1"><li>[Init] We are given a single image and a function <spanclass="math inline">\(\boldsymbol{p}_{\text {prior }}(\cdot)\)</span>which computes the likelihood of an input image <spanclass="math inline">\(\boldsymbol{I}\)</span>. We choose a camera viewand represent our scene with a differentiably-renderable representation<span class="math inline">\(\boldsymbol{x}\)</span>, for example aNeRF.</li><li>[Reconstruction] We render <spanclass="math inline">\(\boldsymbol{x}\)</span> from our given view andminimize the loss with respect to the real input image <spanclass="math inline">\(\mathbf{I}\)</span>.</li><li>[Prior] We render images <spanclass="math inline">\(\boldsymbol{I}_{\text {prior }}\)</span> of <spanclass="math inline">\(\boldsymbol{x}\)</span> from randomly-chosen viewson a hemisphere surrounding the origin, and we optimize <spanclass="math inline">\(\boldsymbol{p}_{\text {prior}}\left(\boldsymbol{I}_{\text {priol }}\right)\)</span> to enforce that<span class="math inline">\(\boldsymbol{x}\)</span> looks realistic fromall directions.</li></ol><ul><li>Prior work has explored this question in the domain of 3D generation<ul><li>Dreamfields: CLIP prior</li><li>DreamFusion: Diffusion model prior</li></ul></li><li>In our work, we adopt a diffusion model prior using StableDiffusion, a text-conditional latent diffusion model.</li><li>As currently stated, our set up combines a reconstruction objectivewith a latent diffusion-based prior objective, which is conditioned on amanual text prompt (e.g. "An image of a fish.")</li><li>However, we found that these results were lacking.</li><li>In particular, the 3D shapes that are generated look like the inputobject from the input view, but do not look like the input object fromother views.</li><li>To fix this, we need to modify the prior to place a high likelihoodon our input object, rather than a generic object with the samedescription.</li><li>We do so by performing textual inversion.<ul><li>We optimize a text embedding <spanclass="math inline">\(\mathbf{e}\)</span> in the text encoder of thediffusion model to match our input image.</li><li>Usually textual inversion is performed with multiple views of anobject, but we substitute these views with heavy imageaugmentations.</li></ul></li><li>We also add other pieces of regularization:<ol type="1"><li>A regularization on rendered normals</li><li>A coarse-to-fine training setup</li></ol></li><li>However, the key piece of the puzzle is the textual inversion.</li></ul><h2 id="experiment">Experiment</h2><p><imgsrc="https://img.enderfga.cn/img/image-20230227165614420.png" /></p><p><imgsrc="https://img.enderfga.cn/img/image-20230227165925713.png" /></p><p><imgsrc="https://img.enderfga.cn/img/image-20230227165956615.png" /></p><p><imgsrc="https://img.enderfga.cn/img/image-20230227170021757.png" /></p><p><imgsrc="https://img.enderfga.cn/img/image-20230227170047165.png" /></p><p><imgsrc="https://img.enderfga.cn/img/image-20230227170111552.png" /></p><h2 id="limitations">Limitations</h2><ul><li>Requires per-image optimization<ul><li>Both the textual inversion and the 3D optimization procedure must beperformed separately for each input image.</li><li>As a result, the process is relatively slow and difficult to applyto large datasets</li></ul></li><li>In some cases, reconstruction fails to produce a solid shape<ul><li>Perhaps this could be alleviated with better inductive biases orregularization terms</li></ul></li><li>In some cases, reconstruction produces two-headed objects<ul><li>This is known as the Janus Problem</li></ul></li></ul>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;realfusion&quot;&gt;RealFusion：&lt;/h1&gt;
&lt;h3 id=&quot;reconstruction-of-any-object-from-a-single-image&quot;&gt;360°
Reconstruction of Any Object from a Single Image&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Oxford University&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;2023.2.23&lt;/p&gt;</summary>
    
    
    
    
    <category term="计算机视觉" scheme="http://enderfga.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>Outstanding Wordle</title>
    <link href="http://enderfga.cn/2023/02/22/wordle/"/>
    <id>http://enderfga.cn/2023/02/22/wordle/</id>
    <published>2023-02-22T13:38:54.000Z</published>
    <updated>2023-02-22T13:49:05.366Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>唯一一次认真参加数学建模，虽然曾经的我确实不喜欢这类赛事。假如拿到O/F奖了就考虑写写经验或者录个视频，捞了就当我没说。</p><span id="more"></span><embed src="./wordle.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    <summary type="html">&lt;p&gt;唯一一次认真参加数学建模，虽然曾经的我确实不喜欢这类赛事。假如拿到O/F奖了就考虑写写经验或者录个视频，捞了就当我没说。&lt;/p&gt;</summary>
    
    
    
    
    <category term="数学建模" scheme="http://enderfga.cn/tags/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/"/>
    
  </entry>
  
  <entry>
    <title>亚太数模论文</title>
    <link href="http://enderfga.cn/2023/02/20/apmcm/"/>
    <id>http://enderfga.cn/2023/02/20/apmcm/</id>
    <published>2023-02-20T11:02:17.000Z</published>
    <updated>2023-02-20T11:05:59.145Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>这么烂的学术垃圾都能拿奖，这比赛确实没含金量</p><span id="more"></span><embed src="./apmcm.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    <summary type="html">&lt;p&gt;这么烂的学术垃圾都能拿奖，这比赛确实没含金量&lt;/p&gt;</summary>
    
    
    
    
    <category term="数学建模" scheme="http://enderfga.cn/tags/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/"/>
    
  </entry>
  
  <entry>
    <title>MAV3D:Text-To-4D Dynamic Scene Generation</title>
    <link href="http://enderfga.cn/2023/02/11/mav/"/>
    <id>http://enderfga.cn/2023/02/11/mav/</id>
    <published>2023-02-11T15:42:41.000Z</published>
    <updated>2023-02-11T15:51:26.196Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>论文阅读笔记</p><span id="more"></span><div style="text-align:center"><iframe width="100%" height="500" src="https://player.bilibili.com/player.html?aid=479164157&amp;bvid=BV1aT411R77Z&amp;cid=1003655006&amp;page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"></iframe></iframe></div><h1 id="mav3dtext-to-4d-dynamic-scene-generation">MAV3D:Text-To-4DDynamic Scene Generation</h1><ul><li><p><strong>Meta AI</strong></p></li><li><p><strong>2023.1.26</strong></p><h2 id="demo">Demo</h2><p><ahref="https://make-a-video3d.github.io/">https://make-a-video3d.github.io/</a></p><video src="./rotating_grid.mp4"></video></li></ul><h2 id="motivation">Motivation</h2><ol type="1"><li><strong>需要一个有效的、端到端可学习的动态三维场景表征；</strong></li><li><strong>需要一个有监督学习的数据源，因为目前并不存在大规模的（文本，4D）对的数据集可供学习；</strong></li><li><strong>需要在空间和时间维度上扩展输出的分辨率，因为4D输出需要大量的内存和计算能力；</strong></li></ol><h2 id="proposal">Proposal</h2><ol type="1"><li><strong>本文提出了MAV3D，利用了T2V模型和动态NeRFs，实现从自然语言描述生成动态三维时间表示；</strong></li><li><strong>提出了一个从静态到动态的多阶段优化方案，逐步纳入静态、时间和超分辨率模型的梯度信息。</strong></li></ol><h2 id="related-work">Related Work</h2><h3 id="dynamic-nerfs">dynamic NeRFs</h3><p><strong>适用于动态场景的NeRF变体</strong></p><h3 id="mav">MAV</h3><p><strong>Make AVideo，通过在未标记的视频上训练，拓展了文本到图像（T2I）模型。</strong></p><h3 id="dreamfusion">DreamFusion</h3><p><strong>以NeRF的形式从文本描述中学习3D表示，提出了一个基于概率密度蒸馏的loss（SDS）</strong></p><h2 id="method">Method</h2><p><imgsrc="https://img.enderfga.cn/img/image-20230209142352394.png" /></p><h3 id="d-scene-representation">4D Scene Representation</h3><p><imgsrc="https://img.enderfga.cn/img/image-20230209141807225.png" /></p><p><span class="math display">\[\left(\tau, c_i\right)=f_\theta(x, y, z, t)\]</span></p><p><span class="math display">\[\left[P_{x y}^{X Y R_1}+P_{z t}^{Z T R_1} ; P_{x z}^{X Z R_2}+P_{y t}^{YT R_2} ; P_{y z}^{Y Z R_3}+P_{y z}^{X T R_3}\right]\]</span></p><h3 id="dynamic-scene-optimization">Dynamic Scene Optimization</h3><h4id="为了监督4d场景与文本提示p匹配引入sds-ttemporal-score-distillation-sampling">为了监督4D场景与文本提示p匹配，引入SDS-T（temporalScore Distillation Sampling ）</h4><p><span class="math display">\[\nabla_\theta \mathcal{L}_{S D S-T}=E_{\sigma,\epsilon}\left[w(\sigma)\left(\hat{\epsilon}\left(V_{(\bar{\theta},\sigma, \epsilon)} \mid y, \sigma\right)-\epsilon\right) \frac{\partialV_\theta}{\partial \theta}\right]\\\]</span></p><p><span class="math display">\[\nabla_\theta \mathcal{L}_{\mathrm{SDS}}(\phi, \mathbf{x}=g(\theta))\triangleq \mathbb{E}_{t,\epsilon}\left[w(t)\left(\hat{\epsilon}_\phi\left(\mathbf{z}_t ; y,t\right)-\epsilon\right) \frac{\partial \mathbf{x}}{\partial\theta}\right]\]</span></p><h4 id="从静态到动态的场景优化">从静态到动态的场景优化</h4><h4 id="动态相机">动态相机</h4><h4 id="fps-采样">FPS 采样</h4><h4 id="高斯退火">高斯退火</h4><h4 id="全变分损失">全变分损失</h4><h3 id="super-resolution-fine-tuning">Super-Resolution Fine-Tuning</h3><p><imgsrc="https://img.enderfga.cn/img/image-20230209152345831.png" /></p><h2 id="experiment">Experiment</h2><p><strong>Metrics</strong>：R-Precision and human preference</p><p><imgsrc="https://img.enderfga.cn/img/image-20230209152747908.png" /></p><p><imgsrc="https://img.enderfga.cn/img/image-20230209152759865.png" /></p><h2 id="limitations">Limitations</h2><ul><li><strong>将动态NeRFs转换为实时应用的不连续网格序列的效率很低，如果能直接预测顶点的轨迹，就能得到改善。</strong></li><li><strong>利用超分辨率信息提高了表示的质量，但对于更高细节的纹理还需要进一步改进。</strong></li><li><strong>文本到四维动态场景生成的表示质量取决于T2V模型从不同视角生成视频的能力。</strong></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;论文阅读笔记&lt;/p&gt;</summary>
    
    
    
    
    <category term="计算机视觉" scheme="http://enderfga.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>Test to 3d随笔</title>
    <link href="http://enderfga.cn/2023/02/09/3d/"/>
    <id>http://enderfga.cn/2023/02/09/3d/</id>
    <published>2023-02-09T08:51:10.000Z</published>
    <updated>2023-02-09T08:53:02.663Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>随便记的笔记</p><span id="more"></span><h1 id="text-to-3d">Text-to-3D</h1><h2id="nerf-representing-scenes-as-neural-radiance-fields-for-view-synthesis">NeRF:Representing Scenes as Neural Radiance Fields for View Synthesis</h2><p><imgsrc="https://img.enderfga.cn/img/ed4df06e919cae0e638015fa78d935eb_1_Figure_1.png" /></p><p><strong>输入为连续的5维坐标（xyz坐标，以及视野角度theta和phi）；输出是空间位置的体密度以及该位置的发射射线（这里射线是根据视角变化的）。</strong></p><ol type="1"><li><strong>用 network 存体素信息: </strong>(x, y, z, , ) (, )</li><li><strong>然后用体素渲染方程获得生成视角图片：光线采样+积分</strong><span class="math display">\[C(\mathbf{r})=\int_{t_n}^{t_f} T(t) \sigma(\mathbf{r}(t))\mathbf{c}(\mathbf{r}(t), \mathbf{d}) d t, \text { where } T(t)=\exp\left(-\int_{t_n}^t \sigma(\mathbf{r}(s)) d s\right)\]</span></li><li><strong>最后与原视角图片计算损失更新网络</strong></li></ol><h2 id="dreamfusion-text-to-3d-using-2d-diffusion">DreamFusion:Text-to-3D using 2D Diffusion</h2><p><imgsrc="https://img.enderfga.cn/img/image-20230202213651008.png" /></p><p><strong>三维合成并不存在大规模的标注数据，也没有一个高效的模型架构对3D数据进行降噪</strong></p><p><strong>使用NERF的格式，使用预训练的text to2d，加上他们提出的一个基于概率密度蒸馏的loss，证明了预训练图像扩散模型作为先验模型的有效性</strong></p><h2 id="magic3d-high-resolution-text-to-3d-content-creation">Magic3D:High-Resolution Text-to-3D Content Creation</h2><p><imgsrc="https://img.enderfga.cn/img/f3fcff88aa23d692c243bda5b3dd5467_3_Figure_2_1114637308.png" /></p><p><strong>用一个两阶段的优化框架来提高速度和分辨率：利用低分辨率的扩散先验获得一个粗略的模型，并以稀疏的三维哈希网格结构加速。使用粗略表示作为初始化，进一步优化纹理三维网格模型，用高效的可微分渲染器与高分辨率的stablediffusion模型交互。</strong></p><h2id="point-e-a-system-for-generating-3d-point-clouds-from-complex-prompts">Point-E:A System for Generating 3D Point Clouds from Complex Prompts</h2><p><imgsrc="https://img.enderfga.cn/img/v2-b0ca9d9f44550ec6ab34dfe21797ea7e_720w.webp" /></p><p><strong>不输出传统意义上的 3D 图像，它会生成点云，或空间中代表 3D形状的离散数据点集</strong></p><p><strong>点云更容易合成，但它们无法捕获对象的细粒度形状或纹理，训练了一个额外的人工智能系统来将Point-E 的点云转换为网格</strong></p><p><strong>算力和时间需求小 但质量差</strong></p><h2id="dream3d-zero-shot-text-to-3d-synthesis-using-3d-shape-prior-and-text-to-image-diffusion-models">Dream3D:Zero-Shot Text-to-3D Synthesis Using 3D Shape Prior and Text-to-ImageDiffusion Models</h2><p><imgsrc="https://img.enderfga.cn/img/image-20230202234404950.png" /></p><p><strong>引入一个显式3D先验形状，来优化CLIP引导的3D优化任务。具体的讲，首先在文本到形状转换时，使用输入文本生成了一个质量的3D形状来作为先验知识。然后使用它来初始化神经辐射场，并使用完整prompt进行优化</strong></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;随便记的笔记&lt;/p&gt;</summary>
    
    
    
    
    <category term="笔记" scheme="http://enderfga.cn/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>三维手势姿态估计算法研究</title>
    <link href="http://enderfga.cn/2023/01/13/nyu/"/>
    <id>http://enderfga.cn/2023/01/13/nyu/</id>
    <published>2023-01-13T14:08:50.000Z</published>
    <updated>2023-01-13T14:19:04.839Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>人工智能原理实验期末项目</p><span id="more"></span><embed src="./nyu.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    <summary type="html">&lt;p&gt;人工智能原理实验期末项目&lt;/p&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://enderfga.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Mind-Diffusion</title>
    <link href="http://enderfga.cn/2023/01/13/diff/"/>
    <id>http://enderfga.cn/2023/01/13/diff/</id>
    <published>2023-01-13T14:04:03.000Z</published>
    <updated>2023-01-13T14:19:04.833Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>计算机视觉之diffusion model with mindspore</p><span id="more"></span><embed src="./diff.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    <summary type="html">&lt;p&gt;计算机视觉之diffusion model with mindspore&lt;/p&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://enderfga.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>基于 JMAG 软件的电机仿真分析</title>
    <link href="http://enderfga.cn/2023/01/13/mach/"/>
    <id>http://enderfga.cn/2023/01/13/mach/</id>
    <published>2023-01-13T13:59:11.000Z</published>
    <updated>2023-01-13T14:19:04.835Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>压根没上过课，不知道这写的是啥</p><span id="more"></span><embed src="./mach.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    <summary type="html">&lt;p&gt;压根没上过课，不知道这写的是啥&lt;/p&gt;</summary>
    
    
    
    
    <category term="电机与拖动技术" scheme="http://enderfga.cn/tags/%E7%94%B5%E6%9C%BA%E4%B8%8E%E6%8B%96%E5%8A%A8%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title>智能车协同实验</title>
    <link href="http://enderfga.cn/2023/01/13/car/"/>
    <id>http://enderfga.cn/2023/01/13/car/</id>
    <published>2023-01-13T13:52:04.000Z</published>
    <updated>2023-01-13T14:19:04.830Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>多智能体集群控制技术智能车实验报告</p><span id="more"></span><embed src="./1.pdf" width="100%" height="750" type="application/pdf"><embed src="./2.pdf" width="100%" height="750" type="application/pdf"><embed src="./3.pdf" width="100%" height="750" type="application/pdf"><embed src="./4.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    <summary type="html">&lt;p&gt;多智能体集群控制技术智能车实验报告&lt;/p&gt;</summary>
    
    
    
    
    <category term="多智能体集群" scheme="http://enderfga.cn/tags/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E9%9B%86%E7%BE%A4/"/>
    
  </entry>
  
  <entry>
    <title>四旋翼集群编队</title>
    <link href="http://enderfga.cn/2023/01/13/multi/"/>
    <id>http://enderfga.cn/2023/01/13/multi/</id>
    <published>2023-01-13T13:37:11.000Z</published>
    <updated>2023-01-14T12:23:24.113Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>多智能体集群控制技术期末项目报告</p><span id="more"></span><h1 id="单积分模型sysu编队设计">单积分模型SYSU编队设计</h1><p><img src="https://img.enderfga.cn/img/SI.gif" /></p><p><imgsrc="https://img.enderfga.cn/img/image-20230113213938616.png" /></p><h1id="控制四旋翼飞行器实现编队方式设计">控制四旋翼飞行器实现编队方式设计</h1><p><img src="https://img.enderfga.cn/img/test.gif" /></p><p><imgsrc="https://img.enderfga.cn/img/image-20230113214008450.png" /></p><p><imgsrc="https://img.enderfga.cn/img/image-20230113214017680.png" /></p><ul><li><p>Position control</p><p><span class="math display">\[\begin{array}{ll}\text { PID: } &amp; \ddot{\boldsymbol{p}}_{i,c}=\ddot{\boldsymbol{p}}_i{ }^{d e s}+K_{d,i}\left(\dot{\boldsymbol{p}}_i{ }^{\text {des}}-\dot{\boldsymbol{p}}_i\right)+K_{p, i}\left(\boldsymbol{p}_i{}^{\text {des }}-\boldsymbol{p}_i\right) \\\text { Model: } &amp; u_1=m\left(g+\ddot{\boldsymbol{p}}_{3, c}\right)\quad \text { (Newton Equation) } \\&amp; \phi_c=\frac{1}{g}\left(\ddot{\boldsymbol{p}}_{1, c} \sin\Psi-\ddot{\boldsymbol{p}}_{2, c} \cos \psi \right) \quad\theta_c=\frac{1}{g}\left(\ddot{\boldsymbol{p}}_{1, c} \cos\Psi+\ddot{\boldsymbol{p}}_{2, c} \sin \Psi\right)\end{array}\]</span></p></li><li><p>Attitude control</p></li></ul><p><span class="math display">\[PID: \quad\left[\begin{array}{c}\ddot{\phi}_c \\ \ddot{\theta}_c \\\ddot{\psi}_c\end{array}\right]=\left[\begin{array}{c}K_{p,\phi}\left(\phi_c-\phi\right)+K_{d,\phi}\left(\dot{\phi}_c-\dot{\phi}\right) \\ K_{p,\theta}\left(\theta_c-\theta\right)+K_{d,\phi}\left(\dot{\theta}_c-\dot{\theta}\right) \\ K_{p,\psi}\left(\psi_c-\psi\right)+K_{d,\psi}\left(\dot{\psi}_c-\dot{\psi}\right)\end{array}\right]\]</span></p><p><span class="math display">\[Model: \quad \boldsymbol{u}_2=\boldsymbol{I}\cdot\left[\begin{array}{c}\ddot{\phi}_c \\ \ddot{\theta}_c \\\ddot{\psi}_c\end{array}\right]+\left[\begin{array}{c}\omega_x \\\omega_y \\ \omega_z\end{array}\right] \times \boldsymbol{I}\cdot\left[\begin{array}{c}\omega_x \\ \omega_y \\\omega_z\end{array}\right] (Euler Equation)\]</span></p><p>我使用的控制器遵循上述公式采取了PID控制，结合单积分模型的控制共同决定了结果分数。当然字母间距、运行时间等也能对误差产生一定影响。</p><p>单积分模型中控制增益kv与刚度矩阵R、距离误差z和期望速度dv相乘，起到了限制距离误差和期望速度之间的平衡作用。具体来说，当kv增加时，系统会更快地收敛到目标状态，但是可能会出现振荡。当kv减小时，系统会更缓慢地收敛到目标状态，甚至会导致无人机几乎不动的情况。</p><p>控制器的原理是输入期望控制，输出飞行器整体推力与力矩。公式整体的推导较为复杂，涉及机器人运动学与动力学，且会解欧拉牛顿方程，但对公式的直观理解可以更好理解公式；这个公式基本是外环位置，内环姿态，计算扭矩与推力，可见推力与飞行器质量与z轴加速度有关，通过计算期望角度计算扭矩。</p><p>我使用了Ziegler-Nichols整定方法来调节PID参数，首先将积分和微分增益设置为0，然后比例增益从零开始逐渐增加，直到到达极限增益<em>KU</em>，此时控制器输出值以恒定值振荡。<em>KU</em>和振荡周期<em>TU</em>根据不同的类型，按下表中的方式来设置比例、积分和微分增益。</p><p><span class="math display">\[\begin{array}{|c|c|c|c|}\hline \text { Controller } &amp; K_p &amp; K_d &amp; K_i \\\hline \text { P } &amp; 0.5 K_u &amp; - &amp; - \\\hline \text { PD } &amp; 0.8 K_u &amp; K_p T_u / 8 &amp; - \\\hline \text { PID } &amp; 0.6 K_u &amp; K_p T_u / 8 &amp; 2 K_p / T_u\\\hline\end{array}\]</span></p><p>在位置控制中，使用了三轴PID控制器来控制x, y,z轴上的运动。使用了误差积分来消除误差；在姿态控制中，通过计算出当前的欧拉角（phi,theta, psi），并使用欧拉角的导数来控制飞行器的姿态。</p><p>Kp是比例系数，它控制着系统的稳定性和响应速度。当Kp增大时，系统的响应速度会变快，但同时也会增加系统的震荡。</p><p>Ki是积分系数，它控制着系统的累计误差。当Ki增大时，系统会更快地消除误差，但同时也会增加系统的积分饱和。</p><p>Kd是微分系数，它控制着系统的响应速度。当Kd增大时，系统的响应速度会变快，但同时也会增加系统的偏差。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;多智能体集群控制技术期末项目报告&lt;/p&gt;</summary>
    
    
    
    
    <category term="多智能体集群" scheme="http://enderfga.cn/tags/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E9%9B%86%E7%BE%A4/"/>
    
  </entry>
  
  <entry>
    <title>Tiny SSD目标检测</title>
    <link href="http://enderfga.cn/2022/12/11/ssd/"/>
    <id>http://enderfga.cn/2022/12/11/ssd/</id>
    <published>2022-12-11T13:23:46.000Z</published>
    <updated>2022-12-11T13:27:07.369Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Github: https://github.com/Enderfga/TinySSD_sysu</p><span id="more"></span><h1id="tiny-ssd-a-tiny-single-shot-detection-deep-convolutional-neural-network-for-real-time-embedded-object-detection">TinySSD: A Tiny Single-shot Detection Deep Convolutional Neural Network forReal-time Embedded Object Detection</h1><p>This repo contains the code, data and trained models for the paper <ahref="https://arxiv.org/pdf/1802.06488.pdf">Tiny SSD: A Tiny Single-shotDetection Deep Convolutional Neural Network for Real-time EmbeddedObject Detection</a>.</p><h2 id="quick-links">Quick Links</h2><ul><li><a href="#Overview">Overview</a></li><li><a href="#Requirements">Requirements</a></li><li><a href="#How-to-Install">How to Install</a></li><li><a href="#Description-of-Codes">Description of Codes</a></li><li><a href="#Preprocessing">Preprocessing</a><ul><li><a href="#Preprocessed-Data">Preprocessed Data</a></li></ul></li><li><a href="#How-to-Run">How to Run</a><ul><li><a href="#Train">Train</a><ul><li><a href="#Finetuning-from-an-existing-checkpoint">Finetuning from anexisting checkpoint</a></li></ul></li><li><a href="#Evaluate">Evaluate</a></li></ul></li><li><a href="#Results-Outputs-Checkpoints">Results, Outputs,Checkpoints</a></li></ul><h2 id="overview">Overview</h2><p>Tiny SSD is a single-shot detection deep convolutional neural networkfor real-time embedded object detection. It brings together theefficieny of Fire microarchitecture introduced in<strong>SqueezeNet</strong> and object detection performance of<strong>SSD (Single Shot Object Detector)</strong>.</p><p><img src="https://img.enderfga.cn/img/ssd.svg" /></p><p><imgsrc="https://img.enderfga.cn/img/image-20221018133431973.png" /></p><h2 id="requirements">Requirements</h2><ul><li>numpy</li><li>pandas</li><li>matplotlib</li><li>opencv-python</li><li>torch</li><li>torchvision</li></ul><h2 id="how-to-install">How to Install</h2><p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">conda create -n env python=3.8 -y<br>conda activate env<br></code></pre></td></tr></table></figure> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">pip install -r requirements.txt<br></code></pre></td></tr></table></figure></p><h2 id="description-of-files">Description of Files</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs stylus">│──<span class="hljs-selector-tag">main</span><span class="hljs-selector-class">.py</span>                 -&gt; Run models using different models<br>│──README.md<br>│──requirements.txt<br>│──test<span class="hljs-selector-class">.py</span>                 -&gt; Testing Model<br>│──train<span class="hljs-selector-class">.py</span>                -&gt; Training Model<br>│<br>├─data<br>│  │  dataloader<span class="hljs-selector-class">.py</span>         -&gt; dataloader and <span class="hljs-attribute">transform</span><br>│  │  __init__.py<br>│  │<br>│  └─detection<br>│      │  create_train<span class="hljs-selector-class">.py</span>   -&gt; data preprocessing<br>│      │<br>│      ├─<span class="hljs-attribute">background</span><br>│      ├─sysu_train<br>│      │  │  <span class="hljs-selector-tag">label</span>.csv<br>│      │  │<br>│      │  └─images<br>│      ├─target<br>│      │      <span class="hljs-number">0</span>.jpg<br>│      │      <span class="hljs-number">0</span>.png<br>│      │      <span class="hljs-number">1</span>.png<br>│      │<br>│      └─test<br>│              <span class="hljs-number">1</span>.jpg<br>│              <span class="hljs-number">2</span>.jpg<br>│<br>├─model<br>│  │  TinySSD<span class="hljs-selector-class">.py</span>             -&gt; Definition of the model<br>│  │  __init__.py<br>│  │<br>│  └─checkpoints             -&gt; Trained model weights<br>│     net_100.pkl<br>└─utils                      -&gt; utility functions<br>        anchor.py<br>        iou.py<br>        utils.py<br>        __init__.py<br></code></pre></td></tr></table></figure><h2 id="preprocessing">Preprocessing</h2><p>We use /data/detection/background to generate the target detectiondataset for our experiments.</p><p>Since the generated data is stored in the repository, there is noneed to run this step.</p><h3 id="preprocessed-data">Preprocessed Data</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">cd data/detection/<br>python create_train.py<br></code></pre></td></tr></table></figure><h2 id="how-to-run">How to Run</h2><h3 id="train">Train</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">python main.py --mode=train --batch_size=256 --epochs=100<br></code></pre></td></tr></table></figure><p>The checkpoints will be saved in a subfolder of<code>./model/checkpoints/</code>.</p><h4 id="finetuning-from-an-existing-checkpoint">Finetuning from anexisting checkpoint</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">python main.py --mode=train --batch_size=256 --epochs=100 --path=[model path]<br></code></pre></td></tr></table></figure><p>model path should be a subdirectory in the<code>./model/checkpoints/</code> directory, e.g.<code>--path=./model/checkpoints/net_100.pkl</code></p><h3 id="evaluate">Evaluate</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">python main.py --mode=test --threshold=<span class="hljs-number">0.3</span> --path=./model/checkpoints/net_100.pkl<br></code></pre></td></tr></table></figure><h2 id="results-outputs-checkpoints">Results, Outputs, Checkpoints</h2><p>the ./model/checkpoints/net_100.pkl：class err 1.54e-03, bbox mae1.90e-03</p><p>I used the following methods to improve performance：</p><ol type="1"><li><p>HD anti-white detection object to adapt to the test image</p><p><imgsrc="https://img.enderfga.cn/img/image-20221018160400682.png" /></p></li><li><p>Flip and rotate images, etc. to improve generalizationperformance</p><p><imgsrc="https://img.enderfga.cn/img/image-20221018155947834.png" /></p></li><li><p>soft_nms</p><p><imgsrc="https://img.enderfga.cn/img/42166d224f4a20a4d58841b70d795a2a730ed0e4.jpeg@f_auto" /></p></li><li><p>smooth_L1</p><p><imgsrc="https://img.enderfga.cn/img/image-20221018155842392.png" /></p></li><li><p>Focal Loss</p><p><img src="https://img.enderfga.cn/img/20221025160416.png" /></p></li></ol><p>If we have more classes, we can further improve the model in thefollowing aspects:</p><ol type="1"><li>When an object is much smaller compared with the image, the modelcould resize the input image bigger.</li><li>There are typically a vast number of negative anchor boxes. To makethe class distribution more balanced, we could downsample negativeanchor boxes.</li><li>In the loss function, assign different weight hyperparameters to theclass loss and the offset loss.</li><li>Use other methods to evaluate the object detection model, such asthose in the single shot multibox detection paper (Liu et al.,2016).</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;Github: https://github.com/Enderfga/TinySSD_sysu&lt;/p&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://enderfga.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
