<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>机器&amp;深度学习代码速查表</title>
    <link href="/2022/03/24/graph/"/>
    <url>/2022/03/24/graph/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>搬运自github.com/OUCMachineLearning/OUCML</p><span id="more"></span><h1 id="机器-amp-深度学习代码速查表"><a href="#机器-amp-深度学习代码速查表" class="headerlink" title="机器&amp;深度学习代码速查表"></a>机器&amp;深度学习代码速查表</h1><h3 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h3><p><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/image-20220324225648930.png" alt="image-20220324225648930"><br><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/%E7%BD%91%E7%BB%9C.png" alt="网络"></p><h3 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h3><p><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/liner.jpg" alt="liner"></p><h3 id="python基础"><a href="#python基础" class="headerlink" title="python基础"></a>python基础</h3><p><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/sci.jpg" alt="sci"></p><h3 id="scipy科学计算"><a href="#scipy科学计算" class="headerlink" title="scipy科学计算"></a>scipy科学计算</h3><p><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/sci.png" alt="sci"></p><h3 id="spark"><a href="#spark" class="headerlink" title="spark"></a>spark</h3><p><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/spark.jpeg" alt="spark"></p><h2 id="数据保存及可视化"><a href="#数据保存及可视化" class="headerlink" title="数据保存及可视化"></a>数据保存及可视化</h2><h3 id="numpy"><a href="#numpy" class="headerlink" title="numpy"></a>numpy</h3><p><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/np.png" alt="np"></p><h3 id="pandas"><a href="#pandas" class="headerlink" title="pandas"></a>pandas</h3><p><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/pd.png" alt="pd"><br><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/df.jpeg" alt="df"><br><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/df2.jpeg" alt="df2"></p><h3 id="bokeh"><a href="#bokeh" class="headerlink" title="bokeh"></a>bokeh</h3><p><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/bokeh.jpg" alt="bokeh"></p><h2 id="画图"><a href="#画图" class="headerlink" title="画图"></a>画图</h2><h3 id="matplotlib"><a href="#matplotlib" class="headerlink" title="matplotlib"></a>matplotlib</h3><p><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/matplot.png" alt="matplot"></p><h3 id="ggplot"><a href="#ggplot" class="headerlink" title="ggplot"></a>ggplot</h3><p><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/data%20vis.jpeg" alt="data vis"></p><p><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/gg.jpeg" alt="gg"></p><h2 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h2><h3 id="sklearn"><a href="#sklearn" class="headerlink" title="sklearn"></a>sklearn</h3><p><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/sk.jpg" alt="sk"></p><p><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/scikit.png" alt="scikit"></p><h3 id="keras"><a href="#keras" class="headerlink" title="keras"></a>keras</h3><p><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/keras.jpeg" alt="keras"></p><h3 id="tensorflow"><a href="#tensorflow" class="headerlink" title="tensorflow"></a>tensorflow</h3><p><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/TF.png" alt="TF"></p><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><h3 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h3><p><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/datastruct.png" alt="datastruct"></p><h3 id="复杂度"><a href="#复杂度" class="headerlink" title="复杂度"></a>复杂度</h3><p><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/O.png" alt="O"></p><h3 id="排序算法"><a href="#排序算法" class="headerlink" title="排序算法"></a>排序算法</h3><p><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/sort.png" alt="sort"></p>]]></content>
    
    
    
    <tags>
      
      <tag>course</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>智能机器人技术——正向运动学</title>
    <link href="/2022/03/24/robot3/"/>
    <url>/2022/03/24/robot3/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>智能机器人技术作业记录</p><span id="more"></span><p><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/image-20220324223524468.png" alt="image-20220324223524468"></p><h1 id="智能机器人技术——正向运动学"><a href="#智能机器人技术——正向运动学" class="headerlink" title="智能机器人技术——正向运动学"></a>智能机器人技术——正向运动学</h1><ol><li>如下图所示有一处于初始位形的 PRRRRR 空间开链机器人, 试确定末端初始位形 $M$ 、在 ${0}$ 系描述的螺旋轴 Si 、$ 在 ${b} 系描述的螺旋轴 Bi （如讲义那样列表即可)。</li></ol><p><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/image-20220323164358687.png" alt="image-20220323164358687"></p><p><strong>解得：</strong></p><p>相对基坐标系的$\mathrm{PoE}$ :</p><script type="math/tex; mode=display">T(\theta)=e^{\left[\mathcal{S}_{1}\right] \theta_{1}} \cdots e^{\left[\mathcal{S}_{n-1}\right] \theta_{n-1}} e^{\left[\mathcal{S}_{n}\right] \theta_{n}} M</script><p>相对末端坐标系的$\mathrm{PoE}$ :</p><script type="math/tex; mode=display">T(\theta)=M e^{\left[\mathcal{B}_{1}\right] \theta_{1}} \cdots e^{\left[\mathcal{B}_{n-1}\right] \theta_{n-1}} e^{\left[\mathcal{B}_{n}\right] \theta_{n}}</script><p>末端初始位形 $M$：</p><script type="math/tex; mode=display">\left[\begin{array}{cccc}1 & 0 & 0 & 0 \\0 & 1 & 0 & L_1+L_2+L_3+L_4 \\0 & 0 & 1 & h \\0 & 0 & 0 & 1\end{array}\right]</script><p>在 ${0}$ 系描述的螺旋轴 $\mathcal{S}_{i} $：</p><script type="math/tex; mode=display">[\mathcal{S}]=\left[\begin{array}{cc}{[\omega]} & v \\0 & 0\end{array}\right] \in \operatorname{se}(3) \longrightarrow \mathcal{S}=\left[\begin{array}{c}\omega \\v\end{array}\right] \in \mathbb{R}^{6}</script><script type="math/tex; mode=display">\because v_i=-\omega_{i} \times q_i \\\begin{array}{|c||c|c|c|}\hline i & \omega_{i} & q_i & v_{i} \\\hline \hline 1 & (0,0,1) & (0, L_1,0)& (L_1, 0,0) \\\hline 2 & (-1,0,0) & (0, L_1,h)& (0, -h,L_1) \\\hline 3 & (-1,0,0) & (0,L_1+L_2,h)& (0, -h,L_1+L_2) \\\hline 4 & (-1,0,0) & (0,L_1+L_2+L_3,h)& (0, -h,L_1+L_2+L_3) \\\hline 5 & (0,1,0) & (0,0,h) & (-h, 0,0)\\\hline 6 & (0,0,0) & NULL & (0, 1,0) \\\hline\end{array}</script><p>在 ${b}$ 系描述的螺旋轴 $\mathcal{B}_{i}$：</p><script type="math/tex; mode=display">[\mathcal{B}]=\left[\begin{array}{cc}{[\omega]} & v \\0 & 0\end{array}\right] \in \operatorname{se}(3) \longrightarrow \mathcal{B}=\left[\begin{array}{c}\omega \\v\end{array}\right] \in \mathbb{R}^{6}</script><script type="math/tex; mode=display">\because v_i=-\omega_{i} \times q_i \\\begin{array}{|c||c|c|c|}\hline i & \omega_{i} & q_i & v_{i} \\\hline \hline 1 & (0,0,1) & (0, -L_2-L_3-L_4,0)& (-L_2-L_3-L_4, 0,0) \\\hline 2 & (-1,0,0) & (0, -L_2-L_3-L_4,0)& (0, 0,-L_2-L_3-L_4) \\\hline 3 & (-1,0,0) & (0,-L_3-L_4,0)& (0, 0,-L_3-L_4) \\\hline 4 & (-1,0,0) & (0,-L_4,0)& (0, 0,-L_4) \\\hline 5 & (0,1,0) & (0,0,0) & (0, 0,0)\\\hline 6 & (0,0,0) & NULL & (0, 1,0) \\\hline\end{array}</script><ol><li><p>如下图所示有一处于初始位形的 RRRRPR 空间开链机器人, 试确定末端初始位形 $M$ 、在 ${0}$ 系描述的螺旋轴 Si 、$ 在 ${b} 系描述的螺旋轴 Bi （如讲义那样列表即可)。<br> <img src="https://gitee.com/sysu_20354027/fig/raw/master/img/image-20220323164443717.png" alt="image-20220323164443717"></p><p><strong>解得：</strong></p></li></ol><p>相对基坐标系的$\mathrm{PoE}$ :</p><script type="math/tex; mode=display">T(\theta)=e^{\left[\mathcal{S}_{1}\right] \theta_{1}} \cdots e^{\left[\mathcal{S}_{n-1}\right] \theta_{n-1}} e^{\left[\mathcal{S}_{n}\right] \theta_{n}} M</script><p>相对末端坐标系的$\mathrm{PoE}$ :</p><script type="math/tex; mode=display">T(\theta)=M e^{\left[\mathcal{B}_{1}\right] \theta_{1}} \cdots e^{\left[\mathcal{B}_{n-1}\right] \theta_{n-1}} e^{\left[\mathcal{B}_{n}\right] \theta_{n}}</script><p>末端初始位形 $M$：</p><script type="math/tex; mode=display">\left[\begin{array}{cccc}1 & 0 & 0 & L_1 \\0 & 1 & 0 & L_3+L_4 \\0 & 0 & 1 & -L_5-L_6 \\0 & 0 & 0 & 1\end{array}\right]</script><p>在 ${0}$ 系描述的螺旋轴 $\mathcal{S}_{i} $：</p><script type="math/tex; mode=display">[\mathcal{S}]=\left[\begin{array}{cc}{[\omega]} & v \\0 & 0\end{array}\right] \in \operatorname{se}(3) \longrightarrow \mathcal{S}=\left[\begin{array}{c}\omega \\v\end{array}\right] \in \mathbb{R}^{6}</script><script type="math/tex; mode=display">\because v_i=-\omega_{i} \times q_i \\\begin{array}{|c||c|c|c|}\hline i & \omega_{i} & q_i & v_{i} \\\hline \hline 1 & (1,0,0) & (0, 0,0)& (0, 0,0) \\\hline 2 & (0,0,-1) & (L_1,0,0)& (0, L_1,0) \\\hline 3 & (0,1,0) & (L_1,0,L_2)& (-L_2, 0,L_1) \\\hline 4 & (1,0,0) & (0,L_3,0)& (0, 0,-L_3) \\\hline 5 & (0,0,0) & NULL & (0, 1,0)\\\hline 6 & (0,1,0) & (L_1,0,-L_5) & (L_5, 0,-L_1) \\\hline\end{array}</script><p>在 ${b}$ 系描述的螺旋轴 $\mathcal{B}_{i}$：</p><script type="math/tex; mode=display">[\mathcal{B}]=\left[\begin{array}{cc}{[\omega]} & v \\0 & 0\end{array}\right] \in \operatorname{se}(3) \longrightarrow \mathcal{B}=\left[\begin{array}{c}\omega \\v\end{array}\right] \in \mathbb{R}^{6}</script><script type="math/tex; mode=display">\because v_i=-\omega_{i} \times q_i \\\begin{array}{|c||c|c|c|}\hline i & \omega_{i} & q_i & v_{i} \\\hline \hline 1 & (1,0,0) & (0, -L_3-L_4,L_5+L_6)& (0, L_5+L_6,L_3+L_4) \\\hline 2 & (0,0,-1) & (0, -L_3-L_4,0)& (L_3+L_4, 0,0) \\\hline 3 & (0,1,0) & (0,0,L_2+L_5+L_6)& (-L_2-L_5-L_6, 0,0) \\\hline 4 & (1,0,0) & (0,-L_4,L_5+L_6)& (0, L_5+L_6,L_4) \\\hline 5 & (0,0,0) & NULL & (0, 1,0)\\\hline 6 & (0,1,0) & (0,0,L_6) & (-L_6, 0,0) \\\hline\end{array}</script><ol><li>如下图所示有一处于初始位形的 RRRPRR 空间开链机器人, 试确定末端初始位形 $M$ 、在 ${0}$ 系描述的螺旋轴 Si 、$ 在 ${b} 系描述的螺旋轴 Bi （如讲义那样列表即可)。</li></ol><p><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/image-20220323164454698.png" alt="image-20220323164454698"></p><p><strong>解得：</strong></p><p>相对基坐标系的$\mathrm{PoE}$ :</p><script type="math/tex; mode=display">T(\theta)=e^{\left[\mathcal{S}_{1}\right] \theta_{1}} \cdots e^{\left[\mathcal{S}_{n-1}\right] \theta_{n-1}} e^{\left[\mathcal{S}_{n}\right] \theta_{n}} M</script><p>相对末端坐标系的$\mathrm{PoE}$ :</p><script type="math/tex; mode=display">T(\theta)=M e^{\left[\mathcal{B}_{1}\right] \theta_{1}} \cdots e^{\left[\mathcal{B}_{n-1}\right] \theta_{n-1}} e^{\left[\mathcal{B}_{n}\right] \theta_{n}}</script><p>末端初始位形 $M$：</p><script type="math/tex; mode=display">\left[\begin{array}{cccc}-1 & 0 & 0 & 0 \\0 & 1 & 0 & 4 \\0 & 0 & -1 & 1 \\0 & 0 & 0 & 1\end{array}\right]</script><p>在 ${0}$ 系描述的螺旋轴 $\mathcal{S}_{i} $：</p><script type="math/tex; mode=display">[\mathcal{S}]=\left[\begin{array}{cc}{[\omega]} & v \\0 & 0\end{array}\right] \in \operatorname{se}(3) \longrightarrow \mathcal{S}=\left[\begin{array}{c}\omega \\v\end{array}\right] \in \mathbb{R}^{6}</script><script type="math/tex; mode=display">\because v_i=-\omega_{i} \times q_i \\\begin{array}{|c||c|c|c|}\hline i & \omega_{i} & q_i & v_{i} \\\hline \hline 1 & (0,0,1) & (0, 0,0)& (0, 0,0) \\\hline 2 & (1,0,0) & (0,0,2)& (0, 2,0) \\\hline 3 & (1,0,0) & (0,1,2)& (0, 2,-1) \\\hline 4 & (0,0,0) & NULL & (0, 1,0) \\\hline 5 & (0, \frac{\sqrt{2}}{2} , \frac{\sqrt{2}}{2} ) & (0,3,2) & (\frac{\sqrt{2}}{2}, 0,0)\\\hline 6 & (0,0,-1) & (0,4,0)& (-4, 0,0) \\\hline\end{array}</script><p>在 ${b}$ 系描述的螺旋轴 $\mathcal{B}_{i}$：</p><script type="math/tex; mode=display">[\mathcal{B}]=\left[\begin{array}{cc}{[\omega]} & v \\0 & 0\end{array}\right] \in \operatorname{se}(3) \longrightarrow \mathcal{B}=\left[\begin{array}{c}\omega \\v\end{array}\right] \in \mathbb{R}^{6}</script><script type="math/tex; mode=display">\because v_i=-\omega_{i} \times q_i \\\begin{array}{|c||c|c|c|}\hline i & \omega_{i} & q_i & v_{i} \\\hline \hline 1 & (0,0,-1) & (0, -4,0)& (4, 0,0) \\\hline 2 & (-1,0,0) & (0,-4,-1)& (0, 1,-4) \\\hline 3 & (-1,0,0) & (0,-3,-1)& (0, 1,-3) \\\hline 4 & (0,0,0) & NULL & (0, 1,0) \\\hline 5 & (0, \frac{\sqrt{2}}{2} , -\frac{\sqrt{2}}{2} ) & (0,-1,-1) & (\sqrt{2}, 0,0)\\\hline 6 & (0,0,1) & (0,0,0)& (0, 0,0) \\\hline\end{array}</script>]]></content>
    
    
    
    <tags>
      
      <tag>course</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数据库原理 Exercises 2</title>
    <link href="/2022/03/23/data2/"/>
    <url>/2022/03/23/data2/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Database System Concepts Exercises of Chapter 3</p><span id="more"></span><p><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/image-20220323230708879.png" alt="image-20220323230708879"></p><h1 id="Database-System-Concepts-Exercises-of-Chapter-3"><a href="#Database-System-Concepts-Exercises-of-Chapter-3" class="headerlink" title="Database System Concepts Exercises of Chapter 3"></a>Database System Concepts Exercises of Chapter 3</h1><p><strong>Exercises 3.9</strong> Consider the employee database of Figure $3.20$, where the primary keys are underlined. Give an expression in SQL for each of the following queries.</p><p>a. Find the names and cities of residence of all employees who work for “First Bank Corporation”.</p><p>b. Find the names, street addresses, and cities of residence of all employees who work for “First Bank Corporation” and earn more than $$ 10,000$.</p><p>c. Find all employees in the database who do not work for “First Bank Corporation”.</p><p>d. Find all employees in the database who earn more than each employee of “Small Bank Corporation”.</p><p>e. Assume that the companies may be located in several cities. Find all companies located in every city in which “Small Bank Corporation” is located.</p><p>f. Find the company that has the most employees.</p><p>g. Find those companies whose employees earn a higher salary, on average, than the average salary at “First Bank Corporation”.</p><script type="math/tex; mode=display">employee (\underline{employee\_name}, street, city)\\works (\underline{employee\_name}, company\_name, salary)\\company (\underline{company\_name}, city)\\manages ( \underline{ employee\_name, } manager\_name )\\Figure~~3.20~~Employee~~database~~for~~Exercises~~3.9</script><p><strong>My answer：</strong></p><p>a.</p><p><strong>select</strong> e.employee_name, city<br><strong>from</strong> employee <strong>as</strong> e, works <strong>as</strong> w<br><strong>where</strong> w.company_name $=$ ‘First Bank Corporation’ <strong>and</strong> w.employee_name $=$ e.employee_name</p><p>b.</p><p><strong>select</strong> <em><br><strong>from</strong> employee<br><strong>where</strong> employee_name <strong>in</strong><br>(<strong>select</strong> employee_name<br><strong>from</strong> works<br><strong>where</strong> company_name = ‘First Bank Corporation’ <em>*and</em></em> salary $&gt;10000$ )</p><p>c.</p><p><strong>select</strong> employee_name<br><strong>from</strong> works<br><strong>where</strong> company_name $\neq$ ‘First Bank Corporation’</p><p>d.</p><p><strong>select</strong> employee_name<br><strong>from</strong> works<br><strong>where</strong> salary $&gt;$ <strong>all</strong><br>(<strong>select</strong> salary<br><strong>from</strong> works<br><strong>where</strong> company_name $=$ ‘Small Bank Corporation’)</p><p>e.</p><p><strong>select</strong> S.company_name<br><strong>from</strong> company <strong>as</strong> $S$<br><strong>where</strong> <strong>not exists</strong> ((<strong>select</strong> city<br><strong>from</strong> company<br><strong>where</strong> company_name $=$ ‘Small Bank Corporation’)<br><strong>except</strong><br>(<strong>select</strong> city<br><strong>from</strong> company <strong>as</strong> $T$<br><strong>where</strong> S.company_name $=$ T.company_name ) )</p><p>f.</p><p><strong>select</strong> company_name<br><strong>from</strong> works<br><strong>group by</strong> company_name<br><strong>having count</strong> (<strong>distinct</strong> employee_name) $&gt;=$ <strong>all</strong><br>(<strong>select</strong> <strong>count</strong> (<strong>distinct</strong> employee_name)<br><strong>from</strong> works<br><strong>group by</strong> company_name)</p><p>g.</p><p><strong>select</strong> company_name<br><strong>from</strong> works<br><strong>group by</strong> company_name<br><strong>having</strong> <strong>avg</strong> (salary) $&gt;$ (<strong>select</strong> <strong>avg</strong> (salary)<br><strong>from</strong> works<br><strong>where</strong> company_name $=$ ‘First Bank Corporation’)</p><p><strong>Exercises 3.8</strong> Consider the bank database of Figure $3.19$, where the primary keys are underlined. Construct the following SQL queries for this relational database.</p><p>a. Find all customers of the bank who have an account but not a loan.</p><p>b. Find the names of all customers who live on the same street and in the same city as “Smith”.</p><p>c. Find the names of all branches with customers who have an account in the bank and who live in “Harrison”.</p><script type="math/tex; mode=display">branch(\underline{branch\_name}, branch\_city, assets)\\customer (\underline{customer\_name}, customer\_street, customer\_city)\\ loan (\underline{loan\_numbe}r, branch\_name, amount)\\borrower (\underline{customer\_name}, \underline{loan\_number})\\account (\underline{account\_number}, branch\_name, balance)\\depositor (\underline{customer\_name}, \underline{account\_number})\\Figure~~3.19~~Banking~~database~~for~~Exercises~~3.8~~and~~3.15</script><p><strong>My answer:</strong></p><p>a.</p><p><strong>select</strong> customer_name<br><strong>from</strong> depositor<br><strong>except</strong><br>(<strong>select</strong> customer_name<br><strong>from</strong> borrower)</p><p>b.</p><p><strong>select</strong> F.customer_name<br><strong>from</strong> customer <strong>as</strong> $F$ join customer <strong>as</strong> $S$ using(customer_street, customer_city)<br><strong>where</strong> S.customer_name $=$ ‘Smith’</p><p>c.</p><p><strong>select</strong> <strong>distinct</strong> branch_name<br><strong>from</strong> account <strong>natural join</strong> depositor <strong>natural join</strong> customer<br><strong>where</strong> customer_city = ‘Harrison’</p><p><strong>Exercises 3.15</strong> Consider the bank database of Figure 3.19, where the primary keys are underlined. Construct the following SQL queries for this relational database.<br>a. Find all customers who have an account at all the branches located in “Brooklyn”.<br>b. Find out the total sum of all loan amounts in the bank.<br>c. Find the names of all branches that have assets greater than those of at least one branch located in “Brooklyn”.</p><p><strong>My answer:</strong></p><p>a.</p><p><strong>select</strong> <strong>distinct</strong> S.customer_name</p><p><strong>from</strong> depositor <strong>as</strong> S</p><p><strong>where</strong> <strong>not exists(</strong></p><p>(<strong>select</strong> branch_name</p><p><strong>from</strong> branch</p><p><strong>where</strong> branch_city = ‘Brooklyn’)</p><p><strong>except</strong></p><p>(<strong>select</strong> R.branch_name</p><p><strong>from</strong> depositor <strong>as</strong> T, account <strong>as</strong> R</p><p><strong>where</strong> T.account_number = R.account_number</p><p><strong>and</strong> S.customer_name = T.customer_name))</p><p>b.</p><p><strong>select</strong> <strong>sum</strong>(amount) <strong>as</strong> sum_loan</p><p><strong>from</strong> loan</p><p>c.</p><p><strong>select</strong> branch_name</p><p><strong>from</strong> branch</p><p><strong>where</strong> assets ＞<strong>all</strong></p><p>(<strong>select</strong> assets</p><p><strong>from</strong> branch</p><p><strong>where</strong> branch_city = ‘Brooklyn’)</p>]]></content>
    
    
    
    <tags>
      
      <tag>course</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>智能机器人技术——刚体运动</title>
    <link href="/2022/03/21/robot2/"/>
    <url>/2022/03/21/robot2/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>智能机器人技术作业记录</p><span id="more"></span><p><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/A2.jpg" alt="image-20220321151047649"></p><h1 id="智能机器人技术第三章——刚体运动"><a href="#智能机器人技术第三章——刚体运动" class="headerlink" title="智能机器人技术第三章——刚体运动"></a>智能机器人技术第三章——刚体运动</h1><ol><li><script type="math/tex; mode=display">已知一固定的空间坐标系 \{s\} 及其 \hat{x}_{s} 、 \hat{y}_{s} 、 \hat{z}_{s}轴坐标, 坐标系 \{a\} 的 \hat{x}_{a}轴沿 (0,0,1) 方向,\\ \hat{y}_{a} 轴沿 (-1,0,0) 方向; 坐标系 \{b\} 的 \hat{x}_{b} 轴沿 (1,0,0) 方向, \hat{y}_{b} 轴沿 (0,0,-1)方向。</script></li></ol><p><strong>a)</strong> 手绘 3 个坐标系, 注意画在不同位置以便区分。</p><script type="math/tex; mode=display">由右手定则可知，坐标系 \{a\} 的 \hat{z}_{a} 轴沿 (0,-1,0) 方向，坐标系 \{b\} 的 \hat{z}_{b} 轴沿 (0,1,0) 方向。结果如图所示：</script><p><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/image-20220320155656111.png" alt="image-20220320155656111"></p><p><strong>b)</strong> <script type="math/tex">计算旋转矩阵 R_{s a} 和 R_{s b}</script> 。</p><script type="math/tex; mode=display">\begin{aligned}R_{sa} &=\left[\begin{array}{lll}0 & -1 & 0 \\0 & 0 & -1 \\1 & 0 & 0\end{array}\right] \\R_{sb} &=\left[\begin{array}{ccc}1 & 0 & 0 \\0 & 0 & 1 \\0 & -1 & 0\end{array}\right]\end{aligned}</script><p><strong>c)</strong> <script type="math/tex">已知 R_{s b}, 在不使用逆矩阵的情况下计算 R_{s b}^{-1}, 并验证坐标系画的是否正确。</script></p><p>由旋转矩阵的性质得$R^{-1}=R^T$</p><script type="math/tex; mode=display">\begin{aligned}R_{sb}^{-1}=R^T_{sb} &=\left[\begin{array}{ccc}1 & 0 & 0 \\0 & 0 & -1 \\0 & 1 & 0\end{array}\right]\end{aligned}</script><p>可知坐标系绘画正确。</p><p><strong>d)</strong> <script type="math/tex">已知 R_{s a} 和 R_{s b}, 计算 R_{a b}, 并验证坐标系画的是否正确。</script></p><script type="math/tex; mode=display">\begin{aligned}R_{as}=R_{sa}^{-1}=R^T_{sa} &=\left[\begin{array}{ccc}0 & 0 & 1 \\-1 & 0 & 0 \\0 & -1 & 0\end{array}\right]\end{aligned}</script><script type="math/tex; mode=display">\begin{aligned}R_{ab}=R_{as}R_{sb} &=\left[\begin{array}{ccc}0 & -1 & 0 \\-1 & 0 & 0 \\0 & 0 & -1\end{array}\right]\end{aligned}</script><p>可知坐标系绘画正确。</p><p><strong>e)</strong><script type="math/tex">将 R=R_{s b} 作为变换算子, 表示绕 \hat{x} 轴转动 -90^{\circ} 。计算 R_{1}=R_{s a} R 与 R_{2}=R R_{s a}, 并回答新姿态 R_{1} 与 R_{2} 分别对应的是 R_{s a} 绕哪个坐标系的 \hat{x} 轴转动得到的结果?</script></p><script type="math/tex; mode=display">\begin{aligned}R_{1}=R_{sa}R &=\left[\begin{array}{ccc}0 & 0 & -1 \\0 & 1 & 0 \\1 & 0 & 0\end{array}\right]\end{aligned}</script><script type="math/tex; mode=display">\begin{aligned}R_{2}=RR_{sa} &=\left[\begin{array}{ccc}0 & -1 & 0 \\1 & 0 & 0 \\0 & 0 & 1\end{array}\right]\end{aligned}</script><script type="math/tex; mode=display">R_1表示绕坐标系 \{a\} 的 \hat{x}_{a} 轴，R_2表示绕坐标系 \{s\} 的 \hat{x}_{s} 轴。</script><p><strong>f)</strong> <script type="math/tex">利用 R_{s b} 将点 p_{b}=(1,2,3) 从 \{b\} 系变换到 \{s\} 系。</script></p><script type="math/tex; mode=display">p_{s}=R_{sb}p_{b}=(1,3,-2)</script><p><strong>g)</strong> <script type="math/tex">已知 \{s\} 系中一点 p_{s}=(1,2,3), 计算 p^{\prime}=R_{s b} p_{s} 和 p^{\prime \prime}=R_{s b}^{T} p_{s} 。每一推导过程均可以解释为坐标变换 (无须移动点的位置) 或移动点的位置 (无须改变坐标系)。</script></p><script type="math/tex; mode=display">p^{\prime}=R_{s b} p_{s}=(1,3,-2)：其几何意义为移动点的位置，将向量p_s绕 \hat{x} 轴转动 -90^{\circ}  (无须改变坐标系)。</script><script type="math/tex; mode=display">p^{\prime \prime}=R_{s b}^{T} p_{s}=(1,-3,2)：其几何意义为坐标变换，将 \{s\} 系中一点 p_{s}变换到 \{b\} 系 (无须移动点的位置) 。</script><p><strong>h)</strong>已知 ${s}$ 系中的角速度 $\omega_{s}=(3,2,1)$, 计算其在 ${a}$ 系中的表示。</p><script type="math/tex; mode=display">\omega_{a}=R_{as}\omega_s=(1,-3,-2)</script><p><strong>i)</strong> 计算 $R_{s a}$ 的矩阵对数 $[\widehat{\omega}] \theta$, 并提取其中的元素: 单位角速度 $\widehat{\omega}$ 和转动量 $\theta$ (可以利用编程手段)。</p><p>使用到的函数：</p><p>MatrixLog3：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs matlab"><span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">so3mat</span> = <span class="hljs-title">MatrixLog3</span><span class="hljs-params">(R)</span></span><br>acosinput = (trace(R) - <span class="hljs-number">1</span>) / <span class="hljs-number">2</span>;<br><span class="hljs-keyword">if</span> acosinput &gt;= <span class="hljs-number">1</span><br>    so3mat = <span class="hljs-built_in">zeros</span>(<span class="hljs-number">3</span>);<br><span class="hljs-keyword">elseif</span> acosinput &lt;= <span class="hljs-number">-1</span><br>    <span class="hljs-keyword">if</span> ~NearZero(<span class="hljs-number">1</span> + R(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>))<br>        omg = (<span class="hljs-number">1</span> / <span class="hljs-built_in">sqrt</span>(<span class="hljs-number">2</span> * (<span class="hljs-number">1</span> + R(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>)))) ...<br>              * [R(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>); R(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>); <span class="hljs-number">1</span> + R(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>)];<br>    <span class="hljs-keyword">elseif</span> ~NearZero(<span class="hljs-number">1</span> + R(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>))<br>        omg = (<span class="hljs-number">1</span> / <span class="hljs-built_in">sqrt</span>(<span class="hljs-number">2</span> * (<span class="hljs-number">1</span> + R(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))) ...<br>              * [R(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>); <span class="hljs-number">1</span> + R(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>); R(<span class="hljs-number">3</span>, <span class="hljs-number">2</span>)];<br>    <span class="hljs-keyword">else</span><br>        omg = (<span class="hljs-number">1</span> / <span class="hljs-built_in">sqrt</span>(<span class="hljs-number">2</span> * (<span class="hljs-number">1</span> + R(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)))) ...<br>              * [<span class="hljs-number">1</span> + R(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>); R(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>); R(<span class="hljs-number">3</span>, <span class="hljs-number">1</span>)];<br>    <span class="hljs-keyword">end</span><br>    so3mat = VecToso3(<span class="hljs-built_in">pi</span> * omg);<br><span class="hljs-keyword">else</span><br>theta = <span class="hljs-built_in">acos</span>(acosinput);     <br>    so3mat = theta * (<span class="hljs-number">1</span> / (<span class="hljs-number">2</span> * <span class="hljs-built_in">sin</span>(theta))) * (R - R&#x27;);<br><span class="hljs-keyword">end</span><br><span class="hljs-keyword">end</span><br></code></pre></td></tr></table></figure><p>so3ToVec：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs matlab"><span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">omg</span> = <span class="hljs-title">so3ToVec</span><span class="hljs-params">(so3mat)</span></span><br>omg = [so3mat(<span class="hljs-number">3</span>, <span class="hljs-number">2</span>); so3mat(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>); so3mat(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>)]; <br><span class="hljs-keyword">end</span><br></code></pre></td></tr></table></figure><p>AxisAng3：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs matlab"><span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-params">[omghat, theta]</span> = <span class="hljs-title">AxisAng3</span><span class="hljs-params">(expc3)</span></span><br>theta = norm(expc3);<br>omghat = expc3 / theta;<br><span class="hljs-keyword">end</span><br></code></pre></td></tr></table></figure><p>解得：</p><script type="math/tex; mode=display">[\widehat{\omega}] \theta = \left[\begin{array}{ccc}0 & -1.2092 & -1.2092 \\1.2092 & 0 & -1.2092 \\1.2092 & 1.2092 & 0\end{array}\right]</script><p>单位角速度 $\widehat{\omega}=(0.5774,-0.5774,0.5774)$ 和转动量 $\theta=2.0944$ </p><p><strong>j)</strong> 计算与转动 $\widehat{\omega} \theta=(1,2,0)$ 的指数坐标对应的矩阵指数。</p><p>使用到的函数：</p><p>VecToso3：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs matlab"><span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">so3mat</span> = <span class="hljs-title">VecToso3</span><span class="hljs-params">(omg)</span></span><br>so3mat = [<span class="hljs-number">0</span>, -omg(<span class="hljs-number">3</span>), omg(<span class="hljs-number">2</span>); omg(<span class="hljs-number">3</span>), <span class="hljs-number">0</span>, -omg(<span class="hljs-number">1</span>); -omg(<span class="hljs-number">2</span>), omg(<span class="hljs-number">1</span>), <span class="hljs-number">0</span>];<br><span class="hljs-keyword">end</span><br></code></pre></td></tr></table></figure><p>MatrixExp3：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs matlab"><span class="hljs-function"><span class="hljs-keyword">function</span>  <span class="hljs-title">R</span> = <span class="hljs-title">MatrixExp3</span><span class="hljs-params">(so3mat)</span></span><br>omgtheta = so3ToVec(so3mat);<br><span class="hljs-keyword">if</span> NearZero(norm(omgtheta))<br>    R = <span class="hljs-built_in">eye</span>(<span class="hljs-number">3</span>);<br><span class="hljs-keyword">else</span><br>    [omghat, theta] = AxisAng3(omgtheta);<br>    omgmat = so3mat / theta;<br>    R = <span class="hljs-built_in">eye</span>(<span class="hljs-number">3</span>) + <span class="hljs-built_in">sin</span>(theta) * omgmat + (<span class="hljs-number">1</span> - <span class="hljs-built_in">cos</span>(theta)) * omgmat * omgmat;<br><span class="hljs-keyword">end</span><br><span class="hljs-keyword">end</span><br></code></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{aligned}R &=\left[\begin{array}{ccc}-0.2938 & 0.6469 & 0.7037 \\0.6469 & 0.6765 & -0.3518 \\-0.7037 & 0.3518 & -0.6173\end{array}\right]\end{aligned}</script><p>2.题干如 1 , 并且 ${a}$ 系原点相对 ${s}$ 系的坐标为 $(3,0,0),{b}$ 系原点相对 ${s}$ 系的坐标为 $(0,2,0)$。</p><p><strong>a)</strong> 手绘 3 个坐标系, 注意它们之间的相对位置关系。</p><p>图中${s}$系的三个坐标轴长度均为单位长度，以此绘得结果如下：</p><p><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/image-20220320160304205.png" alt="image-20220320160304205"></p><p><strong>b)</strong> <script type="math/tex">计算齐次变换矩阵 T_{s a} 和 T_{s b} 。</script></p><script type="math/tex; mode=display">\because T=\left[\begin{array}{cc}R & p \\0 & 1\end{array}\right]\\\therefore T_{sa}=\left[\begin{array}{cccc}0 & -1 & 0 & 3 \\0 & 0 & -1 & 0 \\1 & 0 & 0 & 0 \\0 & 0 & 0 & 1\end{array}\right]\\T_{sb}=\left[\begin{array}{cccc}1 & 0 & 0 & 0 \\0 & 0 & 1 & 2 \\0 & -1 & 0 & 0 \\0 & 0 & 0 & 1\end{array}\right]</script><p><strong>c)</strong> <script type="math/tex">已知 T_{s b}, 在不使用逆矩阵的情况下计算 T_{s b}^{-1}, 并验证坐标系画的是否正确。</script></p><script type="math/tex; mode=display">\because T^{-1}=\left[\begin{array}{cc}R & p \\0 & 1\end{array}\right]^{-1}=\left[\begin{array}{cc}R^{\mathrm{T}} & -R^{\mathrm{T}} p \\0 & 1\end{array}\right]\\\therefore T_{sb}^{-1}=\left[\begin{array}{cccc}1 & 0 & 0 & 0 \\0 & 0 & -1 & 0 \\0 & 1 & 0 & -2 \\0 & 0 & 0 & 1\end{array}\right]</script><p>可知坐标系绘画正确。</p><p><strong>d)</strong> <script type="math/tex">已知 T_{s a} 和 T_{s b}, 计算 T_{a b}, 并验证坐标系画的是否正确。</script></p><script type="math/tex; mode=display">\because T_{as}=T_{sa}^{-1}=\left[\begin{array}{cccc}0 & 0 & 1 & 0 \\-1 & 0 & 0 & 3 \\0 & -1 & 0 & 0 \\0 & 0 & 0 & 1\end{array}\right]\\\therefore T_{ab}=T_{as}T_{sb}=\left[\begin{array}{cccc}0 & -1 & 0 & 0 \\-1 & 0 & 0 & 3 \\0 & 0 & -1 & -2 \\0 & 0 & 0 & 1\end{array}\right]</script><p>可知坐标系绘画正确。</p><p><strong>e)</strong> <script type="math/tex">将 T=T_{s b} 作为变换算子, 表示绕\hat{x}轴转动-90^{\circ} 与沿 \hat{y} 移动 2 个单位。计算T_{1}= T_{s a} T 与 T_{2}=T T_{s a}, 并回答新姿态 T_{1} 与 T_{2} 分别对应的是 T_{s a} 绕哪个坐标系的变换得到的结果?</script></p><script type="math/tex; mode=display">T_{1}=T_{sa}T=\left[\begin{array}{cccc}0 & 0 & -1 & 1 \\0 & 1 & 0 & 0 \\1 & 0 & 0 & 0 \\0 & 0 & 0 & 1\end{array}\right]，相对 \{a\} 系变换得到的结果，表示绕\hat{x}_{a}轴转动-90^{\circ} 与沿 \hat{x} 移动 2 个单位。</script><script type="math/tex; mode=display">T_{2}=TT_{sa}=\left[\begin{array}{cccc}0 & -1 & 0 & 3 \\1 & 0 & 0 & 2 \\0 & 0 & 1 & 0 \\0 & 0 & 0 & 1\end{array}\right]，相对 \{s\} 系变换得到的结果，表示绕\hat{x}_{s}轴转动-90^{\circ} 与沿 \hat{x} 移动 3 个单位，沿 \hat{y} 移动 2 个单位。</script><p><strong>f)</strong> <script type="math/tex">利用 T_{s b} 将点 p_{b}=(1,2,3) 从 \{b\} 系变换到 \{s\} 系。</script></p><script type="math/tex; mode=display">p_{s}=T_{sb}p_{b}=(1,5,-2)</script><p><strong>k)</strong> <script type="math/tex">已知 \{s\} 系中一点 p_{s}=(1,2,3), 计算 p^{\prime}=T_{s b} p_{s} 和 p^{\prime \prime}=T_{s b}^{-1} p_{s} 。每一推导过程均可以解释为坐标变换 (无须移动点的位置) 或移动点的位置 (无须改变坐标系)。</script></p><script type="math/tex; mode=display">p^{\prime}=T_{s b} p_{s}=(1,5,-2)：其几何意义为移动点的位置；</script><script type="math/tex; mode=display">p^{\prime \prime}=T_{s b}^{-1} p_{s}=(1,-3,0)：其几何意义为坐标变换。</script><p><strong>g)</strong>已知 ${s}$ 系中的旋量 $\mathcal{V}=(3,2,1,-1,-2,-3)$, 计算其在 ${a}$ 系中的表示。 </p><script type="math/tex; mode=display">\mathcal{V}_{a}=\left[\begin{array}{c}\omega_{a} \\v_{a}\end{array}\right]=\left[\begin{array}{cc}R^{\mathrm{T}} & 0 \\-R^{\mathrm{T}}[p] & R^{\mathrm{T}}\end{array}\right]\left[\begin{array}{c}\omega_{s} \\v_{s}\end{array}\right]=\left[\mathrm{Ad}_{T_{a s}}\right] \mathcal{V}_{s}\\\because \left[\mathrm{Ad}_{T}\right]=\left[\begin{array}{cc}R & 0 \\{[p] R} & R\end{array}\right] \in \mathbb{R}^{6 \times 6} \\\therefore \mathcal{V}_{a}=(1,-3,-2,-9,1,-1)</script><p><strong>h)</strong> 计算 $T_{s a}$ 的矩阵对数 $[\delta] \theta$, 并提取其中的元素: 单位螺旋轴 $\mathcal{S}$ 和转动量 $\theta$ 。</p><p>使用到的函数：</p><p>MatrixLog6：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs matlab"><span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">expmat</span> = <span class="hljs-title">MatrixLog6</span><span class="hljs-params">(T)</span></span><br>[R, p] = TransToRp(T);<br>omgmat = MatrixLog3(R);<br><span class="hljs-keyword">if</span> <span class="hljs-built_in">isequal</span>(omgmat, <span class="hljs-built_in">zeros</span>(<span class="hljs-number">3</span>))<br>    expmat = [<span class="hljs-built_in">zeros</span>(<span class="hljs-number">3</span>), T(<span class="hljs-number">1</span>: <span class="hljs-number">3</span>, <span class="hljs-number">4</span>); <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>];<br><span class="hljs-keyword">else</span><br>    theta = <span class="hljs-built_in">acos</span>((trace(R) - <span class="hljs-number">1</span>) / <span class="hljs-number">2</span>); <br>    expmat = [omgmat, (<span class="hljs-built_in">eye</span>(<span class="hljs-number">3</span>) - omgmat / <span class="hljs-number">2</span> ...<br>                      + (<span class="hljs-number">1</span> / theta - <span class="hljs-built_in">cot</span>(theta / <span class="hljs-number">2</span>) / <span class="hljs-number">2</span>) ...<br>                        * omgmat * omgmat / theta) * p;<br>              <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>];    <br><span class="hljs-keyword">end</span><br><span class="hljs-keyword">end</span><br></code></pre></td></tr></table></figure><p>se3ToVec：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs matlab"><span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">V</span> = <span class="hljs-title">se3ToVec</span><span class="hljs-params">(se3mat)</span></span><br>V = [se3mat(<span class="hljs-number">3</span>, <span class="hljs-number">2</span>); se3mat(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>); se3mat(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>); se3mat(<span class="hljs-number">1</span>: <span class="hljs-number">3</span>, <span class="hljs-number">4</span>)]; <br><span class="hljs-keyword">end</span><br></code></pre></td></tr></table></figure><p>AxisAng6：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs matlab"><span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-params">[S, theta]</span> = <span class="hljs-title">AxisAng6</span><span class="hljs-params">(expc6)</span></span><br>theta = norm(expc6(<span class="hljs-number">1</span>: <span class="hljs-number">3</span>));<br><span class="hljs-keyword">if</span> NearZero(theta)<br>    theta = norm(expc6(<span class="hljs-number">4</span>: <span class="hljs-number">6</span>));<br><span class="hljs-keyword">end</span><br>S = expc6 / theta;      <br><span class="hljs-keyword">end</span><br></code></pre></td></tr></table></figure><p>解得：</p><script type="math/tex; mode=display">[\delta] \theta=\left[\begin{array}{cccc}0 & -1.2092 & -1.2092 & 2.2092 \\1.2092 & 0 & -1.2092 & -2.2092 \\1.2092 & 1.2092 & 0 & -1.4184 \\0 & 0 & 0 & 0\end{array}\right]</script><p>单位螺旋轴   $\mathcal{S}=(0.5774,-0.5774,0.5774,1.0548,-1.0548,-0.6772)$ 和转动量 $\theta=2.0944$ </p><p><strong>i)</strong> 计算与转动 $\mathcal{S} \theta=(0,1,2,3,0,0)$ 的指数坐标对应的矩阵指数。</p><p>使用到的函数：</p><p>VecTose3：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs matlab"><span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">se3mat</span> = <span class="hljs-title">VecTose3</span><span class="hljs-params">(V)</span></span><br>se3mat = [VecToso3(V(<span class="hljs-number">1</span>: <span class="hljs-number">3</span>)), V(<span class="hljs-number">4</span>: <span class="hljs-number">6</span>); <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>];<br><span class="hljs-keyword">end</span><br></code></pre></td></tr></table></figure><p>MatrixExp6：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs matlab"><span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">T</span> = <span class="hljs-title">MatrixExp6</span><span class="hljs-params">(se3mat)</span><span class="hljs-title">omgtheta</span> = <span class="hljs-title">so3ToVec</span><span class="hljs-params">(se3mat(1: 3, 1: 3)</span>);</span><br><span class="hljs-keyword">if</span> NearZero(norm(omgtheta))<br>    T = [<span class="hljs-built_in">eye</span>(<span class="hljs-number">3</span>), se3mat(<span class="hljs-number">1</span>: <span class="hljs-number">3</span>, <span class="hljs-number">4</span>); <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>];<br><span class="hljs-keyword">else</span><br>    [omghat, theta] = AxisAng3(omgtheta);<br>    omgmat = se3mat(<span class="hljs-number">1</span>: <span class="hljs-number">3</span>, <span class="hljs-number">1</span>: <span class="hljs-number">3</span>) / theta; <br>    T = [MatrixExp3(se3mat(<span class="hljs-number">1</span>: <span class="hljs-number">3</span>, <span class="hljs-number">1</span>: <span class="hljs-number">3</span>)), ...<br>         (<span class="hljs-built_in">eye</span>(<span class="hljs-number">3</span>) * theta + (<span class="hljs-number">1</span> - <span class="hljs-built_in">cos</span>(theta)) * omgmat ...<br>          + (theta - <span class="hljs-built_in">sin</span>(theta)) * omgmat * omgmat) ...<br>            * se3mat(<span class="hljs-number">1</span>: <span class="hljs-number">3</span>, <span class="hljs-number">4</span>) / theta;<br>         <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>];<br><span class="hljs-keyword">end</span><br><span class="hljs-keyword">end</span><br></code></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{aligned}T &=\left[\begin{array}{cccc}-0.6173 & -0.7037 & 0.3518 & 1.0555 \\0.7037 & -0.2938 & 0.6469 & 1.9407 \\-0.3518 & 0.6469 & 0.6765 & -0.9704 \\0 & 0 & 0 & 1\end{array}\right]\end{aligned}</script><p>3.目前工业机器人领域经常需要定义 4 种坐标系: 参考坐标系 ${a}$, 末端或工具坐标系 ${b}$ 、图像坐标系 ${c}$ 、件坐标系 ${d}$, 如下所示。</p><p><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/image-20220314230530164.png" alt="image-20220314230530164"></p><p><strong>a)</strong> <script type="math/tex">基于图中所给尺寸, 确定 T_{a d} 和 T_{c d} 。</script></p><script type="math/tex; mode=display">T_{ad}=\left[\begin{array}{cccc}1 & 0 & 0 & -1 \\0 & 1 & 0 & 1 \\0 & 0 & 1 & 0 \\0 & 0 & 0 & 1\end{array}\right]</script><script type="math/tex; mode=display">T_{cd}=\left[\begin{array}{cccc}0 & 1 & 0 & 0 \\1 & 0 & 0 & 0 \\0 & 0 & -1 & 2 \\0 & 0 & 0 & 1\end{array}\right]</script><p><strong>b)</strong> <script type="math/tex">若T_{bc}=[1~~0~~0~~4；0~~1 ~~0~~0；0~~0~~1~~0；0~~0~~0~~1],求T_{ab}。</script></p><script type="math/tex; mode=display">T_{ab}=T_{ac}T_{cb}=T_{ad}T_{dc}T_{bc}^{-1}=T_{ad}T_{cd}^{-1}T_{bc}^{-1}\\=\left[\begin{array}{cccc}0 & 1 & 0 & -1 \\1 & 0 & 0 & -3 \\0 & 0 & -1 & 2 \\0 & 0 & 0 & 1\end{array}\right]</script><p><img src="https://cdn.mathpix.com/cropped/2022_03_20_515d600b91de346f2453g-1.jpg?height=1510&amp;width=1069&amp;top_left_y=122&amp;top_left_x=108" alt=""></p><p><img src="https://cdn.mathpix.com/cropped/2022_03_20_515d600b91de346f2453g-2.jpg?height=556&amp;width=999&amp;top_left_y=598&amp;top_left_x=116" alt=""></p><p><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/image-20220322133139136.png" alt="image-20220322133139136"></p><p><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/image-20220322133103015.png" alt="image-20220322133103015"></p>]]></content>
    
    
    
    <tags>
      
      <tag>course</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数据库原理 Exercises 1</title>
    <link href="/2022/03/17/data1/"/>
    <url>/2022/03/17/data1/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Database System Concepts Exercises of Chapter 2</p><span id="more"></span><p><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/photo-1613068687893-5e85b4638b56" alt="black flat screen computer monitor"></p><h1 id="Database-System-Concepts-Exercises-of-Chapter-2"><a href="#Database-System-Concepts-Exercises-of-Chapter-2" class="headerlink" title="Database System Concepts Exercises of Chapter 2"></a>Database System Concepts Exercises of Chapter 2</h1><p><strong>Exercise 2.1</strong> Consider the relational database of Figure 2.14,</p><p>Employee( person_name, street, city)<br>Works (person_name, company_name, salary)<br>Company(company_name, city)<br><strong>Figure 2.14</strong></p><p>What are the appropriate primary keys?</p><p><strong>My answer:</strong></p><p>$employee (\underline{person-name}, street, city)$</p><p>$works (\underline{person-name}, company-name, salary) $</p><p>$company (\underline{company-name}, city)$</p><p><strong>Exercise</strong> <strong>2.7</strong> Consider the relational database of Figure 2.14. Given an expression in the relational algebra to express each of the following queries:</p><p>a.Find the names of all employees who live in city “Miami”</p><p>b.Find the names of all employees whose salary is greater than $100,000.</p><p>c.Find the names of all employees who live in “Miami” and whose salary is greater than $100,000.</p><p><strong>My answer:</strong></p><p><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/image-20220318132151932.png" alt="image-20220318132151932"></p><p><strong>Exercise</strong> <strong>2.9</strong> Consider the bank database of Figure 2.15.</p><p>branch(branch_name, branch_city, assets)<br>customer(customer_name, customer_street, customer_city)<br>loan(loan_number, branch_name, amount)<br>borrower(customer_name, loan_number)<br>account(account_number, loan_number)<br>depositor(customer_name, account_number)<br><strong>Figure 2.15</strong></p><p>a.  What are the appropriate primary keys?</p><p>b. Given your choice of primary keys, identify appropriate foreign keys. Assume that branch names and customer names uniquely identify branches and customers, but loans and accounts can be associated with more than one customer.</p><p><strong>My answer:</strong></p><p>The primary keys are marked with an $\underline{underline}$, and the foreign keys are marked with a $\overline{overline}$.</p><p>$branch(\underline{branch-name}, branch-city, assets)$</p><p>$customer(\underline{customer-name}, customer-street, customer-city)$</p><p>$loan(\underline{loan-number}, \overline{branch-name}, amount)$</p><p>$borrower(\overline{\underline{customer-name}}, \overline{loan-number})$</p><p>$account(\underline{account-number}, loan-number)$</p><p>$depositor(\overline{\underline{customer-name}}, \overline{account-number})$</p>]]></content>
    
    
    
    <tags>
      
      <tag>course</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>计算机网络和因特网</title>
    <link href="/2022/03/14/net1/"/>
    <url>/2022/03/14/net1/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>第一章计算机网络和因特网小结</p><span id="more"></span><h2 id="1-简述面向连接和无连接两种服务的特点。"><a href="#1-简述面向连接和无连接两种服务的特点。" class="headerlink" title="1.简述面向连接和无连接两种服务的特点。"></a>1.<strong>简述面向连接和无连接两种服务的特点。</strong></h2><p>面向连接服务：质量可靠，确保从发送方发出的数据最终按顺序完整地交付给接收方。</p><p>无连接服务：质量不可靠，不能对最终交付作任何保证。</p><p><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/image-20220313164646441.png" alt="image-20220313164646441"></p><p><strong>面向连接服务</strong>(connection-oriented)</p><p>面向连接服务具有<strong>可靠的数据传送，流控制，拥塞控制</strong>三个特性。</p><p>特点：</p><p>1、面向连接的数据传输过程必须经过建立连接、连接维护和释放连接的3个过程；</p><p>2、数据传输过程中，各分组不需要携带目的地址。</p><p>面向连接数据传输的收发数据顺序不变，因此传输的可靠性好，但协议复杂，通信效率不高。</p><p><strong>无连接服务</strong>(connectionless)</p><p>两个实体之间的通信<strong>不需要先建立好连接</strong>。是一种不可靠的服务。这种服务常被描述为“尽最大努力交付”(best effort delivery)或“尽力而为”。</p><p>特点：</p><p>1、数据传输过程中，每个分组都携带完整的目的地址，各分组在系统中是独立传送的。因此，无连接中的数据传输过程不需要经过3个过程；</p><p>2、由于无连接发送的不同的分组，可能经历不同路径到目的主机，所以先发送的不一定先到，因此无连接的数据分组传输过程中，目的主机接收的数据分组可能出现乱序、重复与丢失的现象。</p><p>无连接的可靠性不是很好，但因其省去许多保征机制，因此通信协议相对简单，通信效率较高。</p><h2 id="2-什么是多路复用？常分为哪两种类型。"><a href="#2-什么是多路复用？常分为哪两种类型。" class="headerlink" title="2.什么是多路复用？常分为哪两种类型。"></a>2.<strong>什么是多路复用？常分为哪两种类型。</strong></h2><p>多路复用是指在一条传输链路上同时建立多条连接，分别传输数据。</p><p>常分为以下两种类型：</p><p><strong>频分多路复用FDM(frequency-division multiplexing)</strong>：链路的频谱由跨越链路创建的连接所共享，按频率划分若干频段，每个频段专用于一个连接。</p><p><strong>时分多路复用TDM (time-division multiplexing)</strong> ：时间划分为固定区间的帧，每帧再划分为固定数量的时隙，每一个时隙专用于一个连接，用于传输数据。</p><p><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/image-20220314223649714.png" alt="image-20220314223649714"></p><h2 id="3-简述电路交换和分组交换特点及工作过程。"><a href="#3-简述电路交换和分组交换特点及工作过程。" class="headerlink" title="3.简述电路交换和分组交换特点及工作过程。"></a>3.<strong>简述电路交换和分组交换特点及工作过程。</strong></h2><p><strong>特点：</strong></p><ol><li><strong>电路交换 (circuit switching)</strong><br>预留端到端资源：端系统之间通信路径上所需要的资源 (缓存，链路带宽)，建立连接；<br>发送方以恒定速率向接收方传送数据。<br>如，电话网络。</li><li><strong>分组交换(packet switching)</strong><br>不需要资源预留；<br>按需使用资源，可能要排队等待，同时有其它分组发送。<br>如，因特网。</li></ol><p><strong>工作过程：</strong></p><ol><li><p>电路交换：</p><p>在两台主机A、B之间创建一条专用的端到端连接，分别占用每条链路中的一条电路；</p><p>该连接获得链路带宽的1/n，进行通信。</p></li><li><p>分组交换：</p><p>源端将报文划分为较小的数据块（分组packet）；</p><p>每个分组通过一系列链路和分组交换机传送，直到目的端</p><p>目的端恢复原报文。</p></li></ol><p><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/image-20220314220812885.png" alt="image-20220314220812885"></p><h2 id="4-什么是协议？分层的服务模型？"><a href="#4-什么是协议？分层的服务模型？" class="headerlink" title="4.什么是协议？分层的服务模型？"></a>4.<strong>什么是协议？分层的服务模型？</strong></h2><p><strong>协议</strong>：控制网络中信息的发送和接收。定义了通信实体之间交换报文的格式和次序，以及在报文传输和/或接收或其他事件所采取的动作。</p><p><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/image-20220314223735346.png" alt="image-20220314223735346"></p><p><strong>分层的服务模型</strong>：上层调用下层的服务，下层为上层提供服务。</p><p>通过第n层本身执行某些动作，或再使用其相邻下层（第n-1层）的服务，来完成向其上层（第n+1层）提供的服务。</p><h2 id="5-简述分层的特点和分层后数据的传递过程。"><a href="#5-简述分层的特点和分层后数据的传递过程。" class="headerlink" title="5.简述分层的特点和分层后数据的传递过程。"></a>5.<strong>简述分层的特点和分层后数据的传递过程。</strong></h2><p><strong>分层特点：</strong></p><ul><li>每层功能独立；</li><li>每两个相邻层之间有一逻辑接口，可交换信息；</li><li>上一层建立在下一层基础上，上一层可调用下一层的服务，下一层为上一层提供服务。</li></ul><p><strong>分层后数据的传递过程</strong>：主机（端系统）间数据传送实际上并不是在对等层间直接进行，而是通过相邻层间的传递合作完成。</p><p>发送方：将用户数据由高层向低层逐层传递，每经过一层，加上该层的控制信息，直到最低层（物理层），然后直接通过物理媒体传输到目的方。（逐层封装）</p><p>接收方：将收到的数据由低层向高层逐层传递，每经过一层，去掉该层的控制信息，直到最高层，恢复为用户数据。（逐层解封）</p><p><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/image-20220314222653398.png" alt="image-20220314222653398"></p><h2 id="6-因特网分层模型及各层功能。"><a href="#6-因特网分层模型及各层功能。" class="headerlink" title="6.因特网分层模型及各层功能。"></a>6.<strong>因特网分层模型及各层功能。</strong></h2><p>因特网的协议栈由5个层次组成：物理层、链路层、网络层、运输层和应用层。</p><p><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/image-20220314222941459.png" alt="image-20220314222941459"></p><ol><li>应用层：提供各种网络应用。传输应用报文。<br>FTP、 SMTP、 HTTP</li><li>运输层：在应用程序的客户机和服务器之间提供传输应用层报文服务。传输报文段。<br>TCP、 UDP</li><li>网络层：主机和主机之间传输网络层分组（数据报）。<br>IP协议、 选路协议</li><li>链路层： 在邻近单元之间传输数据（帧 ）。<br>PPP、以太网</li><li>物理层：在节点之间传输比特流。<br>传输媒体</li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>course</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>智能机器人技术——位形空间</title>
    <link href="/2022/03/02/robot1/"/>
    <url>/2022/03/02/robot1/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>智能机器人技术作业记录</p><span id="more"></span><p><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/image-20220302214614790.png" alt="image-20220302214614790"></p><h1 id="智能机器人技术——位形空间"><a href="#智能机器人技术——位形空间" class="headerlink" title="智能机器人技术——位形空间"></a>智能机器人技术——位形空间</h1><p>一、选择/填空题（10 分）。</p><ol><li>机器人的自由度是 (<strong>D</strong>) ?<br>A. 机器人上点的数量<br>B. 机器人关节数量<br>C. 组成机器人的刚体的数量<br>D. 组成机器人的刚体的数量, 减去刚体间独立约束的数量</li><li>二维平面刚体的自由度为 (<strong>3</strong>) ； 三维空间刚体的自由度为 (<strong>6</strong>) 。</li><li>根据课堂上推算三维空间内刚体自由度的方法, 推算出四维空间中刚体的自由度 (<strong>10</strong>)、有关角度的自由度 (<strong>6</strong>)、有关平移位置的自由度 (<strong>4</strong>)。(如, 三维空间中分别为 $6,3,3$ )</li><li>假设你的手臂（从肩膀到手掌）, 有 7 个自由度。你如同一位服务生一样水平端着餐盘, 防止洒出酒水。你的手臂此时有几个自由度：<strong>5</strong>， 这个任务空间的自由度是：<strong>4</strong>。</li></ol><p>二、简答题, 请写出解题过程 (10 分)。</p><ol><li><p>考虑两个刚体之间的一个关节。每个刚体有 $\mathrm{m}$ 个自由度（二维空间刚体 $m=3$, 三维空间刚体 $m=6$ ), 并且没有任何约束。关节有 $f$ 个自由度（旋转关节 $\mathrm{f}=1$, 球形关节 $\mathrm{f}=3$ 等)。试问, 用这个关节关联两个刚体, 那么引入了多少个约束（一个刚体相对于另一个, 用字母表示）?</p><p><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/image-20220302212205280.png" alt="image-20220302212205280"></p><p><strong>解：$自由度f+平面约束c=平面自由度3，自由度f＋空间约束c=空间自由度6$</strong></p><p><strong>故$\pmb{c=-f+m}$</strong></p></li><li><p>考虑一个机构包含了 3 个三维空间刚体 (记住, 包括地面, 所以 $\mathrm{N}=4$ ), 和 4 个关节: 1 个转动, 1 个平移, 一个万向, 一个球形。使用 Grubler 公式, 计算机构的自由度。</p><p><strong>解：转动副$f_1=1$，移动副$f_2=1$，万向铰$f_3=2$，球形铰$f_4=3$</strong></p><p><strong>又$\because N=4$，$J=4$，$m=6$</strong></p><p><strong>$\therefore \pmb{dof=m(N-1-j)+\sum_{i=1}^Jf_i=6\times\left(4-1-4\right)+\left(1+1+2+3\right)=1}$</strong></p></li><li>如下图的 SRS (球形-转动-球形) 机构, 正在抓取一个物体。试问, 当机构紧握物体时 (物体与机构中的机械臂最后一段没有相对运动时), 自由度是多少?</li></ol><p><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/2022_02_28_40f28045fe8caa2a438eg-1.jpg" alt=""></p><p><strong>解：Spherical Joint即为球形铰$f=3$，共8个；Revolute Joint即为转动副$f=1$，共4个；</strong></p><p><strong>又$\because N=14$，$J=16$，$m=6$</strong></p><p>$\therefore \pmb{dof=m(N-1-j)+\sum_{i=1}^Jf_i=6\times\left(14-1-16\right)+\left(3\times8+1\times4\right)=10}$</p><ol><li><p>同上题, 如果现在有 $n$ 条这样的机械臂 (题 3 中 $n=4$ ), 机构的自由度是?</p><p><strong>同理：$\pmb{dof=m(N-1-j)+\sum_{i=1}^Jf_i=6\times\left(3n+2-1-4n\right)+\left(3\times2n+1\times n\right)=n+6}$</strong></p></li><li><p>同上题, 假设 $n$ 条机械臂的转动关节, 被替换成了万向关节, 机构的自由度是?</p><p><strong>同理：$\pmb{dof=m(N-1-j)+\sum_{i=1}^Jf_i=6\times\left(3n+2-1-4n\right)+\left(3\times2n+2\times n\right)=2n+6}$</strong></p></li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>course</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2022’summary</title>
    <link href="/2021/12/31/2022/"/>
    <url>/2021/12/31/2022/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/image-20211231192304439.png" alt="image-20211231192304439"></p><p>这是2021年发的第一条朋友圈配图，顺手拿来总结吧。</p><span id="more"></span><p>没想到曾经一个因为不怎么使用微信，不怎么看朋友圈而饱受困扰的人会用看朋友圈来回顾自己的一整年。</p><p>毕竟有感而发，这次就不纠结排版美不美观、结构严不严谨了。</p><h1 id="天时人事日相催，冬至阳生春又来"><a href="#天时人事日相催，冬至阳生春又来" class="headerlink" title="天时人事日相催，冬至阳生春又来"></a>天时人事日相催，冬至阳生春又来</h1><p>21的一月初我应该有在努力复习吧，因为一整个学期的怠惰，因为对一个优秀女生的追逐。大学跟高中差别真大，没有人逼我写必刷题了，没有人逼我额外补课了，没有所谓火箭班了。大一上虽然大部分都在20年这个区间，但是给我的21年真是奠定了乱七八糟的基础。</p><p>1月6号那天开始听YOASOBI的歌。</p><p><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/image-20211231193810905.png" alt="image-20211231193810905"></p><p>我的“好友”为我做的图，可惜这位好友不再联系了，倒是图里的内容实现了。</p><p>放假回家了，回了一趟华美，可惜看到华美帝国回忆录的共享文档的时候我还是写不出东西来。</p><h1 id="鸿雁长飞光不度，鱼龙潜跃水成文"><a href="#鸿雁长飞光不度，鱼龙潜跃水成文" class="headerlink" title="鸿雁长飞光不度，鱼龙潜跃水成文"></a>鸿雁长飞光不度，鱼龙潜跃水成文</h1><p>在二月终于和大鸟转转转酒吧的兄弟们会师了，22年的美食王国的勇者传说继续辉煌！可能这是目前我唯一期待的年度团建了吧，一群我可以无条件信任的人。</p><p>这个月里跟了好多人出去玩，多年未见的童年好友，帮我很多的同乡师兄，关系超好的高中同学···（词穷了，就这样吧）</p><p>反正过了个年，又开学了！</p><p>甚至不记得为什么留下这段感慨：</p><p>凡事都有定期。天下万物都有定时，生有时，死有时，哭有时，笑有时，寻找有时，放手有时。</p><p>好消息是1204棋牌中心初具雏形，为后来的金碧辉煌作了铺垫。</p><h1 id="空里流霜不觉飞，汀上白沙看不见"><a href="#空里流霜不觉飞，汀上白沙看不见" class="headerlink" title="空里流霜不觉飞，汀上白沙看不见"></a>空里流霜不觉飞，汀上白沙看不见</h1><p>三月里买了我的宝贝surface，那我肯定开始家教了，虽然我天天骂它玩4399都卡，但确实给我带来了极大便利，不管去哪都只带着平板可比背着游戏本的习武之人好多了。</p><p><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/image-20211231195659935.png" alt="image-20211231195659935"></p><p>在这个时候参加了大创，虽然可能自己完全没有做出什么东西的能力来，但那个星期还是挺震撼的，原来科研离我并不远。毕竟我也梦想成为拉格多科学院的“疯子”。</p><p>这个月里还参加了几次团建，文体组、智协、微软俱乐部（虽然改名了）···大一参加的每一个社团或组织都让我收获了好多好多，知识也好，朋友也好，不禁感慨，还好我参加了。</p><p>三月十号那天还认识了一个special的人，感谢大物，缘分真奇妙。</p><p>虽然我貌似有很多的朋友，但事实上我是一个很害怕交友的人。害怕我的所作所为会对其他人的人生轨迹造成影响（确实怪中二的）。</p><p>不过好在大部分时候都是正面影响，希望我的朋友毕业后，就业后，10年后，50年后还是我的朋友，在我的轨迹留多点痕迹。</p><h1 id="江水流春去欲尽，江潭落月复西斜"><a href="#江水流春去欲尽，江潭落月复西斜" class="headerlink" title="江水流春去欲尽，江潭落月复西斜"></a>江水流春去欲尽，江潭落月复西斜</h1><p>四月里我开始读诗了！语文老师她看了肯定很惊讶，为什么语文成绩这么差的我居然还会对这些感兴趣，感谢老师的滕王阁序。</p><p>这个月里我应该有好好在写python，matlab，c++和数据结构与算法吧，学基础的编程语言让人充满了成就感。</p><p>这个月最特殊的事应该是认识了another special people，刚好前后差了一个月。</p><p>虽然目前是我生活里最重要的人们，但是真的给我带来了许多的”痛苦“！烦死了！希望来年你们可以不要那么傻，但是请继续傻乐下去。</p><h1 id="谁家今夜扁舟子，何处相思明月楼"><a href="#谁家今夜扁舟子，何处相思明月楼" class="headerlink" title="谁家今夜扁舟子，何处相思明月楼"></a>谁家今夜扁舟子，何处相思明月楼</h1><p>五月Enderfga’s blog 建站了！虽然产生了没几篇高质量内容，但是反正是我的，我说了算！</p><p>看起来这个月里我的朋友圈大部分都是吃吃喝喝，参加了一些小比赛，</p><p>总是会产生一些，我学会了好多东西~然后过了一段时间就发现，原来的自己好傻，连时间复杂度都不知道是什么，连flag都没有听说过。</p><h1 id="玉户帘中卷不去，捣衣砧上拂还来"><a href="#玉户帘中卷不去，捣衣砧上拂还来" class="headerlink" title="玉户帘中卷不去，捣衣砧上拂还来"></a>玉户帘中卷不去，捣衣砧上拂还来</h1><p>这个月我生日了，收到了好多好多礼物呀，虽然我还是会觉得，某一天不会和它的昨天与明天有所区别，但是被大家重视的一天就另当别论了。</p><p>这个月还停止前面说的追逐，不过即使这样还是要告诉自己：人间一趟，积极向上！</p><p>学弟学妹参加了高考，他们信心满满；而我还在为不挂科努力。</p><h1 id="春江潮水连海平，海上明月共潮生"><a href="#春江潮水连海平，海上明月共潮生" class="headerlink" title="春江潮水连海平，海上明月共潮生"></a>春江潮水连海平，海上明月共潮生</h1><p>暑假没有回家，认真打工的七月。</p><p>一天12小时的家教，给我未来的体重激增奠定了经济基础。</p><p>总是会回忆一下人，即使物是人非。</p><p>b站前一段时间是不是火了随机挑战来着？我的七月十七号早知道应该拍成视频，全靠掷骰子随机出发，没想到最后还能到关山月美术馆，莲花山和深圳湾。</p><h1 id="斜月沉沉藏海雾，碣石潇湘无限路"><a href="#斜月沉沉藏海雾，碣石潇湘无限路" class="headerlink" title="斜月沉沉藏海雾，碣石潇湘无限路"></a>斜月沉沉藏海雾，碣石潇湘无限路</h1><p>整个八月估计都在为奥运热血沸腾，16年12年08年的我都没有感受到奥运的魅力。</p><p><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/image-20211231202939847.png" alt="image-20211231202939847"></p><p>当然还有大吃大喝跟兄弟们的沙雕你画我猜。</p><h1 id="不知江月待何人，但见长江送流水"><a href="#不知江月待何人，但见长江送流水" class="headerlink" title="不知江月待何人，但见长江送流水"></a>不知江月待何人，但见长江送流水</h1><p>又开学啦····</p><p>我变成学长了，迎新了好多学弟学妹。</p><p>这个月我估计有点想恋爱了，转了很多有的没的。</p><p>暑假赚钱的心愿：请我的好朋友们吃大餐</p><p>开始在这个月一步步实现了！（钱包肉疼）</p><h1 id="禁街箫鼓，寒轻夜永"><a href="#禁街箫鼓，寒轻夜永" class="headerlink" title="禁街箫鼓，寒轻夜永"></a>禁街箫鼓，寒轻夜永</h1><p>国庆爱hanser五周年纪念日！</p><p>第一次跟朋友们出去旅游的国庆假期，有美女美食相伴的日子比在自习室打代码快乐多了。</p><p>越来越胖了，有钱导致我吃的太好，我应该破产的。</p><p>怎么我乐于助人，你们就乐于送我吃蛋糕奶茶呀，这不好！</p><h1 id="凝霜夜下拂罗衣，浮云中断开明月"><a href="#凝霜夜下拂罗衣，浮云中断开明月" class="headerlink" title="凝霜夜下拂罗衣，浮云中断开明月"></a>凝霜夜下拂罗衣，浮云中断开明月</h1><p>十一月不就是上个月吗？我在干嘛？</p><p>我在陪重要的人吃吃喝喝还有傻乐。</p><p>希望我来年总结的时候我还是在陪重要的人吃喝傻乐，不过得多点努力学习。怎么会有这么懒的人，就知道玩动森···</p><p>十二号那天在大教室里演讲了，感觉自己从小就很有表现欲，奈何没有任何技能，每一次上台前都手抖流汗，</p><p>但是我记得那天我讲的很满意，我自己很满意，有进步就好。</p><p><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/image-20211231204209953.png" alt="image-20211231204209953"></p><h1 id="愿我如星君如月，夜夜流光相皎洁"><a href="#愿我如星君如月，夜夜流光相皎洁" class="headerlink" title="愿我如星君如月，夜夜流光相皎洁"></a>愿我如星君如月，夜夜流光相皎洁</h1><p><img src="https://gitee.com/sysu_20354027/fig/raw/master/img/image-20211231204818624.png" alt="image-20211231204818624"></p><p>12月就不单独总结了，总结全年吧！</p><p>本人2021年年度成就总结:</p><ol><li>学术方面，凭借个人努力，在核酸检测领域产出多份数据真实详尽的报告。</li><li>健康方面，保证膳食纤维摄入，具体表现为每日坚持吃瓜，吃好瓜，吃大瓜。</li><li>商业方面，与各大平台合作，全面参与投资618双11、双12等千亿级重大项目。</li><li>环保方面，股票基金一片绿，绿水青山就是金山银山。在废物利用领域更是成绩斐然:自己作为废物，常常被别人利用。</li><li>运动方面，专注于水上项目，在摸鱼、划水等小项上有突出表现。</li><li>信仰方面，全心全意坚持转发锦鲤不动摇。</li></ol><p>算了不玩梗了。</p><p>这一年有什么值得回顾的吗？</p><p>也许应该是友谊吧，从高中到大学，相识6，7年的朋友们不在了，跳出舒适圈，结识了另外一批可爱的人。</p><p>本来不想花时间写这种东西的，会觉得“你的总结关别人什么事，写出来谁会看呀”。</p><p>后来想了想，真有道理，我应该写给明年31号的自己。</p><p>喂，桂安，你有没有记得刷LeetCode，今天学习强国了吗？和朋友们的关系好不，有没有给他们带来快乐？有给学弟学妹做个好榜样没？</p><p>linux用惯没，c++/python/matlab还记得怎么用吗？绩点到5没？体重下来没？······</p><p><img src="https://v2.jinrishici.com/one.svg" alt="今日诗词"></p><p>2021最大的收获是结识了读到这里的你，谢谢~</p>]]></content>
    
    
    
    <tags>
      
      <tag>self</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Visual Synthesis Pre-training for NUWA</title>
    <link href="/2021/12/26/NUWA/"/>
    <url>/2021/12/26/NUWA/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>基于深度生成模型的多模态视觉合成</p><span id="more"></span><embed src="./NUWA.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    
    <tags>
      
      <tag>course</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于强化学习的无人机自主导航</title>
    <link href="/2021/12/22/UAV/"/>
    <url>/2021/12/22/UAV/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>无人系统导论课程设计报告</p><span id="more"></span><embed src="./UAV.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    
    <tags>
      
      <tag>course</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度学习在新冠肺炎辅助诊断中的应用</title>
    <link href="/2021/12/20/AI/"/>
    <url>/2021/12/20/AI/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>综述论文</p><span id="more"></span><embed src="./COVID-19.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    
    <tags>
      
      <tag>course</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>wsl安装与开发环境搭建</title>
    <link href="/2021/11/26/wsl/"/>
    <url>/2021/11/26/wsl/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>wsl2，vscode，windows terminal，zsh，docker····</p><span id="more"></span><h1 id="Windows-Subsystem-for-Linux"><a href="#Windows-Subsystem-for-Linux" class="headerlink" title="Windows Subsystem for Linux"></a>Windows Subsystem for Linux</h1><p>首先贴一个<a href="https://docs.microsoft.com/zh-cn/windows/wsl/install">官方文档</a></p><h2 id="什么是适用于-Linux-的-Windows-子系统"><a href="#什么是适用于-Linux-的-Windows-子系统" class="headerlink" title="什么是适用于 Linux 的 Windows 子系统"></a>什么是适用于 Linux 的 Windows 子系统</h2><p>适用于 Linux 的 Windows 子系统可让开发人员按原样运行 GNU/Linux 环境 - 包括大多数命令行工具、实用工具和应用程序 - 且不会产生传统虚拟机或双启动设置开销。</p><ul><li><a href="https://aka.ms/wslstore">在 Microsoft Store</a> 中选择你偏好的 GNU/Linux 分发版。</li><li>运行常用的命令行软件工具（例如 <code>grep</code>、<code>sed</code>、<code>awk</code>）或其他 ELF-64 二进制文件。</li><li>运行 Bash shell 脚本和 GNU/Linux 命令行应用程序，包括：<ul><li>工具：vim、emacs、tmux</li><li>语言：<a href="https://docs.microsoft.com/zh-cn/windows/nodejs/setup-on-wsl2">NodeJS</a>、Javascript、<a href="https://docs.microsoft.com/zh-cn/windows/python/web-frameworks">Python</a>、Ruby、C/ C++、C# 与 F#、Rust、Go 等。</li><li>服务：SSHD、<a href="https://docs.microsoft.com/zh-cn/windows/wsl/tutorials/wsl-database">MySQL</a>、Apache、lighttpd、<a href="https://docs.microsoft.com/zh-cn/windows/wsl/tutorials/wsl-database">MongoDB</a>、<a href="https://docs.microsoft.com/zh-cn/windows/wsl/tutorials/wsl-database">PostgreSQL</a>。</li></ul></li><li>使用自己的 GNU/Linux 分发包管理器安装其他软件。</li><li>使用类似于 Unix 的命令行 shell 调用 Windows 应用程序。</li><li>在 Windows 上调用 GNU/Linux 应用程序。</li></ul><h2 id="什么是-WSL-2？"><a href="#什么是-WSL-2？" class="headerlink" title="什么是 WSL 2？"></a>什么是 WSL 2？</h2><p>WSL 2 是适用于 Linux 的 Windows 子系统体系结构的一个新版本，它支持适用于 Linux 的 Windows 子系统在 Windows 上运行 ELF64 Linux 二进制文件。 它的主要目标是 <strong>提高文件系统性能</strong>，以及添加 <strong>完全的系统调用兼容性</strong>。</p><p>这一新的体系结构改变了这些 Linux 二进制文件与Windows 和计算机硬件进行交互的方式，但仍然提供与 WSL 1（当前广泛可用的版本）中相同的用户体验。</p><p>单个 Linux 分发版可以在 WSL 1 或 WSL 2 体系结构中运行。 每个分发版可随时升级或降级，并且你可以并行运行 WSL 1 和 WSL 2 分发版。 WSL 2 使用全新的体系结构，该体系结构受益于运行<strong>真正</strong>的 Linux 内核。</p><h2 id="WSL-2安装"><a href="#WSL-2安装" class="headerlink" title="WSL 2安装"></a>WSL 2安装</h2><p>需要CPU开启VT（Virtualization Technology），这一步根据CPU不同操作方式不同就不细说了。</p><p>需要先启用“适用于 Linux 的 Windows 子系统”可选功能，然后才能在 Windows 上安装 Linux 分发。</p><p>以管理员身份打开 PowerShell 并运行：</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs powershell">dism.exe /online /<span class="hljs-built_in">enable-feature</span> /featurename:Microsoft<span class="hljs-literal">-Windows</span><span class="hljs-literal">-Subsystem</span><span class="hljs-literal">-Linux</span> /all /norestart<br></code></pre></td></tr></table></figure><p>若要更新到 WSL 2，需要运行 Windows 10。</p><ul><li>对于 x64 系统：<strong>版本 1903</strong> 或更高版本，采用 <strong>内部版本 18362</strong> 或更高版本。</li><li>对于 ARM64 系统：<strong>版本 2004</strong> 或更高版本，采用 <strong>内部版本 19041</strong> 或更高版本。</li><li>低于 18362 的版本不支持 WSL 2。 使用 <a href="https://www.microsoft.com/software-download/windows10">Windows Update 助手</a>更新 Windows 版本。</li></ul><p>若要检查 Windows 版本及内部版本号，选择 Windows 徽标键 + R，然后键入“winver”，选择“确定”。 更新到“设置”菜单中的<a href="ms-settings:windowsupdate">最新 Windows 版本</a>。</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211126110320999.png" alt="image-20211126110320999"></p><p>安装 WSL 2 之前，必须启用“虚拟机平台”可选功能。 计算机需要<a href="https://docs.microsoft.com/zh-cn/windows/wsl/troubleshooting#error-0x80370102-the-virtual-machine-could-not-be-started-because-a-required-feature-is-not-installed">虚拟化功能</a>才能使用此功能。</p><p>以管理员身份打开 PowerShell 并运行：</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs powershell">dism.exe /online /<span class="hljs-built_in">enable-feature</span> /featurename:VirtualMachinePlatform /all /norestart<br></code></pre></td></tr></table></figure><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211126110900611.png" alt="image-20211126110900611"></p><p><strong>重新启动</strong> 计算机，以完成 WSL 安装并更新到 WSL 2。</p><p>重启之后<a href="https://wslstorestorage.blob.core.windows.net/wslblob/wsl_update_x64.msi">下载安装</a> Linux 内核更新包并将 WSL 2 设置为默认版本。</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211126111436728.png" alt="image-20211126111436728"></p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs powershell">wsl -<span class="hljs-literal">-set</span><span class="hljs-literal">-default</span><span class="hljs-literal">-version</span> <span class="hljs-number">2</span><br></code></pre></td></tr></table></figure><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211126111557676.png" alt="image-20211126111557676"></p><p>最后，打开 <a href="https://aka.ms/wslstore">Microsoft Store</a>，并选择你偏好的 Linux 分发版。</p><p>单击以下链接会打开每个分发版的 Microsoft Store 页面：</p><ul><li><a href="https://www.microsoft.com/store/apps/9N9TNGVNDL3Q">Ubuntu 18.04 LTS</a></li><li><a href="https://www.microsoft.com/store/apps/9n6svws3rx71">Ubuntu 20.04 LTS</a></li><li><a href="https://www.microsoft.com/store/apps/9NJFZK00FGKV">openSUSE Leap 15.1</a></li><li><a href="https://www.microsoft.com/store/apps/9MZ3D1TRP8T1">SUSE Linux Enterprise Server 12 SP5</a></li><li><a href="https://www.microsoft.com/store/apps/9PN498VPMF3Z">SUSE Linux Enterprise Server 15 SP1</a></li><li><a href="https://www.microsoft.com/store/apps/9PKR34TNCV07">Kali Linux</a></li><li><a href="https://www.microsoft.com/store/apps/9MSVKQC78PK6">Debian GNU/Linux</a></li><li><a href="https://www.microsoft.com/store/apps/9n6gdm4k2hnc">Fedora Remix for WSL</a></li><li><a href="https://www.microsoft.com/store/apps/9NV1GV1PXZ6P">Pengwin</a></li><li><a href="https://www.microsoft.com/store/apps/9N8LP0X93VCP">Pengwin Enterprise</a></li><li><a href="https://www.microsoft.com/store/apps/9p804crf0395">Alpine WSL</a></li><li><a href="https://www.microsoft.com/store/apps/9msmjqd017x7">Raft（免费试用版）</a></li></ul><p>首次启动新安装的 Linux 分发版时，将打开一个控制台窗口，系统会要求你等待一分钟或两分钟，以便文件解压缩并存储到电脑上。 未来的所有启动时间应不到一秒。</p><p>然后，需要为新的 Linux 分发版创建用户帐户和密码。</p><p>另外，以上内容貌似（？）可以使用以下一行命令解决</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">wsl --install -d Ubuntu-20.04<br></code></pre></td></tr></table></figure><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211126142919333.png" alt="image-20211126142919333"></p><p>另外我发现用户名不能用大写，并且密码是不会显示出来的。</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211126143140954.png" alt="image-20211126143140954"></p><h2 id="换源"><a href="#换源" class="headerlink" title="换源"></a>换源</h2><p>如果遇到下载速度较慢，或者下载失败等问题，我们还可以把官方源换成国内源。</p><p><strong>备份list文件</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">cd /etc/apt/<br>sudo cp sources.list sources.list.bak<br></code></pre></td></tr></table></figure><p>备份了，如果下面的哪一步出错了好恢复。</p><p><strong>修改list文件</strong></p><p>管理员权限，使用 vim 进行修改：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo vim sources.list<br></code></pre></td></tr></table></figure><p>把想要更换的源复制在剪切板，这里以阿里源为例：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs shell">deb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse<br>deb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse<br>deb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse<br>deb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse<br>deb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse<br>deb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse<br>deb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse<br>deb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse<br>deb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse<br>deb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse<br></code></pre></td></tr></table></figure><p>我们先通过按<strong>ggdG</strong>这几个字母将里面的内容全部删除，</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plain">通过按下gg后发现光标移动到文件首行了。其中，gg为跳转到文件首行；dG为删除光标所在行以及其下所有行的内容。d为删除，G为跳转到文件末尾行。<br></code></pre></td></tr></table></figure><p>按鼠标右键会进行粘贴，然后输入    :wq!    进行退出与保存。</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211126135607014.png" alt="image-20211126135607014"></p><p>最后复制这两条命令进行更新镜像源列表。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo apt-get update<br>sudo apt-get upgrade<br></code></pre></td></tr></table></figure><h1 id="Windows-terminal-vscode"><a href="#Windows-terminal-vscode" class="headerlink" title="Windows terminal+vscode"></a>Windows terminal+vscode</h1><p><a href="https://www.microsoft.com/en-us/p/windows-terminal/9n0dx20hk701?rtc=1&amp;activetab=pivot:overviewtab">Windows Terminal</a> <a href="https://docs.microsoft.com/en-us/windows/terminal/get-started">https://docs.microsoft.com/en-us/windows/terminal/get-started</a> </p><p>VS Code <a href="https://code.visualstudio.com">https://code.visualstudio.com</a></p><p>这两样软件可以大幅提高效率（还很装逼很好看）</p><p>windows terminal各项设置可以实现许多使用功能，比如我设置了默认启动Ubuntu，添加了git bash，透明亚克力效果等（本次不介绍，感兴趣自行研究）</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211126112846563.png" alt="image-20211126112846563"></p><p>可以在工作区文件夹内右键然后在windows终端打开</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211126112958904.png" alt="image-20211126112958904"></p><p>也可以直接打开window终端，cd到对应文件夹，然后输入</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs powershell">code .<br></code></pre></td></tr></table></figure><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211126113402308.png" alt="image-20211126113402308"></p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211126113507900.png" alt="image-20211126113507900"></p><p>至此就可以实现在windows环境下编程，在linux环境下测试。</p><h1 id="Docker"><a href="#Docker" class="headerlink" title="Docker"></a>Docker</h1><p>使用以下脚本可以实现自动安装。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> install docker</span><br>curl -fsSL get.docker.com -o get-docker.sh<br>sh get-docker.sh<br><br>if [ ! $(getent group docker) ];<br>then <br>    sudo groupadd docker;<br>else<br>    echo &quot;docker user group already exists&quot;<br>fi<br><br>sudo gpasswd -a $USER docker<br>sudo service docker restart<br><br>rm -rf get-docker.sh<br></code></pre></td></tr></table></figure><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211126133921891.png" alt="image-20211126133921891"></p><p>可以直接复制到txt中，然后修改文件类型跟文件名为install-docker.sh（后缀是sh，名字任意）</p><p>例如我在桌面放置了该脚本，右键打开终端之后输入以下命令即可完成安装。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">sh install-docker.sh<br></code></pre></td></tr></table></figure><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211126140126640.png" alt="image-20211126140126640"></p><p>接下来输入：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo service docker start<br>docker version<br></code></pre></td></tr></table></figure><p>就可以启动服务，查看版本，证明我们已经安装成功。</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211126140532065.png" alt="image-20211126140532065"></p><p>如果不希望每次都特地启动docker的服务可以使用以下命令设置自启动：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo systemctl enable docker<br></code></pre></td></tr></table></figure><p>关于docker的使用就大家自己研究啦。</p><h1 id="On-my-zsh"><a href="#On-my-zsh" class="headerlink" title="On-my-zsh"></a>On-my-zsh</h1><p>一个美观且功能强大的终端</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211126145704636.png" alt="image-20211126145704636"></p><p>用windows terminal启动Ubuntu，输入以下命令安装：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo apt-get install -y zsh<br></code></pre></td></tr></table></figure><p>oh-my-zsh中整理了常用的zsh<a href="https://github.com/ohmyzsh/ohmyzsh/tree/master/plugins">扩展</a>和<a href="https://github.com/ohmyzsh/ohmyzsh/wiki/Themes">主题</a>,所以先安装oh-my-zsh</p><p>使用curl安装 :</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">sh -c &quot;$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)&quot;<br></code></pre></td></tr></table></figure><p>使用wget安装：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">sh -c &quot;$(wget https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh -O -)&quot;<br></code></pre></td></tr></table></figure><p>虽然我列出了上面两条命令，但最好还是看看<a href="https://ohmyz.sh/#install">官网</a>的安装页面，确保命令的最新版本。</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211126150308574.png" alt="image-20211126150308574"></p><p>接下来输入：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">cd<br>code .zshrc<br></code></pre></td></tr></table></figure><p>就可以cd到根目录文件夹，用vscode的可视化界面编辑配置文件，添加需要的扩展与主题了。</p><p>扩展与主题根据个人需求添加，本文不再赘述。</p><h1 id="Python开发"><a href="#Python开发" class="headerlink" title="Python开发"></a>Python开发</h1><p>花里胡哨那么多东西了，讲一点实战（假的）。</p><p>默认Ubuntu已经安装了python，我们只需要安装pip，就可以进行一些简单的编程。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo apt-get install python3-pip<br></code></pre></td></tr></table></figure><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211126151809809.png" alt="image-20211126151809809"></p><p>当然我们不能局限于简单的编程，我们需要<strong>创建虚拟环境</strong>，确保各个环境互相隔离，互不影响。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo apt-get install python3-venv<br></code></pre></td></tr></table></figure><p>创建虚拟环境：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo python3 -m venv env<br></code></pre></td></tr></table></figure><p>激活与退出：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">source env/bin/activate<br><br>deactivate<br></code></pre></td></tr></table></figure><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211126181729703.png" alt="image-20211126181729703"></p><p>接下来就可以尽情地pip install了，在工作环境下新建文件夹，再用vscode打开，即可开始编程。</p>]]></content>
    
    
    
    <tags>
      
      <tag>course</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>One-Hot Encoding 独热编码</title>
    <link href="/2021/11/23/onehot/"/>
    <url>/2021/11/23/onehot/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>One-Hot Encoding学习记录</p><span id="more"></span><h1 id="一、One-Hot-Encoding"><a href="#一、One-Hot-Encoding" class="headerlink" title="一、One-Hot Encoding"></a>一、One-Hot Encoding</h1><p>One-Hot 编码，又称为一位有效编码，主要是采用位状态寄存器来对个状态进行编码，每个状态都由他独立的寄存器位，并且在任意时候只有一位有效。<br>在实际的机器学习的应用任务中，特征有时候并不总是连续值，有可能是一些分类值，如性别可分为“ male ”和“ female ”。在机器学习任务中，对于这样的特征，</p><p>One-hot在数位电路中被用来表示一种特殊的位元组合，该字节里，仅容许单一位元为1，其他位元都必须为0。之所以称为one-hot就是因为只能有一个1（hot）。若情况相反，只有一个0，其余为1，则称为one-cold。在机器学习里，也有one-hot向量（英语：one-hot vector）的概念。在一任意维度的向量中，仅有一个维度的值是1，其余为0。譬如向量 ${\displaystyle [0\ 0\ 0\ 0\ 0\ 1\ 0\ 0\ 0\ 0\ 0\ 0\ 0\ 0\ 0]}{\displaystyle [0\ 0\ 0\ 0\ 0\ 1\ 0\ 0\ 0\ 0\ 0\ 0\ 0\ 0\ 0]}$，即为15维空间中的一组one-hot向量。将类别性资料转换成one-hot向量的过程则称one-hot编码（英语：one-hot encoding）。在统计学中，虚拟变数代表了类似的概念。</p><p>One-hot目前并无公认或被广泛使用的中文译名。目前可见的one-hot encoding译名有独热编码以及一位有效编码。</p><p>通常我们需要对其进行特征数字化，如下面的例子，这些特征值并不是连续的，而是离散的，无序的。</p><p>有如下三个特征属性：</p><ul><li>性别：[“male”，”female”]</li><li>地区：[“Europe”，”US”，”Asia”]</li><li>浏览器：[“Firefox”，”Chrome”，”Safari”，”Internet Explorer”]</li></ul><p>对于某一个样本，如[“male”，”US”，”Internet Explorer”]，我们需要将这个分类值的特征数字化，最直接的方法，我们可以采用序列化的方式：[0,1,3]。但是这样的特征处理并不能直接放入机器学习算法中。</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/20190514150006715.jpg" alt="img"></p><h1 id="二、One-Hot-Encoding的处理方法"><a href="#二、One-Hot-Encoding的处理方法" class="headerlink" title="二、One-Hot Encoding的处理方法"></a>二、One-Hot Encoding的处理方法</h1><p>One-Hot 编码是分类变量作为二进制向量的表示。</p><ol><li><p>将分类值映射到整数值。</p></li><li><p>每个整数值被表示为二进制向量，除了整数的索引之外，它都是零值，它被标记为1。</p></li></ol><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211123195029323.png" alt="image-20211123195029323"></p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211123195041636.png" alt="image-20211123195041636"></p><p>对于上述的问题，性别的属性是二维的，同理，地区是三维的，浏览器则是四维的，这样，我们可以采用One-Hot编码的方式对上述的样本“[“male”，”US”，”Internet Explorer”]”编码，“male”则对应着[1，0]，同理“US”对应着[0，1，0]，“Internet Explorer”对应着[0，0，0，1]。</p><p>则完整的特征数字化的结果为：[1,0,0,1,0,0,0,0,1]。这样导致的一个结果就是数据会变得非常的稀疏。</p><h1 id="三、Python代码举例"><a href="#三、Python代码举例" class="headerlink" title="三、Python代码举例"></a>三、Python代码举例</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> preprocessing<br> <br>enc = preprocessing.OneHotEncoder()<br>enc.fit([[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">3</span>],[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>],[<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>],[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">2</span>]])<br> <br>array = enc.transform([[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">3</span>]]).toarray()<br> <br><span class="hljs-built_in">print</span>(array)<br></code></pre></td></tr></table></figure><p>结果：[[ 1.  0.  0.  1.  0.  0.  0.  0.  1.]]</p><h1 id="四、优缺点分析"><a href="#四、优缺点分析" class="headerlink" title="四、优缺点分析"></a>四、优缺点分析</h1><p><strong>优点：</strong></p><p>(1) 解决了分类器不好处理离散数据的问题。</p><pre><code>a. 欧式空间。在回归，分类，聚类等机器学习算法中，特征之间 距离计算 或 相似度计算 是非常重要的，而我们常用的距离或相似度的计算都是在欧式空间的相似度计算，计算余弦相似性，基于的就是欧式空间。b. one-hot 编码。使用 one-hot 编码，将离散特征的取值扩展到了欧式空间，离散特征的某个取值就对应欧式空间的某个点。将离散型特征使用 one-hot 编码，确实会让 特征之间的距离计算更加合理。</code></pre><p>(2) 在一定程度上也起到了扩充特征的作用。</p><p><strong>缺点：</strong></p><p>(1) 它是一个词袋模型，不考虑词与词之间的<strong>顺序</strong>（文本中词的顺序信息也是很重要的）；</p><p>(2) 它<strong>假设词与词相互独立</strong>（在大多数情况下，词与词是相互影响的）；</p><p>(3) 它得到的特征是<strong>离散稀疏</strong>的 （这个问题最严重）。</p><p><strong>与其他编码的差异：</strong></p><ul><li>决定状态机目前状态的时间成本低，因为读取一个正反器的时间成本固定。</li><li>改变机器的状态所需时间成本也是固定，因为每次只需要改变两个正反器的值。</li><li>设计及设计变更容易。</li><li>容易侦测出非法状态。</li><li>可以有效率地使用FPGA的大量正反器。</li><li>相较于其他编码，使用one-hot来实现状态机通常可以达到更高的时脉频率。</li><li>比起其他编码，需要更多的正反器，使得其在PAL装置上不切实际。</li><li>会有很多非法状态存在[7]。这是由于${\displaystyle N}$个正反器构成的计数器总共有${\displaystyle 2^{N}}$个状态（每个正反器可以是0或1，所以总共${\displaystyle 2^{N}}$种可能状态），但是合法状态却只有${\displaystyle N}$个（即同一时间只允许一个正反器是1,其他必须为0），所以总共会有${\displaystyle 2^{N}-N}$个可能的非法状态。</li></ul><h1 id="五、应用"><a href="#五、应用" class="headerlink" title="五、应用"></a>五、应用</h1><p><strong>自然语言处理</strong><br>在自然语言处理中，若有个字典或字库里有${\displaystyle N}$个单字，则每个单字可以被一个${\displaystyle N}$维的one-hot向量代表。譬如若字库里仅有apple（苹果），banana（香蕉），以及pineapple（凤梨）这三个单字，则他们各自的one-hot向量可以为：</p><script type="math/tex; mode=display">{\displaystyle {\begin{array}{ll}apple&=[1\ 0\ 0]\\banana&=[0\ 1\ 0]\\pineapple&=[0\ 0\ 1]\end{array}}}</script><p>由于电脑无法理解非数字类的数据，One-hot编码可以将类别性数据转换成统一的数字格式，方便机器学习的算法进行处理及计算。而转换成固定维度的向量则方便机器学习算法进行线性代数上的计算。另外由于一个one-hot向量中，绝大部分的数字都是0，所以若使用稀疏矩阵的数据结构，则可以节省电脑内存的使用量。</p><p><strong>有限状态机</strong><br>One-hot编码常常被用来表示一个有限状态机的状态。如果使用二进制或格雷码来代表状态，则需要用到解码器才能得知该码代表的状态。使用one-hot来代表状态的话，则不需要解码器，因为若第${\displaystyle n}$个位元为1，就代表机器目前在第${\displaystyle n}$个状态。</p><p>一个有限状态机的例子是由15个状态构成的环状计数器。使用one-hot编码来实现此状态机的话，可以将15个正反器串联在一起，每个正反器的Q输出接到下一个正反器的D输入，而第一个正反器的D输入则是接到第15个的Q输出，形成一个环状。第一个正反器代表机器的第一个状态，第二个正反器代表第二个状态，依此类推。当机器被归零重设时，第一个正反器的值为1，其余为0。当一个时脉边缘抵达正反器时，会将1推进到下一个正反器。依照这种方式，1可一步步推进到第15个正反器，亦即第15个状态，再之后则重新回到第一个状态。</p><p>位址解码器可以将二进制或格雷码转换成one-shot表示形式。而优先编码器则是作用相反。</p><h1 id="六、资料来源"><a href="#六、资料来源" class="headerlink" title="六、资料来源"></a>六、资料来源</h1><ul><li><a href="https://zh.wikipedia.org/wiki/One-hot">https://zh.wikipedia.org/wiki/One-hot</a></li><li><a href="https://blog.csdn.net/google19890102/article/details/44039761">https://blog.csdn.net/google19890102/article/details/44039761</a></li><li><a href="https://blog.csdn.net/qq_15192373/article/details/89552498">https://blog.csdn.net/qq_15192373/article/details/89552498</a></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>course</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>神经网络回归与分类（波士顿房价与红酒分类）</title>
    <link href="/2021/11/16/middle/"/>
    <url>/2021/11/16/middle/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>机器学习期中大作业，神经网络回归与分类</p><span id="more"></span><h1 id="一、描述所使用的神经网络模型"><a href="#一、描述所使用的神经网络模型" class="headerlink" title="一、描述所使用的神经网络模型"></a>一、描述所使用的神经网络模型</h1><h2 id="1-1-神经元模型"><a href="#1-1-神经元模型" class="headerlink" title="1.1 神经元模型"></a>1.1 神经元模型</h2><h3 id="1-1-1-神经元模型的定义"><a href="#1-1-1-神经元模型的定义" class="headerlink" title="1.1.1 神经元模型的定义"></a>1.1.1 神经元模型的定义</h3><p><strong>神经网络是由具有适应性的简单单元组成的广泛并行互联的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互反应。神经网络中最基本的成分是神经元模型，即上述的“简单单元”。</strong></p><h3 id="1-1-2-M-P神经元模型"><a href="#1-1-2-M-P神经元模型" class="headerlink" title="1.1.2 M-P神经元模型"></a>1.1.2 M-P神经元模型</h3><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211114180429830.png" alt="image-20211114180429830"></p><p><strong>输入</strong>：来自其他n个神经元传递过来的输入信号</p><p><strong>处理</strong>：输入信号通过带权重的连接进行传递, 神经元接受到总输入值将其与神经元的阈值进行比较</p><p><strong>输出</strong>：通过激活函数的处理以得到输出</p><h3 id="1-1-3-激活函数"><a href="#1-1-3-激活函数" class="headerlink" title="1.1.3 激活函数"></a>1.1.3 激活函数</h3><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211114180032126.png" alt="image-20211114180032126"></p><h4 id="1-1-3-1-激活函数的介绍"><a href="#1-1-3-1-激活函数的介绍" class="headerlink" title="1.1.3.1 激活函数的介绍"></a>1.1.3.1 激活函数的介绍</h4><p><strong>如下图所示，神经网络中的每个神经元节点接受上一层神经元的输出值作为本神经元的输入值，并将输入值传递给下一层，输入层神经元节点会将输入属性值直接传递给下一层（隐层或输出层）。在多层神经网络中，上层节点的输出和下层节点的输入之间具有一个函数关系，这个函数称为激活函数（又称激励函数）。</strong></p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211113213541281.png" alt="image-20211113213541281"></p><h4 id="1-1-3-2-激活函数的用途"><a href="#1-1-3-2-激活函数的用途" class="headerlink" title="1.1.3.2 激活函数的用途"></a>1.1.3.2 激活函数的用途</h4><p><strong>如果不用激励函数（相当于激励函数是f(x) = x），每一层节点的输入都是上层输出的线性函数，这样无论神经网络有多少层，输出都是输入的线性组合，这种情况就是最原始的感知机，网络的逼近能力就相当有限。当我们引入非线性函数作为激励函数，深层神经网络表达能力就更加强大，不再是输入的线性组合，几乎可以逼近任意函数。</strong></p><h4 id="1-1-3-3-一些常见的激活函数及其性质"><a href="#1-1-3-3-一些常见的激活函数及其性质" class="headerlink" title="1.1.3.3 一些常见的激活函数及其性质"></a>1.1.3.3 一些常见的激活函数及其性质</h4><h5 id="1-1-3-3-1-Relu激活函数"><a href="#1-1-3-3-1-Relu激活函数" class="headerlink" title="1.1.3.3.1 Relu激活函数"></a>1.1.3.3.1 Relu激活函数</h5><p><strong>Relu函数的解析式：</strong></p><script type="math/tex; mode=display">f_{Relu}(x)=max(0,x)</script><p><strong>Relu函数及其导数的图像如下图所示：</strong></p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211113213557078.png" alt="image-20211113213557078"></p><p><strong>优点：</strong></p><p><strong>①计算效率较高</strong></p><p><strong>②兼具线性和非线性特性</strong></p><p><strong>缺点：</strong></p><p><strong>梯度消失问题：在x&lt;0时，神经元保持非激活状态，且在反向传导（backward pass）中梯度为零</strong></p><h5 id="1-1-3-3-2-Sigmoid激活函数"><a href="#1-1-3-3-2-Sigmoid激活函数" class="headerlink" title="1.1.3.3.2 Sigmoid激活函数"></a>1.1.3.3.2 Sigmoid激活函数</h5><p><strong>Sigmoid 函数的解析式：</strong></p><script type="math/tex; mode=display">f_{Sigmoid}(x)=\frac{1}{1+e^{-x}}</script><p><strong>Sigmoid函数及其导数的图像如下图所示：</strong></p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211114180235544.png" alt="image-20211114180235544"></p><p><strong>优点：</strong></p><p><strong>①梯度的“平滑性”</strong></p><p><strong>②输出在“0-1区间”</strong></p><p><strong>缺点：</strong></p><p><strong>①梯度消失问题：神经网络使用 Sigmoid 激活函数进行反向传播时，输出接近0或1的神经元其梯度趋近于0 </strong></p><p><strong>②计算成本问题：涉及指数计算</strong></p><p><strong>③不以零为中心：Sigmoid 输出不以零为中心</strong></p><h5 id="1-1-3-3-3-tanh-激活函数"><a href="#1-1-3-3-3-tanh-激活函数" class="headerlink" title="1.1.3.3.3  tanh 激活函数"></a>1.1.3.3.3  tanh 激活函数</h5><p><strong>tanh函数的解析式：</strong></p><script type="math/tex; mode=display">f_{tanh}(x)=\frac{1-e^{-2x}}{1+e^{-2x}}</script><p><strong>tanh函数及其导数的图像如下图所示：</strong></p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211113213650281.png" alt="image-20211113213650281"></p><p><strong>优点：</strong></p><p><strong>①梯度的“平滑性”</strong></p><p><strong>②输出以零为中心</strong></p><p><strong>缺点：</strong></p><p><strong>①梯度消失问题：神经网络使用 tanh 激活函数进行反向传播时，输出接近-1或1的神经元其梯度趋近于0</strong></p><p><strong>②计算成本问题：涉及指数计算</strong></p><h2 id="1-2-神经网络模型"><a href="#1-2-神经网络模型" class="headerlink" title="1.2 神经网络模型"></a>1.2 神经网络模型</h2><h3 id="1-2-1-神经网络模型的定义"><a href="#1-2-1-神经网络模型的定义" class="headerlink" title="1.2.1 神经网络模型的定义"></a>1.2.1 神经网络模型的定义</h3><p><strong>将若干神经元按一定的层次结构连接起来就得到了神经网络，可将神经网络视为包含了若干参数的数学模型，这个模型是由若干个函数相互（嵌套）代入而得。</strong></p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211113213743041.png" alt="image-20211113213743041"></p><h3 id="1-2-2-多层前馈神经网络"><a href="#1-2-2-多层前馈神经网络" class="headerlink" title="1.2.2 多层前馈神经网络"></a>1.2.2 多层前馈神经网络</h3><h4 id="1-2-2-1-定义"><a href="#1-2-2-1-定义" class="headerlink" title="1.2.2.1 定义"></a>1.2.2.1 定义</h4><p><strong>每层神经元与下一层神经元全互联，神经元之间不存在同层连接也不存在跨层连接。输入层接受外界输入，隐含层与输出层神经元对信号进行加工，最终结果由输出层神经元输出。根据训练数据来调整神经元之间的“连接权”以及每个功能神经元的“阈值”。</strong></p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211113213753140.png" alt="image-20211113213753140"></p><h4 id="1-2-2-2-模型训练"><a href="#1-2-2-2-模型训练" class="headerlink" title="1.2.2.2 模型训练"></a>1.2.2.2 模型训练</h4><p><strong>数据</strong>：</p><script type="math/tex; mode=display">D=\{(x_1,y_1),(x_2,y_2),……,(x_m,y_m)\},x_i∈R^d,y_i∈R^l</script><p><strong>模型</strong>：若干神经元按一定的层次结构连接起来，每层神经元与下一层神经元全互联，神经元之间不存在同层连接也不存在跨层连接，所形成的神经网络模型。</p><p><strong>策略</strong>：</p><p><strong>①平方损失（回归问题）</strong></p><script type="math/tex; mode=display">L(y_i,f(x_i))=(y_i-f(x_i)^2),ERM</script><p><strong> ②交叉熵损失（二分类问题）</strong></p><hr><script type="math/tex; mode=display">L(y_i,f(x_i))=-y_ilog(p(y_i=1|x_i))-(1-y_i)log(p(y_i=0|x_i))</script><p><strong>算法</strong>：误差逆传播算法（error Back Propagation）</p><h1 id="二、描述训练模型所使用的算法"><a href="#二、描述训练模型所使用的算法" class="headerlink" title="二、描述训练模型所使用的算法"></a>二、描述训练模型所使用的算法</h1><h1 id="2-1-误差逆传播算法"><a href="#2-1-误差逆传播算法" class="headerlink" title="2.1 误差逆传播算法"></a>2.1 误差逆传播算法</h1><h3 id="2-1-1-应用领域"><a href="#2-1-1-应用领域" class="headerlink" title="2.1.1 应用领域"></a>2.1.1 应用领域</h3><p><strong>反向传播算法应用较为广泛，从字面意思理解，与前向传播相互对应。在简单的神经网络中，反向传播算法，可以理解为最优化损失函数过程，求解每个参与运算的参数的梯度的方法。在前馈神经网中，反向传播从求解损失函数偏导过程中，步步向前求解每一层的参数梯度。在卷积神经网络中，反向传播可以求解全连接层的参数梯度。在循环神经网络中，反向传播算法可以求解每一个时刻t或者状态t的参数梯度（在RNN\LSTM\GRU中，反向传播更多是BPTT）。如今对于BP的理解，认为是在优化损失函数或者目标函数过程中，求解参与运算的参数的梯度方法，是一种比较普遍的说法。</strong></p><h3 id="2-1-2-网络结构"><a href="#2-1-2-网络结构" class="headerlink" title="2.1.2 网络结构"></a>2.1.2 网络结构</h3><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211113192037294.png" alt="image-20211113192037294"></p><ol><li><strong>正向传播求损失，反向传播回传误差</strong></li><li><strong>根据误差信号修正每层的权重</strong></li><li><strong>f是激活函数；f(netj)是隐层的输出； f(netk）是输出层的输出O; d是target。</strong></li></ol><h3 id="2-1-2-基本参数结构"><a href="#2-1-2-基本参数结构" class="headerlink" title="2.1.2 基本参数结构"></a>2.1.2 基本参数结构</h3><p><strong>为了方便讨论，我们以一个隐层的神经网络结构进行推导。多隐层的神经网络推导思想与此类似，可推广。如下图为一个神经网络结构。</strong></p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211113192136112.png" alt="image-20211113192136112"></p><h4 id="2-1-2-1参数简述"><a href="#2-1-2-1参数简述" class="headerlink" title="2.1.2.1参数简述"></a>2.1.2.1参数简述</h4><script type="math/tex; mode=display">\begin{align}&输入参数：x_1^k,\dots ,x_i^k,\dots,x_d^k\\&输入层到第一隐层第h个神经元的权重：v_{1h},\dots,v_{ih},\dots,v_{dh}\\&第一层第h个神经元输入：\alpha _{h}= \sum_{i=1}^{d}v_{ih}x_i^k\\&第一隐层阙值：\gamma _{1},\dots,\gamma _{h},\dots,\gamma _{q}\\&第一隐层第h个输出：b_h=f_{sigmoid}(\alpha _h-\gamma _h) \\&第一隐层到第j个输出神经元的权重：w_{1j},\dots,w_{hj},\dots,w_{qj}\\&第j个输出神经元的输入：\beta _j=\sum_{h=1}^{q}w_{hj}b_h\\&输出层阙值：\theta _{1},\dots,\theta _{j},\dots,\theta_{l}\\&输出值：y_j^k=f_{sigmoid}(\alpha _h-\gamma _h)\end{align}</script><p><strong>所以前向传播计算误差为：</strong></p><script type="math/tex; mode=display">E_{k}=\frac{1}{2} \sum_{j=1}^{l}\left(y_{j}^{k}-\hat{y}_{j}^{k}\right)^{2}</script><h3 id="2-1-3-参数调整策略"><a href="#2-1-3-参数调整策略" class="headerlink" title="2.1.3 参数调整策略"></a>2.1.3 参数调整策略</h3><p><strong>BP算法的核心思想：使用梯度下降来搜索可能的权向量的假设空间，以找到最佳的拟合样例的权向量。具体而言，即利用损失函数，每次向损失函数负梯度方向移动，直到损失函数取得最小值。或者说，反向传播算法，是根据损失函数，求出损失函数关于每一层的权值及偏置项的偏导数，也称为梯度，用该值更新初始的权值和偏置项，一直更新到损失函数取得最小值或是设置的迭代次数完成为止。以此来计算神经网络中的最佳的参数。</strong></p><script type="math/tex; mode=display">损失函数：E_{k}=\frac{1}{2} \sum_{j=1}^{l}\left(y_{j}^{k}-\hat{y}_{j}^{k}\right)^{2}</script><h4 id="2-1-3-1-计算准备"><a href="#2-1-3-1-计算准备" class="headerlink" title="2.1.3.1 计算准备"></a>2.1.3.1 计算准备</h4><script type="math/tex; mode=display">\frac{\partial E_k}{\partial \hat{y_j^k} }=(y_j^k-\hat{y_j^k})(-1)\\f_{sigmoid}(x)^{(1)}=f_{sigmoid}(x)(1-f_{sigmoid}(x))\\\eta:学习率</script><p><strong>下面，我们讲分别讨论每个参数的更新：</strong></p><h4 id="2-1-3-2-w更新"><a href="#2-1-3-2-w更新" class="headerlink" title="2.1.3.2 w更新"></a>2.1.3.2 <strong>w</strong>更新</h4><script type="math/tex; mode=display">更新公式：w_{hj}=w_{hj}+\bigtriangleup w_{hj}</script><p>下面对<strong>w</strong>进行讨论:</p><script type="math/tex; mode=display">\begin{align}\bigtriangleup w_{hj}=&-\eta  \frac{\partial E_k}{\partial w_{hj}}\\=&-\eta (\frac{\partial E_k}{\partial \hat{y_j^k} }\cdot\frac{\partial \hat{y_j^k}}{\partial \beta _j}  \cdot \frac{\partial \beta _j}{\partial w_{hj}}  )\\=&-\eta (y_j^k-\hat{y_j^k})(-1)\cdot \hat{y_j^k} (1-\hat{y_j^k})\cdot b_h\\令g_j&=(y_j^k-\hat{y_j^k})\cdot \hat{y_j^k} (1-\hat{y_j^k})\\\\最终可得&\bigtriangleup w_{hj}=\eta g_jb_h\end{align}</script><h4 id="2-1-3-3-θ更新"><a href="#2-1-3-3-θ更新" class="headerlink" title="2.1.3.3 θ更新"></a>2.1.3.3 <strong>θ</strong>更新</h4><script type="math/tex; mode=display">更新公式：\theta_{j}=\theta_{j}+\bigtriangleup \theta_{j}</script><p><strong>下面对</strong>θ<strong>进行讨论：</strong></p><script type="math/tex; mode=display">\begin{align}\bigtriangleup \theta_j=&-\eta  \frac{\partial E_k}{\partial \theta_{j}}\\=&-\eta  (\frac{\partial E_k}{\partial \hat{y_j^k} }\cdot\frac{\partial \hat{y_j^k}}{\partial \theta _j}    )\\=&-\eta (y_j^k-\hat{y_j^k})(-1)\cdot \hat{y_j^k} (1-\hat{y_j^k})\cdot(-1)\\=&-\eta g_j\end{align}</script><h4 id="2-1-3-4-v更新"><a href="#2-1-3-4-v更新" class="headerlink" title="2.1.3.4 v更新"></a>2.1.3.4 <strong>v</strong>更新</h4><script type="math/tex; mode=display">更新公式：v_{ih}=v_{ih}+\bigtriangleup v_{ih}</script><p><strong>下面对</strong>v<strong>进行讨论：</strong></p><script type="math/tex; mode=display">\begin{align}\bigtriangleup v_{ih}=&-\eta \frac{\partial E_k}{\partial v_{ih}}\\ =&-\eta\frac{\partial E_k}{\partial b_h}\cdot  \frac{\partial b_h}{\partial \alpha _h}\cdot  \frac{\partial \alpha _h}{\partial v_{ih}} \\=&-\eta(\sum_{j=1}^{l}\frac{\partial E_k}{\partial \hat{y_j^k} }\cdot\frac{\partial \hat{y_j^k}}{\partial \beta _j}  \cdot \frac{\partial \beta _j}{\partial b_{h}})\cdot b_h(1-b_h)\cdot x_i^k\\=&\eta \sum_{j=1}^{l}g_jw_{hj}b_h(1-b_h)x_i^k \end{align}</script><h4 id="2-1-3-5γ更新"><a href="#2-1-3-5γ更新" class="headerlink" title="2.1.3.5γ更新"></a>2.1.3.5γ更新</h4><script type="math/tex; mode=display">更新公式：\gamma_{h}=\gamma_{h}+\bigtriangleup \gamma_{h}</script><p><strong>下面对</strong>γ<strong>进行讨论：</strong></p><script type="math/tex; mode=display">\begin{align}\bigtriangleup \gamma_{h}=&-\eta \frac{\partial E_k}{\partial \gamma_{h}}\\ =&-\eta\frac{\partial E_k}{\partial b_h}\cdot  \frac{\partial b_h}{\partial \gamma _h} \\=&-\eta(\sum_{j=1}^{l}\frac{\partial E_k}{\partial \hat{y_j^k} }\cdot\frac{\partial \hat{y_j^k}}{\partial \beta _j}  \cdot \frac{\partial \beta _j}{\partial b_{h}})\cdot b_h(1-b_h)\cdot (-1)\\=&\eta \sum_{j=1}^{l}g_jw_{hj}b_h(1-b_h)(-1)\end{align}</script><h3 id="2-1-3-算法伪代码"><a href="#2-1-3-算法伪代码" class="headerlink" title="2.1.3 算法伪代码"></a>2.1.3 算法伪代码</h3><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211114180628211.png" alt="image-20211114180628211"></p><h1 id="三、描述模型超参数确定的过程，分析模型训练结果"><a href="#三、描述模型超参数确定的过程，分析模型训练结果" class="headerlink" title="三、描述模型超参数确定的过程，分析模型训练结果"></a>三、描述模型超参数确定的过程，分析模型训练结果</h1><h2 id="3-1-超参数的概念"><a href="#3-1-超参数的概念" class="headerlink" title="3.1 超参数的概念"></a>3.1 超参数的概念</h2><p>大部分机器学习算法都需要花费大量时间去训练，而在训练之前需要提前配置一些变量。这些变量对训练结果影响很大，但没有对任何数据集都适用的一组变量，需要根据具体应用具体配置，这些需要配置的变量称之为超参数（hyperparameters）。区分超参数和模型参数最大的一点就是是否通过数据来进行调整，模型参数通常是有数据来驱动调整，超参数则不需要数据来驱动，而是在训练前或者训练中人为的进行调整的参数。例如卷积核的具体核参数就是指模型参数，这是由数据驱动的。而学习率则是人为来进行调整的超参数。这里需要注意的是，通常情况下卷积核数量、卷积核尺寸这些也是超参数，注意与卷积核的核参数区分。</p><h2 id="3-2-神经网络包含的超参数"><a href="#3-2-神经网络包含的超参数" class="headerlink" title="3.2 神经网络包含的超参数"></a>3.2 神经网络包含的超参数</h2><h3 id="3-2-1-超参数种类"><a href="#3-2-1-超参数种类" class="headerlink" title="3.2.1 超参数种类"></a>3.2.1 超参数种类</h3><p>通常可以将超参数分为三类：网络参数、优化参数、正则化参数。</p><p>网络参数：可指网络层与层之间的交互方式（相加、相乘或者串接等）、卷积核数量和卷积核尺寸、网络层数（也称深度）和激活函数等。</p><p>优化参数：一般指学习率（learning rate）、批样本数量（batch size）、不同优化器的参数以及部分损失函数的可调参数。</p><p>正则化：权重衰减系数，丢弃法比率（dropout）</p><p><strong>神经网络包含的超参数具体为以下十一个：</strong></p><ol><li><strong>学习率 η</strong></li><li><strong>正则化参数 λ</strong></li><li><strong>神经网络的层数 L</strong></li><li><strong>每一个隐层中神经元的个数 j</strong></li><li><strong>学习的回合数Epoch</strong></li><li><strong>小批量数据 minibatch 的大小</strong></li><li><strong>输出神经元的编码方式</strong></li><li><strong>代价函数的选择</strong></li><li><strong>权重初始化的方法</strong></li><li><strong>神经元激活函数的种类</strong></li><li><strong>参加训练模型数据的规模</strong></li></ol><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211112080119294.png" alt="image-20211112080119294"></p><p>在上图中可以看到超参数 2，3，4， 7 主要影响的时神经网络的分类正确率；9 主要影响代价函数曲线下降速度，同时有时也会影响正确率；1，8，10 主要影响学习速度，这点主要体现在训练数据代价函数曲线的下降速度上；5，6，11 主要影响模型分类正确率和训练用总体时间。这上面所提到的时某个超参数对于神经网络想到的首要影响，并不代表着该超参数只影响学习速度或者正确率。</p><h3 id="3-2-2-超参数重要性顺序"><a href="#3-2-2-超参数重要性顺序" class="headerlink" title="3.2.2 超参数重要性顺序"></a>3.2.2 超参数重要性顺序</h3><ul><li>首先， 学习率，损失函数上的可调参数。在网络参数、优化参数、正则化参数中最重要的超参数可能就是学习率了。学习率直接控制着训练中网络梯度更新的量级，直接影响着模型的有效容限能力；损失函数上的可调参数，这些参数通常情况下需要结合实际的损失函数来调整，大部分情况下这些参数也能很直接的影响到模型的的有效容限能力。这些损失一般可分成三类，第一类辅助损失结合常见的损失函数，起到辅助优化特征表达的作用。例如度量学习中的Center loss，通常结合交叉熵损失伴随一个权重完成一些特定的任务。这种情况下一般建议辅助损失值不高于或者不低于交叉熵损失值的两个数量级；第二类，多任务模型的多个损失函数，每个损失函数之间或独立或相关，用于各自任务，这种情况取决于任务之间本身的相关性，目前笔者并没有一个普适的经验由于提供参考；第三类，独立损失函数，这类损失通常会在特定的任务有显著性的效果。例如RetinaNet中的focal loss，其中的参数γ，α，对最终的效果会产生较大的影响。这类损失通常论文中会给出特定的建议值。</li><li>其次，批样本数量，动量优化器（Gradient Descent with Momentum）的动量参数β。批样本决定了数量梯度下降的方向。过小的批数量，极端情况下，例如batch size为1，即每个样本都去修正一次梯度方向，样本之间的差异越大越难以收敛。若网络中存在批归一化（batchnorm），batch size过小则更难以收敛，甚至垮掉。这是因为数据样本越少，统计量越不具有代表性，噪声也相应的增加。而过大的batch size，会使得梯度方向基本稳定，容易陷入局部最优解，降低精度。一般参考范围会取在[1:1024]之间，当然这个不是绝对的，需要结合具体场景和样本情况；动量衰减参数β是计算梯度的指数加权平均数，并利用该值来更新参数，设置为 0.9 是一个常见且效果不错的选择；</li><li>最后，Adam优化器的超参数、权重衰减系数、丢弃法比率（dropout）和网络参数。在这里说明下，这些参数重要性放在最后并不等价于这些参数不重要。而是表示这些参数在大部分实践中不建议过多尝试，例如Adam优化器中的β1，β2，ϵ，常设为 0.9、0.999、10−8就会有不错的表现。权重衰减系数通常会有个建议值，例如0.0005 ，使用建议值即可，不必过多尝试。dropout通常会在全连接层之间使用防止过拟合，建议比率控制在[0.2,0.5]之间。使用dropout时需要特别注意两点：一、在RNN中，如果直接放在memory cell中,循环会放大噪声，扰乱学习。一般会建议放在输入和输出层；二、不建议dropout后直接跟上batchnorm，dropout很可能影响batchnorm计算统计量，导致方差偏移，这种情况下会使得推理阶段出现模型完全垮掉的极端情况；网络参数通常也属于超参数的范围内，通常情况下增加网络层数能增加模型的容限能力，但模型真正有效的容限能力还和样本数量和质量、层之间的关系等有关，所以一般情况下会选择先固定网络层数，调优到一定阶段或者有大量的硬件资源支持可以在网络深度上进行进一步调整。</li></ul><h2 id="3-3-模型超参数确定"><a href="#3-3-模型超参数确定" class="headerlink" title="3.3 模型超参数确定"></a>3.3 模型超参数确定</h2><h3 id="3-3-1-超参数调优的原因"><a href="#3-3-1-超参数调优的原因" class="headerlink" title="3.3.1 超参数调优的原因"></a>3.3.1 超参数调优的原因</h3><p>本质上，这是模型优化寻找最优解和正则项之间的关系。网络模型优化调整的目的是为了寻找到全局最优解（或者相比更好的局部最优解），而正则项又希望模型尽量拟合到最优。两者通常情况下，存在一定的对立，但两者的目标是一致的，即最小化期望风险。模型优化希望最小化经验风险，而容易陷入过拟合，正则项用来约束模型复杂度。所以如何平衡两者之间的关系，得到最优或者较优的解就是超参数调整优化的目的。</p><h3 id="3-3-2-模型超参数的确定"><a href="#3-3-2-模型超参数的确定" class="headerlink" title="3.3.2 模型超参数的确定"></a>3.3.2 模型超参数的确定</h3><p><strong>四种主流超参数调优技术：</strong></p><ol><li><strong>传统或手动调参</strong></li><li><strong>网格搜索</strong></li><li><strong>随机搜索</strong></li><li><strong>贝叶斯搜索</strong></li></ol><p>在传统的调优中，我们通过手动检查随机超参数集来训练算法，并选择最适合我们目标的参数集。但这种方法不能保证得到最佳的参数组合，反复试验会消耗更多的时间。</p><h4 id="3-3-2-1-网格搜索"><a href="#3-3-2-1-网格搜索" class="headerlink" title="3.3.2.1 网格搜索"></a>3.3.2.1 网格搜索</h4><ul><li>网格搜索是一种基本的超参数调整技术。它类似于手动调优，为网格中指定的所有给定超参数值的每个排列建立模型，并评估和选择最佳模型。由于它尝试每一种超参数组合，并根据交叉验证分数选择最佳组合，这使得 GridsearchCV 极其缓慢。</li><li>这种启发式的搜索算法对超参数搜索算法，被称之为网格搜索。(如果人工处理所有可能的超参数组合，通常的办法是，根据超参数的维度，列成相应的表格，比如说k的取值有[2，3，4，5，6，7，8]，另一个系数比如λ取值有[0.01,0.03,0.1,0.3]等，这样就可以列出一个二维表格，组合出7*4种可能性的超参数组合，再对每一个格子中具体的超参数组合，通过交叉验证的方式进行模型性能的评估，然后通过验证性能的比较，最终筛选出最佳的超参数数据组合)</li><li>网格搜索采用交叉验证的方法，来寻找更好的超参数组合的过程非常耗时，由于各个新模型在执行交叉验证的过程中是相互独立的，那么我们可以充分利用多核处理器甚至是分布式的计算资源来从事并行搜索，从而成倍的节省运算时间。<img src="https://gitee.com/sysu_20354027/pic/raw/master/img/482493cc74974f85ba09b50697801c27.jpeg" alt="img"></li></ul><h4 id="3-3-2-2-随机搜索"><a href="#3-3-2-2-随机搜索" class="headerlink" title="3.3.2.2 随机搜索"></a>3.3.2.2 随机搜索</h4><p>使用随机搜索代替网格搜索的动机是，在许多情况下，所有的超参数可能并非同等重要。随机搜索从超参数空间中随机选择参数组合，参数按 n_iter 给定的迭代次数进行选择。随机搜索已经被实践证明比网格搜索得到的结果更好，但随机搜索的问题是它不能保证给出最佳的参数组合。</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211114180715729.png" alt="image-20211114180715729"></p><h4 id="3-3-2-3-贝叶斯优化"><a href="#3-3-2-3-贝叶斯优化" class="headerlink" title="3.3.2.3 贝叶斯优化"></a>3.3.2.3 贝叶斯优化</h4><p>贝叶斯优化属于一类被称为<strong><em>sequential model-based optimization</em>(SMBO)</strong>的优化算法。这些算法使用先前对损失 f 的观测，来确定下一个(最佳)点来取样 f。该算法大致可以概括如下：</p><ol><li><strong>使用先前计算过的点 X1: n，计算损失 f 的后验期望值。</strong></li><li><strong>在一个新的点 Xnew取样损失 f ，它最大化了 f 的期望的某些效用函数。该函数指定 f 域的哪些区域是最适合采样的。</strong></li></ol><p>重复这些步骤，直到达到某种收敛准则。</p><h5 id="3-3-2-3-1-高斯过程"><a href="#3-3-2-3-1-高斯过程" class="headerlink" title="3.3.2.3.1 高斯过程"></a>3.3.2.3.1 高斯过程</h5><p>在贝叶斯调参过程中，假设一组超参数组合是X=x1,x2,…,xn(xn表示某一个超参数的值)，而这组超参数与最后我们需要优化的损失函数存在一个函数关系，最终的评估结果为Y，通过什么样的X可以取得最优的Y，我们假设是f(X)， Y=F(X)</p><p>而目前机器学习其实是一个黑盒子(black box),即我们只知道input和output，所以上面的函数f(x)很难确定。所以我们需要将注意力转移到一个我们可以解决的函数上去。</p><p>于是可以假设这个寻找最优化参数的过程是一个高斯过程。高斯过程有个特点，就是当随机遍历一定的数据点并拿到结果之后，可以大致绘制出整个数据的分布曲线。</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/998084-20180726204924171-1721363009.png" alt="img"></p><h5 id="3-3-2-3-2-贝叶斯优化理论"><a href="#3-3-2-3-2-贝叶斯优化理论" class="headerlink" title="3.3.2.3.2 贝叶斯优化理论"></a>3.3.2.3.2 贝叶斯优化理论</h5><p>还是这张图，把横轴看作是参数组合X，纵轴看作是这个参数的结果Y。可以通过已经构建的曲线，找到曲线上升的方向，从而在这个方向上继续探索，这样就可以大概率拿到更好的结果。在生活的轨迹上，如果找到一条明确通往幸福的路，可以继续向前探索，因为大概率可以成功，但也许也有会错过更好的机会，陷入局部最优解。请看上图中的五角星，如果我们处于它的位置，继续向上走会迎来一个高峰，但是如果后退，在下降一段时间之后可能会迎来更高的波峰，你该如何选择。</p><p>于是，在参数的探索中要掌握一个平衡：</p><p>开发：在明确的曲线上扬方向继续走，大概率获得更好的结果，但是容易陷入局部最优。</p><p>探索：除了在曲线上扬的方向，在其它的区域也不忘寻找</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211114102651780.png" alt="image-20211114102651780"></p><h2 id="3-4-结果分析"><a href="#3-4-结果分析" class="headerlink" title="3.4 结果分析"></a>3.4 结果分析</h2><h3 id="3-4-1-MAPE"><a href="#3-4-1-MAPE" class="headerlink" title="3.4.1 MAPE"></a>3.4.1 MAPE</h3><p><strong>平均绝对百分比误差（Mean Absolute Percentage Error）</strong></p><script type="math/tex; mode=display">M A P E=\frac{100 \%}{n} \sum_{i=1}^{n}\left|\frac{\hat{y}_{i}-y_{i}}{y_{i}}\right|</script><p>范围[0,+∞)，MAPE 为0%表示完美模型，MAPE 大于 100 %则表示劣质模型。</p><p>注意：当真实值有数据等于0时，存在分母0除问题，该公式不可用！</p><h3 id="3-4-2-调参前结果及分析"><a href="#3-4-2-调参前结果及分析" class="headerlink" title="3.4.2 调参前结果及分析"></a>3.4.2 调参前结果及分析</h3><h4 id="3-4-2-1-代码"><a href="#3-4-2-1-代码" class="headerlink" title="3.4.2.1 代码"></a>3.4.2.1 代码</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs routeros">mlp = MLPRegressor(hidden_layer_sizes=(100), <span class="hljs-attribute">activation</span>=<span class="hljs-string">&#x27;relu&#x27;</span>, <span class="hljs-attribute">solver</span>=<span class="hljs-string">&#x27;adam&#x27;</span>, <span class="hljs-attribute">alpha</span>=0.0001, <span class="hljs-attribute">batch_size</span>=<span class="hljs-string">&#x27;auto&#x27;</span>, <span class="hljs-attribute">learning_rate</span>=<span class="hljs-string">&#x27;constant&#x27;</span>, <span class="hljs-attribute">learning_rate_init</span>=0.001, <span class="hljs-attribute">power_t</span>=0.5, <span class="hljs-attribute">max_iter</span>=200, <span class="hljs-attribute">shuffle</span>=<span class="hljs-literal">True</span>, <span class="hljs-attribute">random_state</span>=None, <span class="hljs-attribute">tol</span>=0.0001, <span class="hljs-attribute">verbose</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">warm_start</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">momentum</span>=0.9, <span class="hljs-attribute">nesterovs_momentum</span>=<span class="hljs-literal">True</span>, <span class="hljs-attribute">early_stopping</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">validation_fraction</span>=0.1, <span class="hljs-attribute">beta_1</span>=0.9, <span class="hljs-attribute">beta_2</span>=0.999, <span class="hljs-attribute">epsilon</span>=1e-08, <span class="hljs-attribute">n_iter_no_change</span>=10, <span class="hljs-attribute">max_fun</span>=15000) #所有参数默认<br>mlp.fit(X_std, Y)<br>MAPE = -1<span class="hljs-number">*c</span>ross_val_score(mlp, X_std, Y, <span class="hljs-attribute">cv</span>=rkf,scoring=&#x27;neg_mean_absolute_percentage_error&#x27;).mean()<br><span class="hljs-builtin-name">print</span>(<span class="hljs-string">&#x27;MAPE:&#x27;</span>,MAPE)<br></code></pre></td></tr></table></figure><h4 id="3-4-2-2-结果及分析"><a href="#3-4-2-2-结果及分析" class="headerlink" title="3.4.2.2 结果及分析"></a>3.4.2.2 结果及分析</h4><p>首先我们使用<a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.neural_network"><code>sklearn.neural_network</code></a>.<a href="https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html?highlight=mlp#sklearn.neural_network.MLPRegressor">MLPRegressor</a>中的所有默认参数设置来训练模型，五次五折交叉验证的平均MAPE为：0.2036462204885882</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211114171851292.png" alt="image-20211114171851292"></p><p>Fig.1  调参前残差图</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211114171902074.png" alt="image-20211114171902074"></p><p>Fig.2  调参前预测误差图</p><p>残差是因变量未被自变量解释的部分，线性模型要求残差服从独立同分布，且分布类型为正态分布。通过一系列方法判断残差是否符合这一要求，可以达到检验模型是否符合相应假设的目的。从上图可以看出，我们的训练集和测试集的R^2^在0.75左右，说明我们的模型训练结果具有一定的可信度，但并不理想。下面我们进行调参，尝试提高准确率。</p><h3 id="3-4-3-网格搜索调参"><a href="#3-4-3-网格搜索调参" class="headerlink" title="3.4.3 网格搜索调参"></a>3.4.3 网格搜索调参</h3><p>接下来我们手动调试模型，将各个超参数逐一修改并查看MAPE的变化结果，最终得出’<strong>hidden_layer_sizes</strong>‘，’<strong>activation</strong>‘，’<strong>solver</strong>‘，’<strong>alpha</strong>‘，’<strong>learning_rate</strong>‘</p><p>这五个对结果影响较大的参数。</p><p>最后我们利用GridSearchCV结合一些“经验结论”来搜索出最优的超参数。</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211114222524771.png" alt="image-20211114222524771"></p><h4 id="3-4-3-1-代码"><a href="#3-4-3-1-代码" class="headerlink" title="3.4.3.1 代码"></a>3.4.3.1 代码</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># 超参数调优</span><br><span class="hljs-keyword">from</span> sklearn.model_selection import GridSearchCV<br>parameters = &#123;<span class="hljs-string">&#x27;hidden_layer_sizes&#x27;</span>: [(10,10,10,10,10),(20,20,20,20,20),(30,30,30,30,30),(40,40,40,40,40),(50,50,50,50,50),(60,60,60,60,60),(70,70,70,70,70),(80,80,80,80,80),(90,90,90,90,90),(100,100,100,100,100)],<br>                <span class="hljs-string">&#x27;activation&#x27;</span>: [<span class="hljs-string">&#x27;identity&#x27;</span>, <span class="hljs-string">&#x27;logistic&#x27;</span>,<span class="hljs-string">&#x27;tanh&#x27;</span>, <span class="hljs-string">&#x27;relu&#x27;</span>],<br>                <span class="hljs-string">&#x27;solver&#x27;</span>: [<span class="hljs-string">&#x27;adam&#x27;</span>,<span class="hljs-string">&#x27;lbgfs&#x27;</span>,<span class="hljs-string">&#x27;sgd&#x27;</span>],<br>                <span class="hljs-string">&#x27;alpha&#x27;</span>: [0.0001, 0.001, 0.01, 0.1, 1,10,100],<br>                <span class="hljs-string">&#x27;learning_rate&#x27;</span>: [<span class="hljs-string">&#x27;constant&#x27;</span>, <span class="hljs-string">&#x27;invscaling&#x27;</span>, <span class="hljs-string">&#x27;adaptive&#x27;</span>]&#125;<br>grid = GridSearchCV(mlp, parameters, <span class="hljs-attribute">cv</span>=rkf, <span class="hljs-attribute">scoring</span>=<span class="hljs-string">&#x27;neg_mean_absolute_percentage_error&#x27;</span>,n_jobs=-1)<br>grid.fit(X_std, Y)<br><span class="hljs-builtin-name">print</span>(<span class="hljs-string">&#x27;最优参数：&#x27;</span>,grid.best_params_)<br><span class="hljs-builtin-name">print</span>(<span class="hljs-string">&#x27;最优模型得分：&#x27;</span>,grid.best_score_)<br></code></pre></td></tr></table></figure><h4 id="3-4-3-2-结果及分析"><a href="#3-4-3-2-结果及分析" class="headerlink" title="3.4.3.2 结果及分析"></a>3.4.3.2 结果及分析</h4><ul><li><strong>最优参数： {‘activation’: ‘relu’, ‘alpha’: 1, ‘hidden_layer_sizes’: (100, 100, 100, 100, 100), ‘learning_rate’: ‘adaptive’, ‘solver’: ‘sgd’} </strong></li><li><strong>最优模型得分： 0.10669120458358475</strong></li></ul><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211114221915446.png" alt="image-20211114221915446"></p><p>Fig.3  调参后残差图</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211114221934910.png" alt="image-20211114221934910"></p><p>Fig.4  调参后预测误差图</p><p>经过网格搜索最优参数后，我们的模型得到大幅度提升。从上可见，我们的训练和测试的R^2^均在0.98以上，说明模型对训练集的拟合效果和泛化能力都很强。</p><h1 id="四、总结模型训练过程中的收获"><a href="#四、总结模型训练过程中的收获" class="headerlink" title="四、总结模型训练过程中的收获"></a>四、总结模型训练过程中的收获</h1><h2 id="4-1-神经网络的训练过程"><a href="#4-1-神经网络的训练过程" class="headerlink" title="4.1 神经网络的训练过程"></a>4.1 神经网络的训练过程</h2><p>简单的神经网络的训练过程包括以下几个步骤：</p><ol><li><strong>定义一个包含多个可学习参数（权重）的神经网络；</strong></li><li><strong>对输入的数据集进行迭代计算；</strong></li><li><strong>通过多层网络结构来处理输入数据；</strong></li><li><strong>计算损失值（输出值与目标值的差值）；</strong></li><li><strong>反向传播梯度到神经网络的参数中；</strong></li><li><strong>根据更新规则来更新网络中的权重值。</strong></li></ol><h2 id="4-2-确定超参数"><a href="#4-2-确定超参数" class="headerlink" title="4.2 确定超参数"></a>4.2 确定超参数</h2><p>其中，如何定义一个包含多个可学习参数的神经网络（即如何确定模型的超参数）是重点，会影响神经网络学习速度和最后结果。我们确定超参数的步骤如下：</p><p>①我们根据经验结论手动调试模型，将各个超参数逐一修改并查看MAPE的变化结果，最终得出’<strong>hidden_layer_sizes</strong>‘，’<strong>activation</strong>‘，’<strong>solver</strong>‘，’<strong>alpha</strong>‘，</p><p>‘<strong>learning_rate</strong>‘这五个对结果影响较大的参数。</p><p>②搜集“经验总结”的资料后，我们用网格搜索法对下列超参数进行排列组合，得到10<strong>*</strong>4<strong>*</strong>3<strong>*</strong>7<strong>*</strong>3=2520种超参数的排列组合方式。</p><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs gcode">parameters = &#123;<span class="hljs-string">&#x27;hidden_layer_sizes&#x27;</span>: [<span class="hljs-comment">(10,10,10,10,10)</span>,<span class="hljs-comment">(20,20,20,20,20)</span>,<span class="hljs-comment">(30,30,30,30,30)</span>,<span class="hljs-comment">(40,40,40,40,40)</span>,<span class="hljs-comment">(50,50,50,50,50)</span>,<span class="hljs-comment">(60,60,60,60,60)</span>,<span class="hljs-comment">(70,70,70,70,70)</span>,<span class="hljs-comment">(80,80,80,80,80)</span>,<span class="hljs-comment">(90,90,90,90,90)</span>,<span class="hljs-comment">(100,100,100,100,100)</span>],<br>                <span class="hljs-string">&#x27;activation&#x27;</span>: [<span class="hljs-string">&#x27;identity&#x27;</span>, <span class="hljs-string">&#x27;logistic&#x27;</span>,<span class="hljs-string">&#x27;tanh&#x27;</span>, <span class="hljs-string">&#x27;relu&#x27;</span>],<br>                <span class="hljs-string">&#x27;solver&#x27;</span>: [<span class="hljs-string">&#x27;adam&#x27;</span>,<span class="hljs-string">&#x27;lbgfs&#x27;</span>,<span class="hljs-string">&#x27;sgd&#x27;</span>],<br>                <span class="hljs-string">&#x27;alpha&#x27;</span>: [<span class="hljs-number">0.0001</span>, <span class="hljs-number">0.001</span>, <span class="hljs-number">0.01</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">1</span>,<span class="hljs-number">10</span>,<span class="hljs-number">100</span>],<br>                <span class="hljs-string">&#x27;learning_rate&#x27;</span>: [<span class="hljs-string">&#x27;constant&#x27;</span>, <span class="hljs-string">&#x27;invscaling&#x27;</span>, <span class="hljs-string">&#x27;adaptive&#x27;</span>]&#125;<br></code></pre></td></tr></table></figure><p>③使用五次五折将数据划分为25份，把上述2520种超参数的组合都跑一遍数据（计算神经网络中的最佳的参数用的是误差逆传播算法），每一个组合都会得到25个MAPE值，取平均；之后2520份MAPE中的最小值对应的超参数组合即为我们选定的最优超参数组合。</p><h2 id="4-3-防止过拟合的方法"><a href="#4-3-防止过拟合的方法" class="headerlink" title="4.3 防止过拟合的方法"></a>4.3 防止过拟合的方法</h2><p>在机器学习模型（特别是深度学习模型）的训练过程中，模型是非常容易过拟合的。深度学习模型在不断的训练过程中训练误差会逐渐降低，但测试误差的走势则不一定。</p><p>①正则化方法。正则化方法包括L0正则、L1正则和L2正则，而正则一般是在目标函数之后加上对于的范数。但是在机器学习中一般使用L2正则。</p><p>②数据增强（Data augmentation），增大数据的训练量；还有一个原因就是我们用于训练的数据量太小导致的，训练数据占总数据的比例过小。</p><p>③重新清洗数据，导致过拟合的一个原因也有可能是数据不纯导致的，如果出现了过拟合就需要我们重新清洗数据。</p><p>④提前终止法（Early stopping），对模型进行训练的过程即是对模型的参数进行学习更新的过程，这个参数学习的过程往往会用到一些迭代方法，如梯度下降（Gradient descent）学习算法。提前终止法便是一种迭代次数截断的方法来防止过拟合的方法，即在模型对训练数据集迭代收敛之前停止迭代来防止过拟合。</p><p>⑤丢弃法（Dropout）。这个方法在神经网络里面很常用。丢弃法是ImageNet中提出的一种方法，通俗一点讲就是丢弃法在训练的时候让神经元以一定的概率不工作。</p><p>以下内容出自<strong>复旦大学邱锡鹏教授著作</strong>《神经网络与深度学习》</p><h1 id="一、描述所使用的神经网络模型-1"><a href="#一、描述所使用的神经网络模型-1" class="headerlink" title="一、描述所使用的神经网络模型"></a>一、描述所使用的神经网络模型</h1><h2 id="1-1-人脑神经网络"><a href="#1-1-人脑神经网络" class="headerlink" title="1.1 人脑神经网络"></a>1.1 人脑神经网络</h2><p>人类大脑是人体最复杂的器官，由神经元、神经胶质细胞、神经干细胞和血管组成．其中，神经元（Neuron），也叫神经细胞（NerveCell），是携带和传输信息的细胞，是人脑神经系统中最基本的单元．人脑神经系统是一个非常复杂的组织，包含近860亿个神经元，每个神经元有上千个突触和其他神经元相连接．这些神经元和它们之间的连接形成巨大的复杂网络，其中神经连接的总长度可达数千公里．我们人造的复杂网络，比如全球的计算机网络，和大脑神经网络相比要“简单”得多．</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211119093130413.png" alt="image-20211119093130413"></p><h2 id="1-2-人工神经网络"><a href="#1-2-人工神经网络" class="headerlink" title="1.2 人工神经网络"></a>1.2 人工神经网络</h2><p>人工神经网络是为模拟人脑神经网络而设计的一种计算模型， 它从结构、实现机理和功能上模拟人脑神经网络． 人工神经网络与生物神经元类似， 由多个节点（ 人工神经元） 互相连接而成， 可以用来对数据之间的复杂关系进行建模． 不同节点之间的连接被赋予了不同的权重， 每个权重代表了一个节点对另一个节点的影响大小． 每个节点代表一种特定函数， 来自其他节点的信息经过其相应的权重综合计算， 输入到一个激活函数中并得到一个新的活性值（ 兴奋或抑制）．从系统观点看， 人工神经元网络是由大量神经元通过极其丰富和完善的连接而构成的自适应非线性动态系统．<br>虽然我们可以比较容易地构造一个人工神经网络， 但是如何让人工神经网络具有学习能力并不是一件容易的事情． 早期的神经网络模型并不具备学习能力． 首个可学习的人工神经网络是赫布网络， 采用一种基于赫布规则的无监督学习方法． 感知器是最早的具有机器学习思想的神经网络， 但其学习方法无法扩展到多层的神经网络上． 直到 1980 年左右， 反向传播算法才有效地解决了多层神 经网络的学习问题， 并成为最为流行的神经网络学习算法．</p><p>人工神经网络诞生之初并不是用来解决机器学习问题． 由于人工神经网络可以用作一个通用的函数逼近器（ 一个两层的神经网络可以逼近任意的函数），因此我们可以将人工神经网络看作一个可学习的函数， 并将其应用到机器学习中． 理论上， 只要有足够的训练数据和神经元数量， 人工神经网络就可以学到很多复杂的函数． 我们可以把一个人工神经网络塑造复杂函数的能力称为网络容量（ Network Capacity）， 这与可以被储存在网络中的信息的复杂度以及数量相关．</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211119094549829.png" alt="image-20211119094549829"></p><h2 id="1-3-前馈神经网络"><a href="#1-3-前馈神经网络" class="headerlink" title="1.3 前馈神经网络"></a>1.3 前馈神经网络</h2><p>在本次作业中， 我们主要采用误差反向传播来进行学习的神经网络， 即作为一种机器学习模型的神经网络．从机器学习的角度来看， 神经网络一般可以看作一个非线性模型， 其基本组成单元为具有非线性激活函数的神经元， 通过大量神经元之间的连接， 使得神经网络成为一种高度非线性的模型． 神经元之间的连接权重就是需要学习的参数， 可以在机器学习的框架下通过梯度下降方法来进行学习．</p><h3 id="1-3-1-神经元"><a href="#1-3-1-神经元" class="headerlink" title="1.3.1 神经元"></a>1.3.1 神经元</h3><p>1943 年， 心理学家 McCulloch 和数学家 Pitts 根据生物神经元的结构， 提出了一种非常简单的神经元模型， MP神经元． 现代神经网络中的神经元和 MP 神经元的结构并无太多变化． 不同的是， MP 神经元中的激活函数𝑓 为0或1的阶跃函数， 而现代神经元中的激活函数通常要求是连续可导的函数．</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211119205516708.png" alt="image-20211119205516708"></p><p>净输入  <strong>z</strong>  在经过一个非线性函数  $f(\cdot) $ 后, 得到神经元的活性值 ( Activation ) <strong>a</strong> ,</p><script type="math/tex; mode=display">a=f(z)</script><p>其中非线性函数  $f(\cdot) $ 称为激活函数 ( Activation Function ).</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211119100021496.png" alt="image-20211119100021496"></p><h3 id="1-3-2-激活函数"><a href="#1-3-2-激活函数" class="headerlink" title="1.3.2 激活函数"></a>1.3.2 激活函数</h3><ol><li>激活函数在神经元中非常重要的。为了增强网络的表示能力和学习能力，激活函数需要具备以下几点性质:<br>连续并可导(允许少数点上不可导)的非线性函数.可导的激活函数可以直接利用数值优化的方法来学习网络参数.</li><li>激活函数及其导函数要尽可能的简单,有利于提高网络计算效率.</li><li>激活函数的导函数的值域要在一个合适的区间内,不能太大也不能太小,否则会影响训练的效率和稳定性.</li></ol><p>下面介绍几种在神经网络中常用的激活函数.</p><h4 id="1-3-2-1-Sigmoid型函数"><a href="#1-3-2-1-Sigmoid型函数" class="headerlink" title="1.3.2.1 Sigmoid型函数"></a>1.3.2.1 Sigmoid型函数</h4><p>Sigmoid型函数是指一类S型曲线函数,为两端饱和函数.常用的Sigmoid型函数有Logistic 函数和Tanh 函数.</p><h5 id="1-3-2-1-1-Logistic"><a href="#1-3-2-1-1-Logistic" class="headerlink" title="1.3.2.1.1 Logistic"></a>1.3.2.1.1 Logistic</h5><p>Logistic 函数定义为</p><script type="math/tex; mode=display">\sigma(x)=\frac{1}{1+\exp (-x)}</script><p>Logistic 函数可以看成是一个“挤压” 函数， 把一个实数域的输入“挤压” 到(0, 1)． 当输入值在0附近时， Sigmoid型函数近似为线性函数； 当输入值靠近两端时， 对输入进行抑制． 输入越小， 越接近于 0； 输入越大， 越接近于 1． 这样的特点也和生物神经元类似， 对一些输入会产生兴奋（ 输出为1）， 对另一些输入产生抑制（ 输出为0）． 和感知器使用的阶跃激活函数相比， Logistic函数是连续可导的，其数学性质更好．因为Logistic函数的性质， 使得装备了Logistic激活函数的神经元具有以下两点性质：</p><ol><li>其输出直接可以看作概率分布， 使得神经网络可以更好地和统计学习模型进行结合．</li><li>其可以看作一个软性门（ Soft Gate）， 用来控制其他神经元输出信息的数量.</li></ol><h5 id="1-3-2-1-2-Tanh"><a href="#1-3-2-1-2-Tanh" class="headerlink" title="1.3.2.1.2 Tanh"></a>1.3.2.1.2 Tanh</h5><p>Tanh 函数也是一种 Sigmoid 型函数. 其定义为</p><script type="math/tex; mode=display">\tanh (x)=\frac{\exp (x)-\exp (-x)}{\exp (x)+\exp (-x)}</script><p>Tanh 函数可以看作放大并平移的 Logistic 函数, 其值域是  (-1,1) .</p><script type="math/tex; mode=display">\tanh (x)=2 \sigma(2 x)-1</script><p>下图给出了 Logistic 函数和 Tanh 函数的形状． Tanh 函数的输出是零中心化的（ Zero-Centered）， 而 Logistic 函数的输出恒大于 0． 非零中心化的输出会使得其后一层的神经元的输入发生偏置偏移（ Bias Shift）， 并进一步使得梯度下降的收敛速度变慢 .</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211119100752525.png" alt="image-20211119100752525"></p><p>Logistic函数和Tanh函数都是Sigmoid型函数， 具有饱和性， 但是计算开销较大． 因为这两个函数都是在中间（ 0附近） 近似线性， 两端饱和． 因此， 这两个函数可以通过分段函数来近似．</p><p>以 Logistic 函数 $ \sigma(x) $ 为例, 其导数为  $\sigma^{\prime}(x)=\sigma(x)(1-\sigma(x)) $. Logistic 函数 在 0 附近的一阶泰勒展开 ( Taylor expansion ) 为</p><script type="math/tex; mode=display">\begin{aligned}g_{l}(x) & \approx \sigma(0)+x \times \sigma^{\prime}(0) \\&=0.25 x+0.5\end{aligned}</script><p>这样 Logistic 函数可以用分段函数 hard-logistic  (x)  来近似.</p><script type="math/tex; mode=display">\begin{aligned}\operatorname{hard}-\operatorname{logistic}(x) &=\left\{\begin{array}{ll}1 & g_{l}(x) \geq 1 \\g_{l} & 0<g_{l}(x)<1 \\0 & g_{l}(x) \leq 0\end{array}\right.\\&=\max \left(\min \left(g_{l}(x), 1\right), 0\right) \\&=\max (\min (0.25 x+0.5,1), 0)\end{aligned}</script><p>同样, Tanh 函数在 0 附近的一阶泰勒展开为</p><script type="math/tex; mode=display">\begin{aligned}g_{t}(x) & \approx \tanh (0)+x \times \tanh ^{\prime}(0) \\&=x\end{aligned}</script><p>这样  $\operatorname{Tanh} $ 函数也可以用分段函数$  \operatorname{hard}-\tanh (x)  $来近似.</p><script type="math/tex; mode=display">\begin{aligned}\operatorname{hard}-\tanh (x) &=\max \left(\min \left(g_{t}(x), 1\right),-1\right) \\&=\max (\min (x, 1),-1)\end{aligned}</script><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211119101318245.png" alt="image-20211119101318245"></p><h4 id="1-3-2-2-ReLU函数"><a href="#1-3-2-2-ReLU函数" class="headerlink" title="1.3.2.2 ReLU函数"></a>1.3.2.2 ReLU函数</h4><p>ReLU（ Rectified Linear Unit， 修正线性单元） [Nair et al., 2010]， 也叫Rectifier函数[Glorot et al., 2011]， 是目前深度神经网络中经常使用的激活函数．ReLU实际上是一个斜坡（ ramp） 函数， 定义为</p><script type="math/tex; mode=display">\begin{aligned}\operatorname{ReLU}(x) &=\left\{\begin{array}{ll}x & x \geq 0 \\0 & x<0\end{array}\right.\\&=\max (0, x)\end{aligned}</script><p><strong>优点</strong> 采用 ReLU 的神经元只需要进行加、 乘和比较的操作， 计算上更加高效．ReLU 函数也被认为具有生物学合理性（ Biological Plausibility）， 比如单侧抑制、宽兴奋边界（ 即兴奋程度可以非常高）． 在生物神经网络中， 同时处于兴奋状态的神经元非常稀疏． 人脑中在同一时刻大概只有 1% ∼ 4% 的神经元处于活跃状态． Sigmoid 型激活函数会导致一个非稀疏的神经网络， 而 ReLU 却具有很好的稀疏性， 大约50%的神经元会处于激活状态．<br>在优化方面， 相比于Sigmoid型函数的两端饱和， ReLU函数为左饱和函数，且在 𝑥 &gt; 0 时导数为 1， 在一定程度上缓解了神经网络的梯度消失问题， 加速梯度下降的收敛速度．</p><p><strong>缺点</strong> ReLU 函数的输出是非零中心化的， 给后一层的神经网络引入偏置偏移，会影响梯度下降的效率．此外， ReLU 神经元在训练时比较容易“死亡”． 在训练时， 如果参数在一次不恰当的更新后， 第一个隐藏层中的某个 ReLU 神经元在所有的训练数据上都不能被激活， 那么这个神经元自身参数的梯度永远都会是0， 在以后的训练过程中永远不能被激活． 这种现象称为死亡 ReLU 问题（ Dying ReLU Problem ),并且也有可能会发生在其他隐藏层.</p><p>在实际使用中,为了避免上述情况,有几种 ReLU的变种也会被广泛使用.</p><h5 id="1-3-2-2-1-带泄露的ReLU"><a href="#1-3-2-2-1-带泄露的ReLU" class="headerlink" title="1.3.2.2.1 带泄露的ReLU"></a>1.3.2.2.1 带泄露的ReLU</h5><p>带泄露的ReLU（ Leaky ReLU） 在输入 𝑥 &lt; 0时， 保持一个很小的梯度𝛾． 这样当神经元非激活时也能有一个非零的梯度可以更新参数， 避免永远不能被激活[Maas et al., 2013]． 带泄露的ReLU的定义如下：</p><script type="math/tex; mode=display">\begin{aligned}\operatorname{LeakyReLU}(x) &=\left\{\begin{array}{ll}x & \text { if } x>0 \\\gamma x & \text { if } x \leq 0\end{array}\right.\\&=\max (0, x)+\gamma \min (0, x)\end{aligned}</script><p>其中 𝛾是一个很小的常数， 比如0.01． 当𝛾 &lt; 1时， 带泄露的ReLU也可以写为</p><script type="math/tex; mode=display">\operatorname{LeakyReLU}(x)=\max (x, \gamma x)</script><p>相当于是一个比较简单的maxout单元 .</p><h5 id="1-3-2-2-2-带参数的ReLU"><a href="#1-3-2-2-2-带参数的ReLU" class="headerlink" title="1.3.2.2.2 带参数的ReLU"></a>1.3.2.2.2 带参数的ReLU</h5><p>带参数的 ReLU（ Parametric ReLU， PReLU） 引入一个可学习的参数， 不同神经元可以有不同的参数． 对于第 𝑖 个神经元， 其 PReLU 的定义为</p><script type="math/tex; mode=display">\begin{aligned}\operatorname{PReLU}_{i}(x) &=\left\{\begin{array}{ll}x & \text { if } x>0 \\\gamma_{i} x & \text { if } x \leq 0\end{array}\right.\\&=\max (0, x)+\gamma_{i} \min (0, x)\end{aligned}</script><p>其中 𝛾𝑖 为 𝑥 ≤ 0 时函数的斜率． 因此， PReLU 是非饱和函数． 如果 𝛾𝑖 = 0， 那么PReLU就退化为ReLU． 如果𝛾𝑖 为一个很小的常数， 则PReLU可以看作带泄露的ReLU． PReLU 可以允许不同神经元具有不同的参数， 也可以一组神经元共享一个参数．</p><h5 id="1-3-2-2-3-ELU函数"><a href="#1-3-2-2-3-ELU函数" class="headerlink" title="1.3.2.2.3 ELU函数"></a>1.3.2.2.3 ELU函数</h5><p>ELU（ Exponential Linear Unit， 指数线性单元）是一个近似的零中心化的非线性函数， 其定义为</p><script type="math/tex; mode=display">\begin{aligned}\operatorname{ELU}(x) &=\left\{\begin{array}{ll}x & \text { if } x>0 \\\gamma(\exp (x)-1) & \text { if } x \leq 0\end{array}\right.\\&=\max (0, x)+\min (0, \gamma(\exp (x)-1))\end{aligned}</script><p>其中 𝛾 ≥ 0是一个超参数， 决定𝑥 ≤ 0时的饱和曲线， 并调整输出均值在0附近．</p><h5 id="1-3-2-2-4-Softplus函数"><a href="#1-3-2-2-4-Softplus函数" class="headerlink" title="1.3.2.2.4 Softplus函数"></a>1.3.2.2.4 Softplus函数</h5><p>Softplus 函数 可以看作 Rectifier 函数的平滑版本， 其定义为</p><script type="math/tex; mode=display">\operatorname{Softplus}(x)=\log (1+\exp (x))</script><p>Softplus函数其导数刚好是Logistic函数． Softplus函数虽然也具有单侧抑制、宽兴奋边界的特性， 却没有稀疏激活性 .</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211119103024771.png" alt="image-20211119103024771"></p><h3 id="1-3-3-多层前馈神经网络模型"><a href="#1-3-3-多层前馈神经网络模型" class="headerlink" title="1.3.3 多层前馈神经网络模型"></a>1.3.3 多层前馈神经网络模型</h3><p>给定一组神经元， 我们可以将神经元作为节点来构建一个网络． 不同的神经网络模型有着不同网络连接的拓扑结构． 一种比较直接的拓扑结构是前馈网络． 前馈神经网络（ Feedforward Neural Network， FNN） 是最早发明的简单人工神经网络． 前馈神经网络也经常称为多层感知器（ Multi-Layer Perceptron MLP）．但多层感知器的叫法并不是十分合理， 因为前馈神经网络其实是由多层的 Logistic 回归模型（ 连续的非线性函数） 组成， 而不是由多层的感知器（ 不连续的非线性函数） 组成 .</p><p>前馈神经网络中， 各神经元分别属于不同的层． 每一层的神经元可以接收前一层神经元的信号， 并产生信号输出到下一层． 第0层称为输入层， 最后一层称为输出层， 其他中间层称为隐藏层． 整个网络中无反馈， 信号从输入层向输出层单向传播， 可用一个有向无环图表示 .</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211119103824459.png" alt="image-20211119103824459"></p><p>对于本次作业中的多分类问题 $ y \in{1, \cdots, C} $, 使用 Softmax 回归分类器, 相当于网络 最后一层设置  C  个神经元, 其激活函数为 Softmax 函数. 网络最后一层 (第  L  层) 的输出可以作为每个类的条件概率, 即</p><script type="math/tex; mode=display">\hat{\boldsymbol{y}}=\operatorname{softmax}\left(\boldsymbol{z}^{(L)}\right)</script><p>其中  $\boldsymbol{z}^{(L)} \in \mathbb{R}^{C}  $为第  L  层神经元的净输入;  $\hat{\boldsymbol{y}} \in \mathbb{R}^{C}  $为第  L  层神经元的活性值, 每 一维分别表示不同类别标签的预测条件概率.</p><p>故采用交叉熵损失函数, 对于样本 $ (\boldsymbol{x}, y) $, 其损失函数为</p><script type="math/tex; mode=display">\mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})=-\boldsymbol{y}^{\top} \log \hat{\boldsymbol{y}},</script><p>其中  $\boldsymbol{y} \in{0,1}^{C}  $为标签  y  对应的 one-hot 向量表示.<br><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211119205657015.png" alt="image-20211119205657015"></p><script type="math/tex; mode=display">\|\boldsymbol{W}\|_{F}^{2}=\sum_{l=1}^{L} \sum_{i=1}^{M_{l}} \sum_{j=1}^{M_{l-1}}\left(w_{i j}^{(l)}\right)^{2}</script><p>有了学习准则和训练样本, 网络参数可以通过梯度下降法来进行学习. 在梯度下降方法的每次迭代中,第 l 层的参数  $\boldsymbol{W}^{(l)} $ 和  $\boldsymbol{b}^{(l)}  $参数更新方式为</p><script type="math/tex; mode=display">\begin{aligned}\boldsymbol{W}^{(l)} & \leftarrow \boldsymbol{W}^{(l)}-\alpha \frac{\partial \mathcal{R}(\boldsymbol{W}, \boldsymbol{b})}{\partial \boldsymbol{W}^{(l)}} \\&=\boldsymbol{W}^{(l)}-\alpha\left(\frac{1}{N} \sum_{n=1}^{N}\left(\frac{\partial \mathcal{L}\left(\boldsymbol{y}^{(n)}, \hat{\boldsymbol{y}}^{(n)}\right)}{\partial \boldsymbol{W}^{(l)}}\right)+\lambda \boldsymbol{W}^{(l)}\right) \\\boldsymbol{b}^{(l)} & \leftarrow \boldsymbol{b}^{(l)}-\alpha \frac{\partial \mathcal{R}(\boldsymbol{W}, \boldsymbol{b})}{\partial \boldsymbol{b}^{(l)}} \\&=\boldsymbol{b}^{(l)}-\alpha\left(\frac{1}{N} \sum_{n=1}^{N} \frac{\partial \mathcal{L}\left(\boldsymbol{y}^{(n)}, \hat{\boldsymbol{y}}^{(n)}\right)}{\partial \boldsymbol{b}^{(l)}}\right)\end{aligned}</script><p>其中  $\alpha $ 为学习率.</p><p>梯度下降法需要计算损失函数对参数的偏导数, 如果通过链式法则逐一对每个参数进行求偏导比较低效. 在神经网络的训练中经常使用<strong>反向传播算法</strong>来高效地计算梯度.</p><h1 id="二、描述训练模型所使用的算法-1"><a href="#二、描述训练模型所使用的算法-1" class="headerlink" title="二、描述训练模型所使用的算法"></a>二、描述训练模型所使用的算法</h1><p>假设采用随机梯度下降进行神经网络参数学习, 给定一个样本$  (\boldsymbol{x}, \boldsymbol{y})$ , 将其输入到神经网络模型中, 得到网络输出为  $\hat{\boldsymbol{y}} $. 假设损失函数为$  \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}}) $, 要进行参数学习就需要计算损失函数关于每个参数的导数.</p><p>不失一般性, 对第  l  层中的参数$  \boldsymbol{W}^{(l)}  $和 $ \boldsymbol{b}^{(l)}  $计算偏导数. 因为$  \frac{\partial \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})}{\partial \boldsymbol{W}^{(l)}}  $的计算 涉及向量对矩阵的微分, 十分繁琐, 因此我们先计算$  \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})  $关于参数矩阵中每个元素的偏导数  $\frac{\partial \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})}{\partial w_{i j}^{(l)}} $. 根据链式法则,</p><script type="math/tex; mode=display">\begin{array}{l}\frac{\partial \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})}{\partial w_{i j}^{(l)}}=\frac{\partial \boldsymbol{z}^{(l)}}{\partial w_{i j}^{(l)}} \frac{\partial \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})}{\partial \boldsymbol{z}^{(l)}} \\\frac{\partial \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})}{\partial \boldsymbol{b}^{(l)}}=\frac{\partial \boldsymbol{z}^{(l)}}{\partial \boldsymbol{b}^{(l)}} \frac{\partial \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})}{\partial \boldsymbol{z}^{(l)}}\end{array}</script><p>以上两个公式中的第二项都是目标函数关于第  l  层的神经元  $\boldsymbol{z}^{(l)}  $的偏导数,称为误差项, 可以一次计算得到. 这样我们只需要计算三个偏导数, 分别为  $\frac{\partial \boldsymbol{z}^{(l)}}{\partial w_{i j}^{(l)}}, \frac{\partial \boldsymbol{z}^{(l)}}{\partial \boldsymbol{b}^{(l)}}  和  \frac{\partial \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})}{\partial \boldsymbol{z}^{(l)}} .$</p><h2 id="2-1-偏导数计算"><a href="#2-1-偏导数计算" class="headerlink" title="2.1 偏导数计算"></a>2.1 偏导数计算</h2><p>下面分别来计算这三个偏导数：</p><p>(1) 计算偏导数  $\frac{\partial z^{(l)}}{\partial w_{i j}^{(l)}}  $因  $z^{(l)}=\boldsymbol{W}^{(l)} \boldsymbol{a}^{(l-1)}+\boldsymbol{b}^{(l)} ,$ 偏导数</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial z^{(l)}}{\partial w_{i j}^{(l)}} &=[\frac{\partial z_{1}^{(l)}}{\partial w_{i j}^{(l)}}, \cdots, \frac{\partial z_{i}^{(l)}}{\partial w_{i j}^{(l)}}\cdots, \frac{\partial z_{M_{l}}^{(l)}}{\partial w_{i j}^{(l)}}]\\&=[0, \cdots, {\frac{\partial\left(\boldsymbol{w}_{i:}^{(l)} \boldsymbol{a}^{(l-1)}+b_{i}^{(l)}\right)}{\partial w_{i j}^{(l)}}}, \cdots, 0] \\&=\left[0, \cdots, a_{j}^{(l-1)}, \cdots, 0\right] \\& \triangleq \mathbb{l}_{i}\left(a_{j}^{(l-1)}\right) \quad \in \mathbb{R}^{1 \times M_{l}}\end{aligned}</script><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211119205757205.png" alt="image-20211119205757205"></p><h2 id="2-2-误差项"><a href="#2-2-误差项" class="headerlink" title="2.2 误差项"></a>2.2 误差项</h2><p>误差项  $\delta^{(l)}  $也间接反映了不同神经元对网络能力的贡献程度, 从而比较好地解决 了贡献度分配问题 ( Credit Assignment Problem,CAP ).<br>根据 $ \boldsymbol{z}^{(l+1)}=\boldsymbol{W}^{(l+1)} \boldsymbol{a}^{(l)}+\boldsymbol{b}^{(l+1)} $, 有</p><script type="math/tex; mode=display">\frac{\partial \boldsymbol{z}^{(l+1)}}{\partial \boldsymbol{\alpha}^{(l)}}=\left(\boldsymbol{W}^{(l+1)}\right)^{\top} \quad \in \mathbb{R}^{M_{l} \times M_{l+1}}</script><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211119205825727.png" alt="image-20211119205825727"></p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial \boldsymbol{\alpha}^{(l)}}{\partial \boldsymbol{z}^{(l)}} &=\frac{\partial f_{l}\left(\boldsymbol{z}^{(l)}\right)}{\partial \boldsymbol{z}^{(l)}} \\&=\operatorname{diag}\left(f_{l}^{\prime}\left(\boldsymbol{z}^{(l)}\right)\right) \quad \in \mathbb{R}^{M_{l} \times M_{l}}\end{aligned}</script><p>因此,根据链式法则,第  l  层的误差项为</p><script type="math/tex; mode=display">\begin{aligned}\delta^{(l)} & \triangleq \frac{\partial \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})}{\partial \boldsymbol{z}^{(l)}} \\&=\frac{\partial \boldsymbol{a}^{(l)}}{\partial \boldsymbol{z}^{(l)}}  \cdot \frac{\partial \boldsymbol{z}^{(l+1)}}{\partial \boldsymbol{a}^{(l)}} \cdot \frac{\partial \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})}{\partial \boldsymbol{z}^{(l+1)}} \\&={\operatorname{diag}\left(f_{l}^{\prime}\left(\boldsymbol{z}^{(l)}\right)\right)} \cdot\left(\boldsymbol{W}^{(l+1)}\right)^{\top}  \cdot{\delta^{(l+1)}} \\&=f_{l}^{\prime}\left(\boldsymbol{z}^{(l)}\right) \odot\left(\left(\boldsymbol{W}^{(l+1)}\right)^{\top} \delta^{(l+1)}\right) \quad \in \mathbb{R}^{M_{l}},\end{aligned}</script><p>其中 $ \odot  $是向量的 Hadamard 积运算符, 表示每个元素相乘.<br>从上面的公式可以看出,第  l  层的误差项可以通过第  l+1  层的误差项计算得到, 这就是误差的反向传播 ( BackPropagation, BP ). 反向传播算法的含义是: 第  l  层的一个神经元的误差项 ( 或敏感性 ) 是所有与该神经元相连的第  l+1  层 的神经元的误差项的权重和. 然后, 再乘上该神经元激活函数的梯度.<br>在计算出上面三个偏导数之后,</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})}{\partial w_{i j}^{(l)}} &={l}_{i}\left(a_{j}^{(l-1)}\right) \delta^{(l)} \\&=\left[0, \cdots, a_{j}^{(l-1)}, \cdots, 0\right]\left[\delta_{1}^{(l)}, \cdots, \delta_{i}^{(l)}, \cdots, \delta_{M_{l}}^{(l)}\right]^{\top} \\&=\delta_{i}^{(l)} a_{j}^{(l-1)}\end{aligned}</script><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211119205855415.png" alt="image-20211119205855415"></p><script type="math/tex; mode=display">\left[\frac{\partial \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})}{\partial \boldsymbol{W}^{(l)}}\right]_{i j}=\left[\delta^{(l)}\left(\boldsymbol{a}^{(l-1)}\right)^{\top}\right]_{i j}</script><p>因此, $ \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})  $关于第  l  层权重 $ \boldsymbol{W}^{(l)}  $的梯度为</p><script type="math/tex; mode=display">\frac{\partial \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})}{\partial \boldsymbol{W}^{(l)}}=\delta^{(l)}\left(\boldsymbol{a}^{(l-1)}\right)^{\top} \quad \in \mathbb{R}^{M_{l} \times M_{l-1}}</script><p>同理,  $\mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})  $关于第  l  层偏置 $ \boldsymbol{b}^{(l)}  $的梯度为</p><script type="math/tex; mode=display">\frac{\partial \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})}{\partial \boldsymbol{b}^{(l)}}=\delta^{(l)} \quad \in \mathbb{R}^{M_{l}}</script><h2 id="2-3-实现步骤与伪代码"><a href="#2-3-实现步骤与伪代码" class="headerlink" title="2.3 实现步骤与伪代码"></a>2.3 实现步骤与伪代码</h2><p>在计算出每一层的误差项之后, 我们就可以得到每一层参数的梯度. 因此, 使用误差反向传播算法的前馈神经网络训练过程可以分为以下三步:</p><ol><li>前馈计算每一层的净输入  $\boldsymbol{z}^{(l)}  $和激活值 $ \boldsymbol{a}^{(l)} $, 直到最后一层;</li><li>反向传播计算每一层的误差项  $\delta^{(l)}$ ;</li><li>计算每一层参数的偏导数, 并更新参数.</li></ol><p>使用反向传播算法的随机梯度下降训练过程：</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211119113727799.png" alt="image-20211119113727799"></p><h1 id="三、描述模型超参数确定的过程，分析模型训练结果-1"><a href="#三、描述模型超参数确定的过程，分析模型训练结果-1" class="headerlink" title="三、描述模型超参数确定的过程，分析模型训练结果"></a>三、描述模型超参数确定的过程，分析模型训练结果</h1><p>在神经网络中， 除了可学习的参数之外， 还存在很多超参数． 这些超参数对网络性能的影响也很大． 不同的机器学习任务往往需要不同的超参数． 常见的超参数有以下三类：</p><ol><li>网络结构,包括神经元之间的连接关系、层数、每层的神经元数量、激活函数的类型等.</li><li>优化参数,包括优化方法、学习率、小批量的样本数量等.</li><li>正则化系数.</li></ol><p>超参数优化（ Hyperparameter Optimization） 主要存在两方面的困难：</p><ul><li>超参数优化是一个组合优化问题， 无法像一般参数那样通过梯度下降方法来优化， 也没有一种通用有效的优化方法；</li><li>评估一组超参数配置（ Configuration）的时间代价非常高， 从而导致一些优化方法（ 比如演化算法（ Evolution Algorithm）） 在超参数优化中难以应用</li></ul><p>对于超参数的配置， 比较简单的方法有网格搜索、随机搜索、贝叶斯优化、动态资源分配和神经架构搜索 ，这次我们介绍最后两种。</p><h2 id="3-1-动态资源分配"><a href="#3-1-动态资源分配" class="headerlink" title="3.1 动态资源分配"></a>3.1 动态资源分配</h2><p>在超参数优化中， 每组超参数配置的评估代价比较高． 如果我们可以在较早的阶段就估计出一组配置的效果会比较差， 那么我们就可以中止这组配置的评估， 将更多的资源留给其他配置．这个问题可以归结为多臂赌博机问题的一个泛化问题： <strong>最优臂问题（ Best-Arm Problem）</strong>， 即在给定有限的机会次数下， 如何玩这些赌博机并找到收益最大的臂． 和多臂赌博机问题类似， 最优臂问题也是在利用和探索之间找到最佳的平衡．</p><p>由于目前神经网络的优化方法一般都采取随机梯度下降， 因此我们可以通过一组超参数的学习曲线来预估这组超参数配置是否有希望得到比较好的结果． 如果一组超参数配置的学习曲线不收敛或者收敛比较差， 我们可以应用<strong>早期停止（ Early-Stopping）</strong> 策略来中止当前的训练．</p><p>动态资源分配的关键是将有限的资源分配给更有可能带来收益的超参数组合． 一种有效方法是<strong>逐次减半（ Successive Halving）</strong> 方法， 将超参数优化看作一种非随机的最优臂问题． 假设要尝试 𝑁 组超参数配置， 总共可利用的资源预算（ 摇臂的次数） 为𝐵， 我们可以通过𝑇 = ⌈log2(𝑁)⌉ - 1轮逐次减半的方法来选取最优的配置．</p><h3 id="3-1-1-实现步骤与伪代码"><a href="#3-1-1-实现步骤与伪代码" class="headerlink" title="3.1.1 实现步骤与伪代码"></a>3.1.1 实现步骤与伪代码</h3><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211119165313867.png" alt="image-20211119165313867"></p><p>在逐次减半方法中， 尝试的超参数配置数量 𝑁 十分关键． 如果𝑁 越大， 得到最佳配置的机会也越大， 但每组配置分到的资源就越少， 这样早期的评估结果可能不准确． 反之， 如果 𝑁 越小， 每组超参数配置的评估会越准确， 但有可能无法得到最优的配置． 因此， 如何设置 𝑁 是平衡“利用-探索” 的一个关键因素． 一种改进的方法是<strong>HyperBand方法</strong>， 通过尝试不同的𝑁 来选取最优参数．</p><h2 id="3-2-神经架构搜索"><a href="#3-2-神经架构搜索" class="headerlink" title="3.2 神经架构搜索"></a>3.2 神经架构搜索</h2><p>上面介绍的超参数优化方法都是在固定（ 或变化比较小） 的超参数空间 𝒳中进行最优配置搜索， 而最重要的神经网络架构一般还是需要由有经验的专家来进行设计 . 神经架构搜索（ Neural Architecture Search， NAS） 是一个新的比较有前景的研究方向， 通过神经网络来自动实现网络架构的设计． 一个神经网络的架构可以用一个变长的字符串来描述． 利用元学习的思想， 神经架构搜索利用一个控制器来生成另一个子网络的架构描述． 控制器可以由一个循环神经网络来实现． 控制器的训练可以通过强化学习来完成， 其奖励信号为生成的子网络在开发集上的准确率  .</p><h2 id="3-3-结果分析"><a href="#3-3-结果分析" class="headerlink" title="3.3 结果分析"></a>3.3 结果分析</h2><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211119191016077.png" alt="image-20211119191016077"></p><p>在两次报告中我们一共介绍了5种超参数调优方式，也都尝试过应用到实验中，但并没有获得很大的提升。且结合数据量较小，网络结构较简单的实际，我们依旧选择了根据一些“经验结论”来进行网格搜索。</p><p>默认参数下，五次五折交叉验证结果：</p><p>Accuracy: 0.95</p><p>使用代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.neural_network <span class="hljs-keyword">import</span> MLPClassifier<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> cross_val_score<br><span class="hljs-keyword">import</span> warnings<br>warnings.filterwarnings(<span class="hljs-string">&#x27;ignore&#x27;</span>)<br>mlp = MLPClassifier(hidden_layer_sizes=(<span class="hljs-number">100</span>), activation=<span class="hljs-string">&#x27;relu&#x27;</span>, solver=<span class="hljs-string">&#x27;adam&#x27;</span>, alpha=<span class="hljs-number">0.0001</span>, batch_size=<span class="hljs-string">&#x27;auto&#x27;</span>, learning_rate=<span class="hljs-string">&#x27;constant&#x27;</span>, learning_rate_init=<span class="hljs-number">0.001</span>, power_t=<span class="hljs-number">0.5</span>, max_iter=<span class="hljs-number">200</span>, shuffle=<span class="hljs-literal">True</span>, random_state=<span class="hljs-literal">None</span>, tol=<span class="hljs-number">0.0001</span>, verbose=<span class="hljs-literal">False</span>, warm_start=<span class="hljs-literal">False</span>, momentum=<span class="hljs-number">0.9</span>, nesterovs_momentum=<span class="hljs-literal">True</span>, early_stopping=<span class="hljs-literal">False</span>, validation_fraction=<span class="hljs-number">0.1</span>, beta_1=<span class="hljs-number">0.9</span>, beta_2=<span class="hljs-number">0.999</span>, epsilon=<span class="hljs-number">1e-08</span>, n_iter_no_change=<span class="hljs-number">10</span>, max_fun=<span class="hljs-number">15000</span>) <span class="hljs-comment">#均以官网默认参数设置</span><br>mlp.fit(X_std, Y)<br>ACC = cross_val_score(mlp, X_std, Y, cv=rkf,scoring=<span class="hljs-string">&#x27;accuracy&#x27;</span>).mean()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Accuracy:&#x27;</span>,ACC)<br></code></pre></td></tr></table></figure><p>网格搜索最优参数的结果为：</p><p>Accuracy: 0.96125</p><p>混淆矩阵如图：</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211119193317068.png" alt="image-20211119193317068"></p><p>ROC曲线如图：</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211119193347579.png" alt="image-20211119193347579"></p><p>评价指标（精确率，召回率和F1值）：</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211119193422505.png" alt="image-20211119193422505"></p><p>从上面可以看出，我们的模型分类结果完全正确，甚至有可能出现了过拟合。为了避免这种情况，我们继续修改参数，发现即使不设置任何超参数也可以达到这样的准确率，也从另一方面证明了神经网络拟合能力的强悍。</p><p>网格搜索代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 超参数调优</span><br><span class="hljs-keyword">import</span> warnings<br>warnings.filterwarnings(<span class="hljs-string">&#x27;ignore&#x27;</span>)<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> GridSearchCV<br>parameters = &#123;<span class="hljs-string">&#x27;hidden_layer_sizes&#x27;</span>: [(<span class="hljs-number">10</span>,<span class="hljs-number">10</span>,<span class="hljs-number">10</span>,<span class="hljs-number">10</span>,<span class="hljs-number">10</span>),(<span class="hljs-number">20</span>,<span class="hljs-number">20</span>,<span class="hljs-number">20</span>,<span class="hljs-number">20</span>,<span class="hljs-number">20</span>),(<span class="hljs-number">30</span>,<span class="hljs-number">30</span>,<span class="hljs-number">30</span>,<span class="hljs-number">30</span>,<span class="hljs-number">30</span>),(<span class="hljs-number">40</span>,<span class="hljs-number">40</span>,<span class="hljs-number">40</span>,<span class="hljs-number">40</span>,<span class="hljs-number">40</span>),(<span class="hljs-number">50</span>,<span class="hljs-number">50</span>,<span class="hljs-number">50</span>,<span class="hljs-number">50</span>,<span class="hljs-number">50</span>),(<span class="hljs-number">60</span>,<span class="hljs-number">60</span>,<span class="hljs-number">60</span>,<span class="hljs-number">60</span>,<span class="hljs-number">60</span>),(<span class="hljs-number">70</span>,<span class="hljs-number">70</span>,<span class="hljs-number">70</span>,<span class="hljs-number">70</span>,<span class="hljs-number">70</span>),(<span class="hljs-number">80</span>,<span class="hljs-number">80</span>,<span class="hljs-number">80</span>,<span class="hljs-number">80</span>,<span class="hljs-number">80</span>),(<span class="hljs-number">90</span>,<span class="hljs-number">90</span>,<span class="hljs-number">90</span>,<span class="hljs-number">90</span>,<span class="hljs-number">90</span>),(<span class="hljs-number">100</span>,<span class="hljs-number">100</span>,<span class="hljs-number">100</span>,<span class="hljs-number">100</span>,<span class="hljs-number">100</span>)],<br>                <span class="hljs-string">&#x27;activation&#x27;</span>: [<span class="hljs-string">&#x27;identity&#x27;</span>, <span class="hljs-string">&#x27;logistic&#x27;</span>,<span class="hljs-string">&#x27;tanh&#x27;</span>, <span class="hljs-string">&#x27;relu&#x27;</span>],<br>                <span class="hljs-string">&#x27;solver&#x27;</span>: [<span class="hljs-string">&#x27;adam&#x27;</span>,<span class="hljs-string">&#x27;lbgfs&#x27;</span>,<span class="hljs-string">&#x27;sgd&#x27;</span>],<br>                <span class="hljs-string">&#x27;alpha&#x27;</span>: [<span class="hljs-number">0.0001</span>, <span class="hljs-number">0.001</span>, <span class="hljs-number">0.01</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">1</span>,<span class="hljs-number">10</span>,<span class="hljs-number">100</span>],<br>                <span class="hljs-string">&#x27;learning_rate&#x27;</span>: [<span class="hljs-string">&#x27;constant&#x27;</span>, <span class="hljs-string">&#x27;invscaling&#x27;</span>, <span class="hljs-string">&#x27;adaptive&#x27;</span>]&#125;<br>grid = GridSearchCV(mlp, parameters, cv=rkf, scoring=<span class="hljs-string">&#x27;neg_mean_absolute_percentage_error&#x27;</span>,n_jobs=-<span class="hljs-number">1</span>)<br>grid.fit(X_std, Y)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;最优参数：&#x27;</span>,grid.best_params_)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;最优模型得分：&#x27;</span>,grid.best_score_)<br></code></pre></td></tr></table></figure><h1 id="四、总结模型训练过程中的收获-1"><a href="#四、总结模型训练过程中的收获-1" class="headerlink" title="四、总结模型训练过程中的收获"></a>四、总结模型训练过程中的收获</h1><h2 id="4-1-优化与正则化"><a href="#4-1-优化与正则化" class="headerlink" title="4.1 优化与正则化"></a>4.1 优化与正则化</h2><p>神经网络的优化和正则化是既对立又统一的关系． 一方面我们希望优化算法能找到一个全局最优解（ 或较好的局部最优解）， 另一方面我们又不希望模型优化到最优解， 这可能陷入过拟合． 优化和正则化的统一目标是期望风险最小化．</p><h3 id="4-1-1-优化"><a href="#4-1-1-优化" class="headerlink" title="4.1.1 优化"></a>4.1.1 优化</h3><p>在优化方面， 训练神经网络时的主要难点是非凸优化以及梯度消失问题． 在深度学习技术发展的初期， 我们通常需要利用预训练和逐层训练等比较低效的<br>方法来辅助优化．随着深度学习技术的发展， 我们目前通常可以高效地、 端到端地训练一个深度神经网络． 这些提高训练效率的方法通常分为以下 3 个方面：</p><ol><li>修改网络模型来得到更好的优化地形， 比如使用逐层归一化、 残差连接以及ReLU激活函数等；</li><li>使用更有效的优化算法， 比如动态学习率以及梯度估计修正等；</li><li>使用更好的参数初始化方法</li></ol><h3 id="4-1-2-泛化"><a href="#4-1-2-泛化" class="headerlink" title="4.1.2 泛化"></a>4.1.2 泛化</h3><p>在泛化方面， 传统的机器学习中有一些很好的理论可以帮助我们在模型的表示能力、复杂度和泛化能力之间找到比较好的平衡， 但是这些理论无法解释深度神经网络在实际应用中的泛化能力表现． 根据通用近似定理，神经网络的表示能力十分强大． 从直觉上， 一个过度参数化的神经网络很容易产生过拟合现象， 因为它的容量足够记住所有训练数据． 但是实验表明， 神经网络在训练过程中依然优先记住训练数据中的一般模式（ Pattern）， 即具有高泛化能力的模式 ． 但目前， 神经网络的泛化能力还没有很好的理论支持． 在传统机器学习模型上比较有效的ℓ1 或ℓ2 正则化在深度神经网络中作用也比较有限， 而一些经验的做法（ 比如小的批量大小、大的学习率、提前停止、丢弃法、数据增强） 会更有效  。</p><h2 id="4-2-与回归的差异"><a href="#4-2-与回归的差异" class="headerlink" title="4.2 与回归的差异"></a>4.2 与回归的差异</h2><p>和回归问题不同， 分类问题中的目标标签 𝑦 是离散的类别标签， 因此分类问题中的决策函数需要输出离散值或是标签的后验概率． 线性分类模型一般是一个广义线性函数， 即一个或多个线性判别函数加上一个非线性激活函数． 所谓“线性”是指决策边界由一个或多个超平面组成．</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211119171348116.png" alt="image-20211119171348116"></p><h2 id="4-3-对深度学习框架的初步了解"><a href="#4-3-对深度学习框架的初步了解" class="headerlink" title="4.3 对深度学习框架的初步了解"></a>4.3 对深度学习框架的初步了解</h2><p>在深度学习中， 一般通过误差反向传播算法来进行参数学习． 采用手工方式来计算梯度再写代码实现的方式会非常低效， 并且容易出错． 此外， 深度学习模型需要的计算机资源比较多， 一般需要在 CPU 和 GPU 之间不断进行切换， 开发难度也比较大． 因此， 一些支持自动梯度计算、无缝CPU和GPU切换等功能的深度学习框架就应运而生．比较有代表性的框架包括： Theano、Caffe、TensorFlow、Pytorch、飞桨（ PaddlePaddle）、Chainer和MXNet等。</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211119185714226.png" alt="image-20211119185714226"></p><p>因此本次作业我们没有局限在scikit-learn中，我们了解并学习了TensorFlow，pytorch等框架的使用，搭建了简易的神经网络对本次作业的数据进行分析。虽然最终得到的效果不及sklearn，但是在体验的过程加深了对神经网络的理解。</p>]]></content>
    
    
    
    <tags>
      
      <tag>course</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>岭回归，特征工程分析advertising.csv</title>
    <link href="/2021/10/28/ridge/"/>
    <url>/2021/10/28/ridge/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>机器学习第二次作业记录。小组成员：方桂安，刘玥，周敏。</p><span id="more"></span><h1 id="一、数据分析"><a href="#一、数据分析" class="headerlink" title="一、数据分析"></a>一、数据分析</h1><h2 id="1-1-数据缺失检查"><a href="#1-1-数据缺失检查" class="headerlink" title="1.1 数据缺失检查"></a>1.1 数据缺失检查</h2><p>首先，为了我们能正常进行数据分析，我们进行了数据缺失分布情况检查。代码及结果如下：</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211026214742931.png" alt="image-20211026214742931"></p><p>缺失值总数：</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211026214800042.png" alt="image-20211026214800042"></p><p>由上可知，我们的数据中没有缺失值，不需要进行插值处理。</p><h2 id="1-2-销售量与各媒体投入关系分析"><a href="#1-2-销售量与各媒体投入关系分析" class="headerlink" title="1.2 销售量与各媒体投入关系分析"></a>1.2 销售量与各媒体投入关系分析</h2><h3 id="1-2-1-散点图"><a href="#1-2-1-散点图" class="headerlink" title="1.2.1 散点图"></a>1.2.1 散点图</h3><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211026214824135.png" alt="image-20211026214824135"></p><p>以上是销售量与各项媒体投入量的散点图。从上图我们可以看出，sales和TV投入量有明显的正相关关系，随着TV投入增多，sales大体上呈上升趋势。sales和radio投入量也有较弱的正相关趋势，但sales分布在以radio投入量为指标时，分布较零散，相关关系弱于sales与TV投入量。而sales和newspaper的相关性最弱，sales集中分布在newspaper低投入区域内。</p><h3 id="1-2-2-各项数据分析"><a href="#1-2-2-各项数据分析" class="headerlink" title="1.2.2 各项数据分析"></a>1.2.2 各项数据分析</h3><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211026215012236.png" alt="image-20211026215012236"></p><p>由上图可知，TV类广告的平均投入量最大，其投入量的最小值，二分位数，中位数和四分位数，最大值均大于其他类型的广告，说明企业偏向于在TV类广告投入更多资金。</p><h3 id="1-2-3-相关系数"><a href="#1-2-3-相关系数" class="headerlink" title="1.2.3 相关系数"></a>1.2.3 相关系数</h3><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211026215047222.png" alt="image-20211026215047222"></p><p>上图为四个变量的相关系数热力图，由此可以看出，销售量和TV，radio，newspaper的相关性依次减弱。</p><h3 id="1-2-4-散点图矩阵，多变量之间的关系可视化"><a href="#1-2-4-散点图矩阵，多变量之间的关系可视化" class="headerlink" title="1.2.4 散点图矩阵，多变量之间的关系可视化"></a>1.2.4 散点图矩阵，多变量之间的关系可视化</h3><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211026215104208.png" alt="image-20211026215104208"></p><h2 id="1-3-得出结论"><a href="#1-3-得出结论" class="headerlink" title="1.3 得出结论"></a>1.3 得出结论</h2><p>由上面的分析可知，销售量和TV投入量相关性最大，其次是radio，newspaper，这也符合我们目前的社会情况，人们更多的是在电视等电子产品上获取信息。所以，加大上述三种广告方式的投入会对销售量有依次递减的增幅影响。</p><h1 id="二、描述10折交叉验证对数据集的处理"><a href="#二、描述10折交叉验证对数据集的处理" class="headerlink" title="二、描述10折交叉验证对数据集的处理"></a>二、描述10折交叉验证对数据集的处理</h1><h2 id="2-1-引入10折交叉验证的原因"><a href="#2-1-引入10折交叉验证的原因" class="headerlink" title="2.1 引入10折交叉验证的原因"></a>2.1 引入10折交叉验证的原因</h2><p>泛化能力是指模型在训练集上训练后,对新数据进行准确预测的能力。在机器学习的模型选择中，我们要对候选模型的泛化误差进行评估，然后选择泛化误差最小的那个模型。而实际应用中，我们无法直接获得泛化误差，而训练误差又由于过拟合现象的存在而不适合作为标准，所以我们随机将数据集切为三部分：</p><ul><li>训练集：用来训练模型，对应训练误差。</li><li>验证集：用来选择模型，对应测试误差。</li><li>测试集：用来最终对学习方法进行评估，对应泛化误差的近似。</li></ul><p>但是在实际应用中数据往往是不充足的，为了选择泛化能力更好的模型，我们可以对数据集D进行适当的处理，从中产生出训练集S和测试集T。几种常见的做法有：简单交叉验证(holdout cross-validation)、留一交叉验证(leave-one-out cross-validation,LOOCV)、<em>k</em>折交叉验证(<em>k</em>-fold cross-validation)、多重<em>k</em>折交叉验证、分层法(stratification-split cross-validation)、自助法(bootstraps)等。而综合考虑几种方法的特点后，本次我们选择的处理方法是10折交叉验证法。</p><h2 id="2-2-10折交叉验证的基本原理"><a href="#2-2-10折交叉验证的基本原理" class="headerlink" title="2.2 10折交叉验证的基本原理"></a>2.2 10折交叉验证的基本原理</h2><p>10折交叉验证是指将原始数据集随机划分为样本数量近乎相等的10个子集，轮流将其中的9个合并作为训练集，其余1个作为测试集。在每次试验中计算正确率等评价指标，最终通过k次试验后取评价指标的平均值来评估该模型的泛化能力。</p><p>10折交叉验证的基本步骤如下:</p><ol><li>原始数据集划分为10个样本量尽可能均衡的子集；</li><li>使用第1个子集作为测试集，第2～9个子集合并作为训练集；</li><li>使用训练集对模型进行训练,计算多种评价指标在测试集下的结果；</li><li>重复2-3步骤,轮流将第2-10个子集作为测试集；</li><li>计算各评价指标的平均值作为最终结果，最终选出10次测评中平均测试误差最小的模型。</li></ol><p>10折交叉验证的原理示意见下图。</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211026214227086.png" alt="image-20211026214227086"></p><p>由于将数据集D划分为k个子集同样存在多种划分方式，为了减小因样本划分不同而引入的差别，k折交叉验证通常要随机使用不同的划分重复p次。故我们可以采用“10次10折交叉验证”。</p><h2 id="2-3-10折交叉验证函数python代码"><a href="#2-3-10折交叉验证函数python代码" class="headerlink" title="2.3 10折交叉验证函数python代码"></a>2.3 10折交叉验证函数python代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> KFold  <span class="hljs-comment"># 从sklearn导入KFold包</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">Ten_Flod_spilt</span>(<span class="hljs-params">fold,data,label</span>):</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    param fold: 要取第几折的数据。</span><br><span class="hljs-string">    param data: 需要分块的数据</span><br><span class="hljs-string">    param label: 对应的需要分块标签</span><br><span class="hljs-string">    return: 对应折的训练集、测试集和对应的标签</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    split_list = []<br>    kf = KFold(n_splits=<span class="hljs-number">10</span>)<br>    <span class="hljs-keyword">for</span> train, test <span class="hljs-keyword">in</span> kf.split(data):<br>        split_list.append(train.tolist())<br>        split_list.append(test.tolist())<br>    train,test=split_list[<span class="hljs-number">2</span> * fold],split_list[<span class="hljs-number">2</span> * fold + <span class="hljs-number">1</span>]<br>    <span class="hljs-keyword">return</span>  data[train], data[test], label[train], label[test]  <span class="hljs-comment">#已经分好块的数据集</span><br></code></pre></td></tr></table></figure><p>在后续使用中只需循环调用该函数即可达到10折交叉验证的目的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>        x_train, x_test, y_train, y_test = Ten_Flod_spilt(i,X_s,y_s)<br></code></pre></td></tr></table></figure><h1 id="三、描述所使用的线性模型"><a href="#三、描述所使用的线性模型" class="headerlink" title="三、描述所使用的线性模型"></a>三、描述所使用的线性模型</h1><h2 id="3-1-基本形式"><a href="#3-1-基本形式" class="headerlink" title="3.1 基本形式"></a>3.1 基本形式</h2><p>给定由d个属性描述的示例<strong>x</strong>=(x~1~;x~2~;… ; x~d~)，其中x~i~是<strong>x</strong>在第i个属性上的取值，线性回归(linear regression)试图学得一个通过属性的线性组合来进行预测的函数，即</p><script type="math/tex; mode=display">f(\boldsymbol{x})=w_{1} x_{1}+w_{2} x_{2}+\ldots+w_{d} x_{d}+b</script><p>一般用向量形式写成</p><script type="math/tex; mode=display">f(\boldsymbol{x})=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b</script><p>其中<strong>w</strong>=(w~1~;w~2~;… ; w~d~)。<strong>w</strong>和b学得之后，模型就得以确定。</p><p>故本题中的模型应该为</p><script type="math/tex; mode=display">\widehat{Sales}=w_{1} ·TV+w_{2} ·radio+w_{3} ·newspaper+b，使得\widehat{Sales}\cong Sales</script><p>此处有三个属性描述样本，故又称为多元线性回归(multivariate linear regression)。</p><p>其基本形式为</p><script type="math/tex; mode=display">\hat{f}\left(\hat{x}_{N+1}\right)=\hat{x}_{N+1}^{T} \widehat{\omega}^{*}\\其中  \hat{x}_{N+1}=\left(x_{N+1} ; 1\right) \in \mathbb{R}^{n+1}, \widehat{\omega}^{*}=\left(\omega^{*} ; b^{*}\right) \in \mathbb{R}^{n+1}</script><h2 id="3-2-岭回归"><a href="#3-2-岭回归" class="headerlink" title="3.2 岭回归"></a>3.2 岭回归</h2><p>吉洪诺夫正则化以安德烈·尼古拉耶维奇·吉洪诺夫命名，为非适定性问题的正则化中最常见的方法。在统计学中，本方法被称为脊回归或岭回归（ridge regression）；在机器学习领域则称为权重衰减或权值衰减（weight decay）。因为有不同的数学家独立发现此方法，此方法又称做吉洪诺夫－米勒法（Tikhonov–Miller method）、菲利浦斯－图米法（Phillips–Twomey method）、受限线性反演（constrained linear inversion method），或线性正规化（linear regularization）。</p><script type="math/tex; mode=display">min\ L(W)=\frac{1}{2}(XW-y)^T(XW-y)+\frac{1}{2}\alpha||W||^2_2</script><script type="math/tex; mode=display">W=(X^TX+\alpha I)^{-1}X^Ty</script><p>根据4.2、4.3的分析，我们最终决定在最小二乘法的基础上采取L2正则化，即岭回归。相应地，为了使用岭回归和缩减技术，首先需要对特征做标准化处理。因为，我们需要使每个维度特征具有相同的重要性，故采用了z-score标准化。随着模型复杂度的提升，在训练集上的效果就越好，即模型的偏差就越小；但是同时模型的方差就越大。对于岭回归的α而言，随着α的增大，$|X^TX+\alpha I|$就越大，$(X^TX+\alpha I)^{-1}$ 就越小，模型的方差就越小；而α越大使得<strong>W</strong>的估计值更加偏离真实值，模型的偏差就越大。所以岭回归的关键是找到一个合理的α值来平衡模型的方差和偏差。</p><p>本次使用10折交叉验证法来确定α值，每一种训练集和测试集下都会有对应的一个模型及模型评分（如均方误差），进而可以得到一个平均评分。对于α值则选择平均评分最优的α值。</p><h2 id="3-3-特征工程"><a href="#3-3-特征工程" class="headerlink" title="3.3 特征工程"></a>3.3 特征工程</h2><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211027233323219.png" alt="image-20211027233323219"></p><p>如图所示为梯度下降法，最小二乘法和sklearn调用所得结果与真实值的对比折线图。从中可以看出，三种折线都已经接近重合，但又与真实值存在差异。查阅资料后，我们了解了特征工程的相关知识。</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/967090-20170116151505067-1134887580.png" alt="img"></p><p>“数据决定了机器学习的上限，而算法只是尽可能逼近这个上限”，这里的数据指的就是经过特征工程得到的数据。特征工程指的是把原始数据转变为模型的训练数据的过程，它的目的就是获取更好的训练数据特征，使得机器学习模型逼近这个上限。特征工程能使得模型的性能得到提升，有时甚至在简单的模型上也能取得不错的效果。特征工程在机器学习中占有非常重要的作用，一般认为括特征构建、特征提取、特征选择三个部分。特征构建比较麻烦，需要一定的经验。 特征提取与特征选择都是为了从原始特征中找出最有效的特征。它们之间的区别是特征提取强调通过特征转换的方式得到一组具有明显物理或统计意义的特征；而特征选择是从特征集合中挑选一组具有明显物理或统计意义的特征子集。两者都能帮助减少特征的维度、数据冗余，特征提取有时能发现更有意义的特征属性，特征选择的过程经常能表示出每个特征的重要性对于模型构建的重要性。</p><p>本次作业中主要使用了特征构建、特征选择、特征缩放，具体结果将在第五部分讨论。</p><h1 id="四、描述训练模型所使用的算法"><a href="#四、描述训练模型所使用的算法" class="headerlink" title="四、描述训练模型所使用的算法"></a>四、描述训练模型所使用的算法</h1><h2 id="4-1-数据预处理"><a href="#4-1-数据预处理" class="headerlink" title="4.1 数据预处理"></a>4.1 数据预处理</h2><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211027090542621.png" alt="image-20211027090542621"></p><p>本次数据处理使用的是z-score标准化，转换公式为：</p><script type="math/tex; mode=display">z=\frac{x-\mu}{\sigma}</script><p>使用python具体实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fit_transform</span>(<span class="hljs-params">x</span>):</span><br>    x = np.asarray(x)<br>    std_ = np.std(x, axis=<span class="hljs-number">0</span>)  <span class="hljs-comment"># 标准差</span><br>    mean_ = np.mean(x, axis=<span class="hljs-number">0</span>)  <span class="hljs-comment"># 均值</span><br>    <span class="hljs-keyword">return</span> (x - mean_) / std_<br></code></pre></td></tr></table></figure><h2 id="4-2-策略"><a href="#4-2-策略" class="headerlink" title="4.2 策略"></a>4.2 策略</h2><h3 id="4-2-1-经验风险最小化"><a href="#4-2-1-经验风险最小化" class="headerlink" title="4.2.1 经验风险最小化"></a>4.2.1 经验风险最小化</h3><p>均方误差是回归任务中最常用的性能度量，因此我们可试图让均方误差最小化，即</p><script type="math/tex; mode=display">\begin{aligned}\left(w^{*}, b^{*}\right) &=\underset{(w, b)}{\arg \min } \sum_{i=1}^{m}\left(f\left(x_{i}\right)-y_{i}\right)^{2} \\&=\underset{(w, b)}{\arg \min } \sum_{i=1}^{m}\left(y_{i}-w x_{i}-b\right)^{2}\end{aligned}</script><p>均方误差有非常好的几何意义，它对应了常用的欧几里得距离或简称“欧氏距离”(Euclidean distance)。基于均方误差最小化来进行模型求解的方法称为“最小二乘法”(least square method)。具体求解过程在4.3中进行介绍。</p><h3 id="4-2-2-结构风险最小化"><a href="#4-2-2-结构风险最小化" class="headerlink" title="4.2.2 结构风险最小化"></a>4.2.2 结构风险最小化</h3><h4 id="4-2-2-1-正则化"><a href="#4-2-2-1-正则化" class="headerlink" title="4.2.2.1 正则化"></a>4.2.2.1 正则化</h4><p>当模型的复杂度增大时，训练误差会逐渐减小并趋于0；而测试误差会先减小，达到最大值后又增大。当选择的模型复杂度过大时，就会发生过拟合，如下图所示。</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211027095448459.png" alt="image-20211027095448459"></p><p>为了避免因为过拟合问题而导致拟合效果不佳，我们在经验风险上加一个正则化项或罚项，使结构风险最小，这种方法叫做正则化，一般具有如下形式：</p><script type="math/tex; mode=display">\min _{f \in \mathcal{F}} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)+\lambda J(f)</script><h4 id="4-2-2-2-L1与L2正则化"><a href="#4-2-2-2-L1与L2正则化" class="headerlink" title="4.2.2.2 L1与L2正则化"></a>4.2.2.2 L1与L2正则化</h4><p>使用L1范数（也称曼哈顿距离或Taxicab范数，只允许在与空间轴平行行径的距离）又叫<strong>lasso</strong>回归，损失函数变为：</p><script type="math/tex; mode=display">J(\mathbf{W})=\frac{1}{2 n}(\mathbf{X} \mathbf{W}-\mathbf{Y})^{T}(\mathbf{X} \mathbf{W}-\mathbf{Y})+\alpha\|W\|_{1}</script><p>使用L2范数（也称欧几里德距离，是向量到原点的最短距离）又叫<strong>ridge</strong>回归，损失函数变为：</p><script type="math/tex; mode=display">J(\mathbf{W})=\frac{1}{2}(\mathbf{X} \mathbf{W}-\mathbf{Y})^{T}(\mathbf{X} \mathbf{W}-\mathbf{Y})+\frac{1}{2} \alpha\|W\|_{2}^{2}</script><p>L1能使得一些特征的系数变小，甚至还使一些绝对值较小的系数直接变为0，产生稀疏解，起到特征选择的作用，增强模型的泛化能力。</p><p>L2的优点是可以限制|w|的大小，从而使模型更简单，更稳定，即使加入一些干扰样本也不会对模型产生较大的影响，而且还能解决非正定的问题，强制使XTX可逆有解。</p><script type="math/tex; mode=display">\theta=\left(X X^{T}+\alpha I\right)^{-1} X Y</script><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/1335117-20180708193526314-357302334.png" alt="img"></p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/1335117-20180708194712773-1094778410.png" alt="img"></p><p>在上图中，两个坐标分别是要学习到的两个参数ω1和ω2；彩色线是损失函数J的等高线即损失值相等线；方形和圆形就分别是L1和L2所产生的额外误差（约束空间）；最后的目标要是两者最小，即要得到能使两者相加最小的点，也就是图中的黑色交点。在画等差图时，L1的效果就很容易与坐标轴相交了，这就是会产生很多0，即造成参数稀疏的原因。而且同时如果给一个微小的偏移，L2移动不会很大，而L1可能会移动到方形边上产生很多的交点，所以L1比较不稳定。</p><p>L2倾向于使ω的分量取值更均衡，即非零分量个数更稠密，而L1倾向ω的分量取值更稀疏，即非零分量个数更少。所以从图可以看出L1的边缘比较尖锐，与目标函数的等高线相交时，交点会常在那些尖锐的地方，所以很多的参数就是0，即L1能产生稀疏解。所以在调参时如果我们主要的目的只是为了解决过拟合，一般选择L2正则化就够了。但是如果选择L2正则化发现还是过拟合，即预测效果差的时候，就可以考虑L1正则化。另外，如果模型的特征非常多，我们希望一些不重要的特征系数归零，从而让模型系数稀疏化的话，也可以使用L1正则化。</p><p>综合考虑，我们在本次的损失函数中引入的是L2正则化。</p><h2 id="4-3-算法"><a href="#4-3-算法" class="headerlink" title="4.3 算法"></a>4.3 算法</h2><h3 id="4-3-1-最小二乘法"><a href="#4-3-1-最小二乘法" class="headerlink" title="4.3.1 最小二乘法"></a>4.3.1 最小二乘法</h3><h4 id="4-3-1-1-问题分析"><a href="#4-3-1-1-问题分析" class="headerlink" title="4.3.1.1 问题分析"></a>4.3.1.1 问题分析</h4><p>我们的策略是</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211028184420327.png" alt="image-20211028184420327"></p><p>我们进行展开</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211028184438472.png" alt="image-20211028184438472"></p><p>下面，我们进行梯度推导</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211028184450724.png" alt="image-20211028184450724"></p><p>由于$L(W)$是关于$W$的凸函数，所以我们在梯度为零的点，即是我们要求的最优解。</p><script type="math/tex; mode=display">令\frac{\partial L}{\partial W}=0\\得(X^TX+\alpha I)W=X^Ty</script><p>我们要通过此方法求得$W$，需要的条件是$X^TX+\alpha I$可逆，若其可逆，则$W$的解是</p><script type="math/tex; mode=display">W=(X^TX+\alpha I)^{-1}X^Ty</script><p>因为最小二乘法要求$X^TX+\alpha I$必须存在可逆矩阵，在实际问题中可能不满足，于是我们下面采用梯度下降法进行迭代求解。</p><h4 id="4-3-1-2-代码实现"><a href="#4-3-1-2-代码实现" class="headerlink" title="4.3.1.2 代码实现"></a>4.3.1.2 代码实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">lms</span>(<span class="hljs-params">x_train, x_test, y_train, y_test</span>):</span><br>    x_train_=np.c_[np.ones([x_train.shape[<span class="hljs-number">0</span>],<span class="hljs-number">1</span>]),x_train]<br>    x_test_=np.c_[np.ones([x_test.shape[<span class="hljs-number">0</span>],<span class="hljs-number">1</span>]),x_test]<br>    theta_n = np.dot(np.dot(inv(np.dot(x_train_.T, x_train_)+<span class="hljs-number">0.1</span>*np.eye(x_train_.shape[<span class="hljs-number">1</span>])), x_train_.T), y_train)  <span class="hljs-comment"># theta = (X`X)^(-1)X`Y，其中X`表示X的转置，使用L2范数正则化</span><br>    y_pre = np.dot(x_test_, theta_n)<br>    mse = np.average((y_test - y_pre) ** <span class="hljs-number">2</span>)<br>    <span class="hljs-keyword">return</span> theta_n, y_pre, mse<br></code></pre></td></tr></table></figure><h3 id="4-3-2-梯度下降法"><a href="#4-3-2-梯度下降法" class="headerlink" title="4.3.2 梯度下降法"></a>4.3.2 梯度下降法</h3><h4 id="4-3-2-1-问题分析"><a href="#4-3-2-1-问题分析" class="headerlink" title="4.3.2.1 问题分析"></a>4.3.2.1 问题分析</h4><p>首先，我们的目标是下式</p><script type="math/tex; mode=display">令\hat{\omega}=W\\E(\hat{\omega})=\frac{1}{2}(X\hat{\omega}-y)^T(X\hat{\omega}-y)+\frac{1}{2}\alpha||\hat{\omega}||^2_2\quad,\hat{\omega}=\mathop{argmin}_\hat{\omega}\ E(\hat{\omega})</script><p>梯度下降法是一种迭代算法：我们选取适当的初始值$\hat{\omega}^{(0)}$，不断迭代，更新$\hat{\omega}$的值，进行目标函数的极小化，直到收敛。由于负梯度方向是使得函数值下降最快的方向，在迭代的每一步，以负梯度方向更新$\hat{\omega}$的值，从而达到减小函数值的目的。如下图形象化表示：</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211027100605390.png" alt="image-20211027100605390"></p><h4 id="4-3-2-2-核心思想"><a href="#4-3-2-2-核心思想" class="headerlink" title="4.3.2.2 核心思想"></a>4.3.2.2 核心思想</h4><ol><li>$E(\hat{\omega})$是具有一阶连续偏导数的凸函数，其极值点在一阶导数为零的地方取得</li><li>一阶泰勒展开：$E(\hat{\omega})\thickapprox E(\hat{\omega}^{(k)})+\nabla E(\hat{\omega}^{(k)})(\hat{\omega}-\hat{\omega}^{(k)})$，其中，$\nabla E(\hat{\omega}^{(k)})$是$E(\hat{\omega})$在$\hat{\omega}^{(k)}$的梯度：</li></ol><script type="math/tex; mode=display">\nabla E(\hat{\omega}^{(k)})=\frac{\partial E(\hat{\omega})}{\partial \hat{\omega}}|_{\hat{\omega}=\hat{\omega}^{(k)}}</script><ol><li>求取第k+1次迭代值：$\hat{\omega}^{k+1}=\hat{\omega}^{(k)}+\eta_k*(-\nabla E(\hat{\omega}^{(k)}))$，其中$\eta_k$是步长，有我们最初指定。梯度如下（推导在上面部分）：<script type="math/tex; mode=display">\frac{\partial E}{\partial \hat{\omega}}=X^TX\hat{\omega}-X^Ty+\alpha I\hat{\omega}\quad(I是n\times n的单位矩阵)</script></li></ol><h4 id="4-3-2-3-求解步骤"><a href="#4-3-2-3-求解步骤" class="headerlink" title="4.3.2.3 求解步骤"></a>4.3.2.3 求解步骤</h4><p>输入：目标函数$E(\hat{\omega})$，梯度函数$\nabla E(\hat{\omega})$，计算精度ε，步长$\eta_k$；</p><p>输出： $E(\hat{\omega})$的极小点$\hat{\omega}^*$。</p><p>（1）取初始值$\hat{\omega}^{(0)}\in \mathbb{R}^{d+1}$，置k=0；</p><p>（2）计算$E(\hat{\omega}^{(k)})$；</p><p>（3）计算梯度$\nabla E(\hat{\omega}^{(k)})$，当$||\nabla E(\hat{\omega}^{(k)})||&lt;\varepsilon$时，令$\hat{\omega}^*=\hat{\omega}^{(k)}$，</p><p>停止迭代；</p><p>（4）置$\hat{\omega}^{(k+1)}=\hat{\omega}^{(k)}+\eta_k(-\nabla E(\hat{\omega}^{(k)}))$，计算$E(\hat{\omega}^{(k+1)})$，</p><p>当$||E(\hat{\omega}^{(k+1)})-E(\hat{\omega}^{(k)})||&lt;\varepsilon$或$||\hat{\omega}^{(k+1)}-\hat{\omega}^{(k)}||&lt;\varepsilon$时，</p><p>令$\hat{\omega}^*=\hat{\omega}^{(k)}$，停止迭代；</p><p>（5）否则，置k=k+1，转步骤（3）。</p><h4 id="4-3-2-4-代码实现"><a href="#4-3-2-4-代码实现" class="headerlink" title="4.3.2.4 代码实现"></a>4.3.2.4 代码实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">GradientDescent_MultiLine</span>:</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, lr, epochs</span>):</span><br>        self.lr = lr  <span class="hljs-comment"># 学习率，用来控制步长（权重调整幅度）</span><br>        self.epochs = epochs  <span class="hljs-comment"># 循环迭代的次数</span><br>        self.lose = []  <span class="hljs-comment"># 损失值计算（损失函数）：均方误差</span><br><br>    <span class="hljs-string">&#x27;&#x27;&#x27;根据提供的训练数据对模型进行训练&#x27;&#x27;&#x27;</span><br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fit</span>(<span class="hljs-params">self, x, y</span>):</span><br>        x = np.asarray(x)<br>        y = np.asarray(y)<br>        y = np.squeeze(y)  <span class="hljs-comment"># 去掉冗余的维度</span><br><br>        self.w = np.zeros(<span class="hljs-number">1</span> + x.shape[<span class="hljs-number">1</span>])  <span class="hljs-comment"># 初始权重，权重向量初始值为0（或任何其他值），长度比X的特征数量多1（多出来的为截距）</span><br><br>        <span class="hljs-comment"># 开始训练</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.epochs):<br>            y_hat = np.dot(x, self.w[<span class="hljs-number">1</span>:]) + self.w[<span class="hljs-number">0</span>]  <span class="hljs-comment"># 计算预测值</span><br>            error = y - y_hat  <span class="hljs-comment"># 计算真实值与预测值之间的差距</span><br>            self.lose.append(np.<span class="hljs-built_in">sum</span>(error ** <span class="hljs-number">2</span>) / <span class="hljs-number">2</span> + <span class="hljs-number">0.1</span>* np.dot(self.w.T, self.w))  <span class="hljs-comment"># 将损失加入到损失列表中，使用L2范数正则化</span><br>            <span class="hljs-comment">#print(&quot;迭代次数:&#123;0&#125;,进度：&#123;1&#125;%&quot;.format(i + 1, 100.0 * (i + 1) / self.epochs), &quot;  loss:&quot;, np.sum(error ** 2) / 2)</span><br>            <span class="hljs-comment"># j &lt;- j + α * sum((y - y_hat) * x(j))</span><br>            x_=np.c_[np.ones([x.shape[<span class="hljs-number">0</span>],<span class="hljs-number">1</span>]),x]<br>            <span class="hljs-comment">#self.w[0] += self.lr * np.sum(error)</span><br>            <span class="hljs-comment">#self.w[1:] += self.lr * np.dot(x.T, error)</span><br>            I=np.identity(x_.shape[<span class="hljs-number">1</span>])<br>            self.w=self.w-self.lr*(np.dot((np.dot(x_.T, x_)+<span class="hljs-number">0.2</span>*I), self.w)-np.dot(x_.T, y))<br></code></pre></td></tr></table></figure><h1 id="五、分析模型训练结果，包括训练误差和测试误差"><a href="#五、分析模型训练结果，包括训练误差和测试误差" class="headerlink" title="五、分析模型训练结果，包括训练误差和测试误差"></a>五、分析模型训练结果，包括训练误差和测试误差</h1><h2 id="5-1-评估指标计算公式"><a href="#5-1-评估指标计算公式" class="headerlink" title="5.1 评估指标计算公式"></a>5.1 评估指标计算公式</h2><p>训练误差是模型关于训练数据集的平均损失；测试误差是模型关于测试数据集的平均损失。计算公式如下：</p><script type="math/tex; mode=display">R_{e m p}(\hat{f})=\frac{1}{N} \sum_{i=1}^{N} L\left(y, \hat{f}\left(x_{i}\right)\right)</script><script type="math/tex; mode=display">e_{t e s t}=\frac{1}{N^{\prime}} \sum_{i=1}^{N^{\prime}} L\left(y_{i}, \hat{f}\left(x_{i}\right)\right)</script><p>其中N为训练样本容量，<em>N</em>′为测试样本容量。由于我们在线性回归中使用的是平方损失函数，故上述计算结果又叫均方误差 MSE（Mean Squared Error）：</p><script type="math/tex; mode=display">M S E=\frac{1}{m} \sum_{i=1}^{m}\left(y_{\text {test }}^{(i)}-\hat{y}_{\text {test }}^{(i)}\right)^{2}</script><p>但是，MSE 公式有一个问题是会改变量纲。因为公式平方了，我们可以对这个MSE开方，得到第二个评价指标：均方根误差 RMSE（Root Mean Squared Error）：</p><script type="math/tex; mode=display">R M S E = \sqrt{M S E}=\sqrt{\frac{1}{m} \sum_{i=1}^{m}\left(y_{\text {test }}^{(i)}-\hat{y}_{\text {test }}^{(i)}\right)^{2}}</script><p>但是MSE不甚全面，某些情况下决定系数 R2（coefficient of determination）显得尤为有用，它可以看作是MSE的标准化版本，用于更好地解释模型的性能。R2值的定义如下：</p><script type="math/tex; mode=display">R^{2}=1-\frac{\left(\sum_{i=1}^{m}\left(\hat{y}^{(i)}-y^{(i)}\right)^{2}\right) / m}{\left(\sum_{i=1}^{m}\left(y^{(i)}-\bar{y}\right)^{2}\right) / m}=1-\frac{M S E(\hat{y}, y)}{\operatorname{Var}(y)}</script><h2 id="5-2-误差分析思路"><a href="#5-2-误差分析思路" class="headerlink" title="5.2 误差分析思路"></a>5.2 误差分析思路</h2><p>结合前文的推导分析，我们最终采用的是线性最小二乘法与L2正则化，即alpha取值为1.0的<strong>Ridge回归</strong>，并结合特征工程中特征构建（将<strong>TV*radio</strong>，<strong>radio*newspaper</strong>作为新的特征），特征选择（加入新的特征，舍弃相关系数较小的newspaper），特征缩放（将TV，radio，newspaper进行开方、平方、三次方等）的思路进行了14种情况的实验，并得出了每一种情况的MSE，RMSE，R^2^。</p><p>所使用的代码为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">X = np.asarray(data_s.get([<span class="hljs-string">&#x27;TV&#x27;</span>,<span class="hljs-string">&#x27;radio&#x27;</span>,<span class="hljs-string">&#x27;newspaper&#x27;</span>]))<br>y = np.asarray(data_s.get(<span class="hljs-string">&#x27;sales&#x27;</span>))<br>clf.fit(X,y)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;MSE=%f&#x27;</span>%(-<span class="hljs-number">0.1</span>*cross_val_score(clf, X, y, cv=<span class="hljs-number">10</span>, scoring=<span class="hljs-string">&#x27;neg_mean_squared_error&#x27;</span>).<span class="hljs-built_in">sum</span>()))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;RMSE=%f&#x27;</span>%(-<span class="hljs-number">0.1</span>*cross_val_score(clf, X, y, cv=<span class="hljs-number">10</span>, scoring=<span class="hljs-string">&#x27;neg_root_mean_squared_error&#x27;</span>).<span class="hljs-built_in">sum</span>()))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;R2=%f&#x27;</span>%(<span class="hljs-number">10</span>*cross_val_score(clf, X, y, cv=<span class="hljs-number">10</span>, scoring=<span class="hljs-string">&#x27;r2&#x27;</span>).<span class="hljs-built_in">sum</span>())+<span class="hljs-string">&#x27;%&#x27;</span>)<br><span class="hljs-built_in">print</span>(clf.coef_)<br><span class="hljs-built_in">print</span>(clf.intercept_)<br></code></pre></td></tr></table></figure><h2 id="5-3-训练结果"><a href="#5-3-训练结果" class="headerlink" title="5.3 训练结果"></a>5.3 训练结果</h2><p>14种情况的训练误差及测试误差记录在jupyter notebook的ipynb文件中，此处展示其中三种。</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211028143446583.png" alt="image-20211028143446583"></p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211028143509634.png" alt="image-20211028143509634"></p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211028144137987.png" alt="image-20211028144137987"></p><p>上图所示的第13种情况训练所得模型的各项评估指标最优，故将其model文件保存，助教老师可以使用test.ipynb自动载入模型，并计算出测试误差MSE。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> joblib<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> mean_squared_error <br>model = joblib.load(<span class="hljs-string">&#x27;model.pickle&#x27;</span>) <span class="hljs-comment">#载入模型</span><br>data=pd.read_csv(<span class="hljs-string">&#x27;5_test.csv&#x27;</span>) <span class="hljs-comment">#读入数据</span><br>data_s = (data-data.<span class="hljs-built_in">min</span>())/(data.<span class="hljs-built_in">max</span>()-data.<span class="hljs-built_in">min</span>()) <span class="hljs-comment">#归一化</span><br>data_s[<span class="hljs-string">&#x27;TV_min&#x27;</span>] = data_s[<span class="hljs-string">&#x27;TV&#x27;</span>].apply(<span class="hljs-keyword">lambda</span> x:x**<span class="hljs-number">0.2</span>)<br>data_s[<span class="hljs-string">&#x27;TV_radio&#x27;</span>]=data_s[<span class="hljs-string">&#x27;TV&#x27;</span>]*data_s[<span class="hljs-string">&#x27;radio&#x27;</span>]<br>X = np.asarray(data_s.get([<span class="hljs-string">&#x27;TV_radio&#x27;</span>,<span class="hljs-string">&#x27;TV_min&#x27;</span>,<span class="hljs-string">&#x27;radio&#x27;</span>,<span class="hljs-string">&#x27;newspaper&#x27;</span>]))<br>y = np.asarray(data_s.get(<span class="hljs-string">&#x27;sales&#x27;</span>))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;测试误差MSE=%f&#x27;</span>%mean_squared_error(y,model.predict(X)))<br></code></pre></td></tr></table></figure><h1 id="六、总结模型训练过程中的收获"><a href="#六、总结模型训练过程中的收获" class="headerlink" title="六、总结模型训练过程中的收获"></a>六、总结模型训练过程中的收获</h1><h2 id="6-1-学习数据分析处理"><a href="#6-1-学习数据分析处理" class="headerlink" title="6.1 学习数据分析处理"></a>6.1 学习数据分析处理</h2><p>在进行计算之前，我们首先对数据进行了预处理和分析。首先，我们检查了数据是否缺失。然后，我们画出了散点图，散点图矩阵，相关系数热力图等，分析了销售量和各项广告投入量之间的数据关系，以便于对数据的进一步处理。在对数据的处理中，我们首先进行了数据标准化，将不同量级的数据统一转化为同一量级，以保证数据之间的可比性。而后，我们查阅资料，为了获取更好的训练数据特征，了解了特征工程相关内容，再根据之前对数据的分析，我们对数据进行了特征构建、特征选择等，具体结果上面已经展示。</p><h2 id="6-2-加深对十折交叉验证的理解"><a href="#6-2-加深对十折交叉验证的理解" class="headerlink" title="6.2 加深对十折交叉验证的理解"></a>6.2 加深对十折交叉验证的理解</h2><p>在机器学习的模型选择中，我们要对候选模型的泛化误差进行评估，然后选择泛化误差最小的那个模型。而实际应用中，我们无法直接获得泛化误差，而训练误差又由于过拟合现象的存在而不适合作为标准，所以我们随机将数据集切为三部分：训练集，验证集和测试集。在十折交叉验证中，我们通过某种特定的划分，将所有数据划分为十个，并依次选取作为测试集，剩下的作为训练集。在这个过程中，我们加深了对十折交叉验证的理解。</p><h2 id="6-3-对于正则化的理解加深"><a href="#6-3-对于正则化的理解加深" class="headerlink" title="6.3 对于正则化的理解加深"></a>6.3 对于正则化的理解加深</h2><p>正则化的目的：防止过拟合。过拟合指的是给定一堆数据，这堆数据带有噪声，利用模型去拟合这堆数据，可能会把噪声数据也给拟合了，这一方面会造成模型比较复杂，比如，原本一次函数能够拟合的数据，由于数据带有噪声，导致需要用五次函数来拟合；另一方面，同时会导致模型的泛化性能很差，在测试集上的结果准确率非常高，但测试新数据时，因为得到的是过拟合的模型，正确率会很低。</p><p>正则化的本质：约束（限制）要优化的参数。本来<strong>解空间</strong>是全部区域，但通过正则化添加了一些约束，使得解空间变小了，甚至在个别正则化方式下，解变得稀疏了。正如下图所示：</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/1335117-20180708193526314-357302334.png" alt="img"><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/1335117-20180708194712773-1094778410.png" alt="img"></p><p>彩色线就是优化过程中遇到的等高线，一圈代表一个目标函数值，圆心就是样本观测值（假设一个样本），半径就是误差值，受限条件就是黑色边界（就是正则化的部分），二者相交处，才是最优参数。</p><p>可以看到，L1 与L2 的不同就在于L1在和每个坐标轴相交的地方都有“角”出现，而目标函数的测地线除非位置摆得非常好，大部分时候都会在角的地方相交。注意到在角的位置就会产生稀疏性，例如图中的相交点就有w1=0，而更高维的时候除了角点以外，还有很多边的轮廓也是既有很大的概率成为第一次相交的地方，又会产生稀疏性。相比之下，L2就没有这样的性质，因为没有角，所以第一次相交的地方出现在具有稀疏性的位置的概率就变得非常小了。这就从直观上来解释了为什么L1-regularization 能产生稀疏性，而L2-regularization 不行的原因了。</p><h2 id="6-4-关于算法的择优"><a href="#6-4-关于算法的择优" class="headerlink" title="6.4 关于算法的择优"></a>6.4 关于算法的择优</h2><p>最开始我们分析结构风险最小化的策略，最小二乘法可能不可逆，同时为了增加模型的泛化能力，我们在损失函数中加入了惩罚项，由于对L1，L2正则化的分析，我们选择L2正则化，经过推导，发现最小二乘法可以直接得到解析解，解决了W系数矩阵非正定问题。由于梯度下降法是通过迭代逼近结果，所以只能得到近似解，所以我们选择最小二乘法来进行计算。</p>]]></content>
    
    
    
    <tags>
      
      <tag>course</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>逻辑回归模型及lbfgs算法公式推导</title>
    <link href="/2021/10/23/logistic/"/>
    <url>/2021/10/23/logistic/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>机器学习第一次作业记录。小组成员：方桂安，刘玥，周敏。</p><span id="more"></span><h2 id="一、描述逻辑回归模型"><a href="#一、描述逻辑回归模型" class="headerlink" title="一、描述逻辑回归模型"></a>一、描述逻辑回归模型</h2><h3 id="1-1数据"><a href="#1-1数据" class="headerlink" title="1.1数据"></a>1.1数据</h3><script type="math/tex; mode=display">D=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\},x_i\in \mathbb{R}^n,y_i\in\{0,1\}</script><h3 id="1-2模型"><a href="#1-2模型" class="headerlink" title="1.2模型"></a>1.2模型</h3><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211023235517425.png" alt="image-20211023235517425"></p><p>最初模型：</p><script type="math/tex; mode=display">f(x_i)=\omega^Tx_i+b,使得f(x_i)\simeq g(y_i)</script><p>我们的标记变量y的范围是0或1，所以我们需要一个函数能够将上述x的线性组合转化为0或1，最理想的是阶跃函数。</p><script type="math/tex; mode=display">阶跃函数：y=g^{-1}(\omega^Tx+b)= \begin{cases}0, & \omega^Tx+b<0\\0.5, & \omega^Tx+b=0\\1, & \omega^Tx+b>0\end{cases}</script><p>但由于阶跃函数不连续，不满足单调可微的条件。所以我们希望通过一个一定程度上近似阶跃函数的“替代函数”，并且希望它单调可微。由此，我们想到了逻辑斯蒂函数。</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211023235558559.png" alt="image-20211023235558559"></p><p>它的图像如下：</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/1990595-20200922165029694-1712738583.png" alt="img"></p><p>因为Logistic 回归主要用于分类问题，以二分类为例，对于所给数据集假设存在这样的一条直线可以将数据完成线性可分。                <img src="https://gitee.com/sysu_20354027/pic/raw/master/img/tW5koNMJrG193KEtuAH7cQ.png" alt="img"></p><p>当我们要找到分类概率 P(Y=1) 与输入向量 x 的直接关系时，我们引入Sigmoid函数，然后通过比较概率值来判断类别。</p><p>引入sigmoid函数具体实现如下：</p><p>但因为逻辑斯蒂函数的值域在[0,1]之间，无法直接输出0或1。在此基础上，考虑到$\omega^Tx+b$取值是连续的，因此它不能拟合离散变量。可以考虑用它来拟合条件概率$p(y=1|x)$，因为概率的取值也是连续的,我们将逻辑斯蒂函数的输出作为输入x能预测到y为1的概率，并利用对数几率函数，得到下面三个式子。通过此方法，我们将线性模型转换为概率模型。</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211023235625407.png" alt="image-20211023235625407"></p><h3 id="1-3策略"><a href="#1-3策略" class="headerlink" title="1.3策略"></a>1.3策略</h3><p>在策略上，我们采用极大似然法。即选择最优的w，b使得我们输入x得到的正确的y的概率最大，即下式：</p><script type="math/tex; mode=display">(w^*,b^*)=\mathop{argmax}\limits_{(w,b)}\prod_{i=1}^Np(y_i|x_i;\omega,b)</script><p>我们这里做一点变换：</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211023235646944.png" alt="image-20211023235646944"></p><p>因为上式是连乘的函数，我们通过对数似然函数将之转化为求和，即下式：</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211023235715659.png" alt="image-20211023235715659"></p><p>为了方便计算，我们做以下处理</p><script type="math/tex; mode=display">assume \ that\  \hat{\omega}=(\omega;b),\hat{x}=(x;1)</script><p>则上式可化为</p><script type="math/tex; mode=display">\hat{\omega^*}=\mathop{argmin}\limits_{\hat{\omega}}\sum_{i=1}^N(-y_i\hat{\omega }x_i+ln(1+e^{\hat{\omega}^T\hat{x}_i}))</script><p>这是一个凸函数，可用经典的数值优化算法，如梯度下降法、牛顿法求解。</p><p>最终，我们学得的逻辑斯蒂回归模型为</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211023235742051.png" alt="image-20211023235742051"></p><h2 id="二、描述训练模型所使用的算法"><a href="#二、描述训练模型所使用的算法" class="headerlink" title="二、描述训练模型所使用的算法"></a>二、描述训练模型所使用的算法</h2><h3 id="2-1梯度下降法"><a href="#2-1梯度下降法" class="headerlink" title="2.1梯度下降法"></a>2.1梯度下降法</h3><h4 id="2-1-1问题分析"><a href="#2-1-1问题分析" class="headerlink" title="2.1.1问题分析"></a>2.1.1问题分析</h4><p>首先，我们的目标是下式</p><script type="math/tex; mode=display">E(\hat{\omega})=\sum_{i=1}^N(-y_i\hat{\omega}^T\hat{x}_i+ln(1+e^{\hat{\omega}^T\hat{x}_i})),\hat{\omega}^*=\mathop{argmin}_{\hat{\omega}}E(\hat{\omega})</script><p>梯度下降法是一种迭代算法：我们选取适当的初始值$\hat{\omega}^{(0)}$，不断迭代，更新$\hat{\omega}$的值，进行目标函数的极小化，直到收敛。由于负梯度方向是使得函数值下降最快的方向，在迭代的每一步，以负梯度方向更新$\hat{\omega}$的值，从而达到减小函数值的目的。如下图形象化表示：</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211023152139696.png" alt="image-20211023152139696"></p><h4 id="2-1-2核心思想："><a href="#2-1-2核心思想：" class="headerlink" title="2.1.2核心思想："></a>2.1.2核心思想：</h4><ol><li>$E(\hat{\omega})$是具有一阶连续偏导数的凸函数，其极值点在一阶导数为零的地方取得</li><li>一阶泰勒展开：$E(\hat{\omega})\thickapprox E(\hat{\omega}^{(k)})+\nabla E(\hat{\omega}^{(k)})(\hat{\omega}-\hat{\omega}^{(k)})$，其中，$\nabla E(\hat{\omega}^{(k)})$是$E(\hat{\omega})$在$\hat{\omega}^{(k)}$的梯度：</li></ol><script type="math/tex; mode=display">\nabla E(\hat{\omega}^{(k)})=\frac{\partial E(\hat{\omega})}{\partial \hat{\omega}}|_{\hat{\omega}=\hat{\omega}^{(k)}}</script><ol><li>求取第k+1次迭代值：$\hat{\omega}^{k+1}=\hat{\omega}^{(k)}+\eta_k*(-\nabla E(\hat{\omega}^{(k)}))$，其中$\eta_k$是步长，由我们最初指定。梯度推导：</li></ol><script type="math/tex; mode=display">E(\hat{\omega})=\sum_{i=1}^N(-y_i\hat{\omega}^T\hat{x}_i+ln(1+e^{\hat{\omega}^T\hat{x}_i}))\\\nabla E(\hat{\omega}^{(k)})=\sum_{i=1}^N-y_i\hat{x}_i+\frac{1}{1+e^{\hat{\omega}^T\hat{x}_i}}*e^{\hat{\omega}^T\hat{x}_i}*\hat{x}_i\\=-\sum_{i=1}^Nx_i(y_i-\frac{e^{\hat{\omega}^T\hat{x}_i}}{1+e^{\hat{\omega}^T\hat{x}_i}})</script><h4 id="2-1-3伪代码："><a href="#2-1-3伪代码：" class="headerlink" title="2.1.3伪代码："></a>2.1.3伪代码：</h4><p>输入：目标函数$E(\hat{\omega})$，梯度函数$\nabla E(\hat{\omega})$，计算精度ε，步长$\eta_k$；</p><p>输出： $E(\hat{\omega})$的极小点$\hat{\omega}^*$。</p><p>（1）取初始值$\hat{\omega}^{(0)}\in \mathbb{R}^{d+1}$，置k=0；</p><p>（2）计算$E(\hat{\omega}^{(k)})$；</p><p>（3）计算梯度$\nabla E(\hat{\omega}^{(k)})$，当$||\nabla E(\hat{\omega}^{(k)})||&lt;\varepsilon$时，令$\hat{\omega}^*=\hat{\omega}^{(k)}$，</p><p>停止迭代；</p><p>（4）置$\hat{\omega}^{(k+1)}=\hat{\omega}^{(k)}+\eta_k(-\nabla E(\hat{\omega}^{(k)}))$，计算$E(\hat{\omega}^{(k+1)})$，</p><p>当$||E(\hat{\omega}^{(k+1)})-E(\hat{\omega}^{(k)})||&lt;\varepsilon$或$||\hat{\omega}^{(k+1)}-\hat{\omega}^{(k)}||&lt;\varepsilon$时，</p><p>令$\hat{\omega}^*=\hat{\omega}^{(k)}$，停止迭代；</p><p>（5）否则，置k=k+1，转步骤（3）。</p><h4 id="2-1-4分析"><a href="#2-1-4分析" class="headerlink" title="2.1.4分析"></a>2.1.4分析</h4><p>优点：方法简单，易理解</p><p>缺点：迭代次数多，下降速度慢，如下图，我们采用梯度下降法，迭代近50000次才收敛</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211023152205584.png" alt="image-20211023152205584"></p><p>且准确率如下，可以看出准确率不高。</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211023152217061.png" alt="image-20211023152217061"></p><h3 id="2-2牛顿法"><a href="#2-2牛顿法" class="headerlink" title="2.2牛顿法"></a>2.2牛顿法</h3><h4 id="2-2-1核心思想："><a href="#2-2-1核心思想：" class="headerlink" title="2.2.1核心思想："></a>2.2.1核心思想：</h4><p>$E(\hat{\omega})$是具有二阶连续偏导数的函数</p><p>二阶泰勒展开：$E(\hat{\omega})\thickapprox E(\hat{\omega}^{(k)})+\nabla E(\hat{\omega}^{(k)})(\hat{\omega}-\hat{\omega}^{(k)})+\frac{1}{2}(\hat{\omega}-\hat{\omega}^{(k)})^TH(\hat{\omega}^{(k)})(\hat{\omega}-\hat{\omega}^{(k)})$</p><script type="math/tex; mode=display">\nabla E(\hat{\omega})=\frac{\partial E(\hat{\omega})}{\partial \hat{\omega}}|_{(d+1)\times 1},H(\hat{\omega})=\frac{\partial ^2E(\hat{\omega})}{\partial \hat{\omega}_i \partial \hat{\omega}_j}|_{(d+1)\times 1}</script><p>利用二阶泰勒展开$E(\hat{\omega})$取极小点的必要条件$\nabla E(\hat{\omega})=0$，在第k次迭代$\hat{\omega}^{(k)}$，求$\nabla E(\hat{\omega}^{(k)})+H(\hat{\omega}^{(k)})(\hat{\omega}-\hat{\omega}^{(k)})=0$的点，作为第k+1次迭代值$\hat{\omega}^{(k+1)}$</p><h4 id="2-2-2伪代码"><a href="#2-2-2伪代码" class="headerlink" title="2.2.2伪代码"></a>2.2.2伪代码</h4><p>输入：目标函数$E(\hat{\omega})$，梯度函数$\nabla E(\hat{\omega})$，海森矩阵$H(\hat{\omega})$，精度ε；</p><p>输出：$E(\hat{\omega})$的极小点$\hat{\omega}^*$。</p><p>（1）取初始值$\hat{\omega}^{(0)}\in \mathbb{R}^{n+1}$，置k=0；</p><p>（2）计算梯度$\nabla E(\hat{\omega}^{(k)})$；</p><p>（3）当$||E(\hat{\omega}^{(k)})||&lt;\varepsilon$时，令$\hat{\omega}^*=\hat{\omega}^{(k)}$，停止迭代；</p><p>否则，计算海森矩阵$H(\hat{\omega}^{(k)})$ ；</p><p>（4）置$\hat{\omega}^{(k+1)}=\hat{\omega}^{(k)}-(H(\hat{\omega}))^{(-1)}\nabla E(\hat{\omega}^{(k)})$；</p><p>（5）置k=k+1，转步骤（2）。</p><h4 id="2-2-3分析"><a href="#2-2-3分析" class="headerlink" title="2.2.3分析"></a>2.2.3分析</h4><p>牛顿法优点：下降速度快，属于二次收敛</p><p>缺点：海森矩阵计算复杂度高，且要求可逆才能计算，所以我们查阅资料，将采用拟牛顿法。</p><h3 id="2-3-BFGS算法"><a href="#2-3-BFGS算法" class="headerlink" title="2.3 BFGS算法:"></a>2.3 BFGS算法:</h3><p>由于上述牛顿公式中可以看出，我们的海森矩阵不易得到，因此我们有以下迭代公式来逼近海森矩阵：</p><script type="math/tex; mode=display">H_{k+1}=H_k+\frac{y_ky_k^T}{y_k^Ts_k}-\frac{H_ks_ks_k^TH_k^T}{s_k^TH_k^Ts_k}</script><p>但计算量还是很大，矩阵相乘太多。所以我们最终采取$Sherman-Morrison$公式进行变换可得：</p><script type="math/tex; mode=display">H_{k+1}=\left(I-\frac{s_{k} y_{k}^{T}}{y_{k}^{T} s_{k}}\right) H_{k}\left(I-\frac{y_{k} s_{k}^{T}}{y_{k}^{T} s_{k}}\right)+\frac{s_{k} s_{k}^{T}}{y_{k}^{T} s_{k}} \quad(1)</script><p>公式推导如下：</p><script type="math/tex; mode=display">\begin{array}{l}\text { Sherman Morrison 公式: }\\\left(\mathrm{A}+\frac{u u^{T}}{t}\right)^{-1}=A^{-1}-\frac{A^{-1} u u^{T} A^{-1}}{t+u^{T} A^{-1} u}\\\left(\mathrm{H}+\frac{y y^{T}}{y^{T} \mathrm{~s}}-\frac{H s s^{T} \mathrm{H}}{s^{T} H s}\right)^{-1}\\=\left(\mathrm{H}+\frac{y y^{T}}{y^{T} \mathrm{~s}}\right)^{-1}+\left(\mathrm{H}+\frac{y y^{T}}{y^{T} \mathrm{~s}}\right)^{-1} \frac{H s s^{T} H}{s^{T} H^{T} s-s^{T} H\left(\mathrm{H}+\frac{y y^{T}}{y^{T} \mathrm{~S}}\right)^{-1} \mathrm{Hs}}\left(\mathrm{H}+\frac{y y^{T}}{y^{T} \mathrm{~S}}\right)^{-1}\\=\left(H^{-1}-\frac{H^{-1} y y^{T} H^{-1}}{y^{T} S+y^{T} H^{-1} y}\right)+\left(H^{-1}-\frac{H^{-1} y y^{T} H^{-1}}{y^{T} S+y^{T} H^{-1} y}\right) \frac{H s s^{T} H}{s^{T} H S-s^{T} H\left(H^{-1}-\frac{H^{-1} y y^{T}-1}{y^{T} s+y^{T} H^{-1} y}\right) H s}\left(H^{-1}-\frac{H^{-1} y y^{T} H^{-1}}{y^{T} S+y^{T} H^{-1} y}\right)\\=\left(H^{-1}-\frac{H^{-1} y y^{T} H^{-1}}{y^{T} \mathrm{~s}+y^{T} H^{-1} y}\right)+\left(H^{-1}-\frac{H^{-1} y y^{T} H^{-1}}{y^{T} \mathrm{~s}+y^{T} H^{-1} y}\right) \frac{H s s^{T} H}{\frac{s^{T} y y^{T} s}{y^{T} \mathrm{~s}+y^{T} H^{-1} y}}\left(H^{-1}-\frac{H^{-1} y y^{T} H^{-1}}{y^{T} \mathrm{~S}+y^{T} H^{-1} y}\right)\\=\left(H^{-1}-\frac{H^{-1} y y^{T} H^{-1}}{y^{T} \mathrm{~s}+y^{T} H^{-1} y}\right)+\frac{H^{-1} H s s^{T} H H^{-1}}{\frac{s^{T} y y^{T} S}{y^{T} s+y^{T} H^{-1} y}}-\frac{H^{-1} H s s^{T} H}{\frac{s^{T} y y^{T} s}{}}{y^{T} \mathrm{~s}+y^{T} H^{-1} y} H^{-1} \frac{y y^{T}}{y^{T} \mathrm{~s}+y^{T} H^{-1} y} H^{-1}\\-\frac{H^{-1} y y^{T} H^{-1}}{y^{T} \mathrm{~s}+y^{T} H^{-1} y} \frac{H s s^{T} H}{\frac{s^{T} y y^{T} S}{y^{T} \mathrm{~s}+y^{T} H^{-1} y}} H^{-1}\\+H^{-1} \frac{y y^{T}}{y^{T} \mathrm{~s}+y^{T} H^{-1} y} H^{-1} \frac{H s s^{T} H}{\frac{s^{T} y y^{T} S}{y^{T} \mathrm{~s}+y^{T} H^{-1} y}} H^{-1} \frac{y y^{T}}{y^{T} \mathrm{~s}+y^{T} H^{-1} y} H^{-1}\\=\left(H^{-1}-\frac{H^{-1} y y^{T} H^{-1}}{y^{T} \mathrm{~s}+y^{T} H^{-1} y}\right)+\frac{s s^{T}\left(y^{T} \mathrm{~s}+y^{T} H^{-1} y\right)}{s^{T} y y^{T} s}-\frac{s s^{T} y y^{T} H^{-1}}{s^{T} y y^{T} S}-\frac{H^{-1} y y^{T} S S^{T}}{s^{T} y y^{T} S}\\+\frac{H^{-1} y y^{T} S s^{T} y y^{T} H^{-1}}{\left(y^{T} \mathrm{~s}+y^{T} H^{-1} y\right) s^{T} y y^{T} S}\\=\left(H^{-1}-\frac{H^{-1} y y^{T} H^{-1}}{y^{T} \mathrm{~s}+y^{T} H^{-1} y}\right)+\frac{s s^{T}\left(y^{T} \mathrm{~s}+y^{T} H^{-1} y\right)}{\left(s^{T} y\right)^{2}}-\frac{s\left(s^{T} y\right) y^{T} H^{-1}}{\left(s^{T} y\right)^{2}}-\frac{H^{-1} y\left(y^{T} s\right) s^{T}}{\left(s^{T} y\right)^{2}}\\+\frac{H^{-1} y\left(y^{T} S s^{T} y\right) y^{T} H^{-1}}{\left(y^{T} \mathrm{~s}+y^{T} H^{-1} y\right) s^{T} y y^{T} S}\\=\left(H^{-1}-\frac{H^{-1} y y^{T} H^{-1}}{y^{T} \mathrm{~s}+y^{T} H^{-1} y}\right)+\frac{s s^{T}\left(y^{T} \mathrm{~s}+y^{T} H^{-1} y\right)}{\left(s^{T} y\right)^{2}}-\frac{s y^{T} H^{-1}}{s^{T} y}-\frac{H^{-1} y s^{T}}{s^{T} y}+\frac{H^{-1} y y^{T} H^{-1}}{\left(y^{T} \mathrm{~s}+y^{T} H^{-1} y\right)}\\=H^{-1}+\frac{s s^{T}\left(y^{T} \mathrm{~s}+y^{T} H^{-1} y\right)}{\left(s^{T} y\right)^{2}}-\frac{s y^{T} H^{-1}}{s^{T} y}-\frac{H^{-1} y s^{T}}{s^{T} y}\\=H^{-1}+\frac{s s^{T} y^{T} \mathrm{~s}}{\left(s^{T} y\right)^{2}}+\frac{s s^{T} y^{T} H^{-1} y}{\left(s^{T} y\right)^{2}}-\frac{s y^{T} H^{-1}}{s^{T} y}-\frac{H^{-1} y s^{T}}{s^{T} y}\\=H^{-1}\left(I-\frac{y s^{T}}{s^{T} y}\right)-\frac{s y^{T} H^{-1}}{s^{T} y}\left(I-\frac{y s^{T}}{s^{T} y}\right)+\frac{s s^{T}}{s^{T} y}\\=\left(I-\frac{s y^{T}}{s^{T} y}\right) H^{-1}\left(I-\frac{y s^{T}}{s^{T} y}\right)+\frac{s s^{T}}{s^{T} y}\end{array}</script><h3 id="2-4-L-BFGS算法"><a href="#2-4-L-BFGS算法" class="headerlink" title="2.4 L-BFGS算法"></a>2.4 L-BFGS算法</h3><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211023235811630.png" alt="image-20211023235811630"></p><script type="math/tex; mode=display">\begin{array}{c}H_{k+1}=V_{k}^{T} H_{k} V_{k}+\rho_{k} s_{k} s_{k}^{T} \\\end{array}</script><p>给定初始矩阵$H_0=I$，利用上式，可得：</p><script type="math/tex; mode=display">\begin{aligned}H_{1}&=V_{0}^{T} H_{0} V_{0}+\rho_{0} s_{0} s_{0}^{T}\\H_{2} &=V_{1}^{T} H_{1} V_{1}+\rho_{1} s_{1} s_{1}^{T} \\&=V_{1}^{T}\left(V_{0}^{T} H_{0} V_{0}+\rho_{0} s_{0} s_{0}^{T}\right) V_{1}+\rho_{1} s_{1} s_{1}^{T} \\&\left.=V_{1}^{T} V_{0}^{T} H_{0} V_{0} V_{1}+V_{1}^{T} \rho_{0} s_{0} s_{0}^{T}\right) V_{1}+\rho_{1} s_{1} s_{1}^{T} \\& \\\quad H_{k+1} &=\left(V_{k}^{T} V_{k-1}^{T} \ldots V_{1}^{T} V_{0}^{T}\right) H_{0}\left(V_{0} V_{1} \ldots V_{k-1} V_{k}\right) \\&+\left(V_{k}^{T} V_{k-1}^{T} \ldots V_{1}^{T}\right) \rho_{1} s_{1} s_{1}^{T}\left(V_{1} \ldots V_{k-1} V_{k}\right) \\&+\ldots \\&+\left(V_{k}^{T}\right) \rho_{k-1} s_{k-1} s_{k-1}^{T}\left(V_{k}\right) \\&+\rho_{k} s_{k} s_{k}^{T}\end{aligned}</script><p>只保留最近的m步后，上式的迭代公式变为：</p><script type="math/tex; mode=display">\begin{aligned}H_{k+1} &=\left(V_{k}^{T} V_{k-1}^{T} \ldots V_{k-m}^{T}\right) H_{0}\left(V_{k-m} \ldots V_{k-1} V_{k}\right) \\&+\left(V_{k}^{T} V_{k-1}^{T} \ldots V_{k-m+1}^{T}\right) \rho_{k-m} s_{k-m} s_{k-m}^{T}\left(V_{k-m+1} \ldots V_{k-1} V_{k}\right) \\+& \ldots \\&+\left(V_{k}^{T}\right) \rho_{k-1} s_{k-1} s_{k-1}^{T}\left(V_{k}\right) \\&+\rho_{k} s_{k} s_{k}^{T}\end{aligned}</script><p>所求方向为：</p><script type="math/tex; mode=display">\begin{aligned}H_{k} \nabla f &=\left(V_{K-1}^{T} V_{K-2}^{T} \ldots V_{K-m}^{T}\right) H_{0}\left(V_{K-m} V_{K-m+1} \ldots V_{K-1}\right) \nabla f \\&+\left(V_{K-1}^{T} \ldots V_{K-m+1}^{T}\right)\rho_{k-m} s_{k-m} s_{k-m}^T(V_{k-m+1}\dots V_{k-1}V_{k}) \nabla f\\&+\ldots \\&+V_{k-1} \rho_{k-1} s_{k-1}s_{k-1}^TV_k\nabla f \\&+\rho_{k} s_{k}s_{k}^T\nabla f\end{aligned}</script><p>Two-Loop 算法：</p><script type="math/tex; mode=display">\begin{array}{l}q_{k} \leftarrow \nabla f_{k} \\\text { for } i=k-1 \text { to } k-m \text { do } \\\quad \alpha_{i}=\rho_{i} s_{i}^{T} q_{i+1} \\q_{i}=q_{i+1}-\alpha_{i} y_{i} \\\text { end for } \\r_{k-m-1}=H_{0} q_{k-m} \\\text { for } i=k-m, k-m+1 \text { to } k-1 \text { do } \\\quad \beta_{i}=\rho_{i} y_{i}^{T} r_{i-1} \\r_{i}=r_{i-1}+s_{i} \alpha_{i}-\beta_{i} \\\text { end for } \\\text { End, The result is } H_{k+1} \nabla f=r\end{array}</script><p>Two-Loop算法解析—-第一个循环：</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211023235846434.png" alt="image-20211023235846434"></p><p>重写公式：</p><script type="math/tex; mode=display">\begin{aligned}H_{k} \nabla f &=\left(V_{K-1}^{T} V_{K-2}^{T} \ldots V_{K-m}^{T}\right) H_{0}\left(V_{K-m} V_{K-m+1} \ldots V_{K-1}\right) \nabla f \\&+\left(V_{K-1}^{T} \ldots V_{K-m+1}^{T}\right) s_{k-m} \alpha_{k-m} \\&+\ldots \\&+V_{k-1} s_{k-1} \alpha_{k-2} \\&+s_{k-1} \alpha_{k-1}\end{aligned}</script><p>Two-Loop算法解析—-第二个循环：</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211023235910775.png" alt="image-20211023235910775"></p><p>初始:</p><script type="math/tex; mode=display">r_{k-\mathrm{m}}=V_{k-\mathrm{m}} H_{0} V_{k-\mathrm{m}} V_{k-\mathrm{m}+1} \ldots V_{k-1} \nabla \mathrm{f}+\mathrm{s}_{k-\mathrm{m}} \alpha_{k-m}</script><p>得：</p><script type="math/tex; mode=display">\begin{aligned}r_{k-\mathrm{m}+\mathrm{i}} &=V_{k-\mathrm{m}+\mathrm{i}} \ldots V_{k-\mathrm{m}} H_{0} V_{k-\mathrm{m}} \ldots V_{k-\mathrm{m}+\mathrm{i}} \nabla \mathrm{f} \\&+\left(V_{k-\mathrm{m}+\mathrm{i}} \ldots V_{k-\mathrm{m}+1}\right) \mathrm{s}_{k-\mathrm{m}} \alpha_{k-m} \\&+\left(V_{k-\mathrm{m}+\mathrm{i}} \ldots V_{k-\mathrm{m}+2}\right) \mathrm{s}_{k-\mathrm{m}+1} \alpha_{k-m+1}\\&+\dots\\&s_{k-m+1}\alpha_{k-m+i}\end{aligned}</script><p>$r_{k-1}$即是所求的搜索方向d。</p><p>使用LBFGS求解逻辑回归模型代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score<br><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-comment"># 进一步处理数据集和测试集，将输入和输出分割</span><br>train.columns=<span class="hljs-built_in">list</span>([<span class="hljs-string">&#x27;x1&#x27;</span>,<span class="hljs-string">&#x27;x2&#x27;</span>,<span class="hljs-string">&#x27;y&#x27;</span>])<br>test.columns=<span class="hljs-built_in">list</span>([<span class="hljs-string">&#x27;x1&#x27;</span>,<span class="hljs-string">&#x27;x2&#x27;</span>,<span class="hljs-string">&#x27;y&#x27;</span>])<br>X_train = np.asarray(train.get([<span class="hljs-string">&#x27;x1&#x27;</span>, <span class="hljs-string">&#x27;x2&#x27;</span>]))<br>y_train = np.asarray(train.get(<span class="hljs-string">&#x27;y&#x27;</span>))<br>X_test = np.asarray(test.get([<span class="hljs-string">&#x27;x1&#x27;</span>, <span class="hljs-string">&#x27;x2&#x27;</span>]))<br>y_test = np.asarray(test.get(<span class="hljs-string">&#x27;y&#x27;</span>))<br><span class="hljs-comment"># 使用 sklearn 的 LogisticRegression 作为模型</span><br><span class="hljs-comment"># 其中有 penalty，solver，multi_class 几个比较重要的参数，不同的参数有不同的准确率</span><br>model = LogisticRegression(solver=<span class="hljs-string">&#x27;newton-cg&#x27;</span>)<br><span class="hljs-comment"># newton-cg sag lbfgs liblinear</span><br><br><br><span class="hljs-comment"># 对数据进行标准化</span><br>ss = StandardScaler()<br>X_train = ss.fit_transform(X_train) <br>X_test = ss.fit_transform(X_test)<br><span class="hljs-comment"># 拟合</span><br>model.fit(X_train, y_train)<br><br><span class="hljs-comment"># 预测测试集</span><br>predictions = model.predict(X_test)<br><br><span class="hljs-comment"># 打印准确率</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;测试集准确率：&#x27;</span>, accuracy_score(y_test, predictions))<br><br>weights = np.column_stack((model.intercept_, model.coef_)).transpose()<br><span class="hljs-comment">#print(weights)</span><br></code></pre></td></tr></table></figure><h2 id="三、绘制ROC曲线和PR曲线"><a href="#三、绘制ROC曲线和PR曲线" class="headerlink" title="三、绘制ROC曲线和PR曲线"></a>三、绘制ROC曲线和PR曲线</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs markdown">该部分出现的英语缩写：<br>TP: True Positive<br>FP: False Positive<br>FN: False Negative<br>TN: True Negative<br>P: Precision<br>R: Recall<br>TPR: True Positive Rate<br>FPR: False Positive Rate<br></code></pre></td></tr></table></figure><h3 id="3-1-ROC曲线"><a href="#3-1-ROC曲线" class="headerlink" title="3.1 ROC曲线"></a>3.1 ROC曲线</h3><h4 id="3-1-1介绍"><a href="#3-1-1介绍" class="headerlink" title="3.1.1介绍"></a>3.1.1介绍</h4><p>ROC全称是“受试者工作特征”(Receiver Operating Characteristic)曲线，它源于“二战”中用于敌机检测的雷达信号分析技术，二十世纪六七十年代开始被用于一些心理学、医学检测应用中，此后被引入机器学习领域，用来评判分类、检测结果的好坏。因此，ROC曲线是非常重要和常见的统计分析方法。</p><p>为了绘制ROC曲线，我们需要计算出两个重要量的值（</p><p><strong>TPR</strong>、<strong>FPR</strong>），分别以它们为横、纵坐标作图。其中的TP、FP、TN、FN来自于<strong>混淆矩阵</strong>，且TP+FP+TN+FN=样本总数。</p><script type="math/tex; mode=display">TPR=\frac{TP}{TP+FN}</script><script type="math/tex; mode=display">FPR=\frac{FP}{FP + TN}</script><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211023004557339.png" alt="image-20211023004557339"></p><h4 id="3-1-2画图流程"><a href="#3-1-2画图流程" class="headerlink" title="3.1.2画图流程"></a>3.1.2画图流程</h4><ol><li>给定m+个正例和m-个负例，根据学习器预测结果对样例进行排序</li><li>然后把分类阈值设为最大，即把所有样例均预测为反例，此时真正例率和假正例率均为0，在坐标(0,0)处标记一个点</li><li>将分类阈值依次设为每个样例的预测值，即依次将每个样例划分为正例，设前一个标记点坐标为(x,y)，当前若为真正例，则对应标记点的坐标为$\left ( x,y+\frac{1}{m^{+}} \right )$；当前若为假正例，则对应标记点的坐标为$\left ( x+\frac{1}{m^{-}},y \right )$</li><li>最后用线段连接相邻点</li></ol><h4 id="3-1-3-AUC分析"><a href="#3-1-3-AUC分析" class="headerlink" title="3.1.3 AUC分析"></a>3.1.3 AUC分析</h4><p>ROC曲线下方的面积也有着重要意义（英语：Area under the Curve of ROC (AUC ROC)），其意义是：</p><ul><li>因为是在1x1的方格里求面积，AUC必在0~1之间。</li><li>假设阈值以上是正例，以下是反例；</li><li>简单说：<strong>AUC值越大的分类器，正确率越高。</strong></li></ul><p>从AUC判断分类器（预测模型）优劣的标准：</p><ul><li>AUC = 1，是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数预测的场合，不存在完美分类器。</li><li>0.5 &lt; AUC &lt; 1，优于随机猜测。这个分类器（模型）妥善设置阈值的话，能有预测价值。</li><li>AUC = 0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。</li><li>AUC &lt; 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。</li></ul><p>假设ROC曲线由为{ ( x1,y1 ),⋯,( xN′,yN′ ) }的点按需连接而成且有x~1~=0,x~N’~=1，则AUC可估算为：</p><script type="math/tex; mode=display">AUC=\frac{1}{2} \sum_{j=1}^{N{}'-1} \left ( x_{j+1}-x_{j}  \right ) \left ( y_{j+1}+y_{j}  \right )</script><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211023143342594.png" alt="image-20211023143342594"></p><p>如图即为使用本次作业所提供数据绘制的ROC曲线。由于测试样例有限，所以仅能获得有限个（真正例率，假正例率）坐标对，无法产生光滑的ROC曲线；由此计算得到的AUC的值为0.9648，可以得知该模型的性能较优。</p><p>完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">draw_roc</span>(<span class="hljs-params">confidence_scores, data_labels</span>):</span><br>    <span class="hljs-comment">#真正率，假正率</span><br>    fpr, tpr, thresholds = roc_curve(data_labels, confidence_scores)<br>    plt.figure()<br>    plt.grid()<br>    plt.title(<span class="hljs-string">&#x27;ROC Curve&#x27;</span>)<br>    plt.xlabel(<span class="hljs-string">&#x27;FPR&#x27;</span>)<br>    plt.ylabel(<span class="hljs-string">&#x27;TPR&#x27;</span>)<br> <br>    <span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> auc<br>    auc=auc(fpr, tpr) <span class="hljs-comment">#AUC计算</span><br>    plt.plot(fpr,tpr,<span class="hljs-string">&#x27;k--&#x27;</span>, label = <span class="hljs-string">&#x27;roc_curve(AUC=%0.4f)&#x27;</span> % auc)<br>    plt.legend()<br>    plt.show()<br></code></pre></td></tr></table></figure><h3 id="3-2-PR曲线"><a href="#3-2-PR曲线" class="headerlink" title="3.2 PR曲线"></a>3.2 PR曲线</h3><h4 id="3-2-1介绍"><a href="#3-2-1介绍" class="headerlink" title="3.2.1介绍"></a>3.2.1介绍</h4><p>PR曲线全称为查准率-查全率曲线，查准率P与查全率R分别定义为：</p><script type="math/tex; mode=display">P=\frac{TP}{TP+FP}，R=\frac{TP}{TP+FN}</script><p>查准率和查全率是一对矛盾的度量。一般来说，查准率高时，查全率往往偏低；而查全率高时，查准率往往偏低。</p><h4 id="3-2-2画图流程"><a href="#3-2-2画图流程" class="headerlink" title="3.2.2画图流程"></a>3.2.2画图流程</h4><p>绘制PR曲线的流程与ROC曲线类似，我们需要根据学习器的预测结果按正例可能性大小对样例进行排序，再逐个样本的选择阈值，在该样本之前的都属于正例，该样本之后的都属于负例。每一个样本作为划分阈值时，都可以计算对应的precision和recall，那么就可以以此绘制曲线。</p><h4 id="3-2-3-AP分析"><a href="#3-2-3-AP分析" class="headerlink" title="3.2.3 AP分析"></a>3.2.3 AP分析</h4><p>其中平衡点是曲线上“查准率=查全率”时的取值，可用于度量PR曲线有交叉的分类器性能高低。与AUC类似，PR曲线下方面积也有重要意义。PR曲线下的面积称之为AP(Average Precision)，通常来说一个越好的分类器，AP值越高。</p><p>对于连续的PR曲线，有：</p><script type="math/tex; mode=display">AP=\int_{0}^{1} p\left ( r \right ) \mathrm{d}r</script><p>但由于曲线可能出现不可导的部分，故我们常常求其近似值：</p><script type="math/tex; mode=display">p_{\text {interp }}(r)=\max _{\tilde{r} \geq r} p(\tilde{r})</script><p>对于离散的PR曲线，有：</p><script type="math/tex; mode=display">\mathrm{AP}=\sum_{k=1}^{n} p(k) \Delta r(k)</script><p>另外PR曲线平衡点更用常用的是F1度量：</p><script type="math/tex; mode=display">F 1=\frac{2 \times P \times R}{P+R}=\frac{2 \times T P}{\text { 样例总数 }+T P-T N}</script><p>比F1度量更一般的形式是F~β~：</p><script type="math/tex; mode=display">F_{\beta}=\frac{\left(1+\beta^{2}\right) \times P \times R}{\left(\beta^{2} \times P\right)+R}</script><ul><li>β=1：标准F1</li><li>β&gt;1：偏重查全率（逃犯信息检索）</li><li>β&lt;1：偏重查准率（商品推荐系统）</li></ul><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211023150341520.png" alt="image-20211023150341520"></p><p>如图即为使用本次作业所提供数据绘制的PR曲线。在现实任务中，PR曲线是非单调、不平滑的，在很多局部有上下波动；由此计算得到的AP的值为0.9751，可以得知该模型的性能较优。</p><p>完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">draw_pr</span>(<span class="hljs-params">confidence_scores, data_labels</span>):</span><br>    plt.figure()<br>    plt.title(<span class="hljs-string">&#x27;PR Curve&#x27;</span>)<br>    plt.xlabel(<span class="hljs-string">&#x27;Recall&#x27;</span>)<br>    plt.ylabel(<span class="hljs-string">&#x27;Precision&#x27;</span>)<br>    plt.grid()<br> <br>    <span class="hljs-comment">#精确率，召回率，阈值</span><br>    precision,recall,thresholds = precision_recall_curve(data_labels,confidence_scores)<br> <br>    <span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> average_precision_score<br>    AP = average_precision_score(data_labels, confidence_scores) <span class="hljs-comment"># 计算AP</span><br>    plt.plot(recall, precision,<span class="hljs-string">&#x27;k--&#x27;</span>, label = <span class="hljs-string">&#x27;pr_curve(AP=%0.4f)&#x27;</span> % AP)<br>    plt.legend()<br>    plt.show()<br></code></pre></td></tr></table></figure><h2 id="四、总结模型训练过程中的收获"><a href="#四、总结模型训练过程中的收获" class="headerlink" title="四、总结模型训练过程中的收获"></a>四、总结模型训练过程中的收获</h2><h3 id="4-1加深了对逻辑斯蒂回归的理解"><a href="#4-1加深了对逻辑斯蒂回归的理解" class="headerlink" title="4.1加深了对逻辑斯蒂回归的理解"></a>4.1加深了对逻辑斯蒂回归的理解</h3><h4 id="4-1-1简述对模型的理解："><a href="#4-1-1简述对模型的理解：" class="headerlink" title="4.1.1简述对模型的理解："></a>4.1.1简述对模型的理解：</h4><p>因为线性回归模型产生的预测值是一系列实值。为了使得输出的预测结果变成分类所需的0和1，我们需要在线性回归的基础式子外再套一个函数将其输出变成0和1，又要求该函数单调可微，所以我们引入logistic函数，将输出的预测结果成功转为概率值。这样，逻辑斯蒂回归模型被成功应用于解决分类模型。</p><h4 id="4-1-2关于算法的择优："><a href="#4-1-2关于算法的择优：" class="headerlink" title="4.1.2关于算法的择优："></a>4.1.2关于算法的择优：</h4><p>在代码实现过程中，我们最开始使用的是梯度下降法，但是迭代速度较慢，拟合效果不是很好；之后我们选择了牛顿法，但是因为计算海森矩阵的复杂度太高，我们选择用一种拟牛顿法——‘L-BFGS’来逼近海森矩阵，最终达到了我们理想的效果。</p><p>梯度下降法和牛顿法/拟牛顿法相比，两者都是迭代求解，不过梯度下降法是梯度求解，而牛顿法/拟牛顿法是用二阶的海森矩阵的逆矩阵或伪逆矩阵求解。相对而言，使用牛顿法/拟牛顿法收敛更快。</p><h3 id="4-2实现了代码技能的提升"><a href="#4-2实现了代码技能的提升" class="headerlink" title="4.2实现了代码技能的提升"></a>4.2实现了代码技能的提升</h3><p>在代码实现过程中，我们调用了机器学习工具包sklearn中的重要函数——LogisticRegression函数，熟悉了它的常用参数及意义，下面以表格形式列出我们在此次模型训练中使用到的参数。</p><div class="table-container"><table><thead><tr><th>参数</th><th>意义</th><th>备注</th></tr></thead><tbody><tr><td>penalty</td><td>str类型，可选项有{‘L1’,‘L2’}，用来确定惩罚项的规范。‘newton-cg’，‘sag’和‘lbfgs’仅支持‘L2’惩罚项。</td><td>该参数是为了添加惩罚项，避免过拟合，用以提高函数的泛化能力。我们在本次模型训练中使用的是‘L2’。</td></tr><tr><td>solver</td><td>可选的优化算法有{‘newton-cg’，‘lbfgs’,‘liblinear’,‘sag’}</td><td>小数据集中，liblinear是一个好选择，sag和saga对大数据更快； 多分类问题中，除了liblinear其它四种算法都可以使用；newton-cg，lbfgs和sag仅能使用L2惩罚项；  我们经过对比，选择的算法是lbfgs。</td></tr><tr><td>multi_class</td><td>str类型，可选参数有{‘ovr’，‘multinomial’}  如果是二元分类问题则两个选项一样，如果是多元分类则ovr将进行多次二分类，分别为一类别和剩余其它所有类别;  multinomial则分别进行两两分类，需要T(T-1)/2次分类。</td><td>在多分类中，ovr快，精度低; multinomial慢，精度高。</td></tr></tbody></table></div><h3 id="4-3提高了公式推导和文章排版能力"><a href="#4-3提高了公式推导和文章排版能力" class="headerlink" title="4.3提高了公式推导和文章排版能力"></a>4.3提高了公式推导和文章排版能力</h3><p>报告中的所有公式，我们都脚踏实地，一步步手动推导，并学习使用latex将其手动输入并排版。在这个过程中，我们对算法中公式的来源更加清楚，对其原理理解更加深透。这提高了我们的公式推导能力和文章排版能力。</p><h3 id="4-4锻炼了小组合作精神，提高了小组合作能力"><a href="#4-4锻炼了小组合作精神，提高了小组合作能力" class="headerlink" title="4.4锻炼了小组合作精神，提高了小组合作能力"></a>4.4锻炼了小组合作精神，提高了小组合作能力</h3><p>在正式写报告之前，我们对本次作业任务以及对逻辑斯蒂回归模型的理解进行了讨论；然后为了加深彼此对知识的掌握程度，每个人都对代码进行了独立编写，在实现的过程中探讨互助；最后，我们根据彼此的优势项对任务进行了分工合作，齐心协力创作出了这份尽可能完善的报告。</p>]]></content>
    
    
    
    <tags>
      
      <tag>course</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>typora+picgo+gitee高效写作</title>
    <link href="/2021/10/20/picgo/"/>
    <url>/2021/10/20/picgo/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>解决hexo博客图片上传问题</p><span id="more"></span><p>Markdown 是一种轻量级标记语言，它允许人们使用易读易写的纯文本格式编写文档。</p><p>Markdown 语言在 2004 由约翰·格鲁伯（英语：John Gruber）创建。</p><p>Markdown 编写的文档可以导出 HTML 、Word、图像、PDF、Epub 等多种格式的文档。</p><p>Markdown 编写的文档后缀为 <strong>.md</strong>, <strong>.markdown</strong>。</p><hr><p><a href="https://typora.io/">Typora</a> gives you a seamless experience as both a reader and a writer. </p><p>It removes the preview window, mode switcher, syntax symbols of </p><p>markdown source code, and all other unnecessary distractions. Instead, </p><p>it provides a real live preview feature to help you concentrate on the content itself.</p><hr><p>懒得解释了，总之markdown真好用，typora yyds！</p><p>但是一直以来我都觉得写blog是个很痛苦的过程，除了因为我懒，就是每次都得一张一张上传图片，所以迟迟未更新。</p><p>直到我使用了typora+picgo+gitee这一组合。</p><hr><p><a href="https://github.com/PicGo/">Picgo</a>的GitHub页面上包含了<a href="https://github.com/PicGo/PicGo-Core">core</a>，<a href="https://github.com/PicGo/vs-picgo">vscode的扩展版</a>和各种各样<a href="https://github.com/PicGo/Awesome-PicGo">awesome的插件</a>。</p><p>不过我选择的是编译好的<a href="https://github.com/Molunerfinn/PicGo/releases">发行版程序</a>。</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211020213735806.png" alt="image-20211020213735806"></p><p>安装过程中没什么需要注意的，总之就是一路点下去。</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211020213944319.png" alt="image-20211020213944319"></p><p>打开后应该是一个这样的界面，我首先排除GitHub和Imgur了，毕竟不能保证每一位用户都科学上网。</p><p>接下来我根据网上的教程按顺序尝试了SMMS，七牛云，又拍云···</p><p>第一个配置好上传不了，后面俩配置得也很完美，什么域名绑定，cdn加速···结果本地图片显示，上传到博客上就显示不了了，原因不得而知。</p><p>最后我选择了免费，访问速度还快的Gitee。</p><p>首先在插件设置中搜索并下载：</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211020215002022.png" alt="image-20211020215002022"></p><p>如果没有Gitee账户，先进行<a href="https://gitee.com/">注册</a>。</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211020220550100.png" alt="image-20211020220550100"></p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211020220758789.png" alt="image-20211020220758789"></p><p>记好我红框出来的内容，然后到个人设置里生成私人令牌。</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211020221019061.png" alt="image-20211020221019061"></p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211020221038756.png" alt="image-20211020221038756"></p><p>描述由你决定，只需要选择图中两项，然后把令牌<strong>复制</strong>好备用。（关闭之后就不再明文显示了！）</p><p>最后在picgo中填好各项参数：</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211020221334908.png" alt="image-20211020221334908"></p><p>把gitee设置为默认图床之后至此已经可以使用Ctrl+C复制图片，Ctrl＋Shift＋P上传至图床，Ctrl+V粘贴到typora中。</p><p>为了简化操作，可以在typora的偏好设置中进行如下修改：</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211020222002514.png" alt="image-20211020222002514"></p><p>（顺便验证一下成功与否）</p><p>大功告成，从此写作可以完全抛弃图片的苦恼，一键上传，so easy~</p><p>另外推荐两个我常常使用的图片<a href="https://bigjpg.com/">无损放大</a>和<a href="https://www.picdiet.com/zh-cn">无损压缩</a>的网站。</p><p>虽然自动化上传之后我就不会去调整图片的大小了······</p>]]></content>
    
    
    
    <tags>
      
      <tag>course</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用搜索算法解决八数码问题</title>
    <link href="/2021/10/19/8puzzle/"/>
    <url>/2021/10/19/8puzzle/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>人工智能导论作业记录。</p><span id="more"></span><h2 id="一、问题描述与分析"><a href="#一、问题描述与分析" class="headerlink" title="一、问题描述与分析"></a>一、问题描述与分析</h2><p>八数码问题就是在一个大小为3×3的九宫格上,放置8块编号为1-8的木块，九宫格中有一个空格，周围(上下左右)的木块可以和空格交换位置。对于问题，给定一个初始状态，目标状态是期望达到1-8顺序排列的序列，并且空格在右下角，问题的实质就是寻找一个合法的移动序列。</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/20211020204607.png" alt=""></p><p>不是每一个给定的初始状态都存在解，在分析之前，引入线性代数中的几个概念：</p><ul><li>逆序数：在一个排列中，如果一对数字的前后位置与大小顺序相反，即前面的数大于后面的数，那么它们就称为一个逆序。一个排列中序的总数就称为这个排列的逆序数。</li><li>奇排列：逆序数为奇数的排列称为奇排列</li><li><p>偶排列：逆序数为偶数的排列称为偶排列</p><p>使用线性代数理论可以得知，对于任意目标状态，只有初始状态的逆序数和目标状态的逆序数的奇偶性相同才有解(逆序数计算不包括0的逆序数)。</p><p>证明：</p><p>∵八数码问题每一个步骤都可以视作 0 的移动， 0 的移动至多有四个可能的方向<br>又∵ 0 是序列中最小的数，序列的奇偶性不会跟随 0 的移动而改变<br>且对于其余数字而言，要么与 0 互换，要么跨过两个数字和 0 互换<br>∴逆序数的改变只有变化为 0、 -2、 +2 这三种情况<br>又∵奇数±偶数=奇数，偶数±偶数=偶数<br>∴序列在变换过程中，它的奇偶性不会发生改变<br>∴如果初始序列和目标序列不是同为奇排列或者偶排列，那么这个八数码问题就是无解的。</p></li></ul><p>以图中所给状态为例，初始状态的逆序数t=0+6+5+1+2+1+1=16，目标状态的逆序数t’=0，故有解。</p><h2 id="二、深度优先遍历搜索-DFS"><a href="#二、深度优先遍历搜索-DFS" class="headerlink" title="二、深度优先遍历搜索(DFS)"></a>二、深度优先遍历搜索(DFS)</h2><h3 id="2-1算法介绍"><a href="#2-1算法介绍" class="headerlink" title="2.1算法介绍"></a>2.1算法介绍</h3><p><strong>深度优先搜索算法</strong>（英语：Depth-First-Search，DFS）是一种用于遍历或搜索树或图的算法。这个算法会尽可能深的搜索树的分支。当节点v的所在边都己被探寻过，搜索将回溯到发现节点v的那条边的起始节点。这一过程一直进行到已发现从源节点可达的所有节点为止。如果还存在未被发现的节点，则选择其中一个作为源节点并重复以上过程，整个进程反复进行直到所有节点都被访问为止，属于盲目搜索。以下图为例，DFS方法首先从根节点1开始，其最终得到的遍历顺序是“1-2-3-4-5-6-7-8-9-10-11-12”。（假定左分枝和右分枝中优先选择左分枝）</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/20211020204640.png" alt=""></p><p>我们将其应用于八数码问题的解决。解八数码问题实际上就是找出从初始状态到达目标状态所经过的一系列中间过渡状态。前文提到DFS遍历的树是已经存在的，我们只需要按照规定的遍历方法就能完成遍历，而对于八数码问题，没有已经存在的路径供我们遍历，需要我们从初始状态向下延伸（也就是上下左右移动）才能构造出类似的树。</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/20211020204711.png" alt=""></p><p>以上图为例。在使用DFS进行搜索时，每个状态都会按照一定的顺序进行上下左右移动（在上图中是下、左、右、上的顺序），一次移动后会产生一个新的状态，然后以新状态为起点继续按约定的顺序（例如先向下）移动。终止的条件是找到解或者达到深度界限。那么如果按照图中下、左、右、上的顺序搜索后的结果将会是最左边的一条路一直是优先向下移动，如果不能向下则依次会是左、右、上的一种。</p><h3 id="2-2实验代码"><a href="#2-2实验代码" class="headerlink" title="2.2实验代码"></a>2.2实验代码</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//初始状态压入队列</span><br>    D_open.<span class="hljs-built_in">push</span>(<span class="hljs-keyword">new</span> <span class="hljs-built_in">borad</span>(<span class="hljs-literal">NULL</span>, start, <span class="hljs-number">0</span>, INT_MAX - <span class="hljs-number">1</span>));<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;DFS：\n&quot;</span>);<br>    <span class="hljs-keyword">while</span> (!D_open.<span class="hljs-built_in">empty</span>()) &#123;<br>        <span class="hljs-comment">//弹出一个状态</span><br>        borad *cur = D_open.<span class="hljs-built_in">top</span>();<br>           D_open.<span class="hljs-built_in">pop</span>();<br>        <span class="hljs-comment">//if (cur-&gt;depth == 5) &#123;</span><br>        <span class="hljs-comment">//    break;</span><br>        <span class="hljs-comment">//&#125;</span><br>        <span class="hljs-comment">//与目标状态的距离，为0即到达目标状态</span><br>        <span class="hljs-keyword">if</span> (<span class="hljs-built_in">hn</span>(cur-&gt;status, target) == <span class="hljs-number">0</span>) &#123;<br>            <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;到达目标状态\nclose表大小为%d\n目标状态深度为%d\n\n&quot;</span>, close.<span class="hljs-built_in">size</span>(), cur-&gt;depth);<br>            <span class="hljs-comment">//printans(cur);</span><br>            <span class="hljs-keyword">break</span>;<br>        &#125;<br>        <span class="hljs-comment">//存放int格式的状态</span><br>        <span class="hljs-keyword">int</span> intstatus = <span class="hljs-built_in">status2int</span>(cur-&gt;status);<br>        <span class="hljs-comment">//出现重复状态</span><br>        <span class="hljs-keyword">if</span> (close.<span class="hljs-built_in">count</span>(intstatus)) &#123;<br>            <span class="hljs-keyword">continue</span>;<br>        &#125;<br>        <span class="hljs-comment">//加入close表，表示已访问过</span><br>        close.<span class="hljs-built_in">insert</span>(intstatus);<br><br>        <span class="hljs-comment">//获得0的坐标</span><br>        <span class="hljs-keyword">int</span> zeroindex = <span class="hljs-built_in">getindex</span>(cur-&gt;status, <span class="hljs-number">0</span>);<br>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">4</span>; i++) &#123;<br>            <span class="hljs-comment">//新建节点，复制当前棋盘状态，深度+1</span><br>            borad *temp = <span class="hljs-keyword">new</span> <span class="hljs-built_in">borad</span>(cur, cur-&gt;status, cur-&gt;depth + <span class="hljs-number">1</span>, INT_MAX - <span class="hljs-number">1</span>);<br>            <span class="hljs-comment">//0向四个方向移动</span><br>            <span class="hljs-keyword">if</span> (<span class="hljs-built_in">swapnum</span>(zeroindex, zeroindex + go[i], temp-&gt;status)) &#123;<br>                <span class="hljs-comment">//移动成功</span><br>                D_open.<span class="hljs-built_in">push</span>(temp);<br>            &#125;<br>            <span class="hljs-keyword">else</span> &#123;<br>                <span class="hljs-comment">//移动失败</span><br>                <span class="hljs-built_in"><span class="hljs-keyword">delete</span></span>(temp);<br>            &#125;<br>        &#125;<br>    &#125;<br></code></pre></td></tr></table></figure><h3 id="2-3实验结果"><a href="#2-3实验结果" class="headerlink" title="2.3实验结果"></a>2.3实验结果</h3><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/20211020204805.png" alt=""></p><p>如图所示，深度优先算法在解决八数码问题时有一个致命缺点，就是必须设置一个深度界限，否则，搜索会一直沿着纵深方向发展，会一直无法搜索到解路径。即使加了限制条件，搜索到了解路径，解路径也不一定是最优解路径。</p><h3 id="2-4实验总结"><a href="#2-4实验总结" class="headerlink" title="2.4实验总结"></a>2.4实验总结</h3><ul><li>缺点：如果目标节点不在搜索进入的分支上，而该分支又是一个无穷分支,就得不到解,因此该算法是不完备的。</li><li>优点：如果目标节点在搜索进入的分支上，则可以较快得到解。</li></ul><h2 id="三、广度优先遍历搜索-BFS"><a href="#三、广度优先遍历搜索-BFS" class="headerlink" title="三、广度优先遍历搜索(BFS)"></a>三、广度优先遍历搜索(BFS)</h2><h3 id="3-1算法介绍"><a href="#3-1算法介绍" class="headerlink" title="3.1算法介绍"></a>3.1算法介绍</h3><p><strong>广度优先搜索算法</strong>（英语：Breadth-First-Search，缩写为BFS），又译作宽度优先搜索，或横向优先搜索，是一种图形搜索算法。简单的说，BFS是从根节点开始，沿着树的宽度遍历树的节点。如果所有节点均被访问，则算法中止。BFS是一种盲目搜索法，目的是系统地展开并检查图中的所有节点，以找寻结果。</p><p>BFS会先访问根节点的所有邻居节点，然后再依次访问邻居节点的邻居节点，直到所有节点都访问完毕。在具体的实现中，使用open和closed两个表，open是一个队列，每次对open进行一次出队操作（并放入closed中），并将其邻居节点进行入队操作。直到队列为空时即完成了所有节点的遍历。closed表在遍历树时其实没有用，因为子节点只能从父节点到达。但在进行图的遍历时，一个节点可能会由多个节点到达，所以此时为了防止重复遍历应该每次都检查下一个节点是否已经在closed中了。 依旧以下图为例，BFS方法首先从根节点1开始，其最终得到的遍历顺序是“1-2-7-8-3-6-9-12-4-5-10-11”。可以看出来BFS进行遍历时是一层一层的搜索的。</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/20211020204640.png" alt=""></p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/20211020204722.png" alt=""></p><p>在应用BFS算法进行八数码问题搜索时需要open和closed两个表。首先将初始状态加入open队列，然后进行出队操作并放入closed中，对出队的状态进行扩展（所谓扩展也就是找出其上下左右移动后的状态），将扩展出的状态加入队列，然后继续循环出队-扩展-入队的操作，直到找到解为止。在上图这个例子中，红圈里的数字是遍历顺序。当找到解时一直往前找父节点即可找出求解的移动路线。</p><h3 id="3-2实验代码"><a href="#3-2实验代码" class="headerlink" title="3.2实验代码"></a>3.2实验代码</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//初始状态压入队列</span><br>    B_open.<span class="hljs-built_in">push</span>(<span class="hljs-keyword">new</span> <span class="hljs-built_in">borad</span>(<span class="hljs-literal">NULL</span>, start, <span class="hljs-number">0</span>, INT_MAX - <span class="hljs-number">1</span>));<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;BFS：\n&quot;</span>);<br>    <span class="hljs-keyword">while</span> (!B_open.<span class="hljs-built_in">empty</span>()) &#123;<br>        <span class="hljs-comment">//弹出一个状态</span><br>        borad* cur = B_open.<span class="hljs-built_in">front</span>();<br>        B_open.<span class="hljs-built_in">pop</span>();<br>        <span class="hljs-comment">//与目标状态的距离，为0即到达目标状态</span><br>        <span class="hljs-keyword">if</span> (<span class="hljs-built_in">hn</span>(cur-&gt;status, target) == <span class="hljs-number">0</span>) &#123;<br>            <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;到达目标状态\nclose表大小为%d\n目标状态深度为%d\n\n&quot;</span>, close.<span class="hljs-built_in">size</span>(), cur-&gt;depth);<br>            <span class="hljs-comment">//printans(cur);</span><br>            <span class="hljs-keyword">break</span>;<br>        &#125;<br>        <span class="hljs-comment">//存放int格式的状态</span><br>        <span class="hljs-keyword">int</span> intstatus = <span class="hljs-built_in">status2int</span>(cur-&gt;status);<br>        <span class="hljs-comment">//出现重复状态</span><br>        <span class="hljs-keyword">if</span> (close.<span class="hljs-built_in">count</span>(intstatus)) &#123;<br>            <span class="hljs-keyword">continue</span>;<br>        &#125;<br>        <span class="hljs-comment">//加入close表，表示已访问过</span><br>        close.<span class="hljs-built_in">insert</span>(intstatus);<br><br>        <span class="hljs-comment">//获得0的坐标</span><br>        <span class="hljs-keyword">int</span> zeroindex = <span class="hljs-built_in">getindex</span>(cur-&gt;status, <span class="hljs-number">0</span>);<br>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">4</span>; i++) &#123;<br>            <span class="hljs-comment">//新建节点，复制当前棋盘状态，深度+1</span><br>            borad* temp = <span class="hljs-keyword">new</span> <span class="hljs-built_in">borad</span>(cur, cur-&gt;status, cur-&gt;depth + <span class="hljs-number">1</span>, INT_MAX - <span class="hljs-number">1</span>);<br>            <span class="hljs-comment">//0向四个方向移动</span><br>            <span class="hljs-keyword">if</span> (<span class="hljs-built_in">swapnum</span>(zeroindex, zeroindex + go[i], temp-&gt;status)) &#123;<br>                <span class="hljs-comment">//移动成功</span><br>                B_open.<span class="hljs-built_in">push</span>(temp);<br>            &#125;<br>            <span class="hljs-keyword">else</span> &#123;<br>                <span class="hljs-comment">//移动失败</span><br>                <span class="hljs-built_in"><span class="hljs-keyword">delete</span></span>(temp);<br>            &#125;<br>        &#125;<br>    &#125;<br>    <span class="hljs-comment">//清空close表</span><br>    close.<span class="hljs-built_in">clear</span>();<br></code></pre></td></tr></table></figure><h3 id="3-3实验结果"><a href="#3-3实验结果" class="headerlink" title="3.3实验结果"></a>3.3实验结果</h3><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/20211020204754.png" alt=""></p><p>如图所示，广度优先算法成功找到了深度为22的最优解，但是close表是DFS中深度46312产生的大小为47788的close表的两倍多，由于𝐵𝐹𝑆算法进行的是盲目的搜索，没有考虑代价，而且利用了空间换取时间的策略，故空间也相对会有更大的复杂度。</p><h3 id="3-4实验总结"><a href="#3-4实验总结" class="headerlink" title="3.4实验总结"></a>3.4实验总结</h3><ul><li>缺点：当目标节点距离初始节点较远时会产生许多无用的节点，搜索效率低，只能适用于到达目标结点步数较少的情况。</li><li>优点：只要问题有解，则总可以得到解，而且是最短路径的解。</li></ul><h2 id="四、A-算法实现八数码问题"><a href="#四、A-算法实现八数码问题" class="headerlink" title="四、A*算法实现八数码问题"></a>四、A*算法实现八数码问题</h2><h3 id="4-1算法介绍"><a href="#4-1算法介绍" class="headerlink" title="4.1算法介绍"></a>4.1算法介绍</h3><p><strong>A*搜索算法</strong>（A* search algorithm）是一种在图形平面上，有多个节点的路径，求出最低通过成本的算法，也是许多其他问题的常用启发式算法。该算法综合了最良优先搜索和Dijkstra算法的优点：在进行启发式搜索提高算法效率的同时，可以保证找到一条最优路径（基于评估函数）。</p><p>在A*算法中，一个结点位置的好坏用估价函数来对它进行评估：</p><script type="math/tex; mode=display">f{}'\left ( n \right )=g{}'\left ( n \right )+h{}'\left ( n \right )</script><p>这里，f’(n)是估价函数，g’(n)是起点到终点的最短路径值(也称为最小耗费或最小代价)，h’(n)是n到目标的最短路经的启发值。由于这个f’(n)其实是无法预先知道的，因而实际上使用的是如下估价函数：</p><script type="math/tex; mode=display">f\left ( n \right )=g\left ( n \right )+h\left ( n \right )</script><p>这个公式遵循以下特性：</p><ul><li>如果g(n)为0，即只计算任意顶点n到目标的评估函数h(n)，而不计算起点到顶点n的距离，则算法转化为使用贪心策略的最良优先搜索，速度最快，但可能得不出最优解；</li><li>如果h(n)不大于顶点n到目标顶点的实际距离，则一定可以求出最优解，而且h(n)越小，需要计算的节点越多，算法效率越低，常见的评估函数有——欧几里得距离、曼哈顿距离、切比雪夫距离；</li><li>如果h(n)为0，即只需求出起点到任意顶点n的最短路径g(n)，而不计算任何评估函数h(n)，则转化为单源最短路径问题，即Dijkstra算法，此时需要计算最多的顶点；</li></ul><p>其中，g(n)是从初始结点到节点n的实际代价，h(n)是从结点n到目标结点的最佳路径的估计代价。在这里主要是h(n)体现了搜索的启发信息，因为g(n)是已知的。用f(n)作为f’(n)的近似，也即用g(n)代替g’(n)，h(n)代替h’(n)。这样必须满足两个条件：</p><ol><li>g(n)≥g’(n)(大多数情况下都是满足的，可以不用考虑)，且f必须保持单调递增；</li><li>h必须小于等于实际的从当前节点到达目标节点的最小耗费h(n)≤h’(n)。（可以证明应用这样的估价函数可以找到最短路径）</li></ol><h3 id="4-2实验代码"><a href="#4-2实验代码" class="headerlink" title="4.2实验代码"></a>4.2实验代码</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//初始状态压入队列</span><br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;A* Fn=Gn+Hn：\n&quot;</span>);<br>    <span class="hljs-keyword">while</span> (!A_open.<span class="hljs-built_in">empty</span>()) &#123;<br>        <span class="hljs-comment">//弹出一个状态</span><br>        borad* cur = A_open.<span class="hljs-built_in">top</span>();<br>        A_open.<span class="hljs-built_in">pop</span>();<br>        <span class="hljs-comment">//hn=Fn-depth为与目标状态的曼哈顿距离，为0即到达目标状态</span><br>        <span class="hljs-keyword">if</span> (cur-&gt;Fn - cur-&gt;depth == <span class="hljs-number">0</span>) &#123;<br>            <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;到达目标状态\nclose表大小为%d\n目标状态深度为%d\n\n&quot;</span>, close.<span class="hljs-built_in">size</span>(), cur-&gt;depth);<br>            <span class="hljs-comment">//printans(cur);</span><br>            <span class="hljs-keyword">break</span>;<br>        &#125;<br>        <span class="hljs-comment">//存放int格式的状态</span><br>        <span class="hljs-keyword">int</span> intstatus = <span class="hljs-built_in">status2int</span>(cur-&gt;status);<br>        <span class="hljs-comment">//出现重复状态</span><br>        <span class="hljs-keyword">if</span> (close.<span class="hljs-built_in">count</span>(intstatus)) &#123;<br>            <span class="hljs-keyword">continue</span>;<br>        &#125;<br>        <span class="hljs-comment">//加入close表，表示已访问过</span><br>        close.<span class="hljs-built_in">insert</span>(intstatus);<br>        <span class="hljs-comment">//获得0的坐标</span><br>        <span class="hljs-keyword">int</span> zeroindex = <span class="hljs-built_in">getindex</span>(cur-&gt;status, <span class="hljs-number">0</span>);<br>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">4</span>; i++) &#123;<br>            <span class="hljs-comment">//新建节点，复制当前棋盘状态，深度+1</span><br>            borad* temp = <span class="hljs-keyword">new</span> <span class="hljs-built_in">borad</span>(cur, cur-&gt;status, cur-&gt;depth + <span class="hljs-number">1</span>, <span class="hljs-number">0</span>);<br>            <span class="hljs-comment">//0向四个方向移动</span><br>            <span class="hljs-keyword">if</span> (<span class="hljs-built_in">swapnum</span>(zeroindex, zeroindex + go[i], temp-&gt;status)) &#123;<br>                <span class="hljs-comment">//移动成功</span><br>                <span class="hljs-comment">//计算启发函数值，并更新节点</span><br>                temp-&gt;Fn = temp-&gt;depth + <span class="hljs-built_in">hn</span>(temp-&gt;status, target);<br>                <span class="hljs-comment">//加入A_open表</span><br>                A_open.<span class="hljs-built_in">push</span>(temp);<br>            &#125;<br>            <span class="hljs-keyword">else</span> &#123;<br>                <span class="hljs-comment">//移动失败</span><br>                <span class="hljs-built_in"><span class="hljs-keyword">delete</span></span>(temp);<br>            &#125;<br>        &#125;<br>    &#125;<br>    <span class="hljs-comment">//清空close表</span><br>    close.<span class="hljs-built_in">clear</span>();<br></code></pre></td></tr></table></figure><h3 id="4-3实验结果"><a href="#4-3实验结果" class="headerlink" title="4.3实验结果"></a>4.3实验结果</h3><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/20211020204738.png" alt=""></p><p>如图所示，A*搜索算法在解决八数码问题时取得了最优的结果，无论是时间复杂度还是空间复杂度都得到了极大的优化。但是𝐴∗算法作为一种预测算法，不能保证解为最优解。</p><h3 id="4-4实验总结"><a href="#4-4实验总结" class="headerlink" title="4.4实验总结"></a>4.4实验总结</h3><ul><li>优点：A*算法在绝大多数的情况下，在性能方面都远远优与DFS和BFS。算法的主要运行性能，取决于估价函数f的选取。</li><li>缺点：由于算法本身的特点，因此根据估价函数找到的解路径不一定是最优解路径。</li></ul><h2 id="五、效率比较及优缺点"><a href="#五、效率比较及优缺点" class="headerlink" title="五、效率比较及优缺点"></a>五、效率比较及优缺点</h2><h3 id="5-1概念"><a href="#5-1概念" class="headerlink" title="5.1概念"></a>5.1概念</h3><p>首先给出几个用来进行效率比价的变量：</p><ol><li>深度(D)：从初始节点到达目标的路径深度；</li><li>时间(T)：搜索程序运行的时间,单位毫秒(ms)；</li><li>状态数(N)：整个过程中生成的状态总数；</li><li><p>外显率(P)：搜索工程中,从初始节点向目标节点进行搜索的区域的宽度。</p><p>其中时间使用C标准库函数 clock_t clock(void) 计算获得，返回三个算法程序执行起，处理器时钟所使用的时间。为了获取 CPU 所使用的秒数，必须除以 CLOCKS_PER_SEC。而外显率定义为以下公式计算获得：</p><script type="math/tex; mode=display">P=\frac{D}{N},P\in \left( 0,1\right]</script></li></ol><h3 id="5-2-实验数据分析"><a href="#5-2-实验数据分析" class="headerlink" title="5.2 实验数据分析"></a>5.2 实验数据分析</h3><p>数据说明：</p><ol><li>环境为Windows系统，语言为C++，使用clock()函数输出算法时间；</li><li>目标状态1 2 3 4 5 6 7 8 0；</li><li>由于运行时间受电脑影响很大，具有一定的随机性，因而每个状态执行3次,取平均数作为最终结果时间。</li></ol><p>以下为题目所给初始状态产生的数据：</p><div class="table-container"><table><thead><tr><th></th><th>深度D</th><th>时间T</th><th>状态数N</th><th>外显率P</th></tr></thead><tbody><tr><td>DFS</td><td>46312</td><td>0.295000</td><td>47788</td><td>0.969113</td></tr><tr><td>BFS</td><td>22</td><td>0.793000</td><td>102868</td><td>0.000213</td></tr><tr><td>A*</td><td>22</td><td>0.005000</td><td>503</td><td>0.043737</td></tr></tbody></table></div><p>以下为随机初始状态产生的数据：</p><div class="table-container"><table><thead><tr><th>状态数N</th><th>DFS</th><th>BFS</th><th>A*</th></tr></thead><tbody><tr><td>1</td><td>37809</td><td>60897</td><td>1114</td></tr><tr><td>2</td><td>13571</td><td>129921</td><td>1289</td></tr><tr><td>3</td><td>39006</td><td>36948</td><td>926</td></tr><tr><td>4</td><td>56982</td><td>38459</td><td>182</td></tr><tr><td>5</td><td>101524</td><td>23754</td><td>610</td></tr><tr><td>6</td><td>62529</td><td>85828</td><td>1175</td></tr><tr><td>7</td><td>119230</td><td>43684</td><td>750</td></tr><tr><td>8</td><td>72091</td><td>129811</td><td>2492</td></tr><tr><td>9</td><td>68716</td><td>40819</td><td>393</td></tr><tr><td>10</td><td>128887</td><td>159858</td><td>6852</td></tr></tbody></table></div><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/20211020204834.png" alt=""></p><div class="table-container"><table><thead><tr><th>深度D</th><th>DFS</th><th>BFS</th><th>A*</th></tr></thead><tbody><tr><td>1</td><td>36756</td><td>20</td><td>20</td></tr><tr><td>2</td><td>13268</td><td>24</td><td>24</td></tr><tr><td>3</td><td>37943</td><td>19</td><td>19</td></tr><tr><td>4</td><td>55007</td><td>19</td><td>19</td></tr><tr><td>5</td><td>95102</td><td>18</td><td>20</td></tr><tr><td>6</td><td>60172</td><td>22</td><td>22</td></tr><tr><td>7</td><td>108378</td><td>20</td><td>20</td></tr><tr><td>8</td><td>69118</td><td>24</td><td>24</td></tr><tr><td>9</td><td>66096</td><td>20</td><td>20</td></tr><tr><td>10</td><td>113307</td><td>25</td><td>27</td></tr></tbody></table></div><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/20211020204843.png" alt=""></p><div class="table-container"><table><thead><tr><th>时间T</th><th>DFS</th><th>BFS</th><th>A*</th></tr></thead><tbody><tr><td>1</td><td>0.226</td><td>0.444</td><td>0.011</td></tr><tr><td>2</td><td>0.079</td><td>1.026</td><td>0.012</td></tr><tr><td>3</td><td>0.231</td><td>0.277</td><td>0.01</td></tr><tr><td>4</td><td>0.347</td><td>0.291</td><td>0.002</td></tr><tr><td>5</td><td>0.675</td><td>0.176</td><td>0.006</td></tr><tr><td>6</td><td>0.372</td><td>0.665</td><td>0.011</td></tr><tr><td>7</td><td>0.749</td><td>0.321</td><td>0.007</td></tr><tr><td>8</td><td>0.429</td><td>0.997</td><td>0.024</td></tr><tr><td>9</td><td>0.439</td><td>0.302</td><td>0.003</td></tr><tr><td>10</td><td>0.796</td><td>1.367</td><td>0.068</td></tr></tbody></table></div><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/20211020204853.png" alt=""></p><div class="table-container"><table><thead><tr><th>外显率P</th><th>DFS</th><th>BFS</th><th>A*</th></tr></thead><tbody><tr><td>1</td><td>0.000972</td><td>0.000328</td><td>0.017953</td></tr><tr><td>2</td><td>0.000978</td><td>0.000185</td><td>0.018619</td></tr><tr><td>3</td><td>0.000973</td><td>0.000514</td><td>0.020518</td></tr><tr><td>4</td><td>0.000965</td><td>0.000494</td><td>0.104396</td></tr><tr><td>5</td><td>0.000937</td><td>0.000758</td><td>0.032787</td></tr><tr><td>6</td><td>0.000962</td><td>0.000256</td><td>0.018723</td></tr><tr><td>7</td><td>0.000909</td><td>0.000458</td><td>0.026667</td></tr><tr><td>8</td><td>0.000959</td><td>0.000185</td><td>0.009631</td></tr><tr><td>9</td><td>0.000962</td><td>0.00049</td><td>0.050891</td></tr><tr><td>10</td><td>0.000879</td><td>0.000156</td><td>0.00394</td></tr></tbody></table></div><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/20211020204901.png" alt=""></p><h3 id="5-3研究结论"><a href="#5-3研究结论" class="headerlink" title="5.3研究结论"></a>5.3研究结论</h3><p>通过研究，可得结论如下：</p><ol><li>DFS搜索效率受深度影响很大，由于深度界限设置得很大，故搜索结点冗余多、速度慢；</li><li>BFS找到的一定是最优解，但是在算法效率上，不一定比DFS好，且远远不如A*算法，同时BFS在搜索深度较深时，产生的冗余结点较多；</li><li>A*算法在效率上相对最优，时间和空间上都比DFS和BFS更优，但缺点是，找到的解不一定是最优解。</li></ol><h2 id="六、参考文献"><a href="#六、参考文献" class="headerlink" title="六、参考文献"></a>六、参考文献</h2><p>[1]付宏杰,王雪莹,周健,周孙静,朱珠,张俊余.八数码问题解法效率比较及改进研究[J].软件导刊,2016,15(09):41-45.</p><p>[2]StuartJ.Russell,PeterNorvig. 人工智能:一种现代的方法(第3版)[J]. 计算机教育, 2011(15):68-68.</p><p>[3]Thomas,H.Cormen,Charles,E.Leiserson,Ronald,L.Rivest,Clifford,Stein,殷建平,徐云,王刚,刘晓光,苏明,邹恒明,王宏志. 算法导论(原书第3版)[J]. 计算机教育(10期):51-51.</p><h2 id="七、完整代码"><a href="#七、完整代码" class="headerlink" title="七、完整代码"></a>七、完整代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;queue&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;stack&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;unordered_set&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;unordered_map&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;string&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;cstdlib&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;ctime&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;time.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;math.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;climits&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><br><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">borad</span> &#123;</span><br>    <span class="hljs-keyword">int</span> status[<span class="hljs-number">9</span>];<span class="hljs-comment">//status[0]到status[8]表示3X3的矩阵，0表示空格</span><br>    <span class="hljs-keyword">int</span> depth;<span class="hljs-comment">//深度</span><br>    <span class="hljs-keyword">int</span> Fn;<span class="hljs-comment">//启发函数值，Fn = depth + hn即深度加曼哈顿距离</span><br>    borad* pre;<span class="hljs-comment">//父指针，指向移动前的棋盘状态</span><br>    <span class="hljs-built_in">borad</span>() : <span class="hljs-built_in">pre</span>(<span class="hljs-number">0</span>), <span class="hljs-built_in">status</span>(), <span class="hljs-built_in">depth</span>(<span class="hljs-number">0</span>), <span class="hljs-built_in">Fn</span>(INT_MAX - <span class="hljs-number">1</span>) &#123;<br>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> j = <span class="hljs-number">0</span>; j &lt; <span class="hljs-number">9</span>; j++) &#123;<br>            status[j] = j;<br>        &#125;<br>    &#125;<br>    <span class="hljs-built_in">borad</span>(borad* x, <span class="hljs-keyword">int</span> i[<span class="hljs-number">9</span>], <span class="hljs-keyword">int</span> y, <span class="hljs-keyword">int</span> z) : <span class="hljs-built_in">pre</span>(x), <span class="hljs-built_in">depth</span>(y), <span class="hljs-built_in">Fn</span>(z) &#123;<br>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> j = <span class="hljs-number">0</span>; j &lt; <span class="hljs-number">9</span>; j++) &#123;<br>            status[j] = i[j];<br>        &#125;<br>    &#125;<br>&#125;;<br><br><span class="hljs-comment">//优先队列自定义排序规则，升序</span><br><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">cmp</span> &#123;</span><br>    <span class="hljs-function"><span class="hljs-keyword">bool</span> <span class="hljs-title">operator</span><span class="hljs-params">()</span> <span class="hljs-params">(<span class="hljs-keyword">const</span> borad* a, <span class="hljs-keyword">const</span> borad* b)</span> </span>&#123;<br>        <span class="hljs-keyword">return</span> a-&gt;Fn &gt; b-&gt;Fn;<br>    &#125;<br>&#125;;<br><br><span class="hljs-function"><span class="hljs-keyword">bool</span> <span class="hljs-title">swapnum</span><span class="hljs-params">(<span class="hljs-keyword">int</span> a, <span class="hljs-keyword">int</span> b, <span class="hljs-keyword">int</span>* status)</span></span>;<span class="hljs-comment">//交换元素</span><br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">getindex</span><span class="hljs-params">(<span class="hljs-keyword">int</span>* status, <span class="hljs-keyword">int</span> num)</span></span>;<span class="hljs-comment">//获得元素在棋盘上的一维坐标</span><br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-keyword">int</span>* status)</span></span>;<span class="hljs-comment">//打印棋盘</span><br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">hn</span><span class="hljs-params">(<span class="hljs-keyword">int</span>* status, <span class="hljs-keyword">int</span>* target)</span></span>;<span class="hljs-comment">//当前状态与目标状态的曼哈顿距离</span><br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">printans</span><span class="hljs-params">(borad* cur)</span></span>;<span class="hljs-comment">//打印解法，回溯</span><br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">status2int</span><span class="hljs-params">(<span class="hljs-keyword">int</span>* status)</span></span>;<span class="hljs-comment">//棋盘状态转为int格式</span><br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">reversesum</span><span class="hljs-params">(<span class="hljs-keyword">int</span>* status)</span></span>;<span class="hljs-comment">//计算逆序数之和</span><br><span class="hljs-function"><span class="hljs-keyword">int</span>* <span class="hljs-title">randstatus</span><span class="hljs-params">(<span class="hljs-keyword">int</span>* target)</span></span>;<span class="hljs-comment">//获得随机初始状态</span><br><br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>    <span class="hljs-keyword">clock_t</span> <span class="hljs-keyword">start_t</span>, <span class="hljs-keyword">end_t</span>;<br>    <span class="hljs-keyword">double</span> <span class="hljs-keyword">total_t</span>;<br>    <span class="hljs-keyword">int</span> go[<span class="hljs-number">4</span>] = &#123; <span class="hljs-number">-1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">-3</span>,<span class="hljs-number">3</span> &#125;;<span class="hljs-comment">//四个移动方向</span><br>    <span class="hljs-keyword">int</span> start[<span class="hljs-number">9</span>] = &#123; <span class="hljs-number">1</span>,<span class="hljs-number">8</span>,<span class="hljs-number">7</span>,<span class="hljs-number">3</span>,<span class="hljs-number">0</span>,<span class="hljs-number">5</span>,<span class="hljs-number">4</span>,<span class="hljs-number">6</span>,<span class="hljs-number">2</span> &#125;;<span class="hljs-comment">//初始状态</span><br>    <span class="hljs-keyword">int</span> target[<span class="hljs-number">9</span>] = &#123; <span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>,<span class="hljs-number">7</span>,<span class="hljs-number">8</span>,<span class="hljs-number">0</span> &#125;;<span class="hljs-comment">//目标状态</span><br>    <span class="hljs-comment">//int* start;//随机初始状态</span><br>    <span class="hljs-comment">//生成随机初始状态</span><br>    <span class="hljs-comment">//start = randstatus(target);</span><br>    stack&lt;borad*&gt; D_open;<span class="hljs-comment">//DFS的open表，使用栈，深度大的在表头</span><br>    queue&lt;borad*&gt; B_open;<span class="hljs-comment">//BFS的open表，使用队列，深度小的在表头</span><br>    priority_queue&lt;borad*, vector&lt;borad*&gt;, cmp&gt; A_open;<span class="hljs-comment">//A*的open表，使用优先队列，启发函数值小的元素在表头</span><br>    unordered_set&lt;<span class="hljs-keyword">int</span>&gt; close;<span class="hljs-comment">//close表，存放已访问过的状态，元素为状态的int格式</span><br>    <span class="hljs-comment">//例：&#123; 1,2,3,8,0,4,7,6,5 &#125;==》123804765(int)</span><br>    <span class="hljs-comment">//&#123; 0,1,3,8,2,4,7,6,5 &#125;==》13824765(int)</span><br><br><br>    A_open.<span class="hljs-built_in">push</span>(<span class="hljs-keyword">new</span> <span class="hljs-built_in">borad</span>(<span class="hljs-literal">NULL</span>, start, <span class="hljs-number">0</span>, INT_MAX - <span class="hljs-number">1</span>));<br>    borad* temp = A_open.<span class="hljs-built_in">top</span>();<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;初始状态：&quot;</span>);<br>    <span class="hljs-built_in">print</span>(temp-&gt;status);<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;目标状态：&quot;</span>);<br>    <span class="hljs-built_in">print</span>(target);<br><br>    <span class="hljs-keyword">start_t</span> = <span class="hljs-built_in">clock</span>();<br>    <span class="hljs-comment">//--------------------------------------------start-A*-------- Fn=Gn+Hn -----------------------------//</span><br>    <span class="hljs-comment">//初始状态压入队列</span><br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;A* Fn=Gn+Hn：\n&quot;</span>);<br>    <span class="hljs-keyword">while</span> (!A_open.<span class="hljs-built_in">empty</span>()) &#123;<br>        <span class="hljs-comment">//弹出一个状态</span><br>        borad* cur = A_open.<span class="hljs-built_in">top</span>();<br>        A_open.<span class="hljs-built_in">pop</span>();<br>        <span class="hljs-comment">//hn=Fn-depth为与目标状态的曼哈顿距离，为0即到达目标状态</span><br>        <span class="hljs-keyword">if</span> (cur-&gt;Fn - cur-&gt;depth == <span class="hljs-number">0</span>) &#123;<br>            <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;到达目标状态\nclose表大小为%ld\n目标状态深度为%d\n&quot;</span>, close.<span class="hljs-built_in">size</span>(), cur-&gt;depth);<br>            <span class="hljs-comment">//printans(cur);</span><br>            <span class="hljs-keyword">break</span>;<br>        &#125;<br>        <span class="hljs-comment">//存放int格式的状态</span><br>        <span class="hljs-keyword">int</span> intstatus = <span class="hljs-built_in">status2int</span>(cur-&gt;status);<br>        <span class="hljs-comment">//出现重复状态</span><br>        <span class="hljs-keyword">if</span> (close.<span class="hljs-built_in">count</span>(intstatus)) &#123;<br>            <span class="hljs-keyword">continue</span>;<br>        &#125;<br>        <span class="hljs-comment">//加入close表，表示已访问过</span><br>        close.<span class="hljs-built_in">insert</span>(intstatus);<br>        <span class="hljs-comment">//获得0的坐标</span><br>        <span class="hljs-keyword">int</span> zeroindex = <span class="hljs-built_in">getindex</span>(cur-&gt;status, <span class="hljs-number">0</span>);<br>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">4</span>; i++) &#123;<br>            <span class="hljs-comment">//新建节点，复制当前棋盘状态，深度+1</span><br>            borad* temp = <span class="hljs-keyword">new</span> <span class="hljs-built_in">borad</span>(cur, cur-&gt;status, cur-&gt;depth + <span class="hljs-number">1</span>, <span class="hljs-number">0</span>);<br>            <span class="hljs-comment">//0向四个方向移动</span><br>            <span class="hljs-keyword">if</span> (<span class="hljs-built_in">swapnum</span>(zeroindex, zeroindex + go[i], temp-&gt;status)) &#123;<br>                <span class="hljs-comment">//移动成功</span><br>                <span class="hljs-comment">//计算启发函数值，并更新节点</span><br>                temp-&gt;Fn = temp-&gt;depth + <span class="hljs-built_in">hn</span>(temp-&gt;status, target);<br>                <span class="hljs-comment">//加入A_open表</span><br>                A_open.<span class="hljs-built_in">push</span>(temp);<br>            &#125;<br>            <span class="hljs-keyword">else</span> &#123;<br>                <span class="hljs-comment">//移动失败</span><br>                <span class="hljs-built_in"><span class="hljs-keyword">delete</span></span>(temp);<br>            &#125;<br>        &#125;<br>    &#125;<br>    <span class="hljs-comment">//清空close表</span><br>    close.<span class="hljs-built_in">clear</span>();<br>    <span class="hljs-comment">//--------------------------------------------end-A*--------- Fn=Gn+Hn -------------------------//</span><br>    <span class="hljs-keyword">end_t</span> = <span class="hljs-built_in">clock</span>();<br>    <span class="hljs-comment">//清空A_open</span><br>    <span class="hljs-keyword">while</span> (!A_open.<span class="hljs-built_in">empty</span>()) &#123;<br>        A_open.<span class="hljs-built_in">pop</span>();<br>    &#125;<br>    <span class="hljs-keyword">total_t</span> = ((<span class="hljs-keyword">double</span>)<span class="hljs-keyword">end_t</span> - (<span class="hljs-keyword">double</span>)<span class="hljs-keyword">start_t</span>) / CLOCKS_PER_SEC;<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;总时间：%f\n\n\n&quot;</span>, <span class="hljs-keyword">total_t</span>);<br>    <span class="hljs-keyword">start_t</span> = <span class="hljs-built_in">clock</span>();<br>    <span class="hljs-comment">//--------------------------------------------start-BFS------------------------------------------//</span><br>    <span class="hljs-comment">//初始状态压入队列</span><br>    B_open.<span class="hljs-built_in">push</span>(<span class="hljs-keyword">new</span> <span class="hljs-built_in">borad</span>(<span class="hljs-literal">NULL</span>, start, <span class="hljs-number">0</span>, INT_MAX - <span class="hljs-number">1</span>));<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;BFS：\n&quot;</span>);<br>    <span class="hljs-keyword">while</span> (!B_open.<span class="hljs-built_in">empty</span>()) &#123;<br>        <span class="hljs-comment">//弹出一个状态</span><br>        borad* cur = B_open.<span class="hljs-built_in">front</span>();<br>        B_open.<span class="hljs-built_in">pop</span>();<br>        <span class="hljs-comment">//与目标状态的距离，为0即到达目标状态</span><br>        <span class="hljs-keyword">if</span> (<span class="hljs-built_in">hn</span>(cur-&gt;status, target) == <span class="hljs-number">0</span>) &#123;<br>            <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;到达目标状态\nclose表大小为%ld\n目标状态深度为%d\n&quot;</span>, close.<span class="hljs-built_in">size</span>(), cur-&gt;depth);<br>            <span class="hljs-comment">//printans(cur);</span><br>            <span class="hljs-keyword">break</span>;<br>        &#125;<br>        <span class="hljs-comment">//存放int格式的状态</span><br>        <span class="hljs-keyword">int</span> intstatus = <span class="hljs-built_in">status2int</span>(cur-&gt;status);<br>        <span class="hljs-comment">//出现重复状态</span><br>        <span class="hljs-keyword">if</span> (close.<span class="hljs-built_in">count</span>(intstatus)) &#123;<br>            <span class="hljs-keyword">continue</span>;<br>        &#125;<br>        <span class="hljs-comment">//加入close表，表示已访问过</span><br>        close.<span class="hljs-built_in">insert</span>(intstatus);<br><br>        <span class="hljs-comment">//获得0的坐标</span><br>        <span class="hljs-keyword">int</span> zeroindex = <span class="hljs-built_in">getindex</span>(cur-&gt;status, <span class="hljs-number">0</span>);<br>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">4</span>; i++) &#123;<br>            <span class="hljs-comment">//新建节点，复制当前棋盘状态，深度+1</span><br>            borad* temp = <span class="hljs-keyword">new</span> <span class="hljs-built_in">borad</span>(cur, cur-&gt;status, cur-&gt;depth + <span class="hljs-number">1</span>, INT_MAX - <span class="hljs-number">1</span>);<br>            <span class="hljs-comment">//0向四个方向移动</span><br>            <span class="hljs-keyword">if</span> (<span class="hljs-built_in">swapnum</span>(zeroindex, zeroindex + go[i], temp-&gt;status)) &#123;<br>                <span class="hljs-comment">//移动成功</span><br>                B_open.<span class="hljs-built_in">push</span>(temp);<br>            &#125;<br>            <span class="hljs-keyword">else</span> &#123;<br>                <span class="hljs-comment">//移动失败</span><br>                <span class="hljs-built_in"><span class="hljs-keyword">delete</span></span>(temp);<br>            &#125;<br>        &#125;<br>    &#125;<br>    <span class="hljs-comment">//清空close表</span><br>    close.<span class="hljs-built_in">clear</span>();<br>    <span class="hljs-comment">//--------------------------------------------end-BFS------------------------------------------//</span><br>    <span class="hljs-keyword">end_t</span> = <span class="hljs-built_in">clock</span>();<br>    <span class="hljs-keyword">total_t</span> = ((<span class="hljs-keyword">double</span>)<span class="hljs-keyword">end_t</span> - (<span class="hljs-keyword">double</span>)<span class="hljs-keyword">start_t</span>) / CLOCKS_PER_SEC;<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;总时间：%f\n\n\n&quot;</span>, <span class="hljs-keyword">total_t</span>);<br>    <span class="hljs-keyword">start_t</span> = <span class="hljs-built_in">clock</span>();<br>    <span class="hljs-comment">//--------------------------------------------start-DFS------------------------------------------//</span><br>    <span class="hljs-comment">//初始状态压入队列</span><br>    D_open.<span class="hljs-built_in">push</span>(<span class="hljs-keyword">new</span> <span class="hljs-built_in">borad</span>(<span class="hljs-literal">NULL</span>, start, <span class="hljs-number">0</span>, INT_MAX - <span class="hljs-number">1</span>));<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;DFS：\n&quot;</span>);<br>    <span class="hljs-keyword">while</span> (!D_open.<span class="hljs-built_in">empty</span>()) &#123;<br>        <span class="hljs-comment">//弹出一个状态</span><br>        borad* cur = D_open.<span class="hljs-built_in">top</span>();<br>        D_open.<span class="hljs-built_in">pop</span>();<br>        <span class="hljs-comment">//if (cur-&gt;depth == 5) &#123;</span><br>        <span class="hljs-comment">//    break;</span><br>        <span class="hljs-comment">//&#125;</span><br>        <span class="hljs-comment">//与目标状态的距离，为0即到达目标状态</span><br>        <span class="hljs-keyword">if</span> (<span class="hljs-built_in">hn</span>(cur-&gt;status, target) == <span class="hljs-number">0</span>) &#123;<br>            <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;到达目标状态\nclose表大小为%ld\n目标状态深度为%d\n&quot;</span>, close.<span class="hljs-built_in">size</span>(), cur-&gt;depth);<br>            <span class="hljs-comment">//printans(cur);</span><br>            <span class="hljs-keyword">break</span>;<br>        &#125;<br>        <span class="hljs-comment">//存放int格式的状态</span><br>        <span class="hljs-keyword">int</span> intstatus = <span class="hljs-built_in">status2int</span>(cur-&gt;status);<br>        <span class="hljs-comment">//出现重复状态</span><br>        <span class="hljs-keyword">if</span> (close.<span class="hljs-built_in">count</span>(intstatus)) &#123;<br>            <span class="hljs-keyword">continue</span>;<br>        &#125;<br>        <span class="hljs-comment">//加入close表，表示已访问过</span><br>        close.<span class="hljs-built_in">insert</span>(intstatus);<br><br>        <span class="hljs-comment">//获得0的坐标</span><br>        <span class="hljs-keyword">int</span> zeroindex = <span class="hljs-built_in">getindex</span>(cur-&gt;status, <span class="hljs-number">0</span>);<br>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">4</span>; i++) &#123;<br>            <span class="hljs-comment">//新建节点，复制当前棋盘状态，深度+1</span><br>            borad* temp = <span class="hljs-keyword">new</span> <span class="hljs-built_in">borad</span>(cur, cur-&gt;status, cur-&gt;depth + <span class="hljs-number">1</span>, INT_MAX - <span class="hljs-number">1</span>);<br>            <span class="hljs-comment">//0向四个方向移动</span><br>            <span class="hljs-keyword">if</span> (<span class="hljs-built_in">swapnum</span>(zeroindex, zeroindex + go[i], temp-&gt;status)) &#123;<br>                <span class="hljs-comment">//移动成功</span><br>                D_open.<span class="hljs-built_in">push</span>(temp);<br>            &#125;<br>            <span class="hljs-keyword">else</span> &#123;<br>                <span class="hljs-comment">//移动失败</span><br>                <span class="hljs-built_in"><span class="hljs-keyword">delete</span></span>(temp);<br>            &#125;<br>        &#125;<br>    &#125;<br>    <span class="hljs-comment">//--------------------------------------------end-DFS------------------------------------------//</span><br>    <span class="hljs-keyword">end_t</span> = <span class="hljs-built_in">clock</span>();<br>    <span class="hljs-keyword">total_t</span> = ((<span class="hljs-keyword">double</span>)<span class="hljs-keyword">end_t</span> - (<span class="hljs-keyword">double</span>)<span class="hljs-keyword">start_t</span>) / CLOCKS_PER_SEC;<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;总时间：%f\n&quot;</span>, <span class="hljs-keyword">total_t</span>);<br>    <span class="hljs-comment">//delete(start);</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br><br><span class="hljs-comment">//打印棋盘</span><br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-keyword">int</span>* status)</span> </span>&#123;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">9</span>; i++) &#123;<br>        <span class="hljs-keyword">if</span> (i % <span class="hljs-number">3</span> == <span class="hljs-number">0</span>) &#123;<br>            <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;\n&quot;</span>);<br>        &#125;<br>        <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;%d &quot;</span>, status[i]);<br><br>    &#125;<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;\n\n&quot;</span>);<br>&#125;<br><br><span class="hljs-comment">//获得元素在棋盘上的一维坐标</span><br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">getindex</span><span class="hljs-params">(<span class="hljs-keyword">int</span>* status, <span class="hljs-keyword">int</span> num)</span> </span>&#123;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">9</span>; i++) &#123;<br>        <span class="hljs-keyword">if</span> (status[i] == num) &#123;<br>            <span class="hljs-keyword">return</span> i;<br>        &#125;<br>    &#125;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br>&#125;<br><br><span class="hljs-comment">//交换元素</span><br><span class="hljs-function"><span class="hljs-keyword">bool</span> <span class="hljs-title">swapnum</span><span class="hljs-params">(<span class="hljs-keyword">int</span> a, <span class="hljs-keyword">int</span> b, <span class="hljs-keyword">int</span>* status)</span> </span>&#123;<br>    <span class="hljs-keyword">if</span> (b &gt;= <span class="hljs-number">0</span> &amp;&amp; b &lt;= <span class="hljs-number">8</span> &amp;&amp; (a / <span class="hljs-number">3</span> == b / <span class="hljs-number">3</span> || a % <span class="hljs-number">3</span> == b % <span class="hljs-number">3</span>)) &#123;<br>        <span class="hljs-built_in">swap</span>(status[a], status[b]);<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">true</span>;<br>    &#125;<br>    <span class="hljs-keyword">else</span> &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>;<br>    &#125;<br>&#125;<br><br><span class="hljs-comment">//当前状态与目标状态的曼哈顿距离</span><br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">hn</span><span class="hljs-params">(<span class="hljs-keyword">int</span>* status, <span class="hljs-keyword">int</span>* target)</span> </span>&#123;<br>    <span class="hljs-comment">//获得当前状态与目标状态的二维x，y坐标</span><br>    <span class="hljs-keyword">int</span> x, y, xt, yt, it, h = <span class="hljs-number">0</span>;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">9</span>; i++) &#123;<br>        x = i % <span class="hljs-number">3</span>;<br>        y = i / <span class="hljs-number">3</span>;<br>        it = <span class="hljs-built_in">getindex</span>(target, status[i]);<br>        xt = it % <span class="hljs-number">3</span>;<br>        yt = it / <span class="hljs-number">3</span>;<br>        h += <span class="hljs-built_in">abs</span>(x - xt) + <span class="hljs-built_in">abs</span>(y - yt);<br>    &#125;<br>    <span class="hljs-keyword">return</span> h;<br>&#125;<br><br><span class="hljs-comment">//打印解法，回溯</span><br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">printans</span><span class="hljs-params">(borad* cur)</span> </span>&#123;<br>    vector&lt;string&gt; ans;<br>    <span class="hljs-keyword">while</span> (cur) &#123;<br>        ans.<span class="hljs-built_in">push_back</span>(<span class="hljs-built_in">to_string</span>(cur-&gt;status[<span class="hljs-number">0</span>]) + <span class="hljs-built_in">to_string</span>(cur-&gt;status[<span class="hljs-number">1</span>]) + <span class="hljs-built_in">to_string</span>(cur-&gt;status[<span class="hljs-number">2</span>]) + <span class="hljs-string">&quot;\n&quot;</span><br>            + <span class="hljs-built_in">to_string</span>(cur-&gt;status[<span class="hljs-number">3</span>]) + <span class="hljs-built_in">to_string</span>(cur-&gt;status[<span class="hljs-number">4</span>]) + <span class="hljs-built_in">to_string</span>(cur-&gt;status[<span class="hljs-number">5</span>]) + <span class="hljs-string">&quot;\n&quot;</span><br>            + <span class="hljs-built_in">to_string</span>(cur-&gt;status[<span class="hljs-number">6</span>]) + <span class="hljs-built_in">to_string</span>(cur-&gt;status[<span class="hljs-number">7</span>]) + <span class="hljs-built_in">to_string</span>(cur-&gt;status[<span class="hljs-number">8</span>]));<br>        cur = cur-&gt;pre;<br>    &#125;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = ans.<span class="hljs-built_in">size</span>() - <span class="hljs-number">1</span>; i &gt;= <span class="hljs-number">0</span>; i--) &#123;<br>        <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;%s\n ↓\n&quot;</span>, ans[i].<span class="hljs-built_in">c_str</span>());<br>    &#125;<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;END\n\n&quot;</span>);<br>&#125;<br><br><span class="hljs-comment">//棋盘状态转为int格式</span><br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">status2int</span><span class="hljs-params">(<span class="hljs-keyword">int</span>* status)</span> </span>&#123;<br>    <span class="hljs-keyword">int</span> res = <span class="hljs-number">0</span>;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>, j = <span class="hljs-number">8</span>; i &lt; <span class="hljs-number">9</span>; i++, j--) &#123;<br>        res += status[i] * <span class="hljs-built_in">pow</span>(<span class="hljs-number">10</span>, j);<br>    &#125;<br>    <span class="hljs-keyword">return</span> res;<br>&#125;<br><br><span class="hljs-comment">//计算逆序数之和</span><br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">reversesum</span><span class="hljs-params">(<span class="hljs-keyword">int</span>* status)</span> </span>&#123;<br>    <span class="hljs-keyword">int</span> sum = <span class="hljs-number">0</span>;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">9</span>; i++) &#123;<br>        <span class="hljs-keyword">if</span> (status[i] != <span class="hljs-number">0</span>) &#123;<br>            <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> j = <span class="hljs-number">0</span>; j &lt; i; j++) &#123;<br>                <span class="hljs-keyword">if</span> (status[j] &gt; status[i]) &#123;<br>                    sum++;<br>                &#125;<br>            &#125;<br>        &#125;<br>    &#125;<br>    <span class="hljs-keyword">return</span> sum;<br>&#125;<br><br><span class="hljs-comment">//获得随机初始状态</span><br><span class="hljs-function"><span class="hljs-keyword">int</span>* <span class="hljs-title">randstatus</span><span class="hljs-params">(<span class="hljs-keyword">int</span>* target)</span> </span>&#123;<br>    <span class="hljs-keyword">int</span>* start = <span class="hljs-keyword">new</span> <span class="hljs-keyword">int</span>[<span class="hljs-number">9</span>]();<br>    unordered_map&lt;<span class="hljs-keyword">int</span>, <span class="hljs-keyword">int</span>&gt; nums;<span class="hljs-comment">//记录已添加的数</span><br>    <span class="hljs-built_in">srand</span>((<span class="hljs-keyword">int</span>)<span class="hljs-built_in">time</span>(<span class="hljs-number">0</span>));<br>    <span class="hljs-keyword">int</span> element, sum1, sum2;<br>    sum2 = <span class="hljs-built_in">reversesum</span>(target);<br>    <span class="hljs-comment">//根据初始状态与目标状态的逆序数之和（sum1、sum2）是否相等，判断初始状态是否有解，不相等（即无解）则重新生成初始状态</span><br>    <span class="hljs-keyword">do</span> &#123;<br>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">9</span>; i++) &#123;<br>            element = <span class="hljs-built_in">rand</span>() % <span class="hljs-number">9</span>;<br>            <span class="hljs-keyword">while</span> (nums[element]) &#123;<br>                element = <span class="hljs-built_in">rand</span>() % <span class="hljs-number">9</span>;<br>            &#125;<br>            nums[element]++;<br>            start[i] = element;<br>        &#125;<br>        <span class="hljs-comment">//清空记录</span><br>        nums.<span class="hljs-built_in">clear</span>();<br>        <span class="hljs-comment">//计算逆序数之和</span><br>        sum1 = <span class="hljs-built_in">reversesum</span>(start);<br>    &#125; <span class="hljs-keyword">while</span> (sum1 % <span class="hljs-number">2</span> != sum2 % <span class="hljs-number">2</span>);<br>    <span class="hljs-keyword">return</span> start;<br>&#125;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>course</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Texlive+VScode</title>
    <link href="/2021/10/18/latex/"/>
    <url>/2021/10/18/latex/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>搭建Latex环境：Texlive+VScode 相关记录</p><span id="more"></span><h2 id="1-安装-Texlive"><a href="#1-安装-Texlive" class="headerlink" title="1.安装 Texlive"></a>1.安装 Texlive</h2><p>鉴于我校没有（我知道的）可用开源软件镜像站，所以在到清华大学开源软件镜像站的<a href="https://mirrors.tuna.tsinghua.edu.cn/CTAN/systems/texlive/Images/">texlive</a>页下载。</p><p><a href="https://imgtu.com/i/5UYncq"><img src="https://z3.ax1x.com/2021/10/18/5UYncq.png" alt="5UYncq.png"></a></p><p>可能由于更新导致你打开之后的页面与上面的内容不一致，总之下载最新的texlive<em>*</em>.iso，虽然很大但各种宏包齐全，用起来能省去各种麻烦。</p><p>在带宽扩容之后的校园网加持下，不用几年就能下载完这个iso文件了。</p><p>如果你是windows 7甚至xp用户，我建议你把iso文件解压然后进行后续操作。</p><p>如果是windows 10/11，系统自带虚拟光驱，直接双击进入即可。</p><p>（ linux/macOS 我不了解，省略）效果如图：</p><p><a href="https://imgtu.com/i/5UUe9H"><img src="https://z3.ax1x.com/2021/10/18/5UUe9H.png" alt="5UUe9H.png"></a></p><p>双击或者右键以管理员身份运行install-tl-advanced.bat，可以点进<strong>Advanced</strong>进入高级安装，点击<strong>Customize</strong>来取消你不需要安装的宏包，比如非中英的语言包，这里我只修改了安装目录，最后开始漫长的等待。</p><p><a href="https://imgtu.com/i/5UaCGQ"><img src="https://z3.ax1x.com/2021/10/18/5UaCGQ.png" alt="5UaCGQ.png"></a></p><p>（安装TeXworks前端也可以取消掉，毕竟都打算用vscode了，加上前面说的语言包之类的，可以省个1G左右，我想着留条后路就啥都没改，也不缺这点空间）</p><p>（在我的电脑上一共安装了57 min 56 s，教程都快写完了，还没有装好）</p><h2 id="2-安装-VSCode"><a href="#2-安装-VSCode" class="headerlink" title="2. 安装 VSCode"></a>2. 安装 VSCode</h2><p>到<a href="https://code.visualstudio.com/Download">官网</a>根据你的系统选择下载安装即可，这部分应该大多数人都安装过了，没什么需要注意的。</p><p><a href="https://imgtu.com/i/5Ud4hD"><img src="https://z3.ax1x.com/2021/10/18/5Ud4hD.png" alt="5Ud4hD.png"></a></p><p>安装完成之后可以在应用商店挑选各种提高使用体验的扩展，跟本文相关的主要是<strong>Latex Workshop</strong>。</p><p><a href="https://imgtu.com/i/5U0KJS"><img src="https://z3.ax1x.com/2021/10/18/5U0KJS.png" alt="5U0KJS.png"></a></p><p>安装完成之后，可以创建或者打开一个tex文件，此时代码已经被高亮显示了。</p><p><a href="https://imgtu.com/i/5U560K"><img src="https://z3.ax1x.com/2021/10/18/5U560K.png" alt="5U560K.png"></a></p><p>按下快捷键Ctrl+Alt+B（build latex project），顺利生成，效果不错。</p><p><a href="https://imgtu.com/i/5U5xcn"><img src="https://z3.ax1x.com/2021/10/18/5U5xcn.png" alt="5U5xcn.png"></a></p><h2 id="3-配置-VSCode-的-插件"><a href="#3-配置-VSCode-的-插件" class="headerlink" title="3. 配置 VSCode 的 插件"></a>3. 配置 VSCode 的 插件</h2><p>按下F1或者Ctrl＋shift＋P，输入setjson，选择第三个（如图所示）。</p><p><a href="https://imgtu.com/i/5Uo9xA"><img src="https://z3.ax1x.com/2021/10/18/5Uo9xA.png" alt="5Uo9xA.png"></a></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><code class="hljs json">&quot;latex-workshop.latex.tools&quot;: [<br>        &#123;<br>   <span class="hljs-comment">// 编译工具和命令</span><br>   <span class="hljs-attr">&quot;name&quot;</span>: <span class="hljs-string">&quot;xelatex&quot;</span>,<br>   <span class="hljs-attr">&quot;command&quot;</span>: <span class="hljs-string">&quot;xelatex&quot;</span>,<br>   <span class="hljs-attr">&quot;args&quot;</span>: [<br>   <span class="hljs-string">&quot;-synctex=1&quot;</span>,<br>   <span class="hljs-string">&quot;-interaction=nonstopmode&quot;</span>,<br>   <span class="hljs-string">&quot;-file-line-error&quot;</span>,<br>   <span class="hljs-string">&quot;-pdf&quot;</span>,<br>   <span class="hljs-string">&quot;%DOCFILE%&quot;</span><br>            ]<br>        &#125;,<br>        &#123;<br>   <span class="hljs-attr">&quot;name&quot;</span>: <span class="hljs-string">&quot;pdflatex&quot;</span>,<br>   <span class="hljs-attr">&quot;command&quot;</span>: <span class="hljs-string">&quot;pdflatex&quot;</span>,<br>   <span class="hljs-attr">&quot;args&quot;</span>: [<br>   <span class="hljs-string">&quot;-synctex=1&quot;</span>,<br>   <span class="hljs-string">&quot;-interaction=nonstopmode&quot;</span>,<br>   <span class="hljs-string">&quot;-file-line-error&quot;</span>,<br>   <span class="hljs-string">&quot;%DOCFILE%&quot;</span><br>            ]<br>        &#125;,<br>        &#123;<br>   <span class="hljs-attr">&quot;name&quot;</span>: <span class="hljs-string">&quot;bibtex&quot;</span>,<br>   <span class="hljs-attr">&quot;command&quot;</span>: <span class="hljs-string">&quot;bibtex&quot;</span>,<br>   <span class="hljs-attr">&quot;args&quot;</span>: [<br>   <span class="hljs-string">&quot;%DOCFILE%&quot;</span><br>            ]<br>        &#125;<br>    ],<br>   &quot;latex-workshop.latex.recipes&quot;: [<br>      &#123;<br>   <span class="hljs-attr">&quot;name&quot;</span>: <span class="hljs-string">&quot;xelatex&quot;</span>,<br>   <span class="hljs-attr">&quot;tools&quot;</span>: [<br>   <span class="hljs-string">&quot;xelatex&quot;</span><br>          ],<br>      &#125;,<br>      &#123;<br>   <span class="hljs-attr">&quot;name&quot;</span>: <span class="hljs-string">&quot;pdflatex&quot;</span>,<br>   <span class="hljs-attr">&quot;tools&quot;</span>: [<br>   <span class="hljs-string">&quot;pdflatex&quot;</span><br>          ]<br>      &#125;,<br>      &#123;<br>   <span class="hljs-attr">&quot;name&quot;</span>: <span class="hljs-string">&quot;xe-&gt;bib-&gt;xe-&gt;xe&quot;</span>,<br>   <span class="hljs-attr">&quot;tools&quot;</span>: [<br>   <span class="hljs-string">&quot;xelatex&quot;</span>,<br>   <span class="hljs-string">&quot;bibtex&quot;</span>,<br>   <span class="hljs-string">&quot;xelatex&quot;</span>,<br>   <span class="hljs-string">&quot;xelatex&quot;</span><br>          ]<br>      &#125;,<br>      &#123;<br>   <span class="hljs-attr">&quot;name&quot;</span>: <span class="hljs-string">&quot;pdf-&gt;bib-&gt;pdf-&gt;pdf&quot;</span>,<br>   <span class="hljs-attr">&quot;tools&quot;</span>: [<br>   <span class="hljs-string">&quot;pdflatex&quot;</span>,<br>   <span class="hljs-string">&quot;bibtex&quot;</span>,<br>   <span class="hljs-string">&quot;pdflatex&quot;</span>,<br>   <span class="hljs-string">&quot;pdflatex&quot;</span><br>          ]<br>      &#125;<br>  ],<br>  &quot;latex-workshop.view.pdf.viewer&quot;: &quot;tab&quot;,<br>&quot;editor.inlineSuggest.enabled&quot;: true,<br>&quot;latex-workshop.latex.autoClean.run&quot;: &quot;onBuilt&quot;,<br>&quot;latex-workshop.latex.autoBuild.run&quot;: &quot;never&quot;,<br></code></pre></td></tr></table></figure><ul><li>Ctrl+Alt+B 是编译</li><li>Ctrl+Alt+V是编译+预览pdf</li></ul><p>我最开始写这些其实是想要把中大的foxitpdf设置成默认的pdf预览软件，不过最终效果并不好，所以作罢。</p><p>（咨询了foxit的技术客服，他们说目前是实现不了的）</p><p>上面这些设置主要是因为默认的编译工具是 latexmk，由于不需要用到 latexmk，因此把其修改为中文环境常用的 xelatex；将 tools 中的 %DOC%替换成%DOCFILE%就可以支持编译中文路径下的文件了。</p><p>还可以研究的设置有很多，什么正向搜索反向搜索之类的，有兴趣的朋友可以自行了解。</p><p><a href="https://ericp.cn/cmd">公式指导手册</a></p><p>如果中文无法显示就加上这一句：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs css">\usepackage<span class="hljs-selector-attr">[UTF8]</span>&#123;ctex&#125;<br></code></pre></td></tr></table></figure><p>Latex的相关公式及使用就不再赘述了。</p>]]></content>
    
    
    
    <tags>
      
      <tag>course</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello world!</title>
    <link href="/2021/04/25/Helloworld/"/>
    <url>/2021/04/25/Helloworld/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>My First blog:Something about Enderfga</p><span id="more"></span><h1 id="Think-twice-code-once"><a href="#Think-twice-code-once" class="headerlink" title="Think twice, code once."></a>Think twice, code once.</h1>        <div id="aplayer-hohivjxY" class="aplayer aplayer-tag-marker" style="margin-bottom: 20px;">            <pre class="aplayer-lrc-content"></pre>        </div>        <script>          var ap = new APlayer({            element: document.getElementById("aplayer-hohivjxY"),            narrow: false,            autoplay: true,            showlrc: false,            music: {              title: "Never Coming Back",              author: "Evan Call",              url: "https://cdn.jsdelivr.net/gh/Enderfga/Enderfga/Backup/music.mp3",              pic: "https://z3.ax1x.com/2021/04/24/cvhTxI.jpg",              lrc: ""            }          });          window.aplayers || (window.aplayers = []);          window.aplayers.push(ap);        </script><p><a href="https://imgtu.com/i/gAAR54"><img src="https://z3.ax1x.com/2021/04/30/gAAR54.jpg" alt="gAAR54.jpg"></a></p><p>自打成为一个程序猿开始，翻阅博客文章学习就成了我的日常（质量确实有够参差不齐···），CSDN、博客园、简书、知乎、GitHub等我都经常光顾，于是萌生了自己写blog的想法，苦于技术力不足一直搁置至今（现在也不怎么样哈哈哈）。在GZTime的协助下，我自己的小破站终于建成啦~希望我早日产出点技术性文章，现在只能拿来记流水账了······</p><h1 id="Enderfga？"><a href="#Enderfga？" class="headerlink" title="Enderfga？"></a>Enderfga？</h1><p>关于我的id来源其实挺傻的，在成为程序猿之前我是一名资深游戏玩家，我还清楚地记得我接触的第一款网络游戏叫植物大战僵尸OL，然后是洛克王国，卡布西游，奥奇传说···直到六年级那年，我玩了第一款我愿将其称之为“游戏”或者说是“第九艺术”的作品——Minecraft。</p><p><a href="https://imgtu.com/i/gAAhG9"><img src="https://z3.ax1x.com/2021/04/30/gAAhG9.jpg" alt="gAAhG9.jpg"></a></p><p>MC一直陪我走到今天，我对编程的兴趣基本也是萌芽于此。虽然课程里没有涉及Java，但有机会我还是会争取好好自学Java和C#的。至于MC，不管怎么更新换代，mod/红石/命令方块/欺负末影龙/种田养猪都挺吸引我的。</p><figure class="highlight mizar"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs mizar">EnderDragon—Enderman-Enderfga,Doesn&#x27;t <span class="hljs-keyword">that</span> sound cool?<br>&quot;This <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> the <span class="hljs-keyword">end</span>,this <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> even the beginning <span class="hljs-keyword">of</span><br> the <span class="hljs-keyword">end</span>,but,perhaps,the <span class="hljs-keyword">end</span> <span class="hljs-keyword">of</span> the beginning.&quot;<br>                   ——Winston Leonard Spencer Churchill<br></code></pre></td></tr></table></figure><p>这个id不知不觉间居然用了七年了，从个别网站到所有账户统一，我也不舍得换新的了（在互联网上统一貌似不是什么好习惯，还容易查到我年轻的时候留下的黑历史···）只希望外国友人看到这么沙雕的英语名不要笑我哈哈哈。（谷歌娘念的好可爱！）</p><p><a href="https://imgtu.com/i/gAm5Bq"><img src="https://z3.ax1x.com/2021/04/30/gAm5Bq.jpg" alt="gAm5Bq.jpg"></a></p><h1 id="ACGN-引きこもり"><a href="#ACGN-引きこもり" class="headerlink" title="ACGN-引きこもり"></a>ACGN-引きこもり</h1><p><a href="https://imgtu.com/i/fgsIRU"><img src="https://z3.ax1x.com/2021/08/15/fgsIRU.jpg" alt="fgsIRU.jpg"></a></p><div style="text-align:center">宿舍一角</div><h2 id="Animation"><a href="#Animation" class="headerlink" title="Animation"></a>Animation</h2><p>二次元浓度++；</p><p>很庆幸自己的童年有虹猫蓝兔七侠传，洛洛历险记，蓝猫淘气三千问，秦时明月，东方神娃······等等优秀国产作品陪伴（甚至顺便在里面学会了普通话），后来在星空卫视的《动漫先锋》栏目里入坑了日漫：犬夜叉，海贼王，钢之炼金术师，七龙珠，<strong>火影忍者</strong>，<strong>银魂</strong>······没有这几部番，肯定也没有现在时而中二热血，时而沙雕废柴的我。<br>至于B站的入站时间是2015-07-10 17:20:10（这是通过答题的时间，终于不是游客了！）（用时间戳查的，我怎么可能记得这种东西）</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/wallhaven-j3dg1m.jpg" alt="General 2560x1440 Aerith Gainsborough Final Fantasy video games women ponytail flowers jacket dress"></p><p>那个时候特地去看了某科学的超电磁炮，lovelive什么的，四舍五入也算是二刺猿入门了（吧？）</p><p>从零开始的异世界生活，一拳超人，灵能百分百，小林家的龙女仆，干物妹小埋，刺客伍六七，<strong>紫罗兰永恒花园</strong>······这些年看番的频率虽然少了，但那种每周等更新看番的热情已经刻进DNA了。每顿饭的时候刷刷B站的剪辑还能感慨一下“爷青回”，泪目一会。</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211208140846878.png" alt="image-20211208140846878"></p><p>另外我的头像其实是桂小太郎，不过因为版权意识的加深，可能有机会还是得重画一个。</p><p><a href="https://imgtu.com/i/fgyay4"><img src="https://z3.ax1x.com/2021/08/15/fgyay4.jpg" alt="fgyay4.jpg"></a></p><p>更新：桂先生成功升级了，参考了尼尔机械纪元中9S的装扮，现在科技感满满！</p><h2 id="Comic"><a href="#Comic" class="headerlink" title="Comic"></a>Comic</h2><p>回忆了一下，我好像不怎么看漫画。起初看漫画是因为死火海更新太慢了，后来在快看上看了几部，记得名字的有阎王不高兴，哑舍，快把我哥带走，<strong>蝉女</strong>。好看是好看，感觉有点像折中选择，不如动画灵动也不如小说全面。</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/20180718145340_ZKfkz.thumb.1000_0.jpeg" alt="蝉女"></p><p>已经完全想不起来蝉女讲什么了，但画风针不戳。最近听了《<a href="https://bilibili.com/video/BV1WX4y1G7ok">鉴情师</a>》这首歌才想起来的。</p><h2 id="Game"><a href="#Game" class="headerlink" title="Game"></a>Game</h2><p>说到这个我可就不困了（zzzzz….）</p><p><a href="https://imgtu.com/i/gV9KCn"><img src="https://z3.ax1x.com/2021/05/01/gV9KCn.jpg" alt="gV9KCn.jpg"></a></p><div style="text-align:center"><iframe width="100%" height="500" src="https://player.bilibili.com/player.html?aid=5868975&bvid=BV1Ts411k73E&cid=9531080&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe></iframe></div><p><a href="https://imgtu.com/i/gV9eEQ"><img src="https://z3.ax1x.com/2021/05/01/gV9eEQ.jpg" alt="gV9eEQ.jpg"></a></p><div style="text-align:center"><iframe width="100%" height="500" src="https://player.bilibili.com/player.html?aid=460307938&bvid=BV1t5411w723&cid=331362469&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe></iframe></div><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211208141258648.png" alt="image-20211208141258648"></p><div style="text-align:center"><iframe width="100%" height="500" src="https://player.bilibili.com/player.html?aid=12069119&bvid=BV1Wx411q7zb&cid=19911067&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe></div><p>我记得当年进b站答题的时候有一道题让我选出“老头滚动条”的原名，百度的过程中我了解到“上古卷轴5”(老滚)mod的丰富程度堪比MC，中世纪剑与魔法的世界也让我着迷，于是我毅然决然地成为了一名抓根宝（龙裔），但通关的过程中我饱受迷路的困扰：黑灯瞎火的洞窟、乱七八糟的陷阱、不可名状的地图···于是我决定找一款我想怎么走就怎么走的游戏——《Assassin‘s Creed》。一入坑就是10年，我愿时间永远停留在佛罗伦萨塔顶的月圆之夜。库里的全套刺客信条通关了，我又想念剑与魔法的故事了，因为久仰其大名我下载了——《The Witcher 3》。这是一部我最喜欢的游戏，没有之一，这也许就是“第九艺术”吧。三言两语不能表达出其中的波澜壮阔，每一个支线，每一部DLC都值得我一遍又一遍地游玩。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs plain">我吻过凯尔莫罕忽然冷冽而至的白雪，<br>我听过史凯利杰伴着海妖清啸的海风。<br>我仰头饮尽诺威格瑞陈年的矮人烈酒，<br>我策马走遍全威伦最艰险的万水千山。<br>对我而言，家是什么地方。<br>是那抹黑白裙摆的倩影，<br>还是那丁香与醋栗的芬芳。<br></code></pre></td></tr></table></figure><p>给我留下深刻印象的游戏有很多，尼尔里的“世界竟然如此美丽”，2077里的”以我残躯化烈火”，死亡搁浅里的“我在冥滩等你”······希望我的程序猿之路最终可以走到像小岛秀夫那样，拥有自己的工作室，书写自己的艺术。</p><p>（另外上面2B那张图的作者是Wlop，我最喜欢的画师）</p><h2 id="Novel"><a href="#Novel" class="headerlink" title="Novel"></a>Novel</h2><p>我还年轻的时候会看一些天蚕土豆，唐家三少，耳根写的小说······对我的文学水平真是没有半点提升。</p><p>在高二语文老师的耳濡目染下，我一个理工男对文学兴趣盎然。即使很忙，也想抽点时间陶冶情操。</p><div style="text-align:center">落霞与孤鹜齐飞，秋水共长天一色</div><p><a href="https://imgtu.com/i/gV41qf"><img src="https://z3.ax1x.com/2021/05/01/gV41qf.jpg" alt="gV41qf.jpg"></a></p><p>除了诗与词，还记得名字的书只剩下《巨人的陨落》、《三体》、《外婆的道歉信》、《自由在高处》······</p><p>剩下的各种悬疑侦探小说就不列举了（不过强推一手《神探夏洛克》《POI疑犯追踪》）。</p><h2 id="Music"><a href="#Music" class="headerlink" title="Music"></a>Music</h2><p>我这个人听的歌有亿点点杂，基本歌单里什么都沾一点（二次元&amp;欧美占比较大）</p><p>特地开了一个Music版块都是因为——hanser！</p><div style="text-align:center"><iframe width="100%" height="500" src="https://player.bilibili.com/player.html?aid=4848309&bvid=BV1Cs411i7B1&cid=7870718&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe></div><p>2016-10-01至今，永远单推憨憨！</p><p><a href="https://imgtu.com/i/4jdP0g"><img src="https://z3.ax1x.com/2021/10/05/4jdP0g.jpg" alt="4jdP0g.jpg"></a></p><h1 id="Enderfga。"><a href="#Enderfga。" class="headerlink" title="Enderfga。"></a>Enderfga。</h1><p>不知不觉写了好多废话了······</p><p>总之，大学生活开始了，希望我能当好一个神奇海螺/哆啦安梦。</p><div align="right">----Nothing is true，everything is permitted.</div>]]></content>
    
    
    
    <tags>
      
      <tag>self</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
