<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>RLHF</title>
    <link href="/2023/03/01/RLHF/"/>
    <url>/2023/03/01/RLHF/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="RLHF"><a href="#RLHF" class="headerlink" title="RLHF"></a>RLHF</h1><h2 id="Aligning-Text-to-Image-Models-using-Human-Feedback"><a href="#Aligning-Text-to-Image-Models-using-Human-Feedback" class="headerlink" title="Aligning Text-to-Image Models using Human Feedback"></a>Aligning Text-to-Image Models using Human Feedback</h2><ul><li><p>Google Research ,University of California</p></li><li><p>2023.2.23</p><span id="more"></span></li></ul><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>深度生成模型在文本到图像合成方面取得了令人印象深刻的成果，但当前的文本到图像模型往往生成与文本提示不够相符的图像。</p><p>本文的动机是改进文本到图像合成模型，使其能够更好地与文本提示对齐。</p><p>作者的方法比预训练模型更准确地生成具有指定颜色、计数和背景的对象。</p><h2 id="Proposal"><a href="#Proposal" class="headerlink" title="Proposal"></a>Proposal</h2><ul><li>提出了一种简单而有效的微调方法，用于使用人类反馈对文本到图像模型进行对齐。</li><li>使用人类反馈进行微调可以显着提高文本到图像模型的图像文本对齐，在人类评估中，我们的模型在图像文本对齐方面达到了高达47％的改善，但图像保真度略有降低。</li><li>学习的奖励函数比CLIP分数更准确地预测了人类对质量的评估。</li><li>基于作者学习的奖励函数的采样也可以显着改善图像文本对齐。</li></ul><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><ul><li>T2I models</li><li>Evaluating image-text alignment</li><li>Learning with human feedback</li></ul><p>与先前关注利用人类反馈改善语言模型和RL代理的工作相比，该工作探索了使用人类反馈来调整多模式文本到图像模型与人类偏好的方法。许多关于利用人类反馈学习的先前工作都包括学习一个奖励函数并最大化奖励加权可能性（通常被称为监督微调）。受其成功的启发，作者提出了一种利用人类反馈进行微调的方法来改善文本到图像模型。</p><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p><img src="https://img.enderfga.cn/img/image-20230228130311025.png" alt=""></p><p>包括三个阶段：</p><ol><li>首先从一系列文本提示中生成一组不同的图像，这些文本提示旨在测试文本到图像模型的各种功能。 </li><li>然后，人类评级者对这些图像提供二进制反馈。 </li><li>接下来，训练一个奖励模型，以文本提示和图像作为输入来预测人类反馈。</li><li>最后，我们使用奖励加权对数似然度来微调文本到图像模型，以改善文本图像对齐。</li></ol><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p>实验部分旨在测试人类反馈参与模型微调的有效性。实验用到的模型为 Stable Diffusion v1.5</p><p><img src="https://img.enderfga.cn/img/image-20230228132659591.png" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20230228132741226.png" alt=""></p><p>本文方法显著提高了图像 - 文本对齐，具体来说，模型生成的图像中有 50% 的样本获得至少三分之二的赞成票（投票数量为 7 票或更多赞成票），然而，微调会稍微降低图像保真度（15% 比 10%）。</p><p><img src="https://img.enderfga.cn/img/image-20230228132824327.png" alt=""></p><p>本文模型生成的图像符合 prompt 指定的颜色、计数和背景。值得注意的是，本文模型还能生成没有见过的文本 prompt 图像，并且质量非常高</p><p><img src="https://img.enderfga.cn/img/image-20230228132908052.png" alt=""></p><p>有奖励（绿色）比 CLIP 分数（红色）更符合典型的人类意图。</p><h2 id="Limitations-and-future-directions"><a href="#Limitations-and-future-directions" class="headerlink" title="Limitations and future directions"></a>Limitations and future directions</h2><ol><li><p><strong>更细致的人类反馈</strong>，存在一些较差的生成，如高饱和度的图像颜色，指示评级者寻找更多样化的失败模式（过度饱和的颜色，不切实际的动物解剖学，物理违规等）将提高这些方面的性能。</p></li><li><p><strong>多样化和大型人类数据集</strong>，为了简化问题，作者考虑了有限的文本类别（计数，颜色，背景），因此人类反馈也相对简单（好或坏）。由于这一点，人类反馈数据的多样性有限。将其扩展到更主观的文本类别（如艺术创作）和更细致的人类反馈将是未来研究的重要方向。</p></li><li><p><strong>不同的目标和算法</strong>，为了更新文本到图像模型，作者使用奖励加权的最大似然。然而，与语言领域的先前工作类似，使用RL算法将是一个有趣的方向。作者相信RLHF微调可能会产生更好的模型，因为</p><p>（a）在更新期间使用在线样本生成</p><p>（b）KL正则化可以减轻对奖励函数的过度拟合。</p></li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>计算机视觉</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Realfusion</title>
    <link href="/2023/03/01/Realfusion/"/>
    <url>/2023/03/01/Realfusion/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="RealFusion："><a href="#RealFusion：" class="headerlink" title="RealFusion："></a>RealFusion：</h1><h3 id="360°-Reconstruction-of-Any-Object-from-a-Single-Image"><a href="#360°-Reconstruction-of-Any-Object-from-a-Single-Image" class="headerlink" title="360° Reconstruction of Any Object from a Single Image"></a>360° Reconstruction of Any Object from a Single Image</h3><ul><li><p>Oxford University</p></li><li><p>2023.2.23</p><span id="more"></span><h2 id="Demo"><a href="#Demo" class="headerlink" title="Demo"></a>Demo</h2><p><a href="https://lukemelas.github.io/realfusion/">https://lukemelas.github.io/realfusion/</a></p><p><img src="https://img.enderfga.cn/img/splash-figure-v2.png" alt="Examples"></p></li></ul><h2 id="Motivation-Single-View-3D-Reconstruction"><a href="#Motivation-Single-View-3D-Reconstruction" class="headerlink" title="Motivation: Single-View 3D Reconstruction"></a>Motivation: Single-View 3D Reconstruction</h2><ul><li>Reconstructing the 3D structure of an object from a single 2D view is a fundamental challenge in computer vision.</li><li>In the case of a single view, the reconstruction problem is highly ill-posed.<br>As a result, the task requires semantic understanding obtained by learning.<br>Despite the difficulty of this task, humans are adept at using a range of monocular cues to infer the 3D structures of objects from single views.</li></ul><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><h3 id="Category-level-3D-Reconstruction"><a href="#Category-level-3D-Reconstruction" class="headerlink" title="Category-level 3D Reconstruction"></a>Category-level 3D Reconstruction</h3><ul><li>Most prior work tackles the problem of category-specific single-view 3D reconstruction by training a category-level reconstruction model.</li><li>The work: Going beyond category-level 3D reconstruction<ul><li>This work aims to go beyond category-specific images to images of arbitrary objects. This setting is highly challenging, but humans perform it effortlessly when they observe new objects.</li></ul></li></ul><h3 id="Single-View-3D-Reconstruction"><a href="#Single-View-3D-Reconstruction" class="headerlink" title="Single-View 3D Reconstruction"></a>Single-View 3D Reconstruction</h3><ul><li>Arbitrary-object 3D reconstruction has been challenging because the problem fundamentally requires the use of large-scale 3D priors over object shapes, which have not been available.</li><li>With the recent rise of large-scale pretraining, this problem has become tractable.<br>Examples include:<ul><li>Contrastive: CLIP</li><li>Autoregressive: DALL-E / Parti</li><li>Diffusion Models: DALL-E 2 / Imagen / Stable Diffusion</li></ul></li><li>These pretrained models may be used as priors for a variety of vision tasks, and we are particularly interested in 3D reconstruction.<ul><li>At a high level, you can think of these models as a tool for optimizing the realism of an input image.</li></ul></li><li>In this way, they enable an elegant approach to 3D generation and reconstruction: using these large-scale pretrained models to enforce that a differentiable scene looks realistic from random views.</li></ul><h2 id="Proposal"><a href="#Proposal" class="headerlink" title="Proposal"></a>Proposal</h2><p>(1) We propose <strong>RealFusion</strong>, a method that can extract from a single image of an object a 360◦ photographic 3D reconstruction without assumptions on the type of object imaged or 3D supervision of any kind; </p><p>(2) We do so by leveraging an existing 2D <strong>diffusion image generator</strong> via a new single image variant of textual inversion; </p><p>(3) We also introduce new regularizers and provide an efficient implementation using <strong>InstantNGP</strong>; </p><p>(4) We demonstrate <strong>state-of-the-art</strong> reconstruction results on a number of in-the-wild images and images from existing datasets when compared to alternative approaches.  </p><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><ul><li>Image-based reconstruction of appearnce and geometry</li><li>Few-view reconstruction</li><li>Single-view reconstruction  </li><li>Extracting 3D models from 2D generators  </li><li>Diffusion Models  </li></ul><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p><img src="https://img.enderfga.cn/img/image-20230227112154724.png" alt=""></p><ul><li>This approach forms the backbone of our method, RealFusion.<br>(0) [Init] We are given a single image and a function $\boldsymbol{p}_{\text {prior }}(\cdot)$ which computes the likelihood of an input image $\boldsymbol{I}$. We choose a camera view and represent our scene with a differentiably-renderable representation $\boldsymbol{x}$, for example a NeRF.<br>(1) [Reconstruction] We render $\boldsymbol{x}$ from our given view and minimize the loss with respect to the real input image $\mathbf{I}$.<br>(2) [Prior] We render images $\boldsymbol{I}_{\text {prior }}$ of $\boldsymbol{x}$ from randomly-chosen views on a hemisphere surrounding the origin, and we optimize $\boldsymbol{p}_{\text {prior }}\left(\boldsymbol{I}_{\text {priol }}\right)$ to enforce that $\boldsymbol{x}$ looks realistic from all directions.</li><li>Prior work has explored this question in the domain of 3D generation<ul><li>Dreamfields: CLIP prior</li><li>DreamFusion: Diffusion model prior</li></ul></li><li>In our work, we adopt a diffusion model prior using Stable Diffusion, a text-conditional latent diffusion model.</li><li>As currently stated, our set up combines a reconstruction objective with a latent diffusion-based prior objective, which is conditioned on a manual text prompt (e.g. “An image of a fish.”)</li><li>However, we found that these results were lacking.</li><li>In particular, the 3D shapes that are generated look like the input object from the input view, but do not look like the input object from other views.</li><li>To fix this, we need to modify the prior to place a high likelihood on our input object, rather than a generic object with the same description.</li><li>We do so by performing textual inversion.<ul><li>We optimize a text embedding $\mathbf{e}$ in the text encoder of the diffusion model to match our input image.</li><li>Usually textual inversion is performed with multiple views of an object, but we substitute these views with heavy image augmentations.</li></ul></li><li>We also add other pieces of regularization:<br>(1) A regularization on rendered normals<br>(2) A coarse-to-fine training setup</li><li>However, the key piece of the puzzle is the textual inversion.</li></ul><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p><img src="https://img.enderfga.cn/img/image-20230227165614420.png" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20230227165925713.png" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20230227165956615.png" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20230227170021757.png" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20230227170047165.png" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20230227170111552.png" alt=""></p><h2 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h2><ul><li>Requires per-image optimization<ul><li>Both the textual inversion and the 3D optimization procedure must be performed separately for each input image.</li><li>As a result, the process is relatively slow and difficult to apply to large datasets</li></ul></li><li>In some cases, reconstruction fails to produce a solid shape<ul><li>Perhaps this could be alleviated with better inductive biases or regularization terms</li></ul></li><li>In some cases, reconstruction produces two-headed objects<ul><li>This is known as the Janus Problem</li></ul></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>计算机视觉</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Outstanding Wordle</title>
    <link href="/2023/02/22/wordle/"/>
    <url>/2023/02/22/wordle/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>唯一一次认真参加数学建模，虽然曾经的我确实不喜欢这类赛事。假如拿到O/F奖了就考虑写写经验或者录个视频，捞了就当我没说。</p><span id="more"></span><embed src="./wordle.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    
    <tags>
      
      <tag>数学建模</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>亚太数模论文</title>
    <link href="/2023/02/20/apmcm/"/>
    <url>/2023/02/20/apmcm/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>这么烂的学术垃圾都能拿奖，这比赛确实没含金量</p><span id="more"></span><embed src="./apmcm.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    
    <tags>
      
      <tag>数学建模</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MAV3D:Text-To-4D Dynamic Scene Generation</title>
    <link href="/2023/02/11/mav/"/>
    <url>/2023/02/11/mav/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>论文阅读笔记</p><span id="more"></span><div style="text-align:center"><iframe width="100%" height="500" src="https://player.bilibili.com/player.html?aid=479164157&bvid=BV1aT411R77Z&cid=1003655006&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe></iframe></div><h1 id="MAV3D-Text-To-4D-Dynamic-Scene-Generation"><a href="#MAV3D-Text-To-4D-Dynamic-Scene-Generation" class="headerlink" title="MAV3D:Text-To-4D Dynamic Scene Generation"></a>MAV3D:Text-To-4D Dynamic Scene Generation</h1><ul><li><strong>Meta AI</strong></li><li><p><strong>2023.1.26</strong></p><h2 id="Demo"><a href="#Demo" class="headerlink" title="Demo"></a>Demo</h2><p><a href="https://make-a-video3d.github.io/">https://make-a-video3d.github.io/</a></p><video src="./rotating_grid.mp4"></video></li></ul><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><ol><li><strong>需要一个有效的、端到端可学习的动态三维场景表征；</strong></li><li><strong>需要一个有监督学习的数据源，因为目前并不存在大规模的（文本，4D）对的数据集可供学习；</strong></li><li><strong>需要在空间和时间维度上扩展输出的分辨率，因为4D输出需要大量的内存和计算能力；</strong></li></ol><h2 id="Proposal"><a href="#Proposal" class="headerlink" title="Proposal"></a>Proposal</h2><ol><li><strong>本文提出了MAV3D，利用了T2V模型和动态NeRFs，实现从自然语言描述生成动态三维时间表示；</strong></li><li><strong>提出了一个从静态到动态的多阶段优化方案，逐步纳入静态、时间和超分辨率模型的梯度信息。</strong></li></ol><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><h3 id="dynamic-NeRFs"><a href="#dynamic-NeRFs" class="headerlink" title="dynamic NeRFs"></a>dynamic NeRFs</h3><p><strong>适用于动态场景的NeRF变体</strong></p><h3 id="MAV"><a href="#MAV" class="headerlink" title="MAV"></a>MAV</h3><p><strong>Make A Video，通过在未标记的视频上训练，拓展了文本到图像（T2I）模型。</strong></p><h3 id="DreamFusion"><a href="#DreamFusion" class="headerlink" title="DreamFusion"></a>DreamFusion</h3><p><strong>以NeRF的形式从文本描述中学习3D表示，提出了一个基于概率密度蒸馏的loss（SDS）</strong></p><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p><img src="https://img.enderfga.cn/img/image-20230209142352394.png" alt=""></p><h3 id="4D-Scene-Representation"><a href="#4D-Scene-Representation" class="headerlink" title="4D Scene Representation"></a>4D Scene Representation</h3><p><img src="https://img.enderfga.cn/img/image-20230209141807225.png" alt=""></p><script type="math/tex; mode=display">\left(\tau, c_i\right)=f_\theta(x, y, z, t)</script><script type="math/tex; mode=display">\left[P_{x y}^{X Y R_1}+P_{z t}^{Z T R_1} ; P_{x z}^{X Z R_2}+P_{y t}^{Y T R_2} ; P_{y z}^{Y Z R_3}+P_{y z}^{X T R_3}\right]</script><h3 id="Dynamic-Scene-Optimization"><a href="#Dynamic-Scene-Optimization" class="headerlink" title="Dynamic Scene Optimization"></a>Dynamic Scene Optimization</h3><h4 id="为了监督4D场景与文本提示p匹配，引入SDS-T（temporal-Score-Distillation-Sampling-）"><a href="#为了监督4D场景与文本提示p匹配，引入SDS-T（temporal-Score-Distillation-Sampling-）" class="headerlink" title="为了监督4D场景与文本提示p匹配，引入SDS-T（temporal Score Distillation Sampling  ）"></a>为了监督4D场景与文本提示p匹配，引入SDS-T（temporal Score Distillation Sampling  ）</h4><script type="math/tex; mode=display">\nabla_\theta \mathcal{L}_{S D S-T}=E_{\sigma, \epsilon}\left[w(\sigma)\left(\hat{\epsilon}\left(V_{(\bar{\theta}, \sigma, \epsilon)} \mid y, \sigma\right)-\epsilon\right) \frac{\partial V_\theta}{\partial \theta}\right]\\</script><script type="math/tex; mode=display">\nabla_\theta \mathcal{L}_{\mathrm{SDS}}(\phi, \mathbf{x}=g(\theta)) \triangleq \mathbb{E}_{t, \epsilon}\left[w(t)\left(\hat{\epsilon}_\phi\left(\mathbf{z}_t ; y, t\right)-\epsilon\right) \frac{\partial \mathbf{x}}{\partial \theta}\right]</script><h4 id="从静态到动态的场景优化"><a href="#从静态到动态的场景优化" class="headerlink" title="从静态到动态的场景优化"></a>从静态到动态的场景优化</h4><h4 id="动态相机"><a href="#动态相机" class="headerlink" title="动态相机"></a>动态相机</h4><h4 id="FPS-采样"><a href="#FPS-采样" class="headerlink" title="FPS 采样"></a>FPS 采样</h4><h4 id="高斯退火"><a href="#高斯退火" class="headerlink" title="高斯退火"></a>高斯退火</h4><h4 id="全变分损失"><a href="#全变分损失" class="headerlink" title="全变分损失"></a>全变分损失</h4><h3 id="Super-Resolution-Fine-Tuning"><a href="#Super-Resolution-Fine-Tuning" class="headerlink" title="Super-Resolution Fine-Tuning"></a>Super-Resolution Fine-Tuning</h3><p><img src="https://img.enderfga.cn/img/image-20230209152345831.png" alt=""></p><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p><strong>Metrics</strong>：R-Precision and human preference</p><p><img src="https://img.enderfga.cn/img/image-20230209152747908.png" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20230209152759865.png" alt=""></p><h2 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h2><ul><li><strong>将动态NeRFs转换为实时应用的不连续网格序列的效率很低，如果能直接预测顶点的轨迹，就能得到改善。</strong></li><li><strong>利用超分辨率信息提高了表示的质量，但对于更高细节的纹理还需要进一步改进。</strong></li><li><strong>文本到四维动态场景生成的表示质量取决于T2V模型从不同视角生成视频的能力。</strong></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>计算机视觉</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Test to 3d随笔</title>
    <link href="/2023/02/09/3d/"/>
    <url>/2023/02/09/3d/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>随便记的笔记</p><span id="more"></span><h1 id="Text-to-3D"><a href="#Text-to-3D" class="headerlink" title="Text-to-3D"></a>Text-to-3D</h1><h2 id="NeRF-Representing-Scenes-as-Neural-Radiance-Fields-for-View-Synthesis"><a href="#NeRF-Representing-Scenes-as-Neural-Radiance-Fields-for-View-Synthesis" class="headerlink" title="NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis"></a>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</h2><p><img src="https://img.enderfga.cn/img/ed4df06e919cae0e638015fa78d935eb_1_Figure_1.png" alt=""></p><p><strong>输入为连续的5维坐标（xyz坐标，以及视野角度theta和phi）；输出是空间位置的体密度以及该位置的发射射线（这里射线是根据视角变化的）。</strong></p><ol><li><strong>用 network 存体素信息: </strong>(x, y, z, \theta, \phi) \rightarrow(\mathbf{c}, \sigma)</li><li><strong>然后用体素渲染方程获得生成视角图片：光线采样+积分</strong><script type="math/tex; mode=display">C(\mathbf{r})=\int_{t_n}^{t_f} T(t) \sigma(\mathbf{r}(t)) \mathbf{c}(\mathbf{r}(t), \mathbf{d}) d t, \text { where } T(t)=\exp \left(-\int_{t_n}^t \sigma(\mathbf{r}(s)) d s\right)</script></li><li><strong>最后与原视角图片计算损失更新网络</strong></li></ol><h2 id="DreamFusion-Text-to-3D-using-2D-Diffusion"><a href="#DreamFusion-Text-to-3D-using-2D-Diffusion" class="headerlink" title="DreamFusion: Text-to-3D using 2D Diffusion"></a>DreamFusion: Text-to-3D using 2D Diffusion</h2><p><img src="https://img.enderfga.cn/img/image-20230202213651008.png" alt=""></p><p><strong>三维合成并不存在大规模的标注数据，也没有一个高效的模型架构对3D数据进行降噪</strong></p><p><strong>使用NERF的格式，使用预训练的text to 2d，加上他们提出的一个基于概率密度蒸馏的loss，证明了预训练图像扩散模型作为先验模型的有效性</strong></p><h2 id="Magic3D-High-Resolution-Text-to-3D-Content-Creation"><a href="#Magic3D-High-Resolution-Text-to-3D-Content-Creation" class="headerlink" title="Magic3D: High-Resolution Text-to-3D Content Creation"></a>Magic3D: High-Resolution Text-to-3D Content Creation</h2><p><img src="https://img.enderfga.cn/img/f3fcff88aa23d692c243bda5b3dd5467_3_Figure_2_1114637308.png" alt=""></p><p><strong>用一个两阶段的优化框架来提高速度和分辨率：利用低分辨率的扩散先验获得一个粗略的模型，并以稀疏的三维哈希网格结构加速。使用粗略表示作为初始化，进一步优化纹理三维网格模型，用高效的可微分渲染器与高分辨率的stable diffusion模型交互。</strong></p><h2 id="Point-E-A-System-for-Generating-3D-Point-Clouds-from-Complex-Prompts"><a href="#Point-E-A-System-for-Generating-3D-Point-Clouds-from-Complex-Prompts" class="headerlink" title="Point-E: A System for Generating 3D Point Clouds from Complex Prompts"></a>Point-E: A System for Generating 3D Point Clouds from Complex Prompts</h2><p><img src="https://img.enderfga.cn/img/v2-b0ca9d9f44550ec6ab34dfe21797ea7e_720w.webp" alt=""></p><p><strong>不输出传统意义上的 3D 图像，它会生成点云，或空间中代表 3D 形状的离散数据点集</strong></p><p><strong>点云更容易合成，但它们无法捕获对象的细粒度形状或纹理，训练了一个额外的人工智能系统来将 Point-E 的点云转换为网格</strong></p><p><strong>算力和时间需求小 但质量差</strong></p><h2 id="Dream3D-Zero-Shot-Text-to-3D-Synthesis-Using-3D-Shape-Prior-and-Text-to-Image-Diffusion-Models"><a href="#Dream3D-Zero-Shot-Text-to-3D-Synthesis-Using-3D-Shape-Prior-and-Text-to-Image-Diffusion-Models" class="headerlink" title="Dream3D: Zero-Shot Text-to-3D Synthesis Using 3D Shape Prior and Text-to-Image Diffusion Models"></a>Dream3D: Zero-Shot Text-to-3D Synthesis Using 3D Shape Prior and Text-to-Image Diffusion Models</h2><p><img src="https://img.enderfga.cn/img/image-20230202234404950.png" alt=""></p><p><strong>引入一个显式3D先验形状，来优化CLIP引导的3D优化任务。具体的讲，首先在文本到形状转换时，使用输入文本生成了一个质量的3D形状来作为先验知识。然后使用它来初始化神经辐射场，并使用完整prompt进行优化</strong></p>]]></content>
    
    
    
    <tags>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>三维手势姿态估计算法研究</title>
    <link href="/2023/01/13/nyu/"/>
    <url>/2023/01/13/nyu/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>人工智能原理实验期末项目</p><span id="more"></span><embed src="./nyu.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Mind-Diffusion</title>
    <link href="/2023/01/13/diff/"/>
    <url>/2023/01/13/diff/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>计算机视觉之diffusion model with mindspore</p><span id="more"></span><embed src="./diff.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于 JMAG 软件的电机仿真分析</title>
    <link href="/2023/01/13/mach/"/>
    <url>/2023/01/13/mach/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>压根没上过课，不知道这写的是啥</p><span id="more"></span><embed src="./mach.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    
    <tags>
      
      <tag>电机与拖动技术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>智能车协同实验</title>
    <link href="/2023/01/13/car/"/>
    <url>/2023/01/13/car/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>多智能体集群控制技术智能车实验报告</p><span id="more"></span><embed src="./1.pdf" width="100%" height="750" type="application/pdf"><embed src="./2.pdf" width="100%" height="750" type="application/pdf"><embed src="./3.pdf" width="100%" height="750" type="application/pdf"><embed src="./4.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    
    <tags>
      
      <tag>多智能体集群</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>四旋翼集群编队</title>
    <link href="/2023/01/13/multi/"/>
    <url>/2023/01/13/multi/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>多智能体集群控制技术期末项目报告</p><span id="more"></span><h1 id="单积分模型SYSU编队设计"><a href="#单积分模型SYSU编队设计" class="headerlink" title="单积分模型SYSU编队设计"></a>单积分模型SYSU编队设计</h1><p><img src="https://img.enderfga.cn/img/SI.gif" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20230113213938616.png" alt=""></p><h1 id="控制四旋翼飞行器实现编队方式设计"><a href="#控制四旋翼飞行器实现编队方式设计" class="headerlink" title="控制四旋翼飞行器实现编队方式设计"></a>控制四旋翼飞行器实现编队方式设计</h1><p><img src="https://img.enderfga.cn/img/test.gif" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20230113214008450.png" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20230113214017680.png" alt=""></p><ul><li><p>Position control</p><script type="math/tex; mode=display">\begin{array}{ll}\text { PID: } & \ddot{\boldsymbol{p}}_{i, c}=\ddot{\boldsymbol{p}}_i{ }^{d e s}+K_{d, i}\left(\dot{\boldsymbol{p}}_i{ }^{\text {des }}-\dot{\boldsymbol{p}}_i\right)+K_{p, i}\left(\boldsymbol{p}_i{ }^{\text {des }}-\boldsymbol{p}_i\right) \\\text { Model: } & u_1=m\left(g+\ddot{\boldsymbol{p}}_{3, c}\right) \quad \text { (Newton Equation) } \\& \phi_c=\frac{1}{g}\left(\ddot{\boldsymbol{p}}_{1, c} \sin \Psi-\ddot{\boldsymbol{p}}_{2, c} \cos \psi \right) \quad \theta_c=\frac{1}{g}\left(\ddot{\boldsymbol{p}}_{1, c} \cos \Psi+\ddot{\boldsymbol{p}}_{2, c} \sin \Psi\right)\end{array}</script></li><li>Attitude control</li></ul><script type="math/tex; mode=display">PID: \quad\left[\begin{array}{c}\ddot{\phi}_c \\ \ddot{\theta}_c \\ \ddot{\psi}_c\end{array}\right]=\left[\begin{array}{c}K_{p, \phi}\left(\phi_c-\phi\right)+K_{d, \phi}\left(\dot{\phi}_c-\dot{\phi}\right) \\ K_{p, \theta}\left(\theta_c-\theta\right)+K_{d, \phi}\left(\dot{\theta}_c-\dot{\theta}\right) \\ K_{p, \psi}\left(\psi_c-\psi\right)+K_{d, \psi}\left(\dot{\psi}_c-\dot{\psi}\right)\end{array}\right]</script><script type="math/tex; mode=display">Model: \quad \boldsymbol{u}_2=\boldsymbol{I} \cdot\left[\begin{array}{c}\ddot{\phi}_c \\ \ddot{\theta}_c \\ \ddot{\psi}_c\end{array}\right]+\left[\begin{array}{c}\omega_x \\ \omega_y \\ \omega_z\end{array}\right] \times \boldsymbol{I} \cdot\left[\begin{array}{c}\omega_x \\ \omega_y \\ \omega_z\end{array}\right] (Euler Equation)</script><p>我使用的控制器遵循上述公式采取了PID控制，结合单积分模型的控制共同决定了结果分数。当然字母间距、运行时间等也能对误差产生一定影响。</p><p>单积分模型中控制增益kv与刚度矩阵R、距离误差z和期望速度dv相乘，起到了限制距离误差和期望速度之间的平衡作用。具体来说，当kv增加时，系统会更快地收敛到目标状态，但是可能会出现振荡。当kv减小时，系统会更缓慢地收敛到目标状态，甚至会导致无人机几乎不动的情况。</p><p>控制器的原理是输入期望控制，输出飞行器整体推力与力矩。公式整体的推导较为复杂，涉及机器人运动学与动力学，且会解欧拉牛顿方程，但对公式的直观理解可以更好理解公式；这个公式基本是外环位置，内环姿态，计算扭矩与推力，可见推力与飞行器质量与z轴加速度有关，通过计算期望角度计算扭矩。</p><p>我使用了Ziegler-Nichols整定方法来调节PID参数，首先将积分和微分增益设置为0，然后比例增益从零开始逐渐增加，直到到达极限增益<em>KU</em>，此时控制器输出值以恒定值振荡。<em>KU</em>和振荡周期<em>TU</em>根据不同的类型，按下表中的方式来设置比例、积分和微分增益。</p><script type="math/tex; mode=display">\begin{array}{|c|c|c|c|}\hline \text { Controller } & K_p & K_d & K_i \\\hline \text { P } & 0.5 K_u & - & - \\\hline \text { PD } & 0.8 K_u & K_p T_u / 8 & - \\\hline \text { PID } & 0.6 K_u & K_p T_u / 8 & 2 K_p / T_u \\\hline\end{array}</script><p>在位置控制中，使用了三轴PID控制器来控制x, y, z轴上的运动。使用了误差积分来消除误差；在姿态控制中，通过计算出当前的欧拉角（phi, theta, psi），并使用欧拉角的导数来控制飞行器的姿态。</p><p>Kp是比例系数，它控制着系统的稳定性和响应速度。当Kp增大时，系统的响应速度会变快，但同时也会增加系统的震荡。</p><p>Ki是积分系数，它控制着系统的累计误差。当Ki增大时，系统会更快地消除误差，但同时也会增加系统的积分饱和。</p><p>Kd是微分系数，它控制着系统的响应速度。当Kd增大时，系统的响应速度会变快，但同时也会增加系统的偏差。</p>]]></content>
    
    
    
    <tags>
      
      <tag>多智能体集群</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Tiny SSD目标检测</title>
    <link href="/2022/12/11/ssd/"/>
    <url>/2022/12/11/ssd/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Github: <a href="https://github.com/Enderfga/TinySSD_sysu">https://github.com/Enderfga/TinySSD_sysu</a></p><span id="more"></span><h1 id="Tiny-SSD-A-Tiny-Single-shot-Detection-Deep-Convolutional-Neural-Network-for-Real-time-Embedded-Object-Detection"><a href="#Tiny-SSD-A-Tiny-Single-shot-Detection-Deep-Convolutional-Neural-Network-for-Real-time-Embedded-Object-Detection" class="headerlink" title="Tiny SSD: A Tiny Single-shot Detection Deep Convolutional Neural Network for Real-time Embedded Object Detection"></a>Tiny SSD: A Tiny Single-shot Detection Deep Convolutional Neural Network for Real-time Embedded Object Detection</h1><p>This repo contains the code, data and trained models for the paper <a href="https://arxiv.org/pdf/1802.06488.pdf">Tiny SSD: A Tiny Single-shot Detection Deep Convolutional Neural Network for Real-time Embedded Object Detection</a>.</p><h2 id="Quick-Links"><a href="#Quick-Links" class="headerlink" title="Quick Links"></a>Quick Links</h2><ul><li><a href="#Overview"> Overview</a></li><li><a href="#Requirements"> Requirements</a></li><li><a href="#How-to-Install">How to Install</a></li><li><a href="#Description-of-Codes">Description of Codes</a></li><li><a href="#Preprocessing"> Preprocessing</a><ul><li><a href="#Preprocessed-Data">Preprocessed Data</a></li></ul></li><li><a href="#How-to-Run">How to Run</a><ul><li><a href="#Train"> Train</a><ul><li><a href="#Finetuning-from-an-existing-checkpoint">Finetuning from an existing checkpoint</a></li></ul></li><li><a href="#Evaluate"> Evaluate</a></li></ul></li><li><a href="#Results-Outputs-Checkpoints">Results, Outputs, Checkpoints</a></li></ul><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>Tiny SSD is a single-shot detection deep convolutional neural network for real-time embedded object detection.<br>It brings together the efficieny of Fire microarchitecture introduced in <strong>SqueezeNet</strong> and object detection performance of <strong>SSD (Single Shot Object Detector)</strong>.</p><p><img src="https://img.enderfga.cn/img/ssd.svg" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20221018133431973.png" alt=""></p><h2 id="Requirements"><a href="#Requirements" class="headerlink" title="Requirements"></a>Requirements</h2><ul><li>numpy</li><li>pandas</li><li>matplotlib</li><li>opencv-python</li><li>torch</li><li>torchvision</li></ul><h2 id="How-to-Install"><a href="#How-to-Install" class="headerlink" title="How to Install"></a>How to Install</h2> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">conda create -n env python=3.8 -y<br>conda activate env<br></code></pre></td></tr></table></figure> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">pip install -r requirements.txt<br></code></pre></td></tr></table></figure><h2 id="Description-of-Files"><a href="#Description-of-Files" class="headerlink" title="Description of Files"></a>Description of Files</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs stylus">│──<span class="hljs-selector-tag">main</span><span class="hljs-selector-class">.py</span>                 -&gt; Run models using different models<br>│──README.md<br>│──requirements.txt<br>│──test<span class="hljs-selector-class">.py</span>                 -&gt; Testing Model<br>│──train<span class="hljs-selector-class">.py</span>                -&gt; Training Model<br>│<br>├─data<br>│  │  dataloader<span class="hljs-selector-class">.py</span>         -&gt; dataloader and <span class="hljs-attribute">transform</span><br>│  │  __init__.py<br>│  │<br>│  └─detection<br>│      │  create_train<span class="hljs-selector-class">.py</span>   -&gt; data preprocessing<br>│      │<br>│      ├─<span class="hljs-attribute">background</span><br>│      ├─sysu_train<br>│      │  │  <span class="hljs-selector-tag">label</span>.csv<br>│      │  │<br>│      │  └─images<br>│      ├─target<br>│      │      <span class="hljs-number">0</span>.jpg<br>│      │      <span class="hljs-number">0</span>.png<br>│      │      <span class="hljs-number">1</span>.png<br>│      │<br>│      └─test<br>│              <span class="hljs-number">1</span>.jpg<br>│              <span class="hljs-number">2</span>.jpg<br>│<br>├─model<br>│  │  TinySSD<span class="hljs-selector-class">.py</span>             -&gt; Definition of the model<br>│  │  __init__.py<br>│  │<br>│  └─checkpoints             -&gt; Trained model weights<br>│     net_100.pkl<br>└─utils                      -&gt; utility functions<br>        anchor.py<br>        iou.py<br>        utils.py<br>        __init__.py<br></code></pre></td></tr></table></figure><h2 id="Preprocessing"><a href="#Preprocessing" class="headerlink" title="Preprocessing"></a>Preprocessing</h2><p>We use /data/detection/background to generate the target detection dataset for our experiments.</p><p>Since the generated data is stored in the repository, there is no need to run this step.</p><h3 id="Preprocessed-Data"><a href="#Preprocessed-Data" class="headerlink" title="Preprocessed Data"></a>Preprocessed Data</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">cd data/detection/<br>python create_train.py<br></code></pre></td></tr></table></figure><h2 id="How-to-Run"><a href="#How-to-Run" class="headerlink" title="How to Run"></a>How to Run</h2><h3 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">python main.py --mode=train --batch_size=256 --epochs=100<br></code></pre></td></tr></table></figure><p>The checkpoints will be saved in a subfolder of <code>./model/checkpoints/</code>.</p><h4 id="Finetuning-from-an-existing-checkpoint"><a href="#Finetuning-from-an-existing-checkpoint" class="headerlink" title="Finetuning from an existing checkpoint"></a>Finetuning from an existing checkpoint</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">python main.py --mode=train --batch_size=256 --epochs=100 --path=[model path]<br></code></pre></td></tr></table></figure><p>model path should be a subdirectory in the <code>./model/checkpoints/</code> directory, e.g. <code>--path=./model/checkpoints/net_100.pkl</code></p><h3 id="Evaluate"><a href="#Evaluate" class="headerlink" title="Evaluate"></a>Evaluate</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">python main.py --mode=test --threshold=<span class="hljs-number">0.3</span> --path=./model/checkpoints/net_100.pkl<br></code></pre></td></tr></table></figure><h2 id="Results-Outputs-Checkpoints"><a href="#Results-Outputs-Checkpoints" class="headerlink" title="Results, Outputs, Checkpoints"></a>Results, Outputs, Checkpoints</h2><p>the ./model/checkpoints/net_100.pkl：class err 1.54e-03, bbox mae 1.90e-03</p><p>I used the following methods to improve performance：</p><ol><li><p>HD anti-white detection object to adapt to the test image</p><p><img src="https://img.enderfga.cn/img/image-20221018160400682.png" alt=""></p></li><li><p>Flip and rotate images, etc. to improve generalization performance</p><p><img src="https://img.enderfga.cn/img/image-20221018155947834.png" alt=""></p></li><li><p>soft_nms</p><p><img src="https://img.enderfga.cn/img/42166d224f4a20a4d58841b70d795a2a730ed0e4.jpeg@f_auto" alt=""></p></li><li><p>smooth_L1</p><p><img src="https://img.enderfga.cn/img/image-20221018155842392.png" alt=""></p></li><li><p>Focal Loss</p><p><img src="https://img.enderfga.cn/img/20221025160416.png" alt=""></p></li></ol><p>If we have more classes, we can further improve the model in the following aspects:</p><ol><li>When an object is much smaller compared with the image, the model could resize the input image bigger.</li><li>There are typically a vast number of negative anchor boxes. To make the class distribution more balanced, we could downsample negative anchor boxes.</li><li>In the loss function, assign different weight hyperparameters to the class loss and the offset loss.</li><li>Use other methods to evaluate the object detection model, such as those in the single shot multibox detection paper (Liu et al., 2016).</li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>人脸检测mtcnn复现</title>
    <link href="/2022/12/11/mtcnn/"/>
    <url>/2022/12/11/mtcnn/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Github: <a href="https://github.com/Enderfga/mtCNN_sysu">https://github.com/Enderfga/mtCNN_sysu</a></p><span id="more"></span><h1 id="Joint-Face-Detection-and-Alignment-using-Multi-task-Cascaded-Convolutional-Networks"><a href="#Joint-Face-Detection-and-Alignment-using-Multi-task-Cascaded-Convolutional-Networks" class="headerlink" title="Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks"></a>Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks</h1><p><strong>This repo contains the code, data and trained models for the paper </strong><a href="https://arxiv.org/ftp/arxiv/papers/1604/1604.02878.pdf">Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks</a>.<strong></strong>Try out the Gradio Web Demo: <a href="https://huggingface.co/spaces/Enderfga/mtCNN_sysu"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue" alt="Hugging Face Spaces"></a><br><img src="https://img.enderfga.cn/img/1faec03527783e6e8ee03d519e167aa.png" alt=""></p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p><strong>MTCNN is a popular algorithm for face detection that uses multiple neural networks to detect faces in images. It is capable of detecting faces under various lighting and pose conditions and can detect multiple faces in an image.</strong></p><p><strong>We have implemented MTCNN using the pytorch framework. Pytorch is a popular deep learning framework that provides tools for building and training neural networks. </strong></p><p><img src="https://img.enderfga.cn/img/image-20221208152130975.png" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20221208152231511.png" alt=""></p><h2 id="Description-of-file"><a href="#Description-of-file" class="headerlink" title="Description of file"></a>Description of file</h2><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs vim">├── README.md                      # explanatory document<br>├── get_data.<span class="hljs-keyword">py</span>                    # Generate corresponding training data depending <span class="hljs-keyword">on</span> the <span class="hljs-built_in">input</span> “--net”<br>├── img                            # mid.png <span class="hljs-keyword">is</span> used <span class="hljs-keyword">for</span> testing visualization effects,other images are the corresponding results.<br>│   ├── mid.png<br>│   ├── onet.png<br>│   ├── pnet.png<br>│   ├── rnet.png<br>│   ├── result.png<br>│   └── result.jpg<br>├── model_store                    # Our <span class="hljs-keyword">pre</span>-trained model<br>│   ├── onet_epoch_20.<span class="hljs-keyword">pt</span><br>│   ├── pnet_epoch_20.<span class="hljs-keyword">pt</span><br>│   └── rnet_epoch_20.<span class="hljs-keyword">pt</span><br>├── requirements.txt               # Environmental <span class="hljs-keyword">version</span> requirements<br>├── test.<span class="hljs-keyword">py</span>                        # Specify different <span class="hljs-string">&quot;--net&quot;</span> <span class="hljs-keyword">to</span> <span class="hljs-built_in">get</span> the corresponding visualization results<br>├── test.<span class="hljs-keyword">sh</span>                        # Used <span class="hljs-keyword">to</span> test mid.png, which will test the output visualization of three networks<br>├── train.out                      # Our <span class="hljs-built_in">complete</span> training <span class="hljs-built_in">log</span> <span class="hljs-keyword">for</span> this experiment<br>├── train.<span class="hljs-keyword">py</span>                       # Specify different <span class="hljs-string">&quot;--net&quot;</span> <span class="hljs-keyword">for</span> the training of the corresponding network<br>├── train.<span class="hljs-keyword">sh</span>                       # Generate data from start <span class="hljs-keyword">to</span> <span class="hljs-keyword">finish</span> <span class="hljs-built_in">and</span> train<br>└── utils                          # Some common tool functions <span class="hljs-built_in">and</span> modules<br>    ├── config.<span class="hljs-keyword">py</span><br>    ├── dataloader.<span class="hljs-keyword">py</span><br>    ├── detect.<span class="hljs-keyword">py</span><br>    ├── models.<span class="hljs-keyword">py</span><br>    ├── tool.<span class="hljs-keyword">py</span><br>    └── vision.<span class="hljs-keyword">py</span><br></code></pre></td></tr></table></figure><h2 id="Requirements"><a href="#Requirements" class="headerlink" title="Requirements"></a>Requirements</h2><ul><li><strong>numpy==1.21.4</strong></li><li><strong>matplotlib==3.5.0</strong></li><li><strong>opencv-python==4.4.0.42</strong></li><li><strong>torch==1.13.0+cu116</strong></li></ul><h2 id="How-to-Install"><a href="#How-to-Install" class="headerlink" title="How to Install"></a>How to Install</h2> <figure class="highlight mel"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs mel">conda create -n <span class="hljs-keyword">env</span> <span class="hljs-keyword">python</span>=<span class="hljs-number">3.8</span> -y<br>conda activate <span class="hljs-keyword">env</span><br></code></pre></td></tr></table></figure> <figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">pip <span class="hljs-keyword">install</span> -r requirements.txt<br></code></pre></td></tr></table></figure><h2 id="Preprocessing"><a href="#Preprocessing" class="headerlink" title="Preprocessing"></a>Preprocessing</h2><ul><li><strong>download </strong><a href="http://shuoyang1213.me/WIDERFACE/">WIDER_FACE</a> face detection data then store it into ./data_set/face_detection</li><li><strong>download </strong><a href="http://mmlab.ie.cuhk.edu.hk/archive/CNN_FacePoint.htm">CNN_FacePoint</a> face detection and landmark data then store it into ./data_set/face_landmark</li></ul><h3 id="Preprocessed-Data"><a href="#Preprocessed-Data" class="headerlink" title="Preprocessed Data"></a>Preprocessed Data</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># Before training Pnet</span><br>python get_data.py <span class="hljs-attribute">--net</span>=pnet<br><span class="hljs-comment"># Before training Rnet, please use your trained model path</span><br>python get_data.py <span class="hljs-attribute">--net</span>=rnet <span class="hljs-attribute">--pnet_path</span>=./model_store/pnet_epoch_20.pt<br><span class="hljs-comment"># Before training Onet, please use your trained model path</span><br>python get_data.py <span class="hljs-attribute">--net</span>=onet <span class="hljs-attribute">--pnet_path</span>=./model_store/pnet_epoch_20.pt <span class="hljs-attribute">--rnet_path</span>=./model_store/rnet_epoch_20.pt<br></code></pre></td></tr></table></figure><h2 id="How-to-Run"><a href="#How-to-Run" class="headerlink" title="How to Run"></a>How to Run</h2><h3 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h3><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs vim"><span class="hljs-keyword">python</span> train.<span class="hljs-keyword">py</span> --net=pnet/rnet/onet #Specify the corresponding network <span class="hljs-keyword">to</span> start training<br>bash train.<span class="hljs-keyword">sh</span>                        #Alternatively, use the <span class="hljs-keyword">sh</span> <span class="hljs-keyword">file</span> <span class="hljs-keyword">to</span> train in order<br></code></pre></td></tr></table></figure><p><strong>The checkpoints will be saved in a subfolder of </strong><code>./model_store/*</code>.</p><h4 id="Finetuning-from-an-existing-checkpoint"><a href="#Finetuning-from-an-existing-checkpoint" class="headerlink" title="Finetuning from an existing checkpoint"></a>Finetuning from an existing checkpoint</h4><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">python train.py --net=pnet<span class="hljs-regexp">/rnet/</span>onet --load=[model path]<br></code></pre></td></tr></table></figure><p><strong>model path should be a subdirectory in the </strong><code>./model_store/</code> directory, e.g. <code>--load=./model_store/pnet_epoch_20.pt</code></p><h3 id="Evaluate"><a href="#Evaluate" class="headerlink" title="Evaluate"></a>Evaluate</h3><h4 id="Use-the-sh-file-to-test-in-order"><a href="#Use-the-sh-file-to-test-in-order" class="headerlink" title="Use the sh file to test in order"></a>Use the sh file to test in order</h4><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stata">bash <span class="hljs-keyword">test</span>.<span class="hljs-keyword">sh</span><br></code></pre></td></tr></table></figure><h4 id="To-detect-a-single-image"><a href="#To-detect-a-single-image" class="headerlink" title="To detect a single image"></a>To detect a single image</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">python test.py <span class="hljs-attribute">--net</span>=pnet/rnet/onet  <span class="hljs-attribute">--path</span>=test.jpg<br></code></pre></td></tr></table></figure><h4 id="To-detect-a-video-stream-from-a-camera"><a href="#To-detect-a-video-stream-from-a-camera" class="headerlink" title="To detect a video stream from a camera"></a>To detect a video stream from a camera</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">python test.py <span class="hljs-attribute">--input_mode</span>=0<br></code></pre></td></tr></table></figure><h4 id="The-result-of-“—net-pnet”"><a href="#The-result-of-“—net-pnet”" class="headerlink" title="The result of  “—net=pnet”"></a>The result of  “—net=pnet”</h4><p><img src="https://img.enderfga.cn/img/20221208160900.png" alt=""></p><h4 id="The-result-of-“—net-rnet”"><a href="#The-result-of-“—net-rnet”" class="headerlink" title="The result of  “—net=rnet”"></a>The result of  “—net=rnet”</h4><p><img src="https://img.enderfga.cn/img/image-20221208155022083.png" alt=""></p><h4 id="The-result-of-“—net-onet”"><a href="#The-result-of-“—net-onet”" class="headerlink" title="The result of  “—net=onet”"></a>The result of  “—net=onet”</h4><p><img src="https://img.enderfga.cn/img/image-20221208155044451.png" alt=""></p><embed src="./mtcnn.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>无人机飞越冰湖</title>
    <link href="/2022/11/22/RL/"/>
    <url>/2022/11/22/RL/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><embed src="./RL.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    
    <tags>
      
      <tag>强化学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Plant Pathology 2021 with MindSpore</title>
    <link href="/2022/11/07/plant/"/>
    <url>/2022/11/07/plant/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><embed src="./plant.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>命令行的艺术</title>
    <link href="/2022/11/03/shell/"/>
    <url>/2022/11/03/shell/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="命令行的艺术"><a href="#命令行的艺术" class="headerlink" title="命令行的艺术"></a>命令行的艺术</h1><p><a href="https://gitter.im/jlevy/the-art-of-command-line?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge&amp;utm_content=badge"><img src="https://badges.gitter.im/Join Chat.svg" alt="Join the chat at https://gitter.im/jlevy/the-art-of-command-line"></a></p><ul><li><a href="#前言">前言</a></li><li><a href="#基础">基础</a></li><li><a href="#日常使用">日常使用</a></li><li><a href="#文件及数据处理">文件及数据处理</a></li><li><a href="#系统调试">系统调试</a></li><li><a href="#单行脚本">单行脚本</a></li><li><a href="#冷门但有用">冷门但有用</a></li><li><a href="#仅限-os-x-系统">仅限 OS X 系统</a></li><li><a href="#仅限-windows-系统">仅限 Windows 系统</a></li><li><a href="#更多资源">更多资源</a></li><li><a href="#免责声明">免责声明</a></li></ul><p><img src="https://img.enderfga.cn/img/image-20221103112627881.png" alt=""></p><p>熟练使用命令行是一种常常被忽视，或被认为难以掌握的技能，但实际上，它会提高你作为工程师的灵活性以及生产力。本文是一份我在 Linux 上工作时，发现的一些命令行使用技巧的摘要。有些技巧非常基础，而另一些则相当复杂，甚至晦涩难懂。这篇文章并不长，但当你能够熟练掌握这里列出的所有技巧时，你就学会了很多关于命令行的东西了。</p><p>这篇文章是<a href="AUTHORS.md">许多作者和译者</a>共同的成果。<br>这里的部分内容<br><a href="http://www.quora.com/What-are-some-lesser-known-but-useful-Unix-commands">首次</a><br><a href="http://www.quora.com/What-are-the-most-useful-Swiss-army-knife-one-liners-on-Unix">出现</a><br>于 <a href="http://www.quora.com/What-are-some-time-saving-tips-that-every-Linux-user-should-know">Quora</a>，<br>但已经迁移到了 Github，并由众多高手做出了许多改进。<br>如果你在本文中发现了错误或者存在可以改善的地方，请<a href="/CONTRIBUTING.md"><strong>贡献你的一份力量</strong></a>。</p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>涵盖范围：</p><ul><li>这篇文章不仅能帮助刚接触命令行的新手，而且对具有经验的人也大有裨益。本文致力于做到<em>覆盖面广</em>（涉及所有重要的内容），<em>具体</em>（给出具体的最常用的例子），以及<em>简洁</em>（避免冗余的内容，或是可以在其他地方轻松查到的细枝末节）。在特定应用场景下，本文的内容属于基本功或者能帮助您节约大量的时间。</li><li>本文主要为 Linux 所写，但在<a href="#仅限-os-x-系统">仅限 OS X 系统</a>章节和<a href="#仅限-windows-系统">仅限 Windows 系统</a>章节中也包含有对应操作系统的内容。除去这两个章节外，其它的内容大部分均可在其他类 Unix 系统或 OS X，甚至 Cygwin 中得到应用。</li><li>本文主要关注于交互式 Bash，但也有很多技巧可以应用于其他 shell 和 Bash 脚本当中。</li><li>除去“标准的”Unix 命令，本文还包括了一些依赖于特定软件包的命令（前提是它们具有足够的价值）。</li></ul><p>注意事项：</p><ul><li>为了能在一页内展示尽量多的东西，一些具体的信息可以在引用的页面中找到。我们相信机智的你知道如何使用 Google 或者其他搜索引擎来查阅到更多的详细信息。文中部分命令需要您使用 <code>apt-get</code>，<code>yum</code>，<code>dnf</code>，<code>pacman</code>，<br><code>pip</code> 或 <code>brew</code>（以及其它合适的包管理器）来安装依赖的程序。</li><li>遇到问题的话，请尝试使用 <a href="http://explainshell.com/">Explainshell</a> 去获取相关命令、参数、管道等内容的解释。</li></ul><h2 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h2><ul><li>学习 Bash 的基础知识。具体地，在命令行中输入 <code>man bash</code> 并至少全文浏览一遍; 它理解起来很简单并且不冗长。其他的 shell 可能很好用，但 Bash 的功能已经足够强大并且到几乎总是可用的（ 如果你<em>只</em>学习 zsh，fish 或其他的 shell 的话，在你自己的设备上会显得很方便，但过度依赖这些功能会给您带来不便，例如当你需要在服务器上工作时）。</li><li>熟悉至少一个基于文本的编辑器。通常而言 Vim （<code>vi</code>） 会是你最好的选择，毕竟在终端中编辑文本时 Vim 是最好用的工具（甚至大部分情况下 Vim 要比 Emacs、大型 IDE 或是炫酷的编辑器更好用）。</li><li>学会如何使用 <code>man</code> 命令去阅读文档。学会使用 <code>apropos</code> 去查找文档。知道有些命令并不对应可执行文件，而是在 Bash 内置好的，此时可以使用 <code>help</code> 和 <code>help -d</code> 命令获取帮助信息。你可以用 <code>type 命令</code> 来判断这个命令到底是可执行文件、shell 内置命令还是别名。</li><li>学会使用 <code>&gt;</code> 和 <code>&lt;</code> 来重定向输出和输入，学会使用 <code>|</code> 来重定向管道。明白 <code>&gt;</code> 会覆盖了输出文件而 <code>&gt;&gt;</code> 是在文件末添加。了解标准输出 stdout 和标准错误 stderr。</li><li>学会使用通配符 <code>*</code> （或许再算上 <code>?</code> 和 <code>[</code>…<code>]</code>） 和引用以及引用中 <code>&#39;</code> 和 <code>&quot;</code> 的区别（后文中有一些具体的例子）。</li><li>熟悉 Bash 中的任务管理工具：<code>&amp;</code>，<strong>ctrl-z</strong>，<strong>ctrl-c</strong>，<code>jobs</code>，<code>fg</code>，<code>bg</code>，<code>kill</code> 等。</li><li>学会使用 <code>ssh</code> 进行远程命令行登录，最好知道如何使用 <code>ssh-agent</code>，<code>ssh-add</code> 等命令来实现基础的无密码认证登录。</li><li>学会基本的文件管理工具：<code>ls</code> 和 <code>ls -l</code> （了解 <code>ls -l</code> 中每一列代表的意义），<code>less</code>，<code>head</code>，<code>tail</code> 和 <code>tail -f</code> （甚至 <code>less +F</code>），<code>ln</code> 和 <code>ln -s</code> （了解硬链接与软链接的区别），<code>chown</code>，<code>chmod</code>，<code>du</code> （硬盘使用情况概述：<code>du -hs *</code>）。 关于文件系统的管理，学习 <code>df</code>，<code>mount</code>，<code>fdisk</code>，<code>mkfs</code>，<code>lsblk</code>。知道 inode 是什么（与 <code>ls -i</code> 和 <code>df -i</code> 等命令相关）。</li><li>学习基本的网络管理工具：<code>ip</code> 或 <code>ifconfig</code>，<code>dig</code>。</li><li>学习并使用一种版本控制管理系统，例如 <code>git</code>。</li><li>熟悉正则表达式，学会使用 <code>grep</code>／<code>egrep</code>，它们的参数中 <code>-i</code>，<code>-o</code>，<code>-v</code>，<code>-A</code>，<code>-B</code> 和 <code>-C</code> 这些是很常用并值得认真学习的。</li><li>学会使用 <code>apt-get</code>，<code>yum</code>，<code>dnf</code> 或 <code>pacman</code> （具体使用哪个取决于你使用的 Linux 发行版）来查找和安装软件包。并确保你的环境中有 <code>pip</code> 来安装基于 Python 的命令行工具 （接下来提到的部分程序使用 <code>pip</code> 来安装会很方便）。</li></ul><h2 id="日常使用"><a href="#日常使用" class="headerlink" title="日常使用"></a>日常使用</h2><ul><li>在 Bash 中，可以通过按 <strong>Tab</strong> 键实现自动补全参数，使用 <strong>ctrl-r</strong> 搜索命令行历史记录（按下按键之后，输入关键字便可以搜索，重复按下 <strong>ctrl-r</strong> 会向后查找匹配项，按下 <strong>Enter</strong> 键会执行当前匹配的命令，而按下右方向键会将匹配项放入当前行中，不会直接执行，以便做出修改）。</li><li>在 Bash 中，可以按下 <strong>ctrl-w</strong> 删除你键入的最后一个单词，<strong>ctrl-u</strong> 可以删除行内光标所在位置之前的内容，<strong>alt-b</strong> 和 <strong>alt-f</strong> 可以以单词为单位移动光标，<strong>ctrl-a</strong> 可以将光标移至行首，<strong>ctrl-e</strong> 可以将光标移至行尾，<strong>ctrl-k</strong> 可以删除光标至行尾的所有内容，<strong>ctrl-l</strong> 可以清屏。键入 <code>man readline</code> 可以查看 Bash 中的默认快捷键。内容有很多，例如 <strong>alt-.</strong> 循环地移向前一个参数，而 <strong>alt-*</strong> 可以展开通配符。</li><li>你喜欢的话，可以执行 <code>set -o vi</code> 来使用 vi 风格的快捷键，而执行 <code>set -o emacs</code> 可以把它改回来。</li><li>为了便于编辑长命令，在设置你的默认编辑器后（例如 <code>export EDITOR=vim</code>），<strong>ctrl-x</strong> <strong>ctrl-e</strong> 会打开一个编辑器来编辑当前输入的命令。在 vi 风格下快捷键则是 <strong>escape-v</strong>。</li><li>键入 <code>history</code> 查看命令行历史记录，再用 <code>!n</code>（<code>n</code> 是命令编号）就可以再次执行。其中有许多缩写，最有用的大概就是 <code>!$</code>， 它用于指代上次键入的参数，而 <code>!!</code> 可以指代上次键入的命令了（参考 man 页面中的“HISTORY EXPANSION”）。不过这些功能，你也可以通过快捷键 <strong>ctrl-r</strong> 和 <strong>alt-.</strong> 来实现。</li><li><code>cd</code> 命令可以切换工作路径，输入 <code>cd ~</code> 可以进入 home 目录。要访问你的 home 目录中的文件，可以使用前缀 <code>~</code>（例如 <code>~/.bashrc</code>）。在 <code>sh</code> 脚本里则用环境变量 <code>$HOME</code> 指代 home 目录的路径。</li><li>回到前一个工作路径：<code>cd -</code>。</li><li>如果你输入命令的时候中途改了主意，按下 <strong>alt-#</strong> 在行首添加 <code>#</code> 把它当做注释再按下回车执行（或者依次按下 <strong>ctrl-a</strong>， <strong>#</strong>， <strong>enter</strong>）。这样做的话，之后借助命令行历史记录，你可以很方便恢复你刚才输入到一半的命令。</li><li>使用 <code>xargs</code> （ 或 <code>parallel</code>）。他们非常给力。注意到你可以控制每行参数个数（<code>-L</code>）和最大并行数（<code>-P</code>）。如果你不确定它们是否会按你想的那样工作，先使用 <code>xargs echo</code> 查看一下。此外，使用 <code>-I&#123;&#125;</code> 会很方便。例如：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">find . -name <span class="hljs-string">&#x27;*.py&#x27;</span> | xargs grep some_function<br>cat hosts | xargs -I&#123;&#125; ssh root@&#123;&#125; hostname<br></code></pre></td></tr></table></figure><ul><li><code>pstree -p</code> 以一种优雅的方式展示进程树。</li><li>使用 <code>pgrep</code> 和 <code>pkill</code> 根据名字查找进程或发送信号（<code>-f</code> 参数通常有用）。</li><li>了解你可以发往进程的信号的种类。比如，使用 <code>kill -STOP [pid]</code> 停止一个进程。使用 <code>man 7 signal</code> 查看详细列表。</li><li>使用 <code>nohup</code> 或 <code>disown</code> 使一个后台进程持续运行。</li><li>使用 <code>netstat -lntp</code> 或 <code>ss -plat</code> 检查哪些进程在监听端口（默认是检查 TCP 端口; 添加参数 <code>-u</code> 则检查 UDP 端口）或者 <code>lsof -iTCP -sTCP:LISTEN -P -n</code> (这也可以在 OS X 上运行)。</li><li><code>lsof</code> 来查看开启的套接字和文件。</li><li>使用 <code>uptime</code> 或 <code>w</code> 来查看系统已经运行多长时间。</li><li>使用 <code>alias</code> 来创建常用命令的快捷形式。例如：<code>alias ll=&#39;ls -latr&#39;</code> 创建了一个新的命令别名 <code>ll</code>。</li><li>可以把别名、shell 选项和常用函数保存在 <code>~/.bashrc</code>，具体看下这篇<a href="http://superuser.com/a/183980/7106">文章</a>。这样做的话你就可以在所有 shell 会话中使用你的设定。</li><li>把环境变量的设定以及登陆时要执行的命令保存在 <code>~/.bash_profile</code>。而对于从图形界面启动的 shell 和 <code>cron</code> 启动的 shell，则需要单独配置文件。</li><li>要想在几台电脑中同步你的配置文件（例如 <code>.bashrc</code> 和 <code>.bash_profile</code>），可以借助 Git。</li><li>当变量和文件名中包含空格的时候要格外小心。Bash 变量要用引号括起来，比如 <code>&quot;$FOO&quot;</code>。尽量使用 <code>-0</code> 或 <code>-print0</code> 选项以便用 NULL 来分隔文件名，例如 <code>locate -0 pattern | xargs -0 ls -al</code> 或 <code>find / -print0 -type d | xargs -0 ls -al</code>。如果 for 循环中循环访问的文件名含有空字符（空格、tab 等字符），只需用 <code>IFS=$&#39;\n&#39;</code> 把内部字段分隔符设为换行符。</li><li>在 Bash 脚本中，使用 <code>set -x</code> 去调试输出（或者使用它的变体 <code>set -v</code>，它会记录原始输入，包括多余的参数和注释）。尽可能地使用严格模式：使用 <code>set -e</code> 令脚本在发生错误时退出而不是继续运行；使用 <code>set -u</code> 来检查是否使用了未赋值的变量；试试 <code>set -o pipefail</code>，它可以监测管道中的错误。当牵扯到很多脚本时，使用 <code>trap</code> 来检测 ERR 和 EXIT。一个好的习惯是在脚本文件开头这样写，这会使它能够检测一些错误，并在错误发生时中断程序并输出信息：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">set</span> -euo pipefail<br><span class="hljs-built_in">trap</span> <span class="hljs-string">&quot;echo &#x27;error: Script failed: see failed command above&#x27;&quot;</span> ERR<br></code></pre></td></tr></table></figure><ul><li>在 Bash 脚本中，子 shell（使用括号 <code>(...)</code>）是一种组织参数的便捷方式。一个常见的例子是临时地移动工作路径，代码如下：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># do something in current dir</span><br>(<span class="hljs-built_in">cd</span> /some/other/dir &amp;&amp; other-command)<br><span class="hljs-comment"># continue in original dir</span><br></code></pre></td></tr></table></figure><ul><li>在 Bash 中，变量有许多的扩展方式。<code>$&#123;name:?error message&#125;</code> 用于检查变量是否存在。此外，当 Bash 脚本只需要一个参数时，可以使用这样的代码 <code>input_file=$&#123;1:?usage: $0 input_file&#125;</code>。在变量为空时使用默认值：<code>$&#123;name:-default&#125;</code>。如果你要在之前的例子中再加一个（可选的）参数，可以使用类似这样的代码 <code>output_file=$&#123;2:-logfile&#125;</code>，如果省略了 $2，它的值就为空，于是 <code>output_file</code> 就会被设为 <code>logfile</code>。数学表达式：<code>i=$(( (i + 1) % 5 ))</code>。序列：<code>&#123;1..10&#125;</code>。截断字符串：<code>$&#123;var%suffix&#125;</code> 和 <code>$&#123;var#prefix&#125;</code>。例如，假设 <code>var=foo.pdf</code>，那么 <code>echo $&#123;var%.pdf&#125;.txt</code>将输出<code>foo.txt</code>。</li><li>使用括号扩展（<code>&#123;</code>…<code>&#125;</code>）来减少输入相似文本，并自动化文本组合。这在某些情况下会很有用，例如 <code>mv foo.&#123;txt,pdf&#125; some-dir</code>（同时移动两个文件），<code>cp somefile&#123;,.bak&#125;</code>（会被扩展成 <code>cp somefile somefile.bak</code>）或者 <code>mkdir -p test-&#123;a,b,c&#125;/subtest-&#123;1,2,3&#125;</code>（会被扩展成所有可能的组合，并创建一个目录树）。</li><li>通过使用 <code>&lt;(some command)</code> 可以将输出视为文件。例如，对比本地文件 <code>/etc/hosts</code> 和一个远程文件：</li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">diff /etc/hosts &lt;(ssh somehost cat /etc/hosts)<br></code></pre></td></tr></table></figure><ul><li>编写脚本时，你可能会想要把代码都放在大括号里。缺少右括号的话，代码就会因为语法错误而无法执行。如果你的脚本是要放在网上分享供他人使用的，这样的写法就体现出它的好处了，因为这样可以防止下载不完全代码被执行。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">&#123;<br>      <span class="hljs-comment"># 在这里写代码</span><br>&#125;<br></code></pre></td></tr></table></figure><ul><li>了解 Bash 中的“here documents”，例如 <code>cat &lt;&lt;EOF ...</code>。</li><li>在 Bash 中，同时重定向标准输出和标准错误：<code>some-command &gt;logfile 2&gt;&amp;1</code> 或者 <code>some-command &amp;&gt;logfile</code>。通常，为了保证命令不会在标准输入里残留一个未关闭的文件句柄捆绑在你当前所在的终端上，在命令后添加 <code>&lt;/dev/null</code> 是一个好习惯。</li><li>使用 <code>man ascii</code> 查看具有十六进制和十进制值的ASCII表。<code>man unicode</code>，<code>man utf-8</code>，以及 <code>man latin1</code> 有助于你去了解通用的编码信息。</li><li>使用 <code>screen</code> 或 <a href="https://tmux.github.io/"><code>tmux</code></a> 来使用多份屏幕，当你在使用 ssh 时（保存 session 信息）将尤为有用。而 <code>byobu</code> 可以为它们提供更多的信息和易用的管理工具。另一个轻量级的 session 持久化解决方案是 <a href="https://github.com/bogner/dtach"><code>dtach</code></a>。</li><li>ssh 中，了解如何使用 <code>-L</code> 或 <code>-D</code>（偶尔需要用 <code>-R</code>）开启隧道是非常有用的，比如当你需要从一台远程服务器上访问 web 页面。</li><li>对 ssh 设置做一些小优化可能是很有用的，例如这个 <code>~/.ssh/config</code> 文件包含了防止特定网络环境下连接断开、压缩数据、多通道等选项：</li></ul><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-attribute">TCPKeepAlive</span>=<span class="hljs-literal">yes</span><br><span class="hljs-attribute">ServerAliveInterval</span>=15<br><span class="hljs-attribute">ServerAliveCountMax</span>=6<br><span class="hljs-attribute">Compression</span>=<span class="hljs-literal">yes</span><br>ControlMaster auto<br>ControlPath /tmp/%r@%h:%p<br>ControlPersist <span class="hljs-literal">yes</span><br></code></pre></td></tr></table></figure><ul><li>一些其他的关于 ssh 的选项是与安全相关的，应当小心翼翼的使用。例如你应当只能在可信任的网络中启用 <code>StrictHostKeyChecking=no</code>，<code>ForwardAgent=yes</code>。</li><li>考虑使用 <a href="https://mosh.mit.edu/"><code>mosh</code></a> 作为 ssh 的替代品，它使用 UDP 协议。它可以避免连接被中断并且对带宽需求更小，但它需要在服务端做相应的配置。</li><li>获取八进制形式的文件访问权限（修改系统设置时通常需要，但 <code>ls</code> 的功能不那么好用并且通常会搞砸），可以使用类似如下的代码：</li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-built_in">stat</span> -c <span class="hljs-string">&#x27;%A %a %n&#x27;</span> /etc/timezone<br></code></pre></td></tr></table></figure><ul><li>使用 <a href="https://github.com/mooz/percol"><code>percol</code></a> 或者 <a href="https://github.com/junegunn/fzf"><code>fzf</code></a> 可以交互式地从另一个命令输出中选取值。</li><li>使用 <code>fpp</code>（<a href="https://github.com/facebook/PathPicker">PathPicker</a>）可以与基于另一个命令(例如 <code>git</code>）输出的文件交互。</li><li>将 web 服务器上当前目录下所有的文件（以及子目录）暴露给你所处网络的所有用户，使用：<br><code>python -m SimpleHTTPServer 7777</code> （使用端口 7777 和 Python 2）或 <code>python -m http.server 7777</code> （使用端口 7777 和 Python 3）。</li><li>以其他用户的身份执行命令，使用 <code>sudo</code>。默认以 root 用户的身份执行；使用 <code>-u</code> 来指定其他用户。使用 <code>-i</code> 来以该用户登录（需要输入_你自己的_密码）。</li><li>将 shell 切换为其他用户，使用 <code>su username</code> 或者 <code>su - username</code>。加入 <code>-</code> 会使得切换后的环境与使用该用户登录后的环境相同。省略用户名则默认为 root。切换到哪个用户，就需要输入_哪个用户的_密码。</li><li>了解命令行的 <a href="https://wiki.debian.org/CommonErrorMessages/ArgumentListTooLong">128K 限制</a>。使用通配符匹配大量文件名时，常会遇到“Argument list too long”的错误信息。（这种情况下换用 <code>find</code> 或 <code>xargs</code> 通常可以解决。）</li><li>当你需要一个基本的计算器时，可以使用 <code>python</code> 解释器（当然你要用 python 的时候也是这样）。例如：</li></ul><figure class="highlight python-repl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python-repl"><span class="hljs-meta">&gt;&gt;&gt;</span> <span class="python"><span class="hljs-number">2</span>+<span class="hljs-number">3</span></span><br>5<br></code></pre></td></tr></table></figure><h2 id="文件及数据处理"><a href="#文件及数据处理" class="headerlink" title="文件及数据处理"></a>文件及数据处理</h2><ul><li>在当前目录下通过文件名查找一个文件，使用类似于这样的命令：<code>find . -iname &#39;*something*&#39;</code>。在所有路径下通过文件名查找文件，使用 <code>locate something</code> （但注意到 <code>updatedb</code> 可能没有对最近新建的文件建立索引，所以你可能无法定位到这些未被索引的文件）。</li><li>使用 <a href="https://github.com/ggreer/the_silver_searcher"><code>ag</code></a> 在源代码或数据文件里检索（<code>grep -r</code> 同样可以做到，但相比之下 <code>ag</code> 更加先进）。</li><li>将 HTML 转为文本：<code>lynx -dump -stdin</code>。</li><li>Markdown，HTML，以及所有文档格式之间的转换，试试 <a href="http://pandoc.org/"><code>pandoc</code></a>。</li><li>当你要处理棘手的 XML 时候，<code>xmlstarlet</code> 算是上古时代流传下来的神器。</li><li>使用 <a href="http://stedolan.github.io/jq/"><code>jq</code></a> 处理 JSON。</li><li>使用 <a href="https://github.com/0k/shyaml"><code>shyaml</code></a> 处理 YAML。</li><li>要处理 Excel 或 CSV 文件的话，<a href="https://github.com/onyxfish/csvkit">csvkit</a> 提供了 <code>in2csv</code>，<code>csvcut</code>，<code>csvjoin</code>，<code>csvgrep</code> 等方便易用的工具。</li><li>当你要处理 Amazon S3 相关的工作的时候，<a href="https://github.com/s3tools/s3cmd"><code>s3cmd</code></a> 是一个很方便的工具而 <a href="https://github.com/bloomreach/s4cmd"><code>s4cmd</code></a> 的效率更高。Amazon 官方提供的 <a href="https://github.com/aws/aws-cli"><code>aws</code></a> 以及  <a href="https://github.com/donnemartin/saws"><code>saws</code></a> 是其他 AWS 相关工作的基础，值得学习。</li><li>了解如何使用 <code>sort</code> 和 <code>uniq</code>，包括 uniq 的 <code>-u</code> 参数和 <code>-d</code> 参数，具体内容在后文单行脚本节中。另外可以了解一下 <code>comm</code>。</li><li>了解如何使用 <code>cut</code>，<code>paste</code> 和 <code>join</code> 来更改文件。很多人都会使用 <code>cut</code>，但遗忘了 <code>join</code>。</li><li>了解如何运用 <code>wc</code> 去计算新行数（<code>-l</code>），字符数（<code>-m</code>），单词数（<code>-w</code>）以及字节数（<code>-c</code>）。</li><li>了解如何使用 <code>tee</code> 将标准输入复制到文件甚至标准输出，例如 <code>ls -al | tee file.txt</code>。</li><li>要进行一些复杂的计算，比如分组、逆序和一些其他的统计分析，可以考虑使用 <a href="https://www.gnu.org/software/datamash/"><code>datamash</code></a>。</li><li>注意到语言设置（中文或英文等）对许多命令行工具有一些微妙的影响，比如排序的顺序和性能。大多数 Linux 的安装过程会将 <code>LANG</code> 或其他有关的变量设置为符合本地的设置。要意识到当你改变语言设置时，排序的结果可能会改变。明白国际化可能会使 sort 或其他命令运行效率下降<em>许多倍</em>。某些情况下（例如集合运算）你可以放心的使用 <code>export LC_ALL=C</code> 来忽略掉国际化并按照字节来判断顺序。</li><li>你可以单独指定某一条命令的环境，只需在调用时把环境变量设定放在命令的前面，例如 <code>TZ=Pacific/Fiji date</code> 可以获取斐济的时间。</li><li>了解如何使用 <code>awk</code> 和 <code>sed</code> 来进行简单的数据处理。 参阅 <a href="#one-liners">One-liners</a> 获取示例。</li><li>替换一个或多个文件中出现的字符串：</li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">perl -pi.bak -e <span class="hljs-string">&#x27;s/old-string/new-string/g&#x27;</span> my-files-*.txt<br></code></pre></td></tr></table></figure><ul><li>使用 <a href="https://github.com/jlevy/repren"><code>repren</code></a> 来批量重命名文件，或是在多个文件中搜索替换内容。（有些时候 <code>rename</code> 命令也可以批量重命名，但要注意，它在不同 Linux 发行版中的功能并不完全一样。）</li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-comment"># 将文件、目录和内容全部重命名 foo -&gt; bar:</span><br>repren --full --preserve-case --from foo --to bar .<br><span class="hljs-comment"># 还原所有备份文件 whatever.bak -&gt; whatever:</span><br>repren --renames --from <span class="hljs-string">&#x27;(.*)\.bak&#x27;</span> --to <span class="hljs-string">&#x27;\1&#x27;</span> *.bak<br><span class="hljs-comment"># 用 rename 实现上述功能（若可用）:</span><br>rename <span class="hljs-string">&#x27;s/\.bak$//&#x27;</span> *.bak<br></code></pre></td></tr></table></figure><ul><li>根据 man 页面的描述，<code>rsync</code> 是一个快速且非常灵活的文件复制工具。它闻名于设备之间的文件同步，但其实它在本地情况下也同样有用。在安全设置允许下，用 <code>rsync</code> 代替 <code>scp</code> 可以实现文件续传，而不用重新从头开始。它同时也是删除大量文件的<a href="https://web.archive.org/web/20130929001850/http://linuxnote.net/jianingy/en/linux/a-fast-way-to-remove-huge-number-of-files.html">最快方法</a>之一：</li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">mkdir empty &amp;&amp; rsync -r --delete empty/ some-dir &amp;&amp; rmdir some-dir<br></code></pre></td></tr></table></figure><ul><li>若要在复制文件时获取当前进度，可使用 <code>pv</code>，<a href="https://github.com/dmerejkowsky/pycp"><code>pycp</code></a>，<a href="https://github.com/Xfennec/progress"><code>progress</code></a>，<code>rsync --progress</code>。若所执行的复制为block块拷贝，可以使用 <code>dd status=progress</code>。</li><li>使用 <code>shuf</code> 可以以行为单位来打乱文件的内容或从一个文件中随机选取多行。</li><li>了解 <code>sort</code> 的参数。显示数字时，使用 <code>-n</code> 或者 <code>-h</code> 来显示更易读的数（例如 <code>du -h</code> 的输出）。明白排序时关键字的工作原理（<code>-t</code> 和 <code>-k</code>）。例如，注意到你需要 <code>-k1，1</code> 来仅按第一个域来排序，而 <code>-k1</code> 意味着按整行排序。稳定排序（<code>sort -s</code>）在某些情况下很有用。例如，以第二个域为主关键字，第一个域为次关键字进行排序，你可以使用 <code>sort -k1，1 | sort -s -k2，2</code>。</li><li>如果你想在 Bash 命令行中写 tab 制表符，按下 <strong>ctrl-v</strong> <strong>[Tab]</strong> 或键入 <code>$&#39;\t&#39;</code> （后者可能更好，因为你可以复制粘贴它）。</li><li>标准的源代码对比及合并工具是 <code>diff</code> 和 <code>patch</code>。使用 <code>diffstat</code> 查看变更总览数据。注意到 <code>diff -r</code> 对整个文件夹有效。使用 <code>diff -r tree1 tree2 | diffstat</code> 查看变更的统计数据。<code>vimdiff</code> 用于比对并编辑文件。</li><li>对于二进制文件，使用 <code>hd</code>，<code>hexdump</code> 或者 <code>xxd</code> 使其以十六进制显示，使用 <code>bvi</code>，<code>hexedit</code> 或者 <code>biew</code> 来进行二进制编辑。</li><li>同样对于二进制文件，<code>strings</code>（包括 <code>grep</code> 等工具）可以帮助在二进制文件中查找特定比特。</li><li>制作二进制差分文件（Delta 压缩），使用 <code>xdelta3</code>。</li><li>使用 <code>iconv</code> 更改文本编码。需要更高级的功能，可以使用 <code>uconv</code>，它支持一些高级的 Unicode 功能。例如，这条命令移除了所有重音符号：</li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">uconv -f utf-8 -t utf-8 -x <span class="hljs-string">&#x27;::Any-Lower; ::Any-NFD; [:Nonspacing Mark:] &gt;; ::Any-NFC; &#x27;</span> &lt; input.txt &gt; output.txt<br></code></pre></td></tr></table></figure><ul><li>拆分文件可以使用 <code>split</code>（按大小拆分）和 <code>csplit</code>（按模式拆分）。</li><li>操作日期和时间表达式，可以用 <a href="http://www.fresse.org/dateutils/"><code>dateutils</code></a> 中的 <code>dateadd</code>、<code>datediff</code>、<code>strptime</code> 等工具。</li><li>使用 <code>zless</code>、<code>zmore</code>、<code>zcat</code> 和 <code>zgrep</code> 对压缩过的文件进行操作。</li><li>文件属性可以通过 <code>chattr</code> 进行设置，它比文件权限更加底层。例如，为了保护文件不被意外删除，可以使用不可修改标记：<code>sudo chattr +i /critical/directory/or/file</code></li><li>使用 <code>getfacl</code> 和 <code>setfacl</code> 以保存和恢复文件权限。例如：</li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sh">getfacl -R /some/path &gt; permissions.txt<br>setfacl --restore=permissions.txt<br></code></pre></td></tr></table></figure><ul><li>为了高效地创建空文件，请使用 <code>truncate</code>（创建<a href="https://zh.wikipedia.org/wiki/稀疏文件">稀疏文件</a>），<code>fallocate</code>（用于 ext4，xfs，btrf 和 ocfs2 文件系统），<code>xfs_mkfile</code>（适用于几乎所有的文件系统，包含在 xfsprogs 包中），<code>mkfile</code>（用于类 Unix 操作系统，比如 Solaris 和 Mac OS）。</li></ul><h2 id="系统调试"><a href="#系统调试" class="headerlink" title="系统调试"></a>系统调试</h2><ul><li><code>curl</code> 和 <code>curl -I</code> 可以被轻松地应用于 web 调试中，它们的好兄弟 <code>wget</code> 也是如此，或者也可以试试更潮的 <a href="https://github.com/jkbrzt/httpie"><code>httpie</code></a>。</li><li>获取 CPU 和硬盘的使用状态，通常使用使用 <code>top</code>（<code>htop</code> 更佳），<code>iostat</code> 和 <code>iotop</code>。而 <code>iostat -mxz 15</code> 可以让你获悉 CPU 和每个硬盘分区的基本信息和性能表现。</li><li>使用 <code>netstat</code> 和 <code>ss</code> 查看网络连接的细节。</li><li><code>dstat</code> 在你想要对系统的现状有一个粗略的认识时是非常有用的。然而若要对系统有一个深度的总体认识，使用 <a href="https://github.com/nicolargo/glances"><code>glances</code></a>，它会在一个终端窗口中向你提供一些系统级的数据。</li><li>若要了解内存状态，运行并理解 <code>free</code> 和 <code>vmstat</code> 的输出。值得留意的是“cached”的值，它指的是 Linux 内核用来作为文件缓存的内存大小，而与空闲内存无关。</li><li>Java 系统调试则是一件截然不同的事，一个可以用于 Oracle 的 JVM 或其他 JVM 上的调试的技巧是你可以运行 <code>kill -3 &lt;pid&gt;</code> 同时一个完整的栈轨迹和堆概述（包括 GC 的细节）会被保存到标准错误或是日志文件。JDK 中的 <code>jps</code>，<code>jstat</code>，<code>jstack</code>，<code>jmap</code> 很有用。<a href="https://github.com/aragozin/jvm-tools">SJK tools</a> 更高级。</li><li>使用 <a href="http://www.bitwizard.nl/mtr/"><code>mtr</code></a> 去跟踪路由，用于确定网络问题。</li><li>用 <a href="https://dev.yorhel.nl/ncdu"><code>ncdu</code></a> 来查看磁盘使用情况，它比寻常的命令，如 <code>du -sh *</code>，更节省时间。</li><li>查找正在使用带宽的套接字连接或进程，使用 <a href="http://www.ex-parrot.com/~pdw/iftop/"><code>iftop</code></a> 或 <a href="https://github.com/raboof/nethogs"><code>nethogs</code></a>。</li><li><code>ab</code> 工具（Apache 中自带）可以简单粗暴地检查 web 服务器的性能。对于更复杂的负载测试，使用 <code>siege</code>。</li><li><a href="https://wireshark.org/"><code>wireshark</code></a>，<a href="https://www.wireshark.org/docs/wsug_html_chunked/AppToolstshark.html"><code>tshark</code></a> 和 <a href="http://ngrep.sourceforge.net/"><code>ngrep</code></a> 可用于复杂的网络调试。</li><li>了解 <code>strace</code> 和 <code>ltrace</code>。这俩工具在你的程序运行失败、挂起甚至崩溃，而你却不知道为什么或你想对性能有个总体的认识的时候是非常有用的。注意 profile 参数（<code>-c</code>）和附加到一个运行的进程参数 （<code>-p</code>）。</li><li>了解使用 <code>ldd</code> 来检查共享库。但是<a href="http://www.catonmat.net/blog/ldd-arbitrary-code-execution/">永远不要在不信任的文件上运行</a>。</li><li>了解如何运用 <code>gdb</code> 连接到一个运行着的进程并获取它的堆栈轨迹。</li><li>学会使用 <code>/proc</code>。它在调试正在出现的问题的时候有时会效果惊人。比如：<code>/proc/cpuinfo</code>，<code>/proc/meminfo</code>，<code>/proc/cmdline</code>，<code>/proc/xxx/cwd</code>，<code>/proc/xxx/exe</code>，<code>/proc/xxx/fd/</code>，<code>/proc/xxx/smaps</code>（这里的 <code>xxx</code> 表示进程的 id 或 pid）。</li><li>当调试一些之前出现的问题的时候，<a href="http://sebastien.godard.pagesperso-orange.fr/"><code>sar</code></a> 非常有用。它展示了 cpu、内存以及网络等的历史数据。</li><li>关于更深层次的系统分析以及性能分析，看看 <code>stap</code>（<a href="https://sourceware.org/systemtap/wiki">SystemTap</a>），<a href="https://en.wikipedia.org/wiki/Perf_(Linux"><code>perf</code></a>)，以及<a href="https://github.com/draios/sysdig"><code>sysdig</code></a>。</li><li>查看你当前使用的系统，使用 <code>uname</code>，<code>uname -a</code>（Unix／kernel 信息）或者 <code>lsb_release -a</code>（Linux 发行版信息）。</li><li>无论什么东西工作得很欢乐（可能是硬件或驱动问题）时可以试试 <code>dmesg</code>。</li><li>如果你删除了一个文件，但通过 <code>du</code> 发现没有释放预期的磁盘空间，请检查文件是否被进程占用：<br><code>lsof | grep deleted | grep &quot;filename-of-my-big-file&quot;</code></li></ul><h2 id="单行脚本"><a href="#单行脚本" class="headerlink" title="单行脚本"></a>单行脚本</h2><p>一些命令组合的例子：</p><ul><li>当你需要对文本文件做集合交、并、差运算时，<code>sort</code> 和 <code>uniq</code> 会是你的好帮手。具体例子请参照代码后面的，此处假设 <code>a</code> 与 <code>b</code> 是两内容不同的文件。这种方式效率很高，并且在小文件和上 G 的文件上都能运用（注意尽管在 <code>/tmp</code> 在一个小的根分区上时你可能需要 <code>-T</code> 参数，但是实际上 <code>sort</code> 并不被内存大小约束），参阅前文中关于 <code>LC_ALL</code> 和 <code>sort</code> 的 <code>-u</code> 参数的部分。</li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sh">sort a b | uniq &gt; c   <span class="hljs-comment"># c 是 a 并 b</span><br>sort a b | uniq -d &gt; c   <span class="hljs-comment"># c 是 a 交 b</span><br>sort a b b | uniq -u &gt; c   <span class="hljs-comment"># c 是 a - b</span><br></code></pre></td></tr></table></figure><ul><li>使用 <code>grep . *</code>（每行都会附上文件名）或者 <code>head -100 *</code>（每个文件有一个标题）来阅读检查目录下所有文件的内容。这在检查一个充满配置文件的目录（如 <code>/sys</code>、<code>/proc</code>、<code>/etc</code>）时特别好用。</li><li>计算文本文件第三列中所有数的和（可能比同等作用的 Python 代码快三倍且代码量少三倍）：</li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">awk <span class="hljs-string">&#x27;&#123; x += $3 &#125; END &#123; print x &#125;&#x27;</span> myfile<br></code></pre></td></tr></table></figure><ul><li>如果你想在文件树上查看大小/日期，这可能看起来像递归版的 <code>ls -l</code> 但比 <code>ls -lR</code> 更易于理解：</li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">find . -<span class="hljs-built_in">type</span> f -ls<br></code></pre></td></tr></table></figure><ul><li>假设你有一个类似于 web 服务器日志文件的文本文件，并且一个确定的值只会出现在某些行上，假设一个 <code>acct_id</code> 参数在 URI 中。如果你想计算出每个 <code>acct_id</code> 值有多少次请求，使用如下代码：</li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">egrep -o <span class="hljs-string">&#x27;acct_id=[0-9]+&#x27;</span> access.log | cut -d= -f2 | sort | uniq -c | sort -rn<br></code></pre></td></tr></table></figure><ul><li>要持续监测文件改动，可以使用 <code>watch</code>，例如检查某个文件夹中文件的改变，可以用 <code>watch -d -n 2 &#39;ls -rtlh | tail&#39;</code>；或者在排查 WiFi 设置故障时要监测网络设置的更改，可以用 <code>watch -d -n 2 ifconfig</code>。</li><li>运行这个函数从这篇文档中随机获取一条技巧（解析 Markdown 文件并抽取项目）：</li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">taocl</span></span>() &#123;<br>  curl -s https://raw.githubusercontent.com/jlevy/the-art-of-command-line/master/README-zh.md|<br>    pandoc -f markdown -t html |<br>    iconv -f <span class="hljs-string">&#x27;utf-8&#x27;</span> -t <span class="hljs-string">&#x27;unicode&#x27;</span> |<br>    xmlstarlet fo --html --dropdtd |<br>    xmlstarlet sel -t -v <span class="hljs-string">&quot;(html/body/ul/li[count(p)&gt;0])[<span class="hljs-variable">$RANDOM</span> mod last()+1]&quot;</span> |<br>    xmlstarlet unesc | fmt -80<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="冷门但有用"><a href="#冷门但有用" class="headerlink" title="冷门但有用"></a>冷门但有用</h2><ul><li><code>expr</code>：计算表达式或正则匹配</li><li><code>m4</code>：简单的宏处理器</li><li><code>yes</code>：多次打印字符串</li><li><code>cal</code>：漂亮的日历</li><li><code>env</code>：执行一个命令（脚本文件中很有用）</li><li><code>printenv</code>：打印环境变量（调试时或在写脚本文件时很有用）</li><li><code>look</code>：查找以特定字符串开头的单词或行</li><li><code>cut</code>，<code>paste</code> 和 <code>join</code>：数据修改</li><li><code>fmt</code>：格式化文本段落</li><li><code>pr</code>：将文本格式化成页／列形式</li><li><code>fold</code>：包裹文本中的几行</li><li><code>column</code>：将文本格式化成多个对齐、定宽的列或表格</li><li><code>expand</code> 和 <code>unexpand</code>：制表符与空格之间转换</li><li><code>nl</code>：添加行号</li><li><code>seq</code>：打印数字</li><li><code>bc</code>：计算器</li><li><code>factor</code>：分解因数</li><li><a href="https://gnupg.org/"><code>gpg</code></a>：加密并签名文件</li><li><code>toe</code>：terminfo 入口列表</li><li><code>nc</code>：网络调试及数据传输</li><li><code>socat</code>：套接字代理，与 <code>netcat</code> 类似</li><li><a href="https://github.com/mattthias/slurm"><code>slurm</code></a>：网络流量可视化</li><li><code>dd</code>：文件或设备间传输数据</li><li><code>file</code>：确定文件类型</li><li><code>tree</code>：以树的形式显示路径和文件，类似于递归的 <code>ls</code></li><li><code>stat</code>：文件信息</li><li><code>time</code>：执行命令，并计算执行时间</li><li><code>timeout</code>：在指定时长范围内执行命令，并在规定时间结束后停止进程</li><li><code>lockfile</code>：使文件只能通过 <code>rm -f</code> 移除</li><li><code>logrotate</code>： 切换、压缩以及发送日志文件</li><li><code>watch</code>：重复运行同一个命令，展示结果并／或高亮有更改的部分</li><li><a href="https://github.com/joh/when-changed"><code>when-changed</code></a>：当检测到文件更改时执行指定命令。参阅 <code>inotifywait</code> 和 <code>entr</code>。</li><li><code>tac</code>：反向输出文件</li><li><code>shuf</code>：文件中随机选取几行</li><li><code>comm</code>：一行一行的比较排序过的文件</li><li><code>strings</code>：从二进制文件中抽取文本</li><li><code>tr</code>：转换字母</li><li><code>iconv</code> 或 <code>uconv</code>：文本编码转换</li><li><code>split</code> 和 <code>csplit</code>：分割文件</li><li><code>sponge</code>：在写入前读取所有输入，在读取文件后再向同一文件写入时比较有用，例如 <code>grep -v something some-file | sponge some-file</code></li><li><code>units</code>：将一种计量单位转换为另一种等效的计量单位（参阅 <code>/usr/share/units/definitions.units</code>）</li><li><code>apg</code>：随机生成密码</li><li><code>xz</code>：高比例的文件压缩</li><li><code>ldd</code>：动态库信息</li><li><code>nm</code>：提取 obj 文件中的符号</li><li><code>ab</code> 或 <a href="https://github.com/wg/wrk"><code>wrk</code></a>：web 服务器性能分析</li><li><code>strace</code>：调试系统调用</li><li><a href="http://www.bitwizard.nl/mtr/"><code>mtr</code></a>：更好的网络调试跟踪工具</li><li><code>cssh</code>：可视化的并发 shell</li><li><code>rsync</code>：通过 ssh 或本地文件系统同步文件和文件夹</li><li><a href="https://wireshark.org/"><code>wireshark</code></a> 和 <a href="https://www.wireshark.org/docs/wsug_html_chunked/AppToolstshark.html"><code>tshark</code></a>：抓包和网络调试工具</li><li><a href="http://ngrep.sourceforge.net/"><code>ngrep</code></a>：网络层的 grep</li><li><code>host</code> 和 <code>dig</code>：DNS 查找</li><li><code>lsof</code>：列出当前系统打开文件的工具以及查看端口信息</li><li><code>dstat</code>：系统状态查看</li><li><a href="https://github.com/nicolargo/glances"><code>glances</code></a>：高层次的多子系统总览</li><li><code>iostat</code>：硬盘使用状态</li><li><code>mpstat</code>： CPU 使用状态</li><li><code>vmstat</code>： 内存使用状态</li><li><code>htop</code>：top 的加强版</li><li><code>last</code>：登入记录</li><li><code>w</code>：查看处于登录状态的用户</li><li><code>id</code>：用户/组 ID 信息</li><li><a href="http://sebastien.godard.pagesperso-orange.fr/"><code>sar</code></a>：系统历史数据</li><li><a href="http://www.ex-parrot.com/~pdw/iftop/"><code>iftop</code></a> 或 <a href="https://github.com/raboof/nethogs"><code>nethogs</code></a>：套接字及进程的网络利用情况</li><li><code>ss</code>：套接字数据</li><li><code>dmesg</code>：引导及系统错误信息</li><li><code>sysctl</code>： 在内核运行时动态地查看和修改内核的运行参数</li><li><code>hdparm</code>：SATA/ATA 磁盘更改及性能分析</li><li><code>lsblk</code>：列出块设备信息：以树形展示你的磁盘以及磁盘分区信息</li><li><code>lshw</code>，<code>lscpu</code>，<code>lspci</code>，<code>lsusb</code> 和 <code>dmidecode</code>：查看硬件信息，包括 CPU、BIOS、RAID、显卡、USB设备等</li><li><code>lsmod</code> 和 <code>modinfo</code>：列出内核模块，并显示其细节</li><li><code>fortune</code>，<code>ddate</code> 和 <code>sl</code>：额，这主要取决于你是否认为蒸汽火车和莫名其妙的名人名言是否“有用”</li></ul><h2 id="仅限-OS-X-系统"><a href="#仅限-OS-X-系统" class="headerlink" title="仅限 OS X 系统"></a>仅限 OS X 系统</h2><p>以下是<em>仅限于</em> OS X 系统的技巧。</p><ul><li>用 <code>brew</code> （Homebrew）或者 <code>port</code> （MacPorts）进行包管理。这些可以用来在 OS X 系统上安装以上的大多数命令。</li><li>用 <code>pbcopy</code> 复制任何命令的输出到桌面应用，用 <code>pbpaste</code> 粘贴输入。</li><li>若要在 OS X 终端中将 Option 键视为 alt 键（例如在上面介绍的 <strong>alt-b</strong>、<strong>alt-f</strong> 等命令中用到），打开 偏好设置 -&gt; 描述文件 -&gt; 键盘 并勾选“使用 Option 键作为 Meta 键”。</li><li>用 <code>open</code> 或者 <code>open -a /Applications/Whatever.app</code> 使用桌面应用打开文件。</li><li>Spotlight：用 <code>mdfind</code> 搜索文件，用 <code>mdls</code> 列出元数据（例如照片的 EXIF 信息）。</li><li>注意 OS X 系统是基于 BSD UNIX 的，许多命令（例如 <code>ps</code>，<code>ls</code>，<code>tail</code>，<code>awk</code>，<code>sed</code>）都和 Linux 中有微妙的不同（ Linux 很大程度上受到了 System V-style Unix 和 GNU 工具影响）。你可以通过标题为 “BSD General Commands Manual” 的 man 页面发现这些不同。在有些情况下 GNU 版本的命令也可能被安装（例如 <code>gawk</code> 和 <code>gsed</code> 对应 GNU 中的 awk 和 sed ）。如果要写跨平台的 Bash 脚本，避免使用这些命令（例如，考虑 Python 或者 <code>perl</code> ）或者经过仔细的测试。</li><li>用 <code>sw_vers</code> 获取 OS X 的版本信息。</li></ul><h2 id="仅限-Windows-系统"><a href="#仅限-Windows-系统" class="headerlink" title="仅限 Windows 系统"></a>仅限 Windows 系统</h2><p>以下是<em>仅限于</em> Windows 系统的技巧。</p><h3 id="在-Winodws-下获取-Unix-工具"><a href="#在-Winodws-下获取-Unix-工具" class="headerlink" title="在 Winodws 下获取 Unix 工具"></a>在 Winodws 下获取 Unix 工具</h3><ul><li>可以安装 <a href="https://cygwin.com/">Cygwin</a> 允许你在 Microsoft Windows 中体验 Unix shell 的威力。这样的话，本文中介绍的大多数内容都将适用。</li><li>在 Windows 10 上，你可以使用 <a href="https://msdn.microsoft.com/commandline/wsl/about">Bash on Ubuntu on Windows</a>，它提供了一个熟悉的 Bash 环境，包含了不少 Unix 命令行工具。好处是它允许 Linux 上编写的程序在 Windows 上运行，而另一方面，Windows 上编写的程序却无法在 Bash 命令行中运行。</li><li>如果你在 Windows 上主要想用 GNU 开发者工具（例如 GCC），可以考虑 <a href="http://www.mingw.org/">MinGW</a> 以及它的 <a href="http://www.mingw.org/wiki/msys">MSYS</a> 包，这个包提供了例如 bash，gawk，make 和 grep 的工具。MSYS 并不包含所有可以与 Cygwin 媲美的特性。当制作 Unix 工具的原生 Windows 端口时 MinGW 将特别地有用。</li><li>另一个在 Windows 下实现接近 Unix 环境外观效果的选项是 <a href="https://github.com/dthree/cash">Cash</a>。注意在此环境下只有很少的 Unix 命令和命令行可用。</li></ul><h3 id="实用-Windows-命令行工具"><a href="#实用-Windows-命令行工具" class="headerlink" title="实用 Windows 命令行工具"></a>实用 Windows 命令行工具</h3><ul><li>可以使用 <code>wmic</code> 在命令行环境下给大部分 Windows 系统管理任务编写脚本以及执行这些任务。</li><li>Windows 实用的原生命令行网络工具包括 <code>ping</code>，<code>ipconfig</code>，<code>tracert</code>，和 <code>netstat</code>。</li><li>可以使用 <code>Rundll32</code> 命令来实现<a href="http://www.thewindowsclub.com/rundll32-shortcut-commands-windows">许多有用的 Windows 任务</a> 。</li></ul><h3 id="Cygwin-技巧"><a href="#Cygwin-技巧" class="headerlink" title="Cygwin 技巧"></a>Cygwin 技巧</h3><ul><li>通过 Cygwin 的包管理器来安装额外的 Unix 程序。</li><li>使用 <code>mintty</code> 作为你的命令行窗口。</li><li>要访问 Windows 剪贴板，可以通过 <code>/dev/clipboard</code>。</li><li>运行 <code>cygstart</code> 以通过默认程序打开一个文件。</li><li>要访问 Windows 注册表，可以使用 <code>regtool</code>。</li><li>注意 Windows 驱动器路径 <code>C:\</code> 在 Cygwin 中用 <code>/cygdrive/c</code> 代表，而 Cygwin 的 <code>/</code> 代表 Windows 中的 <code>C:\cygwin</code>。要转换 Cygwin 和 Windows 风格的路径可以用 <code>cygpath</code>。这在需要调用 Windows 程序的脚本里很有用。</li><li>学会使用 <code>wmic</code>，你就可以从命令行执行大多数 Windows 系统管理任务，并编成脚本。</li><li>要在 Windows 下获得 Unix 的界面和体验，另一个办法是使用 <a href="https://github.com/dthree/cash">Cash</a>。需要注意的是，这个环境支持的 Unix 命令和命令行参数非常少。</li><li>要在 Windows 上获取 GNU 开发者工具（比如 GCC）的另一个办法是使用 <a href="http://www.mingw.org/">MinGW</a> 以及它的 <a href="http://www.mingw.org/wiki/msys">MSYS</a> 软件包，该软件包提供了 bash、gawk、make、grep 等工具。然而 MSYS 提供的功能没有 Cygwin 完善。MinGW 在创建 Unix 工具的 Windows 原生移植方面非常有用。</li></ul><h2 id="更多资源"><a href="#更多资源" class="headerlink" title="更多资源"></a>更多资源</h2><ul><li><a href="https://github.com/alebcay/awesome-shell">awesome-shell</a>：一份精心组织的命令行工具及资源的列表。</li><li><a href="https://github.com/herrbischoff/awesome-osx-command-line">awesome-osx-command-line</a>：一份针对 OS X 命令行的更深入的指南。</li><li><a href="http://redsymbol.net/articles/unofficial-bash-strict-mode/">Strict mode</a>：为了编写更好的脚本文件。</li><li><a href="https://github.com/koalaman/shellcheck">shellcheck</a>：一个静态 shell 脚本分析工具，本质上是 bash／sh／zsh 的 lint。</li><li><a href="http://www.dwheeler.com/essays/filenames-in-shell.html">Filenames and Pathnames in Shell</a>：有关如何在 shell 脚本里正确处理文件名的细枝末节。</li><li><a href="http://datascienceatthecommandline.com/#tools">Data Science at the Command Line</a>：用于数据科学的一些命令和工具，摘自同名书籍。</li></ul><h2 id="免责声明"><a href="#免责声明" class="headerlink" title="免责声明"></a>免责声明</h2><p>除去特别小的工作，你编写的代码应当方便他人阅读。能力往往伴随着责任，你 <em>有能力</em> 在 Bash 中玩一些奇技淫巧并不意味着你应该去做！;)</p><h2 id="授权条款"><a href="#授权条款" class="headerlink" title="授权条款"></a>授权条款</h2><p><a href="http://creativecommons.org/licenses/by-sa/4.0/"><img src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" alt="Creative Commons License"></a></p><p>本文使用授权协议 <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>。</p>]]></content>
    
    
    
    <tags>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>脑影像智能计算及其若干应用研究进展</title>
    <link href="/2022/10/24/medical/"/>
    <url>/2022/10/24/medical/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><embed src="./medical.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    
    <tags>
      
      <tag>智能医疗</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>OS实验报告：Linux、ROS安装与使用</title>
    <link href="/2022/09/26/oslab1/"/>
    <url>/2022/09/26/oslab1/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="https://img.enderfga.cn/img/Aspose.Words.a6f73596-b4e5-4742-b430-3d062b8ab739.001.jpeg" alt=""></p><h1 id="实验一-Linux的安装与使用"><a href="#实验一-Linux的安装与使用" class="headerlink" title="实验一  Linux的安装与使用"></a><strong>实验一  Linux的安装与使用</strong></h1><h2 id="实验目的"><a href="#实验目的" class="headerlink" title="实验目的"></a><strong>实验目的</strong></h2><ol><li>掌握Linux环境下的命令操作，熟悉Linux操作系统的环境和使用，记录各种测试结果；</li><li>了解LINUX系统的安装过程，记录安装流程和界面；</li><li>搭建 ROS 环境，为后续实验做准备。</li></ol><h2 id="实验内容"><a href="#实验内容" class="headerlink" title="实验内容"></a><strong>实验内容</strong></h2><h3 id="任务描述"><a href="#任务描述" class="headerlink" title="任务描述"></a><strong>任务描述</strong></h3><p>1）Linux系统的安装</p><p>1) 调研、选择Linux版本<br>2) 搜索、下载Linux安装所需文件<br>3) 安装Linux</p><p>2）Linux基本操作命令</p><p>熟悉pwd、ls、mkdir、cd、cat、man、cp等命令的使用</p><p>3）编写程序</p><p>1) 编写能输出“Hello world!”问候语的C程序，并在终端中编译、执行。要求记录所使用的命令及结果。<br>2) 编写一个程序：显示信息“Time for Play!”，并能在后台运行一段时间（自定义）后，弹出信息提醒用户。要求记录所使用的命令及结果。（提示：使用sleep(s)函数）</p><p>4）安装ROS</p><p>1) 按照官方教程或者快捷脚本安装好对应版本的ROS；<br>2) 运行ROS代码例程。</p><h3 id="实验说明"><a href="#实验说明" class="headerlink" title="实验说明"></a><strong>实验说明</strong></h3><p>本次实验中我通过VMware和WSL2两种方式安装了Linux操作系统，磁盘管理、文件管理的命令熟悉和程序编写使用WSL2完成，ROS的安装与代码例程使用有可视化界面的VMware完成。</p><h3 id="实验记录"><a href="#实验记录" class="headerlink" title="实验记录"></a><strong>实验记录</strong></h3><h4 id="实施步骤"><a href="#实施步骤" class="headerlink" title="实施步骤"></a><strong>实施步骤</strong></h4><h5 id="使用VMware安装："><a href="#使用VMware安装：" class="headerlink" title="使用VMware安装："></a>使用VMware安装：</h5><ul><li>以管理员身份运行Vmware Workstation</li><li>载入并安装ubuntu-22.04.1-desktop-amd64.iso</li></ul><h5 id="使用WSL2安装："><a href="#使用WSL2安装：" class="headerlink" title="使用WSL2安装："></a>使用WSL2安装：</h5><ul><li>启用“适用于 Linux 的 Windows 子系统”可选功能</li><li>启用“虚拟机平台”可选功能</li><li>下载安装 Linux 内核更新包并将 WSL 2 设置为默认版本</li><li>使用 Microsoft Store，选择并安装偏好的 Linux 分发版</li></ul><ol><li>在终端练习Linux的基本操作命令</li><li>编写程序</li><li>利用脚本安装ROS并运行代码例程</li></ol><h4 id="实验记录-1"><a href="#实验记录-1" class="headerlink" title="实验记录"></a><strong>实验记录</strong></h4><p>1.安装Linux</p><p><img src="https://img.enderfga.cn/img/Aspose.Words.a6f73596-b4e5-4742-b430-3d062b8ab739.002.jpeg" alt=""></p><p>图1-VM安装过程</p><p><img src="https://img.enderfga.cn/img/Aspose.Words.a6f73596-b4e5-4742-b430-3d062b8ab739.003.jpeg" alt=""></p><p>图2-安装成功</p><p><img src="https://img.enderfga.cn/img/Aspose.Words.a6f73596-b4e5-4742-b430-3d062b8ab739.004.png" alt=""></p><p>图3-WSL2安装过程</p><p><img src="https://img.enderfga.cn/img/Aspose.Words.a6f73596-b4e5-4742-b430-3d062b8ab739.005.png" alt=""></p><p>图4-WSL2安装成功</p><p>1) 熟悉操作命令</p><p><strong>pwd命令：</strong></p><p>英文全拼：print work directory，该命令用于显示工作目录，</p><p>执行 pwd 指令可立刻得知您目前所在的工作目录的绝对路径名称。</p><p><strong>ls命令：</strong></p><p>英文全拼：list files，该命令用于显示指定工作目录下之内容（列出目前工作目录所含之文件及子目录)。</p><p>参数说明：</p><ul><li><strong>-a</strong> 显示所有文件及目录 (. 开头的隐藏文件也会列出)</li><li><strong>-l</strong> 除文件名称外，亦将文件型态、权限、拥有者、文件大小等资讯详细列出</li><li><strong>-r</strong> 将文件以相反次序显示(原定依英文字母次序)</li><li><strong>-t</strong> 将文件依建立时间之先后次序列出</li><li><strong>-A</strong> 同 -a ，但不列出 “.” (目前目录) 及 “..” (父目录)</li><li><strong>-F</strong> 在列出的文件名称后加一符号；例如可执行档则加 “*“, 目录则加 “/“</li><li><strong>-R</strong> 若目录下有文件，则以下之文件亦皆依序列出</li></ul><p><strong>mkdir命令：</strong></p><p>英文全拼：make directory，该命令用于创建目录。</p><p>参数说明：</p><ul><li><strong>-p</strong> 确保目录名称存在，不存在的就建一个。</li></ul><p><strong>cd命令：</strong></p><p>英文全拼：change directory，该命令用于切换当前工作目录。</p><p>其中 dirName 表示法可为绝对路径或相对路径。若目录名称省略，则变换至使用者的 home 目录 (也就是刚 login 时所在的目录)。</p><p>另外，~ 也表示为 home 目录 的意思， . 则是表示目前所在的目录， .. 则表示目前目录位置的上一层目录。</p><p><strong>cat命令：</strong></p><p>英文全拼：concatenate,该命令用于连接文件并打印到标准输出设备上。</p><p>参数说明：</p><ul><li><strong>-n 或 —number</strong>：由 1 开始对所有输出的行数编号。</li><li><strong>-b 或 —number-nonblank</strong>：和 -n 相似，只不过对于空白行不编号。</li><li><strong>-s 或 —squeeze-blank</strong>：当遇到有连续两行以上的空白行，就代换为一行的空白行。</li><li><strong>-v 或 —show-nonprinting</strong>：使用 ^ 和 M- 符号，除了 LFD 和 TAB 之外。</li><li><strong>-E 或 —show-ends</strong> : 在每行结束处显示 $。</li><li><strong>-T 或 —show-tabs</strong>: 将 TAB 字符显示为 ^I。</li><li><strong>-A, —show-all</strong>：等价于 -vET。</li><li><strong>-e：</strong>等价于”-vE”选项；</li><li><strong>-t：</strong>等价于”-vT”选项；</li></ul><p><strong>cp命令：</strong></p><p>英文全拼：copy file，该命令主要用于复制文件或目录。</p><p>参数说明：</p><ul><li><strong>-a</strong>：此选项通常在复制目录时使用，它保留链接、文件属性，并复制目录下的所有内容。其作用等于dpR参数组合。</li><li><strong>-d</strong>：复制时保留链接。这里所说的链接相当于 Windows 系统中的快捷方式。</li><li><strong>-f</strong>：覆盖已经存在的目标文件而不给出提示。</li><li><strong>-i</strong>：与 -f 选项相反，在覆盖目标文件之前给出提示，要求用户确认是否覆盖，回答 y 时目标文件将被覆盖。</li><li><strong>-p</strong>：除复制文件的内容外，还把修改时间和访问权限也复制到新文件中。</li><li><strong>-r</strong>：若给出的源文件是一个目录文件，此时将复制该目录下所有的子目录和文件。</li><li><strong>-l</strong>：不复制文件，只是生成链接文件。</li></ul><p>1) 编写程序</p><p><img src="https://img.enderfga.cn/img/Aspose.Words.a6f73596-b4e5-4742-b430-3d062b8ab739.006.png" alt=""></p><p>图5-vim 键盘图</p><p>使用vi文书编辑器来编写要求的程序，代码如下：</p><p><img src="https://img.enderfga.cn/img/Aspose.Words.a6f73596-b4e5-4742-b430-3d062b8ab739.007.png" alt=""></p><p>图6-编写程序一</p><p><img src="https://img.enderfga.cn/img/Aspose.Words.a6f73596-b4e5-4742-b430-3d062b8ab739.008.png" alt=""></p><p>图7-编写程序二</p><p><img src="https://img.enderfga.cn/img/Aspose.Words.a6f73596-b4e5-4742-b430-3d062b8ab739.009.png" alt=""></p><p>图8-安装gcc并编译</p><p>1) 安装ROS</p><p><img src="https://img.enderfga.cn/img/Aspose.Words.a6f73596-b4e5-4742-b430-3d062b8ab739.010.png" alt=""></p><p>图9-使用官方教程手动安装</p><p><img src="https://img.enderfga.cn/img/Aspose.Words.a6f73596-b4e5-4742-b430-3d062b8ab739.010.png" alt=""></p><p>图10-使用一键安装脚本</p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a><strong>实验结果</strong></h2><p>1) 命令熟悉</p><p><img src="https://img.enderfga.cn/img/Aspose.Words.a6f73596-b4e5-4742-b430-3d062b8ab739.010.png" alt=""></p><p>图11-基本操作命令结果一</p><p>如图所示，打开终端之后起始目录显示为“~”，即主目录，也就是当前登录用户的用户目录，故输入pwd的结果为/home/enderfga。</p><p>ls -l会显示当前目录下文件的详细信息，图中是我编写的两个c文件以及编译产生的文件等，ls -al则会将以“.”开头的隐藏文件也展示出来。</p><p><img src="https://img.enderfga.cn/img/Aspose.Words.a6f73596-b4e5-4742-b430-3d062b8ab739.010.png" alt=""></p><p>图12-基本操作命令结果二</p><p>直接使用不带参数的命令cd会改变目录至当前的用户目录，即“~”；</p><p>使用命令cd ../..会到上一级目录的上一级目录中，在此处也就是根目录，即“/”。</p><p><img src="https://img.enderfga.cn/img/Aspose.Words.a6f73596-b4e5-4742-b430-3d062b8ab739.010.png" alt=""></p><p>图13-文件操作命令结果</p><p>值得注意的是，输入命令ls -l后屏幕显示的第一列内容wsl2的结果和VMware的结果并不一样。</p><p>wsl2中显示的是文件大小，如果是文件，则表示该文件的大小，单位为字节；如果是目录，则表示该目录符所占的大小，并不表示该目录下所有文件的大小。</p><p>VMware的结果如图所示：</p><p><img src="https://img.enderfga.cn/img/Aspose.Words.a6f73596-b4e5-4742-b430-3d062b8ab739.011.png" alt=""></p><p>图14-ls -l命令解析</p><p>图中还记录了两个我搜索到的其他文件操作命令，分别是</p><p>“ls -l | grep “^-“ | wc -l”：统计该目录下的文件个数；以及</p><p>“find . -name ‘f*‘ -exec rm {} \;”：搜寻含有特定文件名的文件并删除。</p><p>1) 程序编写</p><p><img src="https://img.enderfga.cn/img/Aspose.Words.a6f73596-b4e5-4742-b430-3d062b8ab739.010.png" alt=""></p><p>图15-C程序输出结果</p><p>根据题目要求，a.c编译的程序实现了输出“Hello world!”问候语；b.c编译的程序会在运行时显示“Time for Play!”，并能在后台运行，每10秒弹出信息提醒用户。</p><p>1) ROS测试与例程</p><p>值得注意的是，本次实验中我安装的是ROS2，故PPT中的命令不能生效。查阅资料之后，我使用ROS2对应的命令成功实现了用键盘控制小乌龟运动。</p><p><img src="https://img.enderfga.cn/img/Aspose.Words.a6f73596-b4e5-4742-b430-3d062b8ab739.012.png" alt=""></p><p>图16-ROS小乌龟测试</p><p>以下是我在过去的学习中使用ROS1和rviz可视化的A*轨迹规划和Turtlebot2轨迹仿真，在此处记录。</p><p><img src="https://img.enderfga.cn/img/Aspose.Words.a6f73596-b4e5-4742-b430-3d062b8ab739.013.png" alt=""><img src="https://img.enderfga.cn/img/Aspose.Words.a6f73596-b4e5-4742-b430-3d062b8ab739.014.png" alt=""></p><p>图17-轨迹规划与实机仿真</p><h2 id="总结与讨论"><a href="#总结与讨论" class="headerlink" title="总结与讨论"></a><strong>总结与讨论</strong></h2><p>1.在Linux中，如何设置前、后台命令和程序的执行？</p><p>在终端中如果对命令不加处理，那么命令会在前台运行；如果打算把命令放到后台运行，这个时候只需要在命令末尾加上&amp;即可，此时终端返回的是[作业号] 进程号。</p><p>需要注意的是，如果程序在后台运行，那么它将无法接受用户的输入，但是其输出将显示在屏幕上（可能用户正在进行其他工作，突然冒出了错误输出），因此在后台执行的程序需是不需要人工干预的、输出被妥善处理（比如重定向）的程序。</p><p>有的时候在程序开始运行之后，想要将程序放在后台执行，这时需要按Ctrl+Z快捷键暂停程序，然后使用bg %作业号命令将其放入后台执行，也可以使用fg %作业号将程序从后台移到前台。</p><p>&amp;是指在后台运行，但当用户推出(挂起)的时候，命令自动也跟着退出。而nohup命令（no hang up）是不挂断的运行，故二者常常结合起来使用，这样就能使命令永久的在后台执行，例如：</p><p>CUDA_VISIBLE_DEVICES=0,1 nohup python train.py  &gt; nohup.log 2&gt;&amp;1 &amp;</p><p><code> </code>即为使用2块GPU后台训练的常用python命令。</p><p>2.你所使用的Linux系统的内核版本是多少？用什么命令查看内核版本？目前你所了解的各发行版本的情况如何？</p><p>可以使用uname -a来查看内核版本，本次实验使用的WSL2和VMware分别输出的结果是：</p><p>Linux Enderfga-PC 5.4.72-microsoft-standard-WSL2 #1 SMP Wed Oct 28 23:40:43 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux</p><p>Linux enderfga-virtual-machine 5.15.0-47-generic #51-Ubuntu SMP Thu Aug 11 07:51:15 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux</p><p>可以看出WSL2相较于虚拟机的差异还是挺大的，即使均为Ubuntu 20.04在内核版本上也有不同。</p><p><code> </code>查阅资料得知，比较流行的发行版本有10个。Ubuntu，Linux Mint和PCLinuxOS是其中最易于使用的。想快速部署使用，就可以选择这几个。尤其对于新手，已经做到了和Windows类似的易用程度了；另一方面，Slackware Linux，Arch Linux和FreeBSD是更激进的发行版，更新比较频繁，所以需要有一定的基础；openSUSE，Fedora，Debian GNU/Linux和Mageia则是比较保守的发行版，稳定性是他们的特点，但是软件包都比较旧，很多桌面版本的新功能没法用；CentOS是一个企业级的发行版，适合那些喜欢稳定性，可靠性和软件长期支持的用户。</p><p>在学习的过程中我使用过Ubuntu和Centos。</p><p>3.你对Linux系统有什么认识？</p><p><code> </code>在我心目中，Linux是一种可以和Windows相媲美的操作系统。Linux一切皆文件、完全开源免费、支持多用户和多任务、同时还支持多种架构平台、可靠的安全性、良好的稳定性、具有强大的网络功能、多样图形界面等等都展现其巨大的魅力。通过Linux我更好地了解学习了计算机网络、操作系统等的相关知识，也实践了深度学习、智能机器人技术等应用。</p><p><strong>五、CPU指令执行过程</strong></p><p><img src="https://img.enderfga.cn/img/Aspose.Words.a6f73596-b4e5-4742-b430-3d062b8ab739.015.png" alt=""><img src="https://img.enderfga.cn/img/Aspose.Words.a6f73596-b4e5-4742-b430-3d062b8ab739.016.png" alt=""></p><p><img src="https://img.enderfga.cn/img/Aspose.Words.a6f73596-b4e5-4742-b430-3d062b8ab739.017.png" alt=""></p><ol><li>首先，我们有一个自动计数器。这个自动计数器会随着时钟主频不断地自增，来作为我们的 PC 寄存器。</li><li>在这个自动计数器的后面，我们连上一个译码器。译码器还要同时连着我们通过大量的 D 触发器组成的内存。</li><li>自动计数器会随着时钟主频不断自增，从译码器当中，找到对应的计数器所表示的内存地址，然后读取出里面的 CPU 指令。</li><li>读取出来的 CPU 指令会通过我们的 CPU 时钟的控制，写入到一个由 D 触发器组成的寄存器，也就是指令寄存器当中。</li><li>在指令寄存器后面，我们可以再跟一个译码器。这个译码器不再是用来寻址的了，而是把我们拿到的指令，解析成 opcode 和对应的操作数。</li><li>当我们拿到对应的 opcode 和操作数，对应的输出线路就要连接 ALU，开始进行各种算术和逻辑运算。对应的计算结果，则会再写回到 D 触发器组成的寄存器或者内存当中。</li></ol><p>模拟代码如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 模拟CPU指令执行过程流水线</span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;iostream&gt;</span></span><br><span class="hljs-comment">//取指令（Instruction Fetch，IF）阶段</span><br><span class="hljs-comment">//译码（Instruction Decode，ID）阶段</span><br><span class="hljs-comment">//执行指令（Execute，EX）阶段</span><br><span class="hljs-comment">//访存取数（Memory，MEM）阶段</span><br><span class="hljs-comment">//结果写回（Writeback，WB）阶段</span><br><span class="hljs-comment">//指令流水线</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">InstructionPipeline</span></span><br><span class="hljs-class">&#123;</span><br><span class="hljs-keyword">public</span>:<br><span class="hljs-built_in">InstructionPipeline</span>()&#123;&#125;<br>~<span class="hljs-built_in">InstructionPipeline</span>()&#123;&#125;<br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">IF</span><span class="hljs-params">()</span></span>;<br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">ID</span><span class="hljs-params">()</span></span>;<br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">EX</span><span class="hljs-params">(<span class="hljs-keyword">int</span> a, <span class="hljs-keyword">int</span> b)</span></span>;<br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">MEM</span><span class="hljs-params">()</span></span>;<br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">WB</span><span class="hljs-params">()</span></span>;<br><span class="hljs-comment">//Program Counter Register</span><br><span class="hljs-built_in"><span class="hljs-keyword">int</span></span> (*p) (<span class="hljs-keyword">int</span> a, <span class="hljs-keyword">int</span> b);<br><span class="hljs-comment">//Instruction Register</span><br><span class="hljs-built_in"><span class="hljs-keyword">int</span></span> (*i) (<span class="hljs-keyword">int</span> a, <span class="hljs-keyword">int</span> b);<br><span class="hljs-comment">//Register</span><br><span class="hljs-built_in"><span class="hljs-keyword">int</span></span> (*r) (<span class="hljs-keyword">int</span> a, <span class="hljs-keyword">int</span> b);<br><span class="hljs-comment">//Memory</span><br><span class="hljs-keyword">int</span> m;<br><span class="hljs-keyword">int</span> t;<br><span class="hljs-keyword">int</span> c = <span class="hljs-number">0</span>;<br>&#125;;<br><span class="hljs-comment">//加法算数指令</span><br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">add</span><span class="hljs-params">(<span class="hljs-keyword">int</span> a, <span class="hljs-keyword">int</span> b)</span> </span>&#123;<br><span class="hljs-keyword">return</span> a + b;<br>&#125;<br><span class="hljs-comment">//IF阶段</span><br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">InstructionPipeline::IF</span><span class="hljs-params">()</span></span><br><span class="hljs-function"></span>&#123;<br><span class="hljs-built_in"><span class="hljs-keyword">int</span></span> (*t) (<span class="hljs-keyword">int</span> a, <span class="hljs-keyword">int</span> b);<span class="hljs-comment">//temp</span><br>    <span class="hljs-comment">//从指令存储器中取指令</span><br>t = p;<br><span class="hljs-comment">//下一条指令地址，自动递增；</span><br><span class="hljs-comment">//p++;</span><br>    <span class="hljs-comment">//将指令存入指令寄存器</span><br>i = p;<br>&#125;<br><span class="hljs-comment">//ID阶段</span><br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">InstructionPipeline::ID</span><span class="hljs-params">()</span></span><br><span class="hljs-function"></span>&#123;<br><span class="hljs-built_in"><span class="hljs-keyword">int</span></span> (*t) (<span class="hljs-keyword">int</span> a, <span class="hljs-keyword">int</span> b);<span class="hljs-comment">//temp</span><br><span class="hljs-comment">//从指令寄存器中取指令</span><br>t = i;<br><span class="hljs-comment">//译码指令</span><br>i = add;<br><span class="hljs-comment">//将译码结果存入译码寄存器</span><br>r = i;<br>&#125;<br><span class="hljs-comment">//EX阶段</span><br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">InstructionPipeline::EX</span><span class="hljs-params">(<span class="hljs-keyword">int</span> a,<span class="hljs-keyword">int</span> b)</span></span><br><span class="hljs-function"></span>&#123;<br><span class="hljs-built_in"><span class="hljs-keyword">int</span></span> (*t) (<span class="hljs-keyword">int</span> a, <span class="hljs-keyword">int</span> b);<span class="hljs-comment">//temp</span><br><span class="hljs-comment">//从译码寄存器中取译码结果</span><br>t = r;<br><span class="hljs-comment">//执行指令</span><br><span class="hljs-keyword">int</span> result = (*t)(a, b);<br><span class="hljs-comment">//将执行结果存入执行寄存器</span><br>m = result;<br>&#125;<br><span class="hljs-comment">//MEM阶段</span><br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">InstructionPipeline::MEM</span><span class="hljs-params">()</span></span><br><span class="hljs-function"></span>&#123;<br><span class="hljs-keyword">int</span> temp = <span class="hljs-number">0</span>;<br><span class="hljs-comment">//访存取数</span><br>temp = m;<br><span class="hljs-comment">//将访存结果存入访存寄存器</span><br>t = temp;<br>&#125;<br><span class="hljs-comment">//WB阶段</span><br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">InstructionPipeline::WB</span><span class="hljs-params">()</span></span><br><span class="hljs-function"></span>&#123;<br><span class="hljs-keyword">int</span> temp = <span class="hljs-number">0</span>;<br><span class="hljs-comment">//从访存寄存器中取访存结果</span><br>temp = t;<br><span class="hljs-comment">//将结果写回</span><br>c = temp;<br>std::cout &lt;&lt; <span class="hljs-string">&quot;result = &quot;</span> &lt;&lt; c &lt;&lt; std::endl;<br>&#125;<br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br><span class="hljs-keyword">int</span> a = <span class="hljs-number">1</span>,b = <span class="hljs-number">1</span>;<br>InstructionPipeline CPU;<br>std::cout &lt;&lt; <span class="hljs-string">&quot;取指...&quot;</span> &lt;&lt; std::endl;<br>CPU.<span class="hljs-built_in">IF</span>();<br>std::cout &lt;&lt; <span class="hljs-string">&quot;译码...&quot;</span> &lt;&lt; std::endl;<br>CPU.<span class="hljs-built_in">ID</span>();<br>std::cout &lt;&lt; <span class="hljs-string">&quot;执行...&quot;</span> &lt;&lt; std::endl;<br>CPU.<span class="hljs-built_in">EX</span>(a, b);<br>std::cout &lt;&lt; <span class="hljs-string">&quot;存结果...&quot;</span> &lt;&lt; std::endl;<br>CPU.<span class="hljs-built_in">MEM</span>();<br>CPU.<span class="hljs-built_in">WB</span>();<br>std::cout &lt;&lt; <span class="hljs-string">&quot;下一条指令地址&quot;</span> &lt;&lt; std::endl;<br><span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br><br></code></pre></td></tr></table></figure><p>结果如下：</p><p><img src="https://img.enderfga.cn/img/Aspose.Words.a6f73596-b4e5-4742-b430-3d062b8ab739.018.png" alt=""></p>]]></content>
    
    
    
    <tags>
      
      <tag>操作系统</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>how to use github?</title>
    <link href="/2022/09/24/github/"/>
    <url>/2022/09/24/github/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>作为全球最大的同性交友网站gayhub，应该怎么用</p><span id="more"></span><h1 id="开始之前"><a href="#开始之前" class="headerlink" title="开始之前"></a>开始之前</h1><p>最好学会科学上网，我没办法教这个，虽然GitHub在国内是可以访问的，但有时候确实慢得难受</p><p>不过加速GitHub的方法千奇百怪，甚至可以开个游戏加速器，这里就不再赘述了</p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>GitHub是全球最大的开源代码网站，这里充满了最前沿的学术、最沙雕的整活、最无私的分享（突然的感慨）。</p><p>科研领域的论文源码发这里，网友们的女装照片发这里，各种各样的知识整合也发在这里，相较于国内那些c**n啥的网站，软件安装包、电子书下载都要钱（发布者都不是原作者），真是天壤之别。</p><h1 id="教程？"><a href="#教程？" class="headerlink" title="教程？"></a>教程？</h1><p>虽然但是，每当我写一个博客想分享点什么的时候还是不禁会怀疑自己，发展到现在这个时代，无论什么内容都能在网上找到很优秀的案例，相较于bilibili这种视频媒体，写出来的文字确实略显苍白，更优质的内容比比皆是。</p><p>所以，想简单地了解一下如何使用的话，不妨从这里看起，浏览一些播放量高跟时间新的视频：</p><p><a href="https://search.bilibili.com/all?keyword=GitHub">https://search.bilibili.com/all?keyword=GitHub</a></p><p><img src="https://img.enderfga.cn/img/image-20220925001854942.png" alt=""></p><p>我写教程前也看了几个，蛮不错的！</p><p>（或许我也应该尝试使用视频录制来分享知识emmmm）</p><h1 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h1><h2 id="任务内容"><a href="#任务内容" class="headerlink" title="任务内容"></a>任务内容</h2><p>写都写了，总不能啥也不教吧，我决定以一个小案例来介绍一下我院学子都是怎么用GitHub炼丹的。</p><p>假设我们现在准备参与某个比赛或者完成某项作业，首先我们手里肯定先有的是数据集，以计算机视觉的第一次作业为例：</p><p><img src="https://img.enderfga.cn/img/image-20220925113333330.png" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20220925113344247.png" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20220925113401643.png" alt=""></p><p>这是一个图像分类任务，评分依据是test集上的结果，所以我们肯定希望追求最好的模型，可以在paperwithcode的imageclassification对应的<a href="https://paperswithcode.com/task/image-classification">sota</a>页面下了解目前哪些模型比较先进：</p><p><img src="https://img.enderfga.cn/img/image-20220925111228440.png" alt=""></p><p>如果手里的数据集是cifar10/100，imagenet等经典数据，那榜单就有很高的参考价值了。鉴于我们手里的plant pathology-2021不在榜单上，我们从作业建议的模型，也是cv领域比较著名的Lenet、VGG16、ResNet50、VIT等试起。</p><h2 id="白嫖代码"><a href="#白嫖代码" class="headerlink" title="白嫖代码"></a>白嫖代码</h2><p>我选择经典到不能再经典的ResNet50来作为本次案例的范本，作为15年提出来的里程碑级别的模型，想要跑resnet实在是太简单不过了。无论是直接百度就能查到网友们各种各样的实现，还是后续的改进优化版本，甚至是直接使用keras.application torch.hub里直接调库，总之，实现目标的途径有很多。</p><p>直接在GitHub上搜索resnet，可以得到以下结果：</p><p><img src="https://img.enderfga.cn/img/image-20220925112920348.png" alt=""></p><p>根据经验我会选择最下面蓝色框中的仓库，因为上面的内容都与我的需求不太相适配（AI不是分类，MXNet、tensorflow、keras不是熟悉的框架，Lua不是熟悉的语言）</p><p>Pytorch-cifar100这个库看标题是使用torch在cifar100数据集上的分类，所以我们只需要下载下来，修改读取数据的部分的代码便可以跑通，得到自己的分类结果。</p><p>如果是用自己的电脑在code处下载代码的zip比较方便，但如果是云服务器，一般都装有git，直接</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">git clone https://github.com/weiaicunzai/pytorch-cifar100.git<br></code></pre></td></tr></table></figure><h2 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h2><p>我点开train.py阅读源码，找到读取数据的代码，一般这些工具函数都写在utils.py中，可以看到这里是利用pytorch自带的cifar100数据的</p><p><img src="https://img.enderfga.cn/img/image-20220925113534402.png" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20220925113713799.png" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20220925121714856.png" alt=""></p><p>读到这里，只要改好我红框圈出来的代码其实就能开始在自己的数据集上训练了，我个人比较习惯使用的是Imagefolder函数，这个函数读取的数据集需要按照类别整理好数据，既train/第一类/第一类的图片，train/第二类/第二类的图片···以此类推。显然我们拿到的数据集不是这样，属于图片和对应的label分开存储的情况，所以我一般会写一个pre_data.py来整理数据，如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 数据集路径（这是我电脑上的，使用时也需要修改）</span><br>path = <span class="hljs-string">r&#x27;C:\Users\User\Downloads\Compressed\plant_dataset\train&#x27;</span> <span class="hljs-comment">#训练集</span><br><span class="hljs-comment"># path = r&#x27;C:\Users\User\Downloads\Compressed\plant_dataset\test&#x27; #测试集</span><br><span class="hljs-comment"># path = r&#x27;C:\Users\User\Downloads\Compressed\plant_dataset\val&#x27; #验证集</span><br><span class="hljs-comment"># 读取csv中的label</span><br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-comment"># 记得把train作对应修改</span><br>df = pd.read_csv(path+<span class="hljs-string">&#x27;/train&#x27;</span>+<span class="hljs-string">&#x27;_label.csv&#x27;</span>)<br><span class="hljs-comment"># 给每一种label创建文件夹，若已存在则跳过</span><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> df[<span class="hljs-string">&#x27;labels&#x27;</span>].unique():<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(path+<span class="hljs-string">&#x27;/&#x27;</span>+i):<br>        os.makedirs(path+<span class="hljs-string">&#x27;/&#x27;</span>+i)<br><span class="hljs-comment"># 将图片移动到对应的文件夹中</span><br><span class="hljs-keyword">import</span> shutil<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(df)):<br>    shutil.copy(path+<span class="hljs-string">&#x27;/images/&#x27;</span>+df[<span class="hljs-string">&#x27;images&#x27;</span>][i],path+<span class="hljs-string">&#x27;/&#x27;</span>+df[<span class="hljs-string">&#x27;labels&#x27;</span>][i]+<span class="hljs-string">&#x27;/&#x27;</span>+df[<span class="hljs-string">&#x27;images&#x27;</span>][i])<br><span class="hljs-comment"># 注意 这里用的是copy不是move，原来的图片还都保存在images文件夹中，训练前需要移走</span><br></code></pre></td></tr></table></figure><h2 id="配置环境"><a href="#配置环境" class="headerlink" title="配置环境"></a>配置环境</h2><p>这一步根据打开的仓库操作，如果你的电脑上已经装好常用的torch，tensorflow等包可能也不需要</p><p>我一般的经验是使用anaconda创建一个虚拟环境</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">conda create -n myenv python=3.6 # 3.6是这个仓库Requirements里写的，注意修改<br>conda activate myenv<br></code></pre></td></tr></table></figure><p>有些readme里Requirements写完了一两个需要包，直接pip install xxx就好，有些则在仓库里准备了requirements.txt，使用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">pip install -r requirements.txt<br></code></pre></td></tr></table></figure><p>总之配置环境这步得对症下药，有时候还需要一些其他工作，具体查看readme对症下药。</p><hr><p>注意，cv作业建议使用mindspore框架，而不是torch，所以情况会有所不同。</p><h1 id="all-in-all"><a href="#all-in-all" class="headerlink" title="all in all"></a>all in all</h1><p>暂时想不到什么可以写的内容，有什么问题不如评论或者私信我吧，GitHub上的精品项目太多了，天天都在疯狂star。例如以上提到的模型，我推荐在这个<a href="https://github.com/WZMIAOMIAO/deep-learning-for-image-processing">仓库</a>里查看代码，还附带有讲解</p><p><img src="https://img.enderfga.cn/img/image-20220925130630621.png" alt=""></p><p>希望看到这里的同学打开我的GitHub主页给我一个<a href="https://github.com/Enderfga/Enderfga">star</a>。</p><p><img src="https://img.enderfga.cn/img/image-20220925130725261.png" alt=""></p>]]></content>
    
    
    
    <tags>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>脑电研究</title>
    <link href="/2022/07/18/mind/"/>
    <url>/2022/07/18/mind/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><strong>脑电研究：通过神经活动和视觉特征的多模态学习</strong>解码大脑表征</p><span id="more"></span><embed src="./mind.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    
    <tags>
      
      <tag>无人系统</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>自动驾驶技术基础赛车仿真</title>
    <link href="/2022/07/18/auto3/"/>
    <url>/2022/07/18/auto3/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>用尽各种手段控制最终依旧屈服于调参</p><span id="more"></span><h2 id="项目简介"><a href="#项目简介" class="headerlink" title="项目简介"></a>项目简介</h2><p>​        拿到GitHub仓库之后我就开始搜索相关资料，代码和苏黎世联邦理工学院自动控制实验室 (IfA)  的MPCC仿真有很大部分重合。但在实际调试之后我发现，老师使用的控制是LQR，车的模型参数是一致的但其他代码基本都是多余的，核心就是slx的仿真文件。</p><h2 id="模型建立"><a href="#模型建立" class="headerlink" title="模型建立"></a>模型建立</h2><p>关于高速转向车辆建模，老师在课上有详细的推导过程，这里只展示结果：</p><script type="math/tex; mode=display">\begin{aligned}{\left[\begin{array}{c}\dot{X} \\\dot{Y} \\\dot{\psi} \\\dot{v}_{x} \\\dot{v}_{y} \\\dot{\omega}\end{array}\right]=\left[\begin{array}{c}v_{x} \cos \psi+v_{x} \sin \psi \\v_{y} \cos \psi-v_{y} \sin \psi \\\omega\\\frac{1}{m}\left(F_{x r}+\cos \delta v_{x} \sin \psi\right. \\\frac{1}{m}\left(F_{y r}+\sin \delta F_{y f}+m v_{y} \omega\right) \\\frac{1}{I_{z}}\left(l_{f} F_{x f} \sin \delta+\cos \delta F_{y f}-m v_{x} \omega\right) \\\end{array}\right] } \\\alpha_{r}=\arctan \left(\frac{v_{y}-l_{r} \omega}{v_{x}}\right) \\\alpha_{f}=-\delta+\arctan \left(\frac{v_{y}+l_{f} \omega}{v_{x}}\right) \\F_{y r}=2 D_{r} \sin \left(C_{r} \arctan \left(-\alpha_{r} B_{r}\right)\right) \\F_{y f}=2 D_{f} \sin \left(C_{f} \arctan \left(-\alpha_{f} B_{f}\right)\right) \\F_{x r}=F_{x}=\left(C_{m 1}-C_{m 2} v_{x}\right) d-C_{r} N-C_{d} v_{x}^{2} \\F_{x f}=-C_{r} N-C_{d} v_{x}^{2}\end{aligned}</script><p>体现在系统的s-function中如下：</p><p><img src="https://img.enderfga.cn/img/image-20220713190531313.png" alt=""></p><h2 id="控制仿真"><a href="#控制仿真" class="headerlink" title="控制仿真"></a>控制仿真</h2><p>​        简单介绍一下使用到的控制方法：</p><h3 id="LQR"><a href="#LQR" class="headerlink" title="LQR"></a>LQR</h3><p>​        最优控制理论主要探讨的是让动力系统以在最小成本来运作，若系统动态可以用一组线性微分方程表示，而其成本为二次泛函，这类的问题称为线性二次（LQ）问题。此类问题的解即为线性二次调节器（英语：linear–quadratic regulator），简称LQR。</p><p>有无限时间/有限时间，离散时间/连续时间几种类型。思路如下：</p><ul><li>选择参数矩阵Q,R</li><li>求解Riccati方程得到矩阵 $P$</li><li>根据P计算 $K=R^{-1} B^{T} P$</li><li>计算控制量 $u=-K x$</li></ul><h3 id="PID"><a href="#PID" class="headerlink" title="PID"></a>PID</h3><p>​        PID控制器（比例-积分-微分控制器），由比例单元（Proportional）、积分单元（Integral）和微分单元（Derivative）组成。可以透过调整这三个单元的增益$K_P$，$K_I$和$K_D$来调定其特性。PID控制器主要适用于基本上线性，且动态特性不随时间变化的系统。</p><script type="math/tex; mode=display">\mathrm{u}(t)=\mathrm{MV}(t)=K_{p} e(t)+K_{i} \int_{0}^{t} e(\tau) d \tau+K_{d} \frac{d}{d t} e(t)</script><p>其中<br>$K_{p}$ : 比例增益，是调适参数<br>$K_{i}$ : 积分增益，也是调适参数<br>$K_{d}$ : 微分增益，也是调适参数<br>$e$ : 误差 $=$ 设定值 $(\mathrm{SP})-$ 回授值 (PV)<br>$t$ : 目前时间<br>$\tau$ : 积分变数，数值从 0 到目前时间 $t$</p><p>​        PID的使用非常简单，在simulink中也只是添加一个单输入单输出的单元，本次实验的提升几乎靠的是PID控制。</p><h3 id="MPC"><a href="#MPC" class="headerlink" title="MPC"></a>MPC</h3><script type="math/tex; mode=display">\begin{array}{ll}\min & \sum_{k=1}^{N}\left[\begin{array}{c}\hat{e}_{k}^{c} \\\hat{e}_{k}^{l}\end{array}\right]^{T}\left[\begin{array}{cc}q_{c} & 0 \\0 & q_{l}\end{array}\right]\left[\begin{array}{c}\hat{e}_{k}^{c} \\\hat{e}_{k}^{l}\end{array}\right]-q_{v} v_{\theta, k}+\Delta u_{k}^{T} R_{\Delta} \Delta u_{k} \\\text { s.t. } & x_{0}=x(0) \\& x_{k+1}=f\left(x_{k}, u_{k}\right) \\& \hat{e}^{c}\left(x_{k}\right)=\sin \left(\Phi^{\mathrm{ref}}\left(\theta_{k}\right)\right)\left(X_{k}-X^{\mathrm{ref}}\left(\theta_{k}\right)\right)-\cos \left(\Phi^{\mathrm{ref}}\left(\theta_{k}\right)\right)\left(Y_{k}-Y^{\mathrm{ref}}\left(\theta_{k}\right)\right) \\& \hat{e}^{l}\left(x_{k}\right)=-\cos \left(\Phi^{\mathrm{ref}}\left(\theta_{k}\right)\right)\left(X_{k}-X^{\mathrm{ref}}\left(\theta_{k}\right)\right)-\sin \left(\Phi^{\mathrm{ref}}\left(\theta_{k}\right)\right)\left(Y_{k}-Y^{\mathrm{ref}}\left(\theta_{k}\right)\right) \\& \Delta u_{k}=u_{k}-u_{k-1} \\& x_{k} \in \mathcal{X}_{\text {Track }} \\& \underline{x} \leq x_{k} \leq \bar{x} \\& \underline{u} \leq u_{k} \leq \bar{u} \\& \Delta u \leq \Delta u_{k} \leq \overline{\Delta u}\end{array}</script><script type="math/tex; mode=display">\begin{array}{ll}F_{f, y}=D_{f} \sin \left(C_{f} \arctan \left(B_{f} \alpha_{f}\right)\right) \quad \text { where } & \alpha_{f}=-\arctan \left(\frac{\dot{\varphi} l_{f}+v_{y}}{v_{x}}\right)+\delta \\F_{r, y}=D_{r} \sin \left(C_{r} \arctan \left(B_{r} \alpha_{r}\right)\right) \quad \text { where } \quad \alpha_{r}=\arctan \left(\frac{\dot{\varphi} l_{r}-v_{y}}{v_{x}}\right) \\F_{r, x}=\left(C_{m 1}-C_{m 2} v_{x}\right) d-C_{r}-C_{d} v_{x}^{2}\end{array}</script><p>​        最后，问题的状态和输入结果如下：</p><script type="math/tex; mode=display">\begin{aligned}&x=\left[X, Y, \varphi, v_{x}, v_{y}, \omega, \theta\right] \\&u=\left[d, \delta, v_{\theta}\right]\end{aligned}</script><p>​        因为有MPCC的这个仓库作为参考，故这也是我最开始的改进方向，接下来会详细介绍过程与结果。</p><h2 id="实验过程与结果分析"><a href="#实验过程与结果分析" class="headerlink" title="实验过程与结果分析"></a>实验过程与结果分析</h2><p>​        首先我把仓库中的所有代码文件简单浏览了一遍，将vx改到16，时间降低到17.2690s。然后我开始研究如何将MPC控制加入进来。</p><p>​        可以选择的方法有两种，一种是根据原理编写m文件，另外一种是利用simulink中的元件。可惜最后实现效果均不理想，我没能将控制成功使用进来，反而是让结果越来越差。</p><p>​        于是我就考虑既然IFA的MPCC仿真我可以成功运行，没能从中借鉴到控制的使用，能否将我的已知轨迹放到程序中规划，再将得到的最优轨迹导出，作为我的仿真的引导。</p><p>​        程序按照我设想地运行，但是结果很不理想，每次都运行一半车就跑出轨道，几个优化包算出来的结果都是inf，估计与约束有关。</p><p>​        不得已我放弃了这种想法，继续在仿真中的PID调参，结果有所提升。其中的某些结果路径比较符合我的期望，结合MPC的滚动优化的思想，我想是否可以将我每次运行的路径保存下来，作为下一次运行的引导线加入仿真中，再结合已有的控制器轨迹追踪，以此来提升效果。</p><p>​        可惜结果依旧不理想，最终我放弃了这个想法。这期间我还试着查询了MPC的相关论文，使用python，pytorch，ros等平台来仿真，但原本的控制没有让我从轨迹修改获得提升。不得已只能采取vx，PID的三个增益这4类参数的调优，调试思路总体沿着齐格勒－尼科尔斯方法，不断修改，直至获得最终时间为：12.866s。</p><p><img src="https://img.enderfga.cn/img/image-20220713202724568.png" alt=""></p><p>​        默认将小车视为质点，已经放大检查过了，上述结果没有与边界发生碰撞。</p><p>​        由于没有使用PID tuner，参数是随机试出来的，且除了$K_i$均为整数。我估计如果愿意从小数点后一点点慢慢调试可能达到更优的结果，但试到这里我觉得再改下去没有什么意义了。本次项目是我目前做到的最“自由”的作业，老师从开学第一节课开始铺垫，我也尝试了卡尔曼滤波，Pure Pursuit，MPC等等方法，即使实力不够没能成功将这些控制实现，但在回忆老师指导内容，查阅资料的过程中也收获了很多乐趣，深刻感受到了控制之美。</p>]]></content>
    
    
    
    <tags>
      
      <tag>自动驾驶</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>自动驾驶技术基础之规划与仿真</title>
    <link href="/2022/07/18/auto2/"/>
    <url>/2022/07/18/auto2/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>自动驾驶技术基础的规划与仿真作业</p><span id="more"></span><embed src="./report.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    
    <tags>
      
      <tag>自动驾驶</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度可分离卷积简介</title>
    <link href="/2022/06/22/dsc/"/>
    <url>/2022/06/22/dsc/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>深度学习理论平时作业</p><span id="more"></span><embed src="./dsc.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>cifar10-horse生成任务</title>
    <link href="/2022/06/20/horse/"/>
    <url>/2022/06/20/horse/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>人工智能综合实验期末大作业horse生成（VAE,AAE,GAN…)</p><span id="more"></span><h1 align="center">Horse Generation</h1><h4 align="center">Comprehensive experiment of artificial intelligence</h4><h3 id="Requirements"><a href="#Requirements" class="headerlink" title="Requirements"></a>Requirements</h3><ol><li>64x,python3.8/3.9.</li><li>CUDA toolkit 11.1.</li><li>GCC 7.</li><li>sh setup.sh/pip install -r requirements.txt</li></ol><h3 id="文件目录"><a href="#文件目录" class="headerlink" title="文件目录"></a>文件目录</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python">├─Intro.pptx  <span class="hljs-comment">#介绍与展示的PowerPoint</span><br>├─tf_aae                      <span class="hljs-comment">#AAE的tensorflow实现</span><br>├─VAE                         <span class="hljs-comment">#VAE的pytorch实现</span><br>├─GAN.ipynb                   <span class="hljs-comment">#所有GAN的pytorch实现与运行结果</span><br>├─requirements.txt  <span class="hljs-comment">#本次任务运行环境中的包</span><br>├─setup.sh  <span class="hljs-comment">#配置环境的安装脚本</span><br>│<br>├─result                      <span class="hljs-comment">#存放用于展示的结果</span><br>├─apps<br>│    └─interpolate_sample.py  <span class="hljs-comment">#用于生成视频</span><br>│<br>├─fake_horse                  <span class="hljs-comment">#一千张生成所得马的图片</span><br>├─mmgen                       <span class="hljs-comment">#一个基于 PyTorch 和MMCV的强有力的生成模型工具箱</span><br>│  ├─apis<br>│  ├─core<br>│  ├─datasets<br>│  ├─models<br>│  ├─ops<br>│  └─utils<br>├─configs                     <span class="hljs-comment">#运行stylegan3的配置文件</span><br>│  ├─styleganv3<br>│  │    └─stylegan3.py<br>│  │<br>│  └─_base_<br>│      ├─ default_runtime.py      <span class="hljs-comment">#训练配置</span><br>│      │<br>│      ├─datasets<br>│      │    └─horse.py                <span class="hljs-comment">#数据处理</span><br>│      │<br>│      └─models<br>│          └─stylegan<br>│                └─stylegan3_base.py  <span class="hljs-comment">#模型搭建</span><br>│<br>└─tools<br>    ├─ dist_train.sh   <span class="hljs-comment">#训练模型的脚本</span><br>    ├─ train.py  <span class="hljs-comment">#训练模型的代码</span><br>    │<br>    └─utils<br>          └─inception_stat.py         <span class="hljs-comment">#生成用于计算fid的inception模型</span><br></code></pre></td></tr></table></figure><h3 id="结果展示"><a href="#结果展示" class="headerlink" title="结果展示"></a>结果展示</h3><h4 id="部分fake马"><a href="#部分fake马" class="headerlink" title="部分fake马"></a>部分fake马</h4><p> <img align="center" src="https://img.enderfga.cn/img/625.png"/></p><h4 id="生成训练可视化"><a href="#生成训练可视化" class="headerlink" title="生成训练可视化"></a>生成训练可视化</h4><p> <img align="center" src="https://img.enderfga.cn/img/GAN_generate_animation.gif"/></p><h4 id="动态结果展示"><a href="#动态结果展示" class="headerlink" title="动态结果展示"></a>动态结果展示</h4><p> <img align="center" src="https://img.enderfga.cn/img/lerp.gif"/></p><h3 id="FID一览"><a href="#FID一览" class="headerlink" title="FID一览"></a>FID一览</h3><p><img src="https://img.enderfga.cn/img/image-20220618194023873.png" alt=""></p><h3 id="代码说明"><a href="#代码说明" class="headerlink" title="代码说明"></a>代码说明</h3><ol><li>VAE/</li><li>tf_aae/</li><li><p>GAN.ipynb</p><p>前两者为文件夹，存放了模型，配置，数据集，辅助函数等相关py文件，主体为main.py;</p><p>后者为ipynb文件，记录了所有GAN模型的运行结果，并用于计算FID分数。</p><p>上述代码均可以通过修改数据路径来训练或测试。</p><embed src="./horse.pdf" width="100%" height="750" type="application/pdf"></li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>呼吸运动伪影的图像质量评估</title>
    <link href="/2022/06/20/cmr/"/>
    <url>/2022/06/20/cmr/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><strong>基于深度学习分类网络的病变诊断之呼吸运动伪影的图像质量评估</strong></p><span id="more"></span><p><img src="https://img.enderfga.cn/img/image-20220620084604845.png" alt=""></p><embed src="./cmr.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度学习在遥感图像分类中的应用</title>
    <link href="/2022/05/30/dl/"/>
    <url>/2022/05/30/dl/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>深度学习期中应用调研报告</p><span id="more"></span><p><img src="https://img.enderfga.cn/img/what-is-deep-learning.jpg" alt=""></p><embed src="./dl.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Thinking Deeper about Cifar10</title>
    <link href="/2022/05/30/cifar/"/>
    <url>/2022/05/30/cifar/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>人工智能综合实验期中大作业cifar10分类</p><span id="more"></span><p><img src="https://img.enderfga.cn/img/20220530151047.png" alt=""></p><embed src="./cifar.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Docker 常用命令与操作</title>
    <link href="/2022/05/29/docker/"/>
    <url>/2022/05/29/docker/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>一张图整理Docker常用命令</p><span id="more"></span><p><img src="https://img.enderfga.cn/img/image-20220529091219215.png" alt=""></p><h2 id="Docker基本概念"><a href="#Docker基本概念" class="headerlink" title="Docker基本概念"></a>Docker基本概念</h2><p>Docker 包括三个基本概念：</p><ul><li>镜像（<code>Image</code>）：Docker 镜像是一个特殊的文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数（如匿名卷、环境变量、用户等）。镜像不包含任何动态数据，其内容在构建之后也不会被改变。</li><li>容器（<code>Container</code>）：镜像（<code>Image</code>）和容器（<code>Container</code>）的关系，就像是面向对象程序设计中的 <code>类</code> 和 <code>实例</code> 一样，镜像是静态的定义，容器是镜像运行时的实体。容器可以被创建、启动、停止、删除、暂停等。</li><li>仓库（<code>Repository</code>）：仓库（<code>Repository</code>）类似Git的远程仓库，集中存放镜像文件。</li></ul><p>三者关系可以用下图表示：</p><p><img src="https://img.enderfga.cn/img/image-20220529091304514.png" alt=""></p><p>接下来看一看Docker的常用命令。</p><h2 id="服务"><a href="#服务" class="headerlink" title="服务"></a>服务</h2><ul><li><p>查看Docker版本信息</p><p>docker version</p></li><li><p>查看docker简要信息</p><p>docker -v</p></li><li><p>启动Docker</p><p>systemctl start docker</p></li><li><p>关闭docker</p><p>systemctl stop docker</p></li><li><p>设置开机启动</p><p>systemctl enable docker</p></li><li><p>重启docker服务</p><p>service docker restart</p></li><li><p>关闭docker服务</p><p>service docker stop</p></li></ul><h2 id="镜像"><a href="#镜像" class="headerlink" title="镜像"></a>镜像</h2><h3 id="镜像仓库"><a href="#镜像仓库" class="headerlink" title="镜像仓库"></a>镜像仓库</h3><p><a href="https://link.segmentfault.com/?enc=eCypHIByefaQ8WlX8AJqSg%3D%3D.IV619qtOOjR2McoljdjR%2FNJXcHjWaOrxy0NrT1051R7Ot%2BYs5eUPDK5lfZoTudgL">Docker Hub</a> 等镜像仓库上有大量的高质量的镜像可以用，可以从仓库获取镜像。</p><ul><li><p>检索镜像</p><p>docker search 关键字</p></li><li><p>拉取镜像</p><p>docker pull [选项] [Docker Registry 地址[:端口号]/]仓库名[:标签]</p></li></ul><h3 id="镜像管理"><a href="#镜像管理" class="headerlink" title="镜像管理"></a>镜像管理</h3><ul><li><p>列出镜像</p><p>docker image ls<br>docker images</p></li><li><p>删除镜像</p><p># 删除指定镜像<br>docker rmi &lt;镜像Id&gt;</p></li><li><p>导出镜像</p><p># 将镜像保存为归档文件<br>docker save</p></li><li><p>导入镜像</p><p>docker load</p></li></ul><h3 id="Dockerfile构建镜像"><a href="#Dockerfile构建镜像" class="headerlink" title="Dockerfile构建镜像"></a>Dockerfile构建镜像</h3><p>Dockerfile 是一个文本格式的配 文件，用户可以使用 Dockerfile 来快速创建自定义的镜像。</p><p>Dockerfile 由一行行行命令语句组成，并且支持以＃开头的注释行.</p><h4 id="Dockerfile常见指令"><a href="#Dockerfile常见指令" class="headerlink" title="Dockerfile常见指令"></a>Dockerfile常见指令</h4><p>下面是Dockerfile中一些常见的指令：</p><ul><li>FROM：指定基础镜像</li><li>RUN：执行命令</li><li>COPY：复制文件</li><li>ADD：更高级的复制文件</li><li>CMD：容器启动命令</li><li>ENV：设置环境变量</li><li>EXPOSE：暴露端口</li></ul><p>其它的指令还有ENTRYPOINT、ARG、VOLUME、WORKDIR、USER、HEALTHCHECK、ONBUILD、LABEL等等。</p><h4 id="镜像构建"><a href="#镜像构建" class="headerlink" title="镜像构建"></a>镜像构建</h4><p> docker build</p><h4 id="镜像运行"><a href="#镜像运行" class="headerlink" title="镜像运行"></a>镜像运行</h4><p>镜像运行，就是新建并运行一个容器。</p><p> docker run [镜像ID]</p><h2 id="容器"><a href="#容器" class="headerlink" title="容器"></a>容器</h2><h3 id="容器生命周期"><a href="#容器生命周期" class="headerlink" title="容器生命周期"></a>容器生命周期</h3><ul><li><p>启动：启动容器有两种方式，一种是基于镜像新建一个容器并启动，另外一个是将在终止状态（stopped）的容器重新启动。</p><p># 新建并启动</p><p>docker run [镜像名/镜像ID]</p><p># 启动已终止容器</p><p>docker start [容器ID]</p></li><li><p>查看容器</p><p># 列出本机运行的容器</p><p>docker ps</p><p># 列出本机所有的容器（包括停止和运行）</p><p>docker ps -a</p></li><li><p>停止容器</p><p># 停止运行的容器</p><p>docker stop [容器ID]</p><p># 杀死容器进程</p><p>docker kill [容器ID]</p></li><li><p>重启容器</p><p>docker restart [容器ID]</p></li><li><p>删除容器</p><p>docker rm [容器ID]</p></li></ul><h3 id="进入容器"><a href="#进入容器" class="headerlink" title="进入容器"></a>进入容器</h3><p>进入容器有两种方式：</p><p> # 如果从这个 stdin 中 exit，会导致容器的停止</p><p> docker attach [容器ID]</p><p> # 交互式进入容器</p><p> docker exec [容器ID]</p><p>进入容器通常使用第二种方式，<code>docker exec</code>后面跟的常见参数如下：</p><p>－ d, —detach 在容器中后台执行命令；</p><p> － i, —interactive=true I false ：打开标准输入接受用户输入命令</p><h3 id="导出和导入"><a href="#导出和导入" class="headerlink" title="导出和导入"></a>导出和导入</h3><ul><li><p>导出容器</p><p>#导出一个已经创建的容器到一个文件</p><p>docker export [容器ID]</p></li><li><p>导入容器</p><p># 导出的容器快照文件可以再导入为镜像</p><p>docker import [路径]</p></li></ul><h3 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h3><ul><li><p>查看日志</p><p># 导出的容器快照文件可以再导入为镜像</p><p>docker logs [容器ID]</p></li></ul><p>这个命令有以下常用参数</p><p> -f : 跟踪日志输出</p><p>—since :显示某个开始时间的所有日志</p><p>-t : 显示时间戳</p><p>—tail :仅列出最新N条容器日志</p><ul><li><p>复制文件</p><p># 从主机复制到容器</p><p>sudo docker cp host_path containerID:container_path</p><p># 从容器复制到主机</p><p>sudo docker cp containerID:container_path host_path</p></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>安の推荐</title>
    <link href="/2022/05/28/suggest/"/>
    <url>/2022/05/28/suggest/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="https://img.enderfga.cn/img/image-20220528213455416.png" alt=""></p><p>一时兴起，乱七八糟的推荐；需要付费的也很多，一分钱一分货，希望大家支持正版</p><span id="more"></span><h1 id="书签"><a href="#书签" class="headerlink" title="书签"></a>书签</h1><h2 id="杂项"><a href="#杂项" class="headerlink" title="杂项"></a>杂项</h2><p><a href="https://enderfga.cn/">我的博客</a></p><p>没想到吧，我的第一个书签居然是自己的博客 虽然没什么高质量文章，但拿来当知识备忘录还是挺方便的~</p><p><a href="https://pan.baidu.com/disk/home?#/all?path=%2F&amp;vmode=list">百度云</a></p><p>上了大学之后使用频率很低，当然主要原因还是我校的网速….不过有时候还是会拿来下下电影</p><p><a href="https://translate.google.cn/">Google 翻译</a></p><p><a href="https://dict.cnki.net/index">CNKI翻译助手</a></p><p><a href="https://www.deepl.com/translator">DeepL翻译</a></p><p>那个知网的翻译助手在b站刷视频被推荐的，还没有怎么用过；一般用deepl＋google对照使用</p><p><a href="http://www.gamersky.com/">游民星空 </a></p><p>使用频率最高的网站，从08年开始每天晚上19点定时打开看囧图，虽然怪傻的</p><p>现在没有盗版游戏下载了，但各种各样的资讯当新闻看还挺有趣的(评论区的游民老哥好玩)</p><p><a href="https://www.youtube.com/">YouTube</a></p><p>没啥好说的，其实我大部分时间也只用bilibili</p><p><a href="https://www.zhihu.com/">知乎</a></p><p>搜搜各种知识，看看文章还有科技产品推荐</p><p><a href="https://portal.sysu.edu.cn/#/index">中山大学统一门户</a></p><p>我校官网的新ui很不错，安卓端也适配了</p><p><a href="https://github.com/Enderfga?tab=stars">Your Stars</a></p><p>习惯了收藏stars来上GitHub，GitHub的使用就不用介绍了</p><p><a href="https://2550505.com/">毛怪俱乐部</a></p><p>毛怪居然有自己的官网了！好像刚刚起步，希望早日能买到hanser的专辑！</p><p><a href="https://www.douyin.com/">抖音-记录美好生活</a></p><p>为了防沉迷，我卸载了抖音；但还是想刷，有时候就看看网页版</p><p><a href="https://eshop-prices.com/?currency=CNY">eShop-Prices.com – The best price comparison tool for Nintendo Switch games – Chinese Renminbi Yuan</a></p><p>一个显示switch游戏最低价格的网站，不过我是大慈善家，买了也没有时间玩</p><p><img src="https://img.enderfga.cn/img/image-20220528220111823.png" alt=""></p><h2 id="影视"><a href="#影视" class="headerlink" title="影视"></a>影视</h2><p>由于下面的网站特殊性，他们常常跑路换域名···</p><p><a href="http://www.rrdyw.net/">人人电影网</a></p><p><a href="http://www.btbtt15.com/">BT之家-BT电影天堂-影视资源交流社区</a></p><p><a href="http://www.kisssub.org/">爱恋动漫</a></p><p>下载电影，番剧···等等的地方，如果能打开的话，都挺好用的</p><p><a href="http://www.age.tv/">在线动画 动漫下载 - AGE动漫</a></p><p>在线看番，无弹幕，更新快</p><p><a href="https://omofun.tv/">OmoFun动漫视频网 - (￣﹃￣)~omO</a></p><p>在线看番，有弹幕</p><p><a href="http://dyxs14.com/">电影先生 - 聚合全网高清影视在线观看、下载</a></p><p>在线看剧</p><p><a href="http://www.549.tv/">影视森林——观影第一站</a></p><p>一个集合导航类型的网站，不怎么用，除非上面的都打不开会试着在里面探索新的</p><h2 id="图片"><a href="#图片" class="headerlink" title="图片"></a>图片</h2><p><a href="https://apod.nasa.gov/">Astronomy Picture of the Day</a></p><p>每天一张宇宙照片和介绍</p><p><a href="https://wallhaven.cc/">Awesome Wallpapers - wallhaven.cc</a></p><p>精美的壁纸，登录解锁全部内容</p><p><a href="https://unsplash.com/">Beautiful Free Images &amp; Pictures | Unsplash</a></p><p>好看的图片，不是壁纸类型的</p><p><a href="https://tinypng.com/">TinyPNG</a></p><p><a href="https://www.picdiet.com/zh-cn">Picdiet - 压缩图片</a></p><p>图片无损压缩</p><p><a href="https://bigjpg.com/">Bigjpg</a></p><p>图片无损放大</p><p><a href="https://wordart.com/">WordArt.com - Word Cloud Art Creator</a></p><p>制作词云图，刚开始我用python，后来发现还是用这个拿去骗人效果好一点</p><p><a href="https://photomosh.com/">PhotoMosh</a></p><p>一个“无聊”的网站，会随机给你的图片来点图像增广，建议自己试试</p><p><img src="https://img.enderfga.cn/img/image-20220528221059628.png" alt=""></p><h2 id="软件"><a href="#软件" class="headerlink" title="软件"></a>软件</h2><p>这里写的是关于软件的书签，并不是推荐软件</p><p><a href="https://amazing-apps.gitbooks.io/windows-apps-that-amaze-us/content/zh-CN/?q=">序章 · 绝赞应用</a></p><p>GitHub上一个旨在介绍 Windows 绝妙项目的网站，现在能用，不过好像断更了</p><p><img src="https://img.enderfga.cn/img/image-20220528221330033.png" alt=""></p><p><a href="https://music.hwkxk.cn/">音乐助手 - 简洁极速搜索解析各大平台音乐</a></p><p>穷人的最爱，不过最近无了，我也是写这个才发现的，希望能回来</p><p><a href="https://www.ghxi.com/">果核剥壳 - 互联网的净土</a></p><p><a href="https://www.rjsos.com/">软件SOS</a></p><p>两个我下载软件的地方，类似于xmind，bandicam等好用的付费软件上面可能有破解版</p><h2 id="知识"><a href="#知识" class="headerlink" title="知识"></a>知识</h2><p>真正的干货</p><p><a href="http://www.ucdrs.superlib.net/">全国图书馆参考咨询联盟</a></p><p><a href="http://libgen.rs/">Library Genesis</a></p><p><a href="https://www.jiumodiary.com/">Jiumo Search 鸠摩搜索</a></p><p><a href="https://zh.z-lib.org/">Z-Library</a></p><p>以上是下载电子书的地方，zlibrary使用频率最高，我还有把我有上面没有的pdf上传，第二个下载国外的书用的，鸠摩像是个百度网盘搜索网站，走投无路的时候会搜；最后这个参考咨询联盟其实本身没什么卵用，但结合油猴脚本可以简单地花1,2块钱买到pdf，比淘宝方便</p><p><a href="https://apps.webofknowledge.com/UA_GeneralSearch_input.do?product=UA&amp;search_mode=GeneralSearch&amp;SID=D4NArRzmguT3sCmspng&amp;preferencesSaved=">Web of Science</a></p><p><a href="https://www.cnki.net/">中国知网</a></p><p><a href="https://scholar.google.com/">Google Scholar</a></p><p><a href="https://xueshu.baidu.com/">百度学术</a></p><p><a href="https://readpaper.com/">论文阅读-专业的学术讨论社区</a></p><p><a href="https://www.semanticscholar.org/">Semantic Scholar</a></p><p>基本只用webofscience和谷歌学术，百度学术用来批量引用；那个社区可以读读文献，划词翻译，不过能实现这个功能的方式实在太多了；知网大一写政治课作业的时候还用，现在我甚至点不开了。</p><p>最近开始用Semantic Scholar，一款基于机器学习的学术搜索引擎，使用体验也很不错。</p><p><a href="https://oi-wiki.org/">OI Wiki</a></p><p><a href="https://labuladong.gitee.io/algo/">labuladong 的算法小抄 </a></p><p>收藏了但就是懒得点开的算法知识</p><p><a href="https://www.runoob.com/">菜鸟教程</a></p><p><a href="https://missing-semester-cn.github.io/">计算机教育中缺失的一课</a></p><p>高质量自学内容，但就是很难有被点开的机会</p><p><a href="https://zh.wikipedia.org/wiki/Wikipedia:首页">维基百科</a></p><p>虽然收藏的是中文首页，但还是用的英文版搜资料（会全很多，翻译不及时）</p><p><a href="https://www.latexlive.com/">在线LaTeX公式编辑器</a></p><p>ocr公式，以前一天50次，现在一天10次，改用mathpix了</p><p><a href="https://paperswithcode.com/sota">State-of-the-Art</a></p><p>机器学习深度学习写作业的时候的灵感来源，或者说是借鉴来源；paperwithcode真的很方便</p><p><a href="https://cn.overleaf.com/latex/templates">Templates - - Overleaf</a></p><p>小组大作业会大家一起用这个在线编译，模板的话我还是习惯自己常备的那3个</p><p><a href="https://spcqwserdvymm.com.vika.cn/share/shryNwH3HRgvzMTaZVAGx/fodkuzz5eaw0w">🔔 Efficiency-follow</a></p><p>乱七八糟的好东西合集，自己探索吧</p><p><a href="https://snip.mathpix.com/">Snip Notes</a></p><p>mathpix他们的一个产品，我拿来上传老师发的pdf然后提取内容放到我的作业里</p><p><a href="https://quillbot.com/">Paraphrasing Tool | QuillBot AI</a></p><p>英文写作小帮手，写出来之后能帮忙提升流畅度、专业性等等等等，功能高级的部分要付费</p><p><a href="https://stackoverflow.com/">Stack Overflow</a></p><p>程序猿的“知乎”，一般有bug在这一搜都能搜到，我没怎么试过问答</p><p><a href="https://zs.symbolab.com/">Symbolab 数学求解器 - 分步计算器</a></p><p>当你高数知识忘的一干二净，遇到问题就用这个来解吧，还有分步过程</p><p><a href="https://www.ilovepdf.com/zh-cn/unlock_pdf">解锁 PDF文件</a></p><p>里面还有各种各样的pdf功能，不过学校提供的正版foxit也基本都有，主要是用来解锁deepl翻译的文档（只读）</p><h1 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h1><h2 id="去广告"><a href="#去广告" class="headerlink" title="去广告"></a>去广告</h2><p><img src="https://img.enderfga.cn/img/image-20220528223252001.png" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20220528223408102.png" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20220528223542478.png" alt=""></p><p><a href="https://chrome.google.com/webstore/detail/adblock-plus-free-ad-bloc/cfhdojbkjhnklbpkdaibdccddilifddb">Adblock Plus</a></p><p><a href="https://chrome.google.com/webstore/detail/adblock-%E2%80%94-best-ad-blocker/gighmmpiobklfepjocnamgkkbiglidom">AdBlock</a></p><p><a href="https://chrome.google.com/webstore/detail/ublock-origin/cjpalhdlnbpafiamejdnhcphjbkeiagm">uBlock Origin</a></p><p>这仨就没什么好解释的，对广告零容忍，也不嫌多就都装了；可以指定屏蔽内容，我把GitHub上那些*独分子都屏蔽了</p><h2 id="邮件"><a href="#邮件" class="headerlink" title="邮件"></a>邮件</h2><p><img src="https://img.enderfga.cn/img/image-20220528223729300.png" alt=""></p><p>方便我查收<a href="https://chrome.google.com/webstore/detail/checker-plus-for-gmail/oeopbcgkkoapgobdbedcemjljbihmemj">gmail</a>的邮件，会有消息提醒等</p><p><img src="https://img.enderfga.cn/img/image-20220528223816247.png" alt=""></p><p>作为一个强迫症，我看到我的每一个盘多了几MB<a href="https://chrome.google.com/webstore/detail/clean-master-the-best-chr/eagiakjmjnblliacokhcalebgnhellfi">垃圾</a>我都会很难受</p><h2 id="翻译写作"><a href="#翻译写作" class="headerlink" title="翻译写作"></a>翻译写作</h2><p><img src="https://img.enderfga.cn/img/image-20220528223919801.png" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20220528223946957.png" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20220528224113309.png" alt=""></p><p>刚开始只用<a href="https://chrome.google.com/webstore/detail/%E6%B2%99%E6%8B%89%E6%9F%A5%E8%AF%8D-%E8%81%9A%E5%90%88%E8%AF%8D%E5%85%B8%E5%88%92%E8%AF%8D%E7%BF%BB%E8%AF%91/cdonnmffkdaoajfknoeeecmchibpmkmg">沙拉查词</a>的，各种功能非常全面；后来用<a href="https://chrome.google.com/webstore/detail/deepl-translate-beta-vers/cofdbpoegempjloogbagkncekinflcnj">deepl</a>边写边译（中译英），方便跟外国友人交流，写完的内容还可以用<a href="https://chrome.google.com/webstore/detail/quillbot-for-chrome/iidnbdjijdkbmajdffnidomddglmieko">quillbot</a>修改润色。虽然以上内容只会显得我英语水平很捞，但我用的蛮开心</p><h2 id="使用体验"><a href="#使用体验" class="headerlink" title="使用体验"></a>使用体验</h2><p><img src="https://img.enderfga.cn/img/image-20220528224252617.png" alt=""></p><p><a href="https://chrome.google.com/webstore/detail/gitzip-for-github/ffabmkklhbepgcgfonabamgnfafbdlkn">gitzip</a>，打包GitHub上指定文件或者文件夹，不用全部clone下来方便很多</p><p><img src="https://img.enderfga.cn/img/image-20220528224450069.png" alt=""></p><p><a href="https://limestart.cn/">青柠起始页</a></p><p>虽然装了扩展但主要还是使用这个网页，我的新标签页主页都是这个，简洁美观，左上角是便签可以倒计时。</p><p><img src="https://img.enderfga.cn/img/image-20220528224715406.png" alt=""></p><p><a href="https://extensions.redeviation.com/">书签侧边栏</a>，如图所示，我那一大堆乱七八糟的书签就是这样打理的</p><p><img src="https://img.enderfga.cn/img/image-20220528224903817.png" alt=""></p><p><a href="https://chrome.google.com/webstore/detail/%E5%93%94%E5%93%A9%E5%93%94%E5%93%A9%E5%8A%A9%E6%89%8B%EF%BC%9Abilibilicom-%E7%BB%BC%E5%90%88%E8%BE%85%E5%8A%A9%E6%89%A9%E5%B1%95/kpbnombpnpcffllnianjibmpadjolanh">哔哩哔哩助手</a>，这就是为什么我的书签里没有b站的原因，我都是从这里跳转的。功能非常非常丰富，包括但不限于下载各个分辨率的视频和弹幕，实现各种各样的默认跳转（自动宽屏，关弹幕，4k···），自动帮我给hanser三连</p><p><img src="https://img.enderfga.cn/img/image-20220528225202730.png" alt=""></p><p>虽然经常说油猴脚本，但我自己用的还是暴力猴，作者是中国人，在哪些网页使用体验不好了我就搜一下</p><p><img src="https://img.enderfga.cn/img/image-20220528225352529.png" alt=""></p><p>往往会有惊喜</p><p>本来想专门写一个栏目介绍了，但我好累</p><p><img src="https://img.enderfga.cn/img/image-20220528225516406.png" alt=""></p><p>大家看名字识功能吧</p><p>比较推荐的有网页限制解除，很多网站不能复制就很烦；秀读图书互转，结合这个上面提到的参考咨询联盟才能轻松下载pdf；</p><p>百度网盘简易下载助手，目前还能用，校园网满速；AC-baidu，对百度谷歌等界面都优化了。（黑是因为我系统设置了暗）</p><p><img src="https://img.enderfga.cn/img/image-20220528225804605.png" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20220528225854431.png" alt=""></p><h1 id="软件-1"><a href="#软件-1" class="headerlink" title="软件"></a>软件</h1><p>没动力写了，摆烂式推荐</p><p><img src="https://img.enderfga.cn/img/image-20220528230140552.png" alt=""></p><p>foxit pdf：功能全面，毕竟是学校帮忙付费了的</p><p><img src="https://img.enderfga.cn/img/image-20220528230232796.png" alt=""></p><p>vscode：啥都能写，加上copilot，一分钟上千行代码不是梦（bushi）</p><p>感觉vscode也可以写一个扩展分享， 但我好懒，感兴趣地可以了解一下图里这个，根据注释自动写代码</p><p><img src="https://img.enderfga.cn/img/image-20220528230544526.png" alt=""></p><p>格式工厂：啥都能给你转转，除了各种格式转换，我还用来简单地剪辑之类的；可以把各大软件的付费格式转换成常见格式</p><p><img src="https://img.enderfga.cn/img/image-20220528230709483.png" alt=""></p><p>anaconda：maybe是python学习必备软件之一（吧）,搭建环境之后我更习惯写jupyter notebook（但用的是vscode）</p><p><img src="https://img.enderfga.cn/img/image-20220528231002743.png" alt=""></p><p>mathpix：上面一张图展示功能，快捷键截图之后就能ocr出来贴typora或者latex，甚至office系列软件</p><p>教育邮箱每月100次</p><p>列举剩下的一些体验良好的软件：</p><p>视频播放我用potplayer；解压缩我用bandizip；视频录制我用bandicam；思维导图我用xmind；</p><p>浏览器当然是chrome，不过edge也很不错；备忘录用microsoft to do；</p><p>备份习惯用google drive，打游戏用一个叫灵缇的小众加速器·····</p><p>暂时到这里吧，如果我又双叒叕心血来潮也许会更新。</p>]]></content>
    
    
    
    <tags>
      
      <tag>闲谈</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>智能机器人技术——感知</title>
    <link href="/2022/05/27/robot7/"/>
    <url>/2022/05/27/robot7/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>智能机器人技术作业记录</p><span id="more"></span><p><img src="https://img.enderfga.cn/img/image-20220527155651383.png" alt=""></p><embed src="./Perception.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    
    <tags>
      
      <tag>机器人</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>可靠数据传输原理</title>
    <link href="/2022/05/17/net5/"/>
    <url>/2022/05/17/net5/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>第三章运输层之可靠数据传输原理</p><p><img src="https://img.enderfga.cn/img/20220517080217.png" alt=""></p><span id="more"></span><embed src="./ARQ.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    
    <tags>
      
      <tag>计算机网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2R机械臂五次多项式轨迹规划</title>
    <link href="/2022/05/16/robot6/"/>
    <url>/2022/05/16/robot6/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="https://img.enderfga.cn/img/20220516000042.png" alt=""></p><p>智能机器人技术作业记录</p><span id="more"></span><embed src="./2R.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    
    <tags>
      
      <tag>机器人</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>C2TCP：一个超低延迟的灵活蜂窝式TCP</title>
    <link href="/2022/05/16/net4/"/>
    <url>/2022/05/16/net4/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>计算机网络前沿论文导读</p><p><img src="https://img.enderfga.cn/img/2432716a34cb4571ac86e2b7f56f617.png" alt=""></p><span id="more"></span><embed src="./C2TCP.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    
    <tags>
      
      <tag>计算机网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>智能机器人技术——机器人运动学奇异分析与性能评价</title>
    <link href="/2022/05/15/robot5/"/>
    <url>/2022/05/15/robot5/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>智能机器人技术作业记录</p><span id="more"></span><h1 id="智能机器人技术——机器人运动学奇异分析与性能评价"><a href="#智能机器人技术——机器人运动学奇异分析与性能评价" class="headerlink" title="智能机器人技术——机器人运动学奇异分析与性能评价"></a>智能机器人技术——机器人运动学奇异分析与性能评价</h1><p>一、给定平面2R机械臂状态参数，</p><p>状态描述：</p><ul><li>关节状态: $\quad\left[\theta_{1}, \theta_{2}\right]^{\mathrm{T}}$</li><li>末端位置: $\quad\left[x_{e}, y_{e}\right]^{\mathrm{T}}$</li></ul><p><img src="https://img.enderfga.cn/img/image-20220508155155580.png" alt=""></p><ol><li><p>计算逆运动学，求解关节角的表达式（已知末端位置$\quad\left[x_{e}, y_{e}\right]^{\mathrm{T}}$，求关节角$\quad\left[\theta_{1}, \theta_{2}\right]^{\mathrm{T}}$）</p><p>根据几何关系, 可推导出机械臂末端位置与机械臂关节变量的关系:</p><script type="math/tex; mode=display">\begin{array}{l}p_{\mathrm{ex}}=l_{1} c_{1}+l_{2} c_{12} \\p_{\mathrm{ey}}=l_{1} s_{1}+l_{2} s_{12}\end{array}</script><p>其中,</p><script type="math/tex; mode=display">\left\{\begin{array}{l}s_{1}=\sin \theta_{1}, c_{1}=\cos \theta_{1} \\s_{12}=\sin \left(\theta_{1}+\theta_{2}\right), c_{12}=\cos \left(\theta_{1}+\theta_{2}\right)\end{array}\right.</script><p>其向量形式为：</p><script type="math/tex; mode=display">p_{\mathrm{e}}=\left[\begin{array}{l}p_{\mathrm{ex}} \\p_{\mathrm{e} y}\end{array}\right]=\left[\begin{array}{l}l_{1} c_{1}+l_{2} c_{12} \\l_{1} s_{1}+l_{2} s_{12}\end{array}\right]=\left[\begin{array}{l}l_{1} \cos \theta_{1}+l_{2} \cos \left(\theta_{1}+\theta_{2}\right) \\l_{1} \sin \theta_{1}+l_{2} \sin \left(\theta_{1}+\theta_{2}\right)\end{array}\right]</script><p>将式子两边的平方相加, 有</p><script type="math/tex; mode=display">p_{\mathrm{e}}^{2}=l_{1}^{2}+l_{2}^{2}+2 l_{1} l_{2}\left(c_{1} c_{12}+s_{1} s_{12}\right)</script><p>$p_{\mathrm{e}}^{2}=p_{\mathrm{ex}}^{2}+p_{\mathrm{ey}}^{2}$ 为基坐标系原点到末端坐标系原点的距离。</p><p>根据三角函数的性质, 有</p><script type="math/tex; mode=display">c_{1} c_{12}+s_{1} s_{12}=c_{2}</script><p>故可化简为</p><script type="math/tex; mode=display">p_{\mathrm{e}}^{2}=l_{1}^{2}+l_{2}^{2}+2 l_{1} l_{2} c_{2}</script><p>因此，</p><script type="math/tex; mode=display">\left\{\begin{array}{l}p_{\mathrm{e}}^{2} \leqslant l_{1}^{2}+l_{2}^{2}+2 l_{1} l_{2}=\left(l_{1}+l_{2}\right)^{2} \\p_{\mathrm{e}}^{2} \geqslant l_{1}^{2}+l_{2}^{2}-2 l_{1} l_{2}=\left(l_{1}-l_{2}\right)^{2}\end{array}\right.\\\left|l_{1}-l_{2}\right| \leqslant p_{e} \leqslant l_{1}+l_{2}</script><p>上式即表示了该 $2 \mathrm{R}$ 机械臂的工作空间范围, 其最小边沿和最大边沿分别对应于 $\theta_{2}=\pi$ 和 $\theta_{2}=0$ 的情况。</p><p>进一步有，</p><script type="math/tex; mode=display">\left|\frac{p_{\mathrm{e}}^{2}-l_{1}^{2}-l_{2}^{2}}{2 l_{1} l_{2}}\right| \leqslant 1</script><p>解得：</p><script type="math/tex; mode=display">\theta_{2}=\pm \arccos \frac{p_{\mathrm{e}}^{2}-l_{1}^{2}-l_{2}^{2}}{2 l_{1} l_{2}}=\pm \arccos \frac{x_{\mathrm{e}}^{2}+y_{\mathrm{e}}^{2}-l_{1}^{2}-l_{2}^{2}}{2 l_{1} l_{2}}</script><p>关节角 $\theta_{2}$ 解出后, 将其代入方程组中, 可进一步解出关节角 $\theta_{1}$ 。首先根据三角函数的性质:</p><script type="math/tex; mode=display">\begin{aligned}&c_{12}=c_{1} c_{2}-s_{1} s_{2} \\&s_{12}=s_{1} c_{2}+c_{1} s_{2}\end{aligned}</script><p>有，</p><script type="math/tex; mode=display">\left\{\begin{array}{l}\left(l_{1}+l_{2} c_{2}\right) c_{1}-l_{2} s_{2} s_{1}=p_{\mathrm{ex}} \\l_{2} s_{2} c_{1}+\left(l_{1}+l_{2} c_{2}\right) s_{1}=p_{\mathrm{e} y}\end{array}\right.</script><p>可写成如下形式:</p><script type="math/tex; mode=display">\left[\begin{array}{cc}l_{1}+l_{2} c_{2} & -l_{2} s_{2} \\l_{2} s_{2} & l_{1}+l_{2} c_{2}\end{array}\right]\left[\begin{array}{l}c_{1} \\s_{1}\end{array}\right]=\left[\begin{array}{l}p_{e x} \\p_{\mathrm{ey}}\end{array}\right]</script><p>方程组的系数矩阵 $\boldsymbol{A}$ 及其行列式分别为</p><script type="math/tex; mode=display">\boldsymbol{A}=\left[\begin{array}{cc}l_{1}+l_{2} c_{2} & -l_{2} s_{2} \\l_{2} s_{2} & l_{1}+l_{2} c_{2}\end{array}\right], \operatorname{det}(\boldsymbol{A})=l_{1}^{2}+l_{2}^{2}+2 l_{1} l_{2} c_{2}</script><p>若 $\operatorname{det}(\boldsymbol{A}) \neq 0$, 则矩阵 $\boldsymbol{A}$ 满秩, 方程组有解, 即:</p><script type="math/tex; mode=display">\left[\begin{array}{l}c_{1} \\s_{1}\end{array}\right]=\left[\begin{array}{cc}l_{1}+l_{2} c_{2} & -l_{2} s_{2} \\l_{2} s_{2} & l_{1}+l_{2} c_{2}\end{array}\right]^{-1}\left[\begin{array}{l}p_{\mathrm{ex}} \\p_{\mathrm{ey}}\end{array}\right]=\frac{1}{l_{1}^{2}+l_{2}^{2}+2 l_{1} l_{2} c_{2}}\left[\begin{array}{c}\left(l_{1}+l_{2} c_{2}\right) p_{\mathrm{ex}}+l_{2} s_{2} p_{\mathrm{ey}} \\-l_{2} s_{2} p_{\mathrm{ex}}+\left(l_{1}+l_{2} c_{2}\right) p_{\mathrm{ey}}\end{array}\right]</script><p>$\theta_{1}$ 可根据解出的 $s_{1}$ 和 $c_{1}$ 求出, 即:</p><script type="math/tex; mode=display">\begin{aligned}\theta_{1} &=\arctan 2\left(s_{1}, c_{1}\right)=\arctan 2\left(\frac{-l_{2} s_{2} p_{\mathrm{ex}}+\left(l_{1}+l_{2} c_{2}\right) p_{\mathrm{ey}}}{l_{1}^{2}+l_{2}^{2}+2 l_{1} l_{2} c_{2}}, \frac{\left(l_{1}+l_{2} c_{2}\right) p_{\mathrm{ex}}+l_{2} s_{2} p_{\mathrm{ey}}}{l_{1}^{2}+l_{2}^{2}+2 l_{1} l_{2} c_{2}}\right) \\&=\arctan 2\left(-l_{2} s_{2} p_{\mathrm{ex}}+\left(l_{1}+l_{2} c_{2}\right) p_{\mathrm{ey}},\left(l_{1}+l_{2} c_{2}\right) p_{\mathrm{ex}}+l_{2} s_{2} p_{\mathrm{ey}}\right)\end{aligned}</script><p>根据基本不等式及三级函数的性质, 令 $\operatorname{det}(\boldsymbol{A})=0$, 则必有 $l_{1}=l_{2}$ 且 $\theta_{2}=\pi$, 即:</p><script type="math/tex; mode=display">\operatorname{det}(\boldsymbol{A})=0 \Rightarrow\left\{\begin{array} { l } { l _ { 1 } ^ { 2 } + l _ { 2 } ^ { 2 } + 2 l _ { 1 } l _ { 2 } c _ { 2 } = 0 } \\{ l _ { 1 } ^ { 2 } + l _ { 2 } ^ { 2 } \geqslant 2 l _ { 1 } l _ { 2 } } \\{ | c _ { 2 } | \leqslant 1 }\end{array} \Rightarrow \left\{\begin{array}{l}l_{1}=l_{2} \\\theta_{2}=\pi\end{array}\right.\right.</script><p>综上所述, 结果可以求出两组解, 对应于机器人的两种臂型, 分别称为高臂 (肘) 和低臂 (肘), 平面 $2 \mathrm{R}$ 机械臂逆运动学多解情况分析下图所示。也就是说, 对于前述的 $2 \mathrm{R}$ 机械臂, 当给定末端点的一个位置 $\boldsymbol{p}_{\mathrm{e}}$ 时, 有两组关节角与之对应, 即位置级逆运动学有多解。</p><p><img src="https://img.enderfga.cn/img/image-20220515125458389.png" alt=""></p></li><li>计算雅克比矩阵</li></ol><p>根据机械臂末端位置与机械臂关节变量的关系求导得：</p><script type="math/tex; mode=display">\left\{\begin{array}{l}\dot{p}_{\mathrm{ex}}=-l_{1} s_{1} \dot{\theta}_{1}-l_{2} s_{12}\left(\dot{\theta}_{1}+\dot{\theta}_{2}\right)=-\left(l_{1} s_{1}+l_{2} s_{12}\right) \dot{\theta}_{1}-l_{2} s_{12} \dot{\theta}_{2} \\\dot{p}_{\mathrm{cy}}=l_{1} c_{1} \dot{\theta}_{1}+l_{2} c_{12}\left(\dot{\theta}_{1}+\dot{\theta}_{2}\right)=\left(l_{1} c_{1}+l_{2} c_{12}\right) \dot{\theta}_{1}+l_{2} c_{12} \dot{\theta}_{2} \\\end{array}\right.</script><p>即:</p><script type="math/tex; mode=display">\left[\begin{array}{l}\dot{p}_{\mathrm{ex}} \\\dot{p}_{\mathrm{ey}}\end{array}\right]=\left[\begin{array}{cc}-l_{1} s_{1}-l_{2} s_{12} & -l_{2} s_{12} \\l_{1} c_{1}+l_{2} c_{12} & l_{2} c_{12}\end{array}\right]\left[\begin{array}{l}\dot{\theta}_{1} \\\dot{\theta}_{2}\end{array}\right]</script><p>其矩阵形式为：</p><script type="math/tex; mode=display">\dot{\boldsymbol{p}}_{\mathrm{e}}=\boldsymbol{J}_{v}(\boldsymbol{\Theta}) \dot{\boldsymbol{\Theta}}</script><p>此时, $\boldsymbol{J}_{v}$ 为 $2 \times 2$ 的方阵:</p><script type="math/tex; mode=display">\boldsymbol{J}_{v}=\left[\begin{array}{cc}-l_{1} s_{1}-l_{2} s_{12} & -l_{2} s_{12} \\l_{1} c_{1}+l_{2} c_{12} & l_{2} c_{12}\end{array}\right]</script><p>二、给定D-H坐标系，填写D-H参数表。</p><p><img src="https://img.enderfga.cn/img/image-20220515131813671.png" alt=""></p><script type="math/tex; mode=display">\begin{array}{ccccc}\hline  \text { 连杆i } & \theta_{i} & \alpha_{i} & a_{i} & d_{i} \\\hline1 & 0 & -90^{\circ} & 0 & d_{1} \\2 & 0 & 0 & a_{2} & 0 \\  3 & 0 & 0 & a_{3} & 0\\  \hline\end{array}</script>]]></content>
    
    
    
    <tags>
      
      <tag>机器人</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PyTorch常用代码段合集</title>
    <link href="/2022/05/10/torch/"/>
    <url>/2022/05/10/torch/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>PyTorch最好的资料是官方文档。本文是PyTorch常用代码段，在参考资料的基础上做了一些修补，方便使用时查阅。</p><p><img src="https://img.enderfga.cn/img/image-20220511003116457.png" alt=""></p><span id="more"></span><h1 id="基本配置"><a href="#基本配置" class="headerlink" title="基本配置"></a><strong>基本配置</strong></h1><h3 id="导入包和版本查询"><a href="#导入包和版本查询" class="headerlink" title="导入包和版本查询"></a><strong>导入包和版本查询</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-built_in">print</span>(torch.__version__)<br><span class="hljs-built_in">print</span>(torch.version.cuda)<br><span class="hljs-built_in">print</span>(torch.backends.cudnn.version())<br><span class="hljs-built_in">print</span>(torch.cuda.get_device_name(<span class="hljs-number">0</span>))<br></code></pre></td></tr></table></figure><h3 id="可复现性"><a href="#可复现性" class="headerlink" title="可复现性"></a><strong>可复现性</strong></h3><p>在硬件设备（CPU、GPU）不同时，完全的可复现性无法保证，即使随机种子相同。但是，在同一个设备上，应该保证可复现性。具体做法是，在程序开始的时候固定torch的随机种子，同时也把numpy的随机种子固定。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">np.random.seed(<span class="hljs-number">0</span>)<br>torch.manual_seed(<span class="hljs-number">0</span>)<br>torch.cuda.manual_seed_all(<span class="hljs-number">0</span>)<br><br>torch.backends.cudnn.deterministic = <span class="hljs-literal">True</span><br>torch.backends.cudnn.benchmark = <span class="hljs-literal">False</span><br></code></pre></td></tr></table></figure><p>显卡设置</p><p>如果只需要一张显卡</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Device configuration</span><br>device = torch.device(<span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span>)<br></code></pre></td></tr></table></figure><p>如果需要指定多张显卡，比如0，1号显卡。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br>os.environ[<span class="hljs-string">&#x27;CUDA_VISIBLE_DEVICES&#x27;</span>] = <span class="hljs-string">&#x27;0,1&#x27;</span><br></code></pre></td></tr></table></figure><p>也可以在命令行运行代码时设置显卡：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">CUDA_VISIBLE_DEVICES=0,1 python train.py<br></code></pre></td></tr></table></figure><p>清除显存</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.cuda.empty_cache()<br></code></pre></td></tr></table></figure><p>也可以使用在命令行重置GPU的指令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">nvidia-smi --gpu-reset -i [gpu_id]<br></code></pre></td></tr></table></figure><h1 id="张量-Tensor-处理"><a href="#张量-Tensor-处理" class="headerlink" title="张量(Tensor)处理"></a><strong>张量(Tensor)处理</strong></h1><h3 id="张量的数据类型"><a href="#张量的数据类型" class="headerlink" title="张量的数据类型"></a><strong>张量的数据类型</strong></h3><p>PyTorch有9种CPU张量类型和9种GPU张量类型。</p><p><img src="https://img.enderfga.cn/img/640" alt=""></p><h3 id="张量基本信息"><a href="#张量基本信息" class="headerlink" title="张量基本信息"></a><strong>张量基本信息</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">tensor = torch.randn(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>)<br><span class="hljs-built_in">print</span>(tensor.<span class="hljs-built_in">type</span>())  <span class="hljs-comment"># 数据类型</span><br><span class="hljs-built_in">print</span>(tensor.size())  <span class="hljs-comment"># 张量的shape，是个元组</span><br><span class="hljs-built_in">print</span>(tensor.dim())   <span class="hljs-comment"># 维度的数量</span><br></code></pre></td></tr></table></figure><h3 id="命名张量"><a href="#命名张量" class="headerlink" title="命名张量"></a><strong>命名张量</strong></h3><p>张量命名是一个非常有用的方法，这样可以方便地使用维度的名字来做索引或其他操作，大大提高了可读性、易用性，防止出错。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 在PyTorch 1.3之前，需要使用注释</span><br><span class="hljs-comment"># Tensor[N, C, H, W]</span><br>images = torch.randn(<span class="hljs-number">32</span>, <span class="hljs-number">3</span>, <span class="hljs-number">56</span>, <span class="hljs-number">56</span>)<br>images.<span class="hljs-built_in">sum</span>(dim=<span class="hljs-number">1</span>)<br>images.select(dim=<span class="hljs-number">1</span>, index=<span class="hljs-number">0</span>)<br><br><span class="hljs-comment"># PyTorch 1.3之后</span><br>NCHW = [‘N’, ‘C’, ‘H’, ‘W’]<br>images = torch.randn(<span class="hljs-number">32</span>, <span class="hljs-number">3</span>, <span class="hljs-number">56</span>, <span class="hljs-number">56</span>, names=NCHW)<br>images.<span class="hljs-built_in">sum</span>(<span class="hljs-string">&#x27;C&#x27;</span>)<br>images.select(<span class="hljs-string">&#x27;C&#x27;</span>, index=<span class="hljs-number">0</span>)<br><span class="hljs-comment"># 也可以这么设置</span><br>tensor = torch.rand(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,names=(<span class="hljs-string">&#x27;C&#x27;</span>, <span class="hljs-string">&#x27;N&#x27;</span>, <span class="hljs-string">&#x27;H&#x27;</span>, <span class="hljs-string">&#x27;W&#x27;</span>))<br><span class="hljs-comment"># 使用align_to可以对维度方便地排序</span><br>tensor = tensor.align_to(<span class="hljs-string">&#x27;N&#x27;</span>, <span class="hljs-string">&#x27;C&#x27;</span>, <span class="hljs-string">&#x27;H&#x27;</span>, <span class="hljs-string">&#x27;W&#x27;</span>)<br></code></pre></td></tr></table></figure><h3 id="数据类型转换"><a href="#数据类型转换" class="headerlink" title="数据类型转换"></a>数据类型转换</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 设置默认类型，pytorch中的FloatTensor远远快于DoubleTensor</span><br>torch.set_default_tensor_type(torch.FloatTensor)<br><br><span class="hljs-comment"># 类型转换</span><br>tensor = tensor.cuda()<br>tensor = tensor.cpu()<br>tensor = tensor.<span class="hljs-built_in">float</span>()<br>tensor = tensor.long()<br></code></pre></td></tr></table></figure><h3 id="torch-Tensor与np-ndarray转换"><a href="#torch-Tensor与np-ndarray转换" class="headerlink" title="torch.Tensor与np.ndarray转换"></a><strong>torch.Tensor与np.ndarray转换</strong></h3><p>除了CharTensor，其他所有CPU上的张量都支持转换为numpy格式然后再转换回来。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">ndarray = tensor.cpu().numpy()<br>tensor = torch.from_numpy(ndarray).<span class="hljs-built_in">float</span>()<br>tensor = torch.from_numpy(ndarray.copy()).<span class="hljs-built_in">float</span>() <span class="hljs-comment"># If ndarray has negative stride.</span><br></code></pre></td></tr></table></figure><h3 id="Torch-tensor与PIL-Image转换"><a href="#Torch-tensor与PIL-Image转换" class="headerlink" title="Torch.tensor与PIL.Image转换"></a><strong>Torch.tensor与PIL.Image转换</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># pytorch中的张量默认采用[N, C, H, W]的顺序，并且数据范围在[0,1]，需要进行转置和规范化</span><br><span class="hljs-comment"># torch.Tensor -&gt; PIL.Image</span><br>image = PIL.Image.fromarray(torch.clamp(tensor*<span class="hljs-number">255</span>, <span class="hljs-built_in">min</span>=<span class="hljs-number">0</span>, <span class="hljs-built_in">max</span>=<span class="hljs-number">255</span>).byte().permute(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">0</span>).cpu().numpy())<br>image = torchvision.transforms.functional.to_pil_image(tensor)  <span class="hljs-comment"># Equivalently way</span><br><br><span class="hljs-comment"># PIL.Image -&gt; torch.Tensor</span><br>path = <span class="hljs-string">r&#x27;./figure.jpg&#x27;</span><br>tensor = torch.from_numpy(np.asarray(PIL.Image.<span class="hljs-built_in">open</span>(path))).permute(<span class="hljs-number">2</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>).<span class="hljs-built_in">float</span>() / <span class="hljs-number">255</span><br>tensor = torchvision.transforms.functional.to_tensor(PIL.Image.<span class="hljs-built_in">open</span>(path)) <span class="hljs-comment">#Equivalently way</span><br></code></pre></td></tr></table></figure><h3 id="np-ndarray与PIL-Image的转换"><a href="#np-ndarray与PIL-Image的转换" class="headerlink" title="np.ndarray与PIL.Image的转换"></a>np.ndarray与PIL.Image的转换</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">image = PIL.Image.fromarray(ndarray.astype(np.uint8))<br>ndarray = np.asarray(PIL.Image.<span class="hljs-built_in">open</span>(path))<br></code></pre></td></tr></table></figure><h3 id="从只包含一个元素的张量中提取值"><a href="#从只包含一个元素的张量中提取值" class="headerlink" title="从只包含一个元素的张量中提取值"></a><strong>从只包含一个元素的张量中提取值</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">value = torch.rand(<span class="hljs-number">1</span>).item()<br></code></pre></td></tr></table></figure><h3 id="张量形变"><a href="#张量形变" class="headerlink" title="张量形变"></a><strong>张量形变</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 在将卷积层输入全连接层的情况下通常需要对张量做形变处理，</span><br><span class="hljs-comment"># 相比torch.view，torch.reshape可以自动处理输入张量不连续的情况。</span><br>tensor = torch.rand(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)<br>shape = (<span class="hljs-number">6</span>, <span class="hljs-number">4</span>)<br>tensor = torch.reshape(tensor, shape)<br></code></pre></td></tr></table></figure><h3 id="打乱顺序"><a href="#打乱顺序" class="headerlink" title="打乱顺序"></a><strong>打乱顺序</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">tensor = tensor[torch.randperm(tensor.size(<span class="hljs-number">0</span>))]  <span class="hljs-comment"># 打乱第一个维度</span><br></code></pre></td></tr></table></figure><h3 id="水平翻转"><a href="#水平翻转" class="headerlink" title="水平翻转"></a><strong>水平翻转</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># pytorch不支持tensor[::-1]这样的负步长操作，水平翻转可以通过张量索引实现</span><br><span class="hljs-comment"># 假设张量的维度为[N, D, H, W].</span><br>tensor = tensor[:,:,:,torch.arange(tensor.size(<span class="hljs-number">3</span>) - <span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>).long()]<br></code></pre></td></tr></table></figure><h3 id="复制张量"><a href="#复制张量" class="headerlink" title="复制张量"></a><strong>复制张量</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Operation                 |  New/Shared memory | Still in computation graph |</span><br>tensor.clone()            <span class="hljs-comment"># |        New         |          Yes               |</span><br>tensor.detach()           <span class="hljs-comment"># |      Shared        |          No                |</span><br>tensor.detach.clone()()   <span class="hljs-comment"># |        New         |          No                | </span><br></code></pre></td></tr></table></figure><h3 id="张量拼接"><a href="#张量拼接" class="headerlink" title="张量拼接"></a><strong>张量拼接</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">注意torch.cat和torch.stack的区别在于torch.cat沿着给定的维度拼接，</span><br><span class="hljs-string">而torch.stack会新增一维。例如当参数是3个10x5的张量，torch.cat的结果是30x5的张量，</span><br><span class="hljs-string">而torch.stack的结果是3x10x5的张量。</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>tensor = torch.cat(list_of_tensors, dim=<span class="hljs-number">0</span>)<br>tensor = torch.stack(list_of_tensors, dim=<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><h3 id="将整数标签转为one-hot编码"><a href="#将整数标签转为one-hot编码" class="headerlink" title="将整数标签转为one-hot编码"></a><strong>将整数标签转为one-hot编码</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># pytorch的标记默认从0开始</span><br>tensor = torch.tensor([<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>])<br>N = tensor.size(<span class="hljs-number">0</span>)<br>num_classes = <span class="hljs-number">4</span><br>one_hot = torch.zeros(N, num_classes).long()<br>one_hot.scatter_(dim=<span class="hljs-number">1</span>, index=torch.unsqueeze(tensor, dim=<span class="hljs-number">1</span>), src=torch.ones(N, num_classes).long())<br></code></pre></td></tr></table></figure><h3 id="得到非零元素"><a href="#得到非零元素" class="headerlink" title="得到非零元素"></a><strong>得到非零元素</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.nonzero(tensor)               <span class="hljs-comment"># index of non-zero elements</span><br>torch.nonzero(tensor==<span class="hljs-number">0</span>)            <span class="hljs-comment"># index of zero elements</span><br>torch.nonzero(tensor).size(<span class="hljs-number">0</span>)       <span class="hljs-comment"># number of non-zero elements</span><br>torch.nonzero(tensor == <span class="hljs-number">0</span>).size(<span class="hljs-number">0</span>)  <span class="hljs-comment"># number of zero elements</span><br></code></pre></td></tr></table></figure><h3 id="判断两个张量相等"><a href="#判断两个张量相等" class="headerlink" title="判断两个张量相等"></a><strong>判断两个张量相等</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.allclose(tensor1, tensor2)  <span class="hljs-comment"># float tensor</span><br>torch.equal(tensor1, tensor2)     <span class="hljs-comment"># int tensor</span><br></code></pre></td></tr></table></figure><h3 id="张量扩展"><a href="#张量扩展" class="headerlink" title="张量扩展"></a><strong>张量扩展</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Expand tensor of shape 64*512 to shape 64*512*7*7.</span><br>tensor = torch.rand(<span class="hljs-number">64</span>,<span class="hljs-number">512</span>)<br>torch.reshape(tensor, (<span class="hljs-number">64</span>, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)).expand(<span class="hljs-number">64</span>, <span class="hljs-number">512</span>, <span class="hljs-number">7</span>, <span class="hljs-number">7</span>)<br></code></pre></td></tr></table></figure><h3 id="矩阵乘法"><a href="#矩阵乘法" class="headerlink" title="矩阵乘法"></a><strong>矩阵乘法</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Matrix multiplcation: (m*n) * (n*p) * -&gt; (m*p).</span><br>result = torch.mm(tensor1, tensor2)<br><br><span class="hljs-comment"># Batch matrix multiplication: (b*m*n) * (b*n*p) -&gt; (b*m*p)</span><br>result = torch.bmm(tensor1, tensor2)<br><br><span class="hljs-comment"># Element-wise multiplication.</span><br>result = tensor1 * tensor2<br></code></pre></td></tr></table></figure><h3 id="计算两组数据之间的两两欧式距离"><a href="#计算两组数据之间的两两欧式距离" class="headerlink" title="计算两组数据之间的两两欧式距离"></a><strong>计算两组数据之间的两两欧式距离</strong></h3><p>利用broadcast机制</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">dist = torch.sqrt(torch.<span class="hljs-built_in">sum</span>((X1[:,<span class="hljs-literal">None</span>,:] - X2) ** <span class="hljs-number">2</span>, dim=<span class="hljs-number">2</span>))<br></code></pre></td></tr></table></figure><h1 id="模型定义和操作"><a href="#模型定义和操作" class="headerlink" title="模型定义和操作"></a><strong>模型定义和操作</strong></h1><h3 id="一个简单两层卷积网络的示例"><a href="#一个简单两层卷积网络的示例" class="headerlink" title="一个简单两层卷积网络的示例"></a>一个简单两层卷积网络的示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># convolutional neural network (2 convolutional layers)</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ConvNet</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, num_classes=<span class="hljs-number">10</span></span>):</span><br>        <span class="hljs-built_in">super</span>(ConvNet, self).__init__()<br>        self.layer1 = nn.Sequential(<br>            nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">16</span>, kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>),<br>            nn.BatchNorm2d(<span class="hljs-number">16</span>),<br>            nn.ReLU(),<br>            nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>))<br>        self.layer2 = nn.Sequential(<br>            nn.Conv2d(<span class="hljs-number">16</span>, <span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>),<br>            nn.BatchNorm2d(<span class="hljs-number">32</span>),<br>            nn.ReLU(),<br>            nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>))<br>        self.fc = nn.Linear(<span class="hljs-number">7</span>*<span class="hljs-number">7</span>*<span class="hljs-number">32</span>, num_classes)<br>  <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><br>        out = self.layer1(x)<br>        out = self.layer2(out)<br>        out = out.reshape(out.size(<span class="hljs-number">0</span>), -<span class="hljs-number">1</span>)<br>        out = self.fc(out)<br>        <span class="hljs-keyword">return</span> out<br><br><br>model = ConvNet(num_classes).to(device)<br></code></pre></td></tr></table></figure><p>卷积层的计算和展示可以用这个网站辅助。</p><p><a href="https://ezyang.github.io/convolution-visualizer/index.html">https://ezyang.github.io/convolution-visualizer/index.html</a></p><h3 id="双线性汇合（bilinear-pooling）"><a href="#双线性汇合（bilinear-pooling）" class="headerlink" title="双线性汇合（bilinear pooling）"></a>双线性汇合（bilinear pooling）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">X = torch.reshape(N, D, H * W)                        <span class="hljs-comment"># Assume X has shape N*D*H*W</span><br>X = torch.bmm(X, torch.transpose(X, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>)) / (H * W)  <span class="hljs-comment"># Bilinear pooling</span><br><span class="hljs-keyword">assert</span> X.size() == (N, D, D)<br>X = torch.reshape(X, (N, D * D))<br>X = torch.sign(X) * torch.sqrt(torch.<span class="hljs-built_in">abs</span>(X) + <span class="hljs-number">1e-5</span>)   <span class="hljs-comment"># Signed-sqrt normalization</span><br>X = torch.nn.functional.normalize(X)                  <span class="hljs-comment"># L2 normalization</span><br></code></pre></td></tr></table></figure><h3 id="多卡同步-BN（Batch-normalization）"><a href="#多卡同步-BN（Batch-normalization）" class="headerlink" title="多卡同步 BN（Batch normalization）"></a><strong>多卡同步 BN（Batch normalization）</strong></h3><p>当使用 torch.nn.DataParallel 将代码运行在多张 GPU 卡上时，PyTorch 的 BN 层默认操作是各卡上数据独立地计算均值和标准差，同步 BN 使用所有卡上的数据一起计算 BN 层的均值和标准差，缓解了当批量大小（batch size）比较小时对均值和标准差估计不准的情况，是在目标检测等任务中一个有效的提升性能的技巧。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">sync_bn = torch.nn.SyncBatchNorm(num_features, eps=<span class="hljs-number">1e-05</span>, momentum=<span class="hljs-number">0.1</span>,   affine=<span class="hljs-literal">True</span>,track_running_stats=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><h3 id="将已有网络的所有BN层改为同步BN层"><a href="#将已有网络的所有BN层改为同步BN层" class="headerlink" title="将已有网络的所有BN层改为同步BN层"></a>将已有网络的所有BN层改为同步BN层</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">convertBNtoSyncBN</span>(<span class="hljs-params">module, process_group=<span class="hljs-literal">None</span></span>):</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;Recursively replace all BN layers to SyncBN layer.</span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        module[torch.nn.Module]. Network</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(module, torch.nn.modules.batchnorm._BatchNorm):<br>        sync_bn = torch.nn.SyncBatchNorm(module.num_features, module.eps, module.momentum, <br>                                         module.affine, module.track_running_stats, process_group)<br>        sync_bn.running_mean = module.running_mean<br>        sync_bn.running_var = module.running_var<br>        <span class="hljs-keyword">if</span> module.affine:<br>            sync_bn.weight = module.weight.clone().detach()<br>            sync_bn.bias = module.bias.clone().detach()<br>        <span class="hljs-keyword">return</span> sync_bn<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">for</span> name, child_module <span class="hljs-keyword">in</span> module.named_children():<br>            <span class="hljs-built_in">setattr</span>(module, name) = convert_syncbn_model(child_module, process_group=process_group))<br>        <span class="hljs-keyword">return</span> module<br></code></pre></td></tr></table></figure><h3 id="类似-BN-滑动平均"><a href="#类似-BN-滑动平均" class="headerlink" title="类似 BN 滑动平均"></a><strong>类似 BN 滑动平均</strong></h3><p>如果要实现类似 BN 滑动平均的操作，在 forward 函数中要使用原地（inplace）操作给滑动平均赋值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">BN</span>(<span class="hljs-params">torch.nn.Module</span>)</span><br><span class="hljs-class">    <span class="hljs-title">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>        ...<br>        self.register_buffer(<span class="hljs-string">&#x27;running_mean&#x27;</span>, torch.zeros(num_features))<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, X</span>):</span><br>        ...<br>        self.running_mean += momentum * (current - self.running_mean)<br></code></pre></td></tr></table></figure><h3 id="计算模型整体参数量"><a href="#计算模型整体参数量" class="headerlink" title="计算模型整体参数量"></a><strong>计算模型整体参数量</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">num_parameters = <span class="hljs-built_in">sum</span>(torch.numel(parameter) <span class="hljs-keyword">for</span> parameter <span class="hljs-keyword">in</span> model.parameters())<br></code></pre></td></tr></table></figure><h3 id="查看网络中的参数"><a href="#查看网络中的参数" class="headerlink" title="查看网络中的参数"></a><strong>查看网络中的参数</strong></h3><p>可以通过model.state_dict()或者model.named_parameters()函数查看现在的全部可训练参数（包括通过继承得到的父类中的参数）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">params = <span class="hljs-built_in">list</span>(model.named_parameters())<br>(name, param) = params[<span class="hljs-number">28</span>]<br><span class="hljs-built_in">print</span>(name)<br><span class="hljs-built_in">print</span>(param.grad)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;-------------------------------------------------&#x27;</span>)<br>(name2, param2) = params[<span class="hljs-number">29</span>]<br><span class="hljs-built_in">print</span>(name2)<br><span class="hljs-built_in">print</span>(param2.grad)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;----------------------------------------------------&#x27;</span>)<br>(name1, param1) = params[<span class="hljs-number">30</span>]<br><span class="hljs-built_in">print</span>(name1)<br><span class="hljs-built_in">print</span>(param1.grad)<br></code></pre></td></tr></table></figure><h3 id="模型可视化（使用pytorchviz）"><a href="#模型可视化（使用pytorchviz）" class="headerlink" title="模型可视化（使用pytorchviz）"></a><strong>模型可视化（使用pytorchviz）</strong></h3><p><a href="https://github.com/szagoruyko/pytorchviz">https://github.com/szagoruyko/pytorchviz</a></p><h3 id="类似-Keras-的-model-summary-输出模型信息（使用pytorch-summary-）"><a href="#类似-Keras-的-model-summary-输出模型信息（使用pytorch-summary-）" class="headerlink" title="类似 Keras 的 model.summary() 输出模型信息（使用pytorch-summary ）"></a><strong>类似 Keras 的 model.summary() 输出模型信息（使用pytorch-summary ）</strong></h3><p><a href="https://github.com/sksq96/pytorch-summary">https://github.com/sksq96/pytorch-summary</a></p><p><strong>模型权重初始化</strong></p><p>注意 model.modules() 和 model.children() 的区别：model.modules() 会迭代地遍历模型的所有子层，而 model.children() 只会遍历模型下的一层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Common practise for initialization.</span><br><span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> model.modules():<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(layer, torch.nn.Conv2d):<br>        torch.nn.init.kaiming_normal_(layer.weight, mode=<span class="hljs-string">&#x27;fan_out&#x27;</span>,<br>                                      nonlinearity=<span class="hljs-string">&#x27;relu&#x27;</span>)<br>        <span class="hljs-keyword">if</span> layer.bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            torch.nn.init.constant_(layer.bias, val=<span class="hljs-number">0.0</span>)<br>    <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(layer, torch.nn.BatchNorm2d):<br>        torch.nn.init.constant_(layer.weight, val=<span class="hljs-number">1.0</span>)<br>        torch.nn.init.constant_(layer.bias, val=<span class="hljs-number">0.0</span>)<br>    <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(layer, torch.nn.Linear):<br>        torch.nn.init.xavier_normal_(layer.weight)<br>        <span class="hljs-keyword">if</span> layer.bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            torch.nn.init.constant_(layer.bias, val=<span class="hljs-number">0.0</span>)<br><br><span class="hljs-comment"># Initialization with given tensor.</span><br>layer.weight = torch.nn.Parameter(tensor)<br></code></pre></td></tr></table></figure><h3 id="提取模型中的某一层"><a href="#提取模型中的某一层" class="headerlink" title="提取模型中的某一层"></a><strong>提取模型中的某一层</strong></h3><p>modules()会返回模型中所有模块的迭代器，它能够访问到最内层，比如self.layer1.conv1这个模块，还有一个与它们相对应的是name_children()属性以及named_modules(),这两个不仅会返回模块的迭代器，还会返回网络层的名字。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 取模型中的前两层</span><br>new_model = nn.Sequential(*<span class="hljs-built_in">list</span>(model.children())[:<span class="hljs-number">2</span>] <br><span class="hljs-comment"># 如果希望提取出模型中的所有卷积层，可以像下面这样操作：</span><br><span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> model.named_modules():<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(layer[<span class="hljs-number">1</span>],nn.Conv2d):<br>         conv_model.add_module(layer[<span class="hljs-number">0</span>],layer[<span class="hljs-number">1</span>])<br></code></pre></td></tr></table></figure><h3 id="部分层使用预训练模型"><a href="#部分层使用预训练模型" class="headerlink" title="部分层使用预训练模型"></a><strong>部分层使用预训练模型</strong></h3><p>注意如果保存的模型是 torch.nn.DataParallel，则当前的模型也需要是</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model.load_state_dict(torch.load(<span class="hljs-string">&#x27;model.pth&#x27;</span>), strict=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure><h3 id="将在-GPU-保存的模型加载到-CPU"><a href="#将在-GPU-保存的模型加载到-CPU" class="headerlink" title="将在 GPU 保存的模型加载到 CPU"></a>将在 GPU 保存的模型加载到 CPU</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model.load_state_dict(torch.load(<span class="hljs-string">&#x27;model.pth&#x27;</span>, map_location=<span class="hljs-string">&#x27;cpu&#x27;</span>))<br></code></pre></td></tr></table></figure><h2 id="导入另一个模型的相同部分到新的模型"><a href="#导入另一个模型的相同部分到新的模型" class="headerlink" title="导入另一个模型的相同部分到新的模型"></a><strong>导入另一个模型的相同部分到新的模型</strong></h2><p>模型导入参数时，如果两个模型结构不一致，则直接导入参数会报错。用下面方法可以把另一个模型的相同的部分导入到新的模型中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># model_new代表新的模型</span><br><span class="hljs-comment"># model_saved代表其他模型，比如用torch.load导入的已保存的模型</span><br>model_new_dict = model_new.state_dict()<br>model_common_dict = &#123;k:v <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> model_saved.items() <span class="hljs-keyword">if</span> k <span class="hljs-keyword">in</span> model_new_dict.keys()&#125;<br>model_new_dict.update(model_common_dict)<br>model_new.load_state_dict(model_new_dict)<br></code></pre></td></tr></table></figure><h1 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a><strong>数据处理</strong></h1><h3 id="计算数据集的均值和标准差"><a href="#计算数据集的均值和标准差" class="headerlink" title="计算数据集的均值和标准差"></a>计算数据集的均值和标准差</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> cv2<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">compute_mean_and_std</span>(<span class="hljs-params">dataset</span>):</span><br>    <span class="hljs-comment"># 输入PyTorch的dataset，输出均值和标准差</span><br>    mean_r = <span class="hljs-number">0</span><br>    mean_g = <span class="hljs-number">0</span><br>    mean_b = <span class="hljs-number">0</span><br><br>    <span class="hljs-keyword">for</span> img, _ <span class="hljs-keyword">in</span> dataset:<br>        img = np.asarray(img) <span class="hljs-comment"># change PIL Image to numpy array</span><br>        mean_r += np.mean(img[:, :, <span class="hljs-number">0</span>])<br>        mean_g += np.mean(img[:, :, <span class="hljs-number">1</span>])<br>        mean_b += np.mean(img[:, :, <span class="hljs-number">2</span>])<br><br>    mean_r /= <span class="hljs-built_in">len</span>(dataset)<br>    mean_g /= <span class="hljs-built_in">len</span>(dataset)<br>    mean_b /= <span class="hljs-built_in">len</span>(dataset)<br><br>    diff_r = <span class="hljs-number">0</span><br>    diff_g = <span class="hljs-number">0</span><br>    diff_b = <span class="hljs-number">0</span><br><br>    N = <span class="hljs-number">0</span><br><br>    <span class="hljs-keyword">for</span> img, _ <span class="hljs-keyword">in</span> dataset:<br>        img = np.asarray(img)<br><br>        diff_r += np.<span class="hljs-built_in">sum</span>(np.power(img[:, :, <span class="hljs-number">0</span>] - mean_r, <span class="hljs-number">2</span>))<br>        diff_g += np.<span class="hljs-built_in">sum</span>(np.power(img[:, :, <span class="hljs-number">1</span>] - mean_g, <span class="hljs-number">2</span>))<br>        diff_b += np.<span class="hljs-built_in">sum</span>(np.power(img[:, :, <span class="hljs-number">2</span>] - mean_b, <span class="hljs-number">2</span>))<br><br>        N += np.prod(img[:, :, <span class="hljs-number">0</span>].shape)<br><br>    std_r = np.sqrt(diff_r / N)<br>    std_g = np.sqrt(diff_g / N)<br>    std_b = np.sqrt(diff_b / N)<br><br>    mean = (mean_r.item() / <span class="hljs-number">255.0</span>, mean_g.item() / <span class="hljs-number">255.0</span>, mean_b.item() / <span class="hljs-number">255.0</span>)<br>    std = (std_r.item() / <span class="hljs-number">255.0</span>, std_g.item() / <span class="hljs-number">255.0</span>, std_b.item() / <span class="hljs-number">255.0</span>)<br>    <span class="hljs-keyword">return</span> mean, std<br></code></pre></td></tr></table></figure><h3 id="得到视频数据基本信息"><a href="#得到视频数据基本信息" class="headerlink" title="得到视频数据基本信息"></a>得到视频数据基本信息</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2<br>video = cv2.VideoCapture(mp4_path)<br>height = <span class="hljs-built_in">int</span>(video.get(cv2.CAP_PROP_FRAME_HEIGHT))<br>width = <span class="hljs-built_in">int</span>(video.get(cv2.CAP_PROP_FRAME_WIDTH))<br>num_frames = <span class="hljs-built_in">int</span>(video.get(cv2.CAP_PROP_FRAME_COUNT))<br>fps = <span class="hljs-built_in">int</span>(video.get(cv2.CAP_PROP_FPS))<br>video.release()<br></code></pre></td></tr></table></figure><h3 id="TSN-每段（segment）采样一帧视频"><a href="#TSN-每段（segment）采样一帧视频" class="headerlink" title="TSN 每段（segment）采样一帧视频"></a>TSN 每段（segment）采样一帧视频</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python">K = self._num_segments<br><span class="hljs-keyword">if</span> is_train:<br>    <span class="hljs-keyword">if</span> num_frames &gt; K:<br>        <span class="hljs-comment"># Random index for each segment.</span><br>        frame_indices = torch.randint(<br>            high=num_frames // K, size=(K,), dtype=torch.long)<br>        frame_indices += num_frames // K * torch.arange(K)<br>    <span class="hljs-keyword">else</span>:<br>        frame_indices = torch.randint(<br>            high=num_frames, size=(K - num_frames,), dtype=torch.long)<br>        frame_indices = torch.sort(torch.cat((<br>            torch.arange(num_frames), frame_indices)))[<span class="hljs-number">0</span>]<br><span class="hljs-keyword">else</span>:<br>    <span class="hljs-keyword">if</span> num_frames &gt; K:<br>        <span class="hljs-comment"># Middle index for each segment.</span><br>        frame_indices = num_frames / K // <span class="hljs-number">2</span><br>        frame_indices += num_frames // K * torch.arange(K)<br>    <span class="hljs-keyword">else</span>:<br>        frame_indices = torch.sort(torch.cat((                            <br>            torch.arange(num_frames), torch.arange(K - num_frames))))[<span class="hljs-number">0</span>]<br><span class="hljs-keyword">assert</span> frame_indices.size() == (K,)<br><span class="hljs-keyword">return</span> [frame_indices[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(K)]<br></code></pre></td></tr></table></figure><h3 id="常用训练和验证数据预处理"><a href="#常用训练和验证数据预处理" class="headerlink" title="常用训练和验证数据预处理"></a><strong>常用训练和验证数据预处理</strong></h3><p>其中 ToTensor 操作会将 PIL.Image 或形状为 H×W×D，数值范围为 [0, 255] 的 np.ndarray 转换为形状为 D×H×W，数值范围为 [0.0, 1.0] 的 torch.Tensor。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python">train_transform = torchvision.transforms.Compose([<br>    torchvision.transforms.RandomResizedCrop(size=<span class="hljs-number">224</span>,<br>                                             scale=(<span class="hljs-number">0.08</span>, <span class="hljs-number">1.0</span>)),<br>    torchvision.transforms.RandomHorizontalFlip(),<br>    torchvision.transforms.ToTensor(),<br>    torchvision.transforms.Normalize(mean=(<span class="hljs-number">0.485</span>, <span class="hljs-number">0.456</span>, <span class="hljs-number">0.406</span>),<br>                                     std=(<span class="hljs-number">0.229</span>, <span class="hljs-number">0.224</span>, <span class="hljs-number">0.225</span>)),<br> ])<br> val_transform = torchvision.transforms.Compose([<br>    torchvision.transforms.Resize(<span class="hljs-number">256</span>),<br>    torchvision.transforms.CenterCrop(<span class="hljs-number">224</span>),<br>    torchvision.transforms.ToTensor(),<br>    torchvision.transforms.Normalize(mean=(<span class="hljs-number">0.485</span>, <span class="hljs-number">0.456</span>, <span class="hljs-number">0.406</span>),<br>                                     std=(<span class="hljs-number">0.229</span>, <span class="hljs-number">0.224</span>, <span class="hljs-number">0.225</span>)),<br>])<br></code></pre></td></tr></table></figure><h1 id="模型训练和测试"><a href="#模型训练和测试" class="headerlink" title="模型训练和测试"></a><strong>模型训练和测试</strong></h1><h3 id="分类模型训练代码"><a href="#分类模型训练代码" class="headerlink" title="分类模型训练代码"></a><strong>分类模型训练代码</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Loss and optimizer</span><br>criterion = nn.CrossEntropyLoss()<br>optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)<br><br><span class="hljs-comment"># Train the model</span><br>total_step = <span class="hljs-built_in">len</span>(train_loader)<br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>    <span class="hljs-keyword">for</span> i ,(images, labels) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_loader):<br>        images = images.to(device)<br>        labels = labels.to(device)<br>      <br>        <span class="hljs-comment"># Forward pass</span><br>        outputs = model(images)<br>        loss = criterion(outputs, labels)<br>      <br>        <span class="hljs-comment"># Backward and optimizer</span><br>        optimizer.zero_grad()<br>        loss.backward()<br>        optimizer.step()<br>      <br>        <span class="hljs-keyword">if</span> (i+<span class="hljs-number">1</span>) % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Epoch: [&#123;&#125;/&#123;&#125;], Step: [&#123;&#125;/&#123;&#125;], Loss: &#123;&#125;&#x27;</span><br>                  .<span class="hljs-built_in">format</span>(epoch+<span class="hljs-number">1</span>, num_epochs, i+<span class="hljs-number">1</span>, total_step, loss.item()))<br></code></pre></td></tr></table></figure><h3 id="分类模型测试代码"><a href="#分类模型测试代码" class="headerlink" title="分类模型测试代码"></a><strong>分类模型测试代码</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Test the model</span><br>model.<span class="hljs-built_in">eval</span>()  <span class="hljs-comment"># eval mode(batch norm uses moving mean/variance </span><br>              <span class="hljs-comment">#instead of mini-batch mean/variance)</span><br><span class="hljs-keyword">with</span> torch.no_grad():<br>    correct = <span class="hljs-number">0</span><br>    total = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> images, labels <span class="hljs-keyword">in</span> test_loader:<br>        images = images.to(device)<br>        labels = labels.to(device)<br>        outputs = model(images)<br>        _, predicted = torch.<span class="hljs-built_in">max</span>(outputs.data, <span class="hljs-number">1</span>)<br>        total += labels.size(<span class="hljs-number">0</span>)<br>        correct += (predicted == labels).<span class="hljs-built_in">sum</span>().item()<br>      <br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Test accuracy of the model on the 10000 test images: &#123;&#125; %&#x27;</span><br>          .<span class="hljs-built_in">format</span>(<span class="hljs-number">100</span> * correct / total))<br></code></pre></td></tr></table></figure><h3 id="自定义loss"><a href="#自定义loss" class="headerlink" title="自定义loss"></a><strong>自定义loss</strong></h3><p>继承torch.nn.Module类写自己的loss。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyLoss</span>(<span class="hljs-params">torch.nn.Moudle</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-built_in">super</span>(MyLoss, self).__init__()<br>      <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x, y</span>):</span><br>        loss = torch.mean((x - y) ** <span class="hljs-number">2</span>)<br>        <span class="hljs-keyword">return</span> loss<br></code></pre></td></tr></table></figure><h3 id="标签平滑（label-smoothing）"><a href="#标签平滑（label-smoothing）" class="headerlink" title="标签平滑（label smoothing）"></a><strong>标签平滑（label smoothing）</strong></h3><p>写一个label_smoothing.py的文件，然后在训练代码里引用，用LSR代替交叉熵损失即可。label_smoothing.py内容如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">LSR</span>(<span class="hljs-params">nn.Module</span>):</span><br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, e=<span class="hljs-number">0.1</span>, reduction=<span class="hljs-string">&#x27;mean&#x27;</span></span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br><br>        self.log_softmax = nn.LogSoftmax(dim=<span class="hljs-number">1</span>)<br>        self.e = e<br>        self.reduction = reduction<br>  <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_one_hot</span>(<span class="hljs-params">self, labels, classes, value=<span class="hljs-number">1</span></span>):</span><br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">            Convert labels to one hot vectors</span><br><span class="hljs-string">      </span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            labels: torch tensor in format [label1, label2, label3, ...]</span><br><span class="hljs-string">            classes: int, number of classes</span><br><span class="hljs-string">            value: label value in one hot vector, default to 1</span><br><span class="hljs-string">      </span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            return one hot format labels in shape [batchsize, classes]</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br><br>        one_hot = torch.zeros(labels.size(<span class="hljs-number">0</span>), classes)<br><br>        <span class="hljs-comment">#labels and value_added  size must match</span><br>        labels = labels.view(labels.size(<span class="hljs-number">0</span>), -<span class="hljs-number">1</span>)<br>        value_added = torch.Tensor(labels.size(<span class="hljs-number">0</span>), <span class="hljs-number">1</span>).fill_(value)<br><br>        value_added = value_added.to(labels.device)<br>        one_hot = one_hot.to(labels.device)<br><br>        one_hot.scatter_add_(<span class="hljs-number">1</span>, labels, value_added)<br><br>        <span class="hljs-keyword">return</span> one_hot<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_smooth_label</span>(<span class="hljs-params">self, target, length, smooth_factor</span>):</span><br>        <span class="hljs-string">&quot;&quot;&quot;convert targets to one-hot format, and smooth</span><br><span class="hljs-string">        them.</span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            target: target in form with [label1, label2, label_batchsize]</span><br><span class="hljs-string">            length: length of one-hot format(number of classes)</span><br><span class="hljs-string">            smooth_factor: smooth factor for label smooth</span><br><span class="hljs-string">      </span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            smoothed labels in one hot format</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        one_hot = self._one_hot(target, length, value=<span class="hljs-number">1</span> - smooth_factor)<br>        one_hot += smooth_factor / (length - <span class="hljs-number">1</span>)<br><br>        <span class="hljs-keyword">return</span> one_hot.to(target.device)<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x, target</span>):</span><br><br>        <span class="hljs-keyword">if</span> x.size(<span class="hljs-number">0</span>) != target.size(<span class="hljs-number">0</span>):<br>            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;Expected input batchsize (&#123;&#125;) to match target batch_size(&#123;&#125;)&#x27;</span><br>                    .<span class="hljs-built_in">format</span>(x.size(<span class="hljs-number">0</span>), target.size(<span class="hljs-number">0</span>)))<br><br>        <span class="hljs-keyword">if</span> x.dim() &lt; <span class="hljs-number">2</span>:<br>            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;Expected input tensor to have least 2 dimensions(got &#123;&#125;)&#x27;</span><br>                    .<span class="hljs-built_in">format</span>(x.size(<span class="hljs-number">0</span>)))<br><br>        <span class="hljs-keyword">if</span> x.dim() != <span class="hljs-number">2</span>:<br>            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;Only 2 dimension tensor are implemented, (got &#123;&#125;)&#x27;</span><br>                    .<span class="hljs-built_in">format</span>(x.size()))<br><br><br>        smoothed_target = self._smooth_label(target, x.size(<span class="hljs-number">1</span>), self.e)<br>        x = self.log_softmax(x)<br>        loss = torch.<span class="hljs-built_in">sum</span>(- x * smoothed_target, dim=<span class="hljs-number">1</span>)<br><br>        <span class="hljs-keyword">if</span> self.reduction == <span class="hljs-string">&#x27;none&#x27;</span>:<br>            <span class="hljs-keyword">return</span> loss<br>      <br>        <span class="hljs-keyword">elif</span> self.reduction == <span class="hljs-string">&#x27;sum&#x27;</span>:<br>            <span class="hljs-keyword">return</span> torch.<span class="hljs-built_in">sum</span>(loss)<br>      <br>        <span class="hljs-keyword">elif</span> self.reduction == <span class="hljs-string">&#x27;mean&#x27;</span>:<br>            <span class="hljs-keyword">return</span> torch.mean(loss)<br>      <br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;unrecognized option, expect reduction to be one of none, mean, sum&#x27;</span>)<br></code></pre></td></tr></table></figure><p>或者直接在训练文件里做label smoothing</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> images, labels <span class="hljs-keyword">in</span> train_loader:<br>    images, labels = images.cuda(), labels.cuda()<br>    N = labels.size(<span class="hljs-number">0</span>)<br>    <span class="hljs-comment"># C is the number of classes.</span><br>    smoothed_labels = torch.full(size=(N, C), fill_value=<span class="hljs-number">0.1</span> / (C - <span class="hljs-number">1</span>)).cuda()<br>    smoothed_labels.scatter_(dim=<span class="hljs-number">1</span>, index=torch.unsqueeze(labels, dim=<span class="hljs-number">1</span>), value=<span class="hljs-number">0.9</span>)<br><br>    score = model(images)<br>    log_prob = torch.nn.functional.log_softmax(score, dim=<span class="hljs-number">1</span>)<br>    loss = -torch.<span class="hljs-built_in">sum</span>(log_prob * smoothed_labels) / N<br>    optimizer.zero_grad()<br>    loss.backward()<br>    optimizer.step()<br></code></pre></td></tr></table></figure><h3 id="Mixup训练"><a href="#Mixup训练" class="headerlink" title="Mixup训练"></a><strong>Mixup训练</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python">beta_distribution = torch.distributions.beta.Beta(alpha, alpha)<br><span class="hljs-keyword">for</span> images, labels <span class="hljs-keyword">in</span> train_loader:<br>    images, labels = images.cuda(), labels.cuda()<br><br>    <span class="hljs-comment"># Mixup images and labels.</span><br>    lambda_ = beta_distribution.sample([]).item()<br>    index = torch.randperm(images.size(<span class="hljs-number">0</span>)).cuda()<br>    mixed_images = lambda_ * images + (<span class="hljs-number">1</span> - lambda_) * images[index, :]<br>    label_a, label_b = labels, labels[index]<br><br>    <span class="hljs-comment"># Mixup loss.</span><br>    scores = model(mixed_images)<br>    loss = (lambda_ * loss_function(scores, label_a)<br>            + (<span class="hljs-number">1</span> - lambda_) * loss_function(scores, label_b))<br>    optimizer.zero_grad()<br>    loss.backward()<br>    optimizer.step()<br></code></pre></td></tr></table></figure><h3 id="L1-正则化"><a href="#L1-正则化" class="headerlink" title="L1 正则化"></a>L1 正则化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">l1_regularization = torch.nn.L1Loss(reduction=<span class="hljs-string">&#x27;sum&#x27;</span>)<br>loss = ...  <span class="hljs-comment"># Standard cross-entropy loss</span><br><span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> model.parameters():<br>    loss += torch.<span class="hljs-built_in">sum</span>(torch.<span class="hljs-built_in">abs</span>(param))<br>loss.backward()<br></code></pre></td></tr></table></figure><h3 id="不对偏置项进行权重衰减（weight-decay）"><a href="#不对偏置项进行权重衰减（weight-decay）" class="headerlink" title="不对偏置项进行权重衰减（weight decay）"></a><strong>不对偏置项进行权重衰减（weight decay）</strong></h3><p>pytorch里的weight decay相当于l2正则</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">bias_list = (param <span class="hljs-keyword">for</span> name, param <span class="hljs-keyword">in</span> model.named_parameters() <span class="hljs-keyword">if</span> name[-<span class="hljs-number">4</span>:] == <span class="hljs-string">&#x27;bias&#x27;</span>)<br>others_list = (param <span class="hljs-keyword">for</span> name, param <span class="hljs-keyword">in</span> model.named_parameters() <span class="hljs-keyword">if</span> name[-<span class="hljs-number">4</span>:] != <span class="hljs-string">&#x27;bias&#x27;</span>)<br>parameters = [&#123;<span class="hljs-string">&#x27;parameters&#x27;</span>: bias_list, <span class="hljs-string">&#x27;weight_decay&#x27;</span>: <span class="hljs-number">0</span>&#125;,              <br>              &#123;<span class="hljs-string">&#x27;parameters&#x27;</span>: others_list&#125;]<br>optimizer = torch.optim.SGD(parameters, lr=<span class="hljs-number">1e-2</span>, momentum=<span class="hljs-number">0.9</span>, weight_decay=<span class="hljs-number">1e-4</span>)<br></code></pre></td></tr></table></figure><h3 id="梯度裁剪（gradient-clipping）"><a href="#梯度裁剪（gradient-clipping）" class="headerlink" title="梯度裁剪（gradient clipping）"></a><strong>梯度裁剪（gradient clipping）</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=<span class="hljs-number">20</span>)<br></code></pre></td></tr></table></figure><h3 id="得到当前学习率"><a href="#得到当前学习率" class="headerlink" title="得到当前学习率"></a><strong>得到当前学习率</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># If there is one global learning rate (which is the common case).</span><br>lr = <span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(optimizer.param_groups))[<span class="hljs-string">&#x27;lr&#x27;</span>]<br><br><span class="hljs-comment"># If there are multiple learning rates for different layers.</span><br>all_lr = []<br><span class="hljs-keyword">for</span> param_group <span class="hljs-keyword">in</span> optimizer.param_groups:<br>    all_lr.append(param_group[<span class="hljs-string">&#x27;lr&#x27;</span>])<br></code></pre></td></tr></table></figure><p>另一种方法，在一个batch训练代码里，当前的lr是optimizer.param_groups[0][‘lr’]</p><h3 id="学习率衰减"><a href="#学习率衰减" class="headerlink" title="学习率衰减"></a>学习率衰减</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Reduce learning rate when validation accuarcy plateau.</span><br>scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=<span class="hljs-string">&#x27;max&#x27;</span>, patience=<span class="hljs-number">5</span>, verbose=<span class="hljs-literal">True</span>)<br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">80</span>):<br>    train(...)<br>    val(...)<br>    scheduler.step(val_acc)<br><br><span class="hljs-comment"># Cosine annealing learning rate.</span><br>scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=<span class="hljs-number">80</span>)<br><span class="hljs-comment"># Reduce learning rate by 10 at given epochs.</span><br>scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[<span class="hljs-number">50</span>, <span class="hljs-number">70</span>], gamma=<span class="hljs-number">0.1</span>)<br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">80</span>):<br>    scheduler.step()  <br>    train(...)<br>    val(...)<br><br><span class="hljs-comment"># Learning rate warmup by 10 epochs.</span><br>scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=<span class="hljs-keyword">lambda</span> t: t / <span class="hljs-number">10</span>)<br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">10</span>):<br>    scheduler.step()<br>    train(...)<br>    val(...)<br></code></pre></td></tr></table></figure><h3 id="优化器链式更新"><a href="#优化器链式更新" class="headerlink" title="优化器链式更新"></a><strong>优化器链式更新</strong></h3><p>从1.4版本开始，torch.optim.lr_scheduler 支持链式更新（chaining），即用户可以定义两个 schedulers，并交替在训练中使用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch.optim <span class="hljs-keyword">import</span> SGD<br><span class="hljs-keyword">from</span> torch.optim.lr_scheduler <span class="hljs-keyword">import</span> ExponentialLR, StepLR<br>model = [torch.nn.Parameter(torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, requires_grad=<span class="hljs-literal">True</span>))]<br>optimizer = SGD(model, <span class="hljs-number">0.1</span>)<br>scheduler1 = ExponentialLR(optimizer, gamma=<span class="hljs-number">0.9</span>)<br>scheduler2 = StepLR(optimizer, step_size=<span class="hljs-number">3</span>, gamma=<span class="hljs-number">0.1</span>)<br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>):<br>    <span class="hljs-built_in">print</span>(epoch, scheduler2.get_last_lr()[<span class="hljs-number">0</span>])<br>    optimizer.step()<br>    scheduler1.step()<br>    scheduler2.step()<br></code></pre></td></tr></table></figure><h3 id="模型训练可视化"><a href="#模型训练可视化" class="headerlink" title="模型训练可视化"></a><strong>模型训练可视化</strong></h3><p>PyTorch可以使用tensorboard来可视化训练过程。</p><p>安装和运行TensorBoard。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">pip install tensorboard<br>tensorboard --logdir=runs<br></code></pre></td></tr></table></figure><p>使用SummaryWriter类来收集和可视化相应的数据，放了方便查看，可以使用不同的文件夹，比如’Loss/train’和’Loss/test’。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>writer = SummaryWriter()<br><br><span class="hljs-keyword">for</span> n_iter <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    writer.add_scalar(<span class="hljs-string">&#x27;Loss/train&#x27;</span>, np.random.random(), n_iter)<br>    writer.add_scalar(<span class="hljs-string">&#x27;Loss/test&#x27;</span>, np.random.random(), n_iter)<br>    writer.add_scalar(<span class="hljs-string">&#x27;Accuracy/train&#x27;</span>, np.random.random(), n_iter)<br>    writer.add_scalar(<span class="hljs-string">&#x27;Accuracy/test&#x27;</span>, np.random.random(), n_iter)<br></code></pre></td></tr></table></figure><h3 id="保存与加载断点"><a href="#保存与加载断点" class="headerlink" title="保存与加载断点"></a><strong>保存与加载断点</strong></h3><p>注意为了能够恢复训练，我们需要同时保存模型和优化器的状态，以及当前的训练轮数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python">start_epoch = <span class="hljs-number">0</span><br><span class="hljs-comment"># Load checkpoint.</span><br><span class="hljs-keyword">if</span> resume: <span class="hljs-comment"># resume为参数，第一次训练时设为0，中断再训练时设为1</span><br>    model_path = os.path.join(<span class="hljs-string">&#x27;model&#x27;</span>, <span class="hljs-string">&#x27;best_checkpoint.pth.tar&#x27;</span>)<br>    <span class="hljs-keyword">assert</span> os.path.isfile(model_path)<br>    checkpoint = torch.load(model_path)<br>    best_acc = checkpoint[<span class="hljs-string">&#x27;best_acc&#x27;</span>]<br>    start_epoch = checkpoint[<span class="hljs-string">&#x27;epoch&#x27;</span>]<br>    model.load_state_dict(checkpoint[<span class="hljs-string">&#x27;model&#x27;</span>])<br>    optimizer.load_state_dict(checkpoint[<span class="hljs-string">&#x27;optimizer&#x27;</span>])<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Load checkpoint at epoch &#123;&#125;.&#x27;</span>.<span class="hljs-built_in">format</span>(start_epoch))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Best accuracy so far &#123;&#125;.&#x27;</span>.<span class="hljs-built_in">format</span>(best_acc))<br><br><span class="hljs-comment"># Train the model</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(start_epoch, num_epochs): <br>    ... <br><br>    <span class="hljs-comment"># Test the model</span><br>    ...<br>      <br>    <span class="hljs-comment"># save checkpoint</span><br>    is_best = current_acc &gt; best_acc<br>    best_acc = <span class="hljs-built_in">max</span>(current_acc, best_acc)<br>    checkpoint = &#123;<br>        <span class="hljs-string">&#x27;best_acc&#x27;</span>: best_acc,<br>        <span class="hljs-string">&#x27;epoch&#x27;</span>: epoch + <span class="hljs-number">1</span>,<br>        <span class="hljs-string">&#x27;model&#x27;</span>: model.state_dict(),<br>        <span class="hljs-string">&#x27;optimizer&#x27;</span>: optimizer.state_dict(),<br>    &#125;<br>    model_path = os.path.join(<span class="hljs-string">&#x27;model&#x27;</span>, <span class="hljs-string">&#x27;checkpoint.pth.tar&#x27;</span>)<br>    best_model_path = os.path.join(<span class="hljs-string">&#x27;model&#x27;</span>, <span class="hljs-string">&#x27;best_checkpoint.pth.tar&#x27;</span>)<br>    torch.save(checkpoint, model_path)<br>    <span class="hljs-keyword">if</span> is_best:<br>        shutil.copy(model_path, best_model_path)<br></code></pre></td></tr></table></figure><h3 id="提取-ImageNet-预训练模型某层的卷积特征"><a href="#提取-ImageNet-预训练模型某层的卷积特征" class="headerlink" title="提取 ImageNet 预训练模型某层的卷积特征"></a>提取 ImageNet 预训练模型某层的卷积特征</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># VGG-16 relu5-3 feature.</span><br>model = torchvision.models.vgg16(pretrained=<span class="hljs-literal">True</span>).features[:-<span class="hljs-number">1</span>]<br><span class="hljs-comment"># VGG-16 pool5 feature.</span><br>model = torchvision.models.vgg16(pretrained=<span class="hljs-literal">True</span>).features<br><span class="hljs-comment"># VGG-16 fc7 feature.</span><br>model = torchvision.models.vgg16(pretrained=<span class="hljs-literal">True</span>)<br>model.classifier = torch.nn.Sequential(*<span class="hljs-built_in">list</span>(model.classifier.children())[:-<span class="hljs-number">3</span>])<br><span class="hljs-comment"># ResNet GAP feature.</span><br>model = torchvision.models.resnet18(pretrained=<span class="hljs-literal">True</span>)<br>model = torch.nn.Sequential(collections.OrderedDict(<br>    <span class="hljs-built_in">list</span>(model.named_children())[:-<span class="hljs-number">1</span>]))<br><br><span class="hljs-keyword">with</span> torch.no_grad():<br>    model.<span class="hljs-built_in">eval</span>()<br>    conv_representation = model(image)<br></code></pre></td></tr></table></figure><h3 id="提取-ImageNet-预训练模型多层的卷积特征"><a href="#提取-ImageNet-预训练模型多层的卷积特征" class="headerlink" title="提取 ImageNet 预训练模型多层的卷积特征"></a>提取 ImageNet 预训练模型多层的卷积特征</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">FeatureExtractor</span>(<span class="hljs-params">torch.nn.Module</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;Helper class to extract several convolution features from the given</span><br><span class="hljs-string">    pre-trained model.</span><br><span class="hljs-string">    Attributes:</span><br><span class="hljs-string">        _model, torch.nn.Module.</span><br><span class="hljs-string">        _layers_to_extract, list&lt;str&gt; or set&lt;str&gt;</span><br><span class="hljs-string">    Example:</span><br><span class="hljs-string">        &gt;&gt;&gt; model = torchvision.models.resnet152(pretrained=True)</span><br><span class="hljs-string">        &gt;&gt;&gt; model = torch.nn.Sequential(collections.OrderedDict(</span><br><span class="hljs-string">                list(model.named_children())[:-1]))</span><br><span class="hljs-string">        &gt;&gt;&gt; conv_representation = FeatureExtractor(</span><br><span class="hljs-string">                pretrained_model=model,</span><br><span class="hljs-string">                layers_to_extract=&#123;&#x27;layer1&#x27;, &#x27;layer2&#x27;, &#x27;layer3&#x27;, &#x27;layer4&#x27;&#125;)(image)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, pretrained_model, layers_to_extract</span>):</span><br>        torch.nn.Module.__init__(self)<br>        self._model = pretrained_model<br>        self._model.<span class="hljs-built_in">eval</span>()<br>        self._layers_to_extract = <span class="hljs-built_in">set</span>(layers_to_extract)<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><br>        <span class="hljs-keyword">with</span> torch.no_grad():<br>            conv_representation = []<br>            <span class="hljs-keyword">for</span> name, layer <span class="hljs-keyword">in</span> self._model.named_children():<br>                x = layer(x)<br>                <span class="hljs-keyword">if</span> name <span class="hljs-keyword">in</span> self._layers_to_extract:<br>                    conv_representation.append(x)<br>            <span class="hljs-keyword">return</span> conv_representation<br></code></pre></td></tr></table></figure><h3 id="微调全连接层"><a href="#微调全连接层" class="headerlink" title="微调全连接层"></a>微调全连接层</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">model = torchvision.models.resnet18(pretrained=<span class="hljs-literal">True</span>)<br><span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> model.parameters():<br>    param.requires_grad = <span class="hljs-literal">False</span><br>model.fc = nn.Linear(<span class="hljs-number">512</span>, <span class="hljs-number">100</span>)  <span class="hljs-comment"># Replace the last fc layer</span><br>optimizer = torch.optim.SGD(model.fc.parameters(), lr=<span class="hljs-number">1e-2</span>, momentum=<span class="hljs-number">0.9</span>, weight_decay=<span class="hljs-number">1e-4</span>)<br></code></pre></td></tr></table></figure><h3 id="以较大学习率微调全连接层，较小学习率微调卷积层"><a href="#以较大学习率微调全连接层，较小学习率微调卷积层" class="headerlink" title="以较大学习率微调全连接层，较小学习率微调卷积层"></a>以较大学习率微调全连接层，较小学习率微调卷积层</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">model = torchvision.models.resnet18(pretrained=<span class="hljs-literal">True</span>)<br>finetuned_parameters = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">map</span>(<span class="hljs-built_in">id</span>, model.fc.parameters()))<br>conv_parameters = (p <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> model.parameters() <span class="hljs-keyword">if</span> <span class="hljs-built_in">id</span>(p) <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> finetuned_parameters)<br>parameters = [&#123;<span class="hljs-string">&#x27;params&#x27;</span>: conv_parameters, <span class="hljs-string">&#x27;lr&#x27;</span>: <span class="hljs-number">1e-3</span>&#125;, <br>              &#123;<span class="hljs-string">&#x27;params&#x27;</span>: model.fc.parameters()&#125;]<br>optimizer = torch.optim.SGD(parameters, lr=<span class="hljs-number">1e-2</span>, momentum=<span class="hljs-number">0.9</span>, weight_decay=<span class="hljs-number">1e-4</span>)<br></code></pre></td></tr></table></figure><h1 id="其他注意事项"><a href="#其他注意事项" class="headerlink" title="其他注意事项"></a><strong>其他注意事项</strong></h1><ul><li>不要使用太大的线性层。因为nn.Linear(m,n)使用的是O(mn)的内存，线性层太大很容易超出现有显存。</li><li>不要在太长的序列上使用RNN。因为RNN反向传播使用的是BPTT算法，其需要的内存和输入序列的长度呈线性关系。</li><li>model(x) 前用 model.train() 和 model.eval() 切换网络状态。</li><li>不需要计算梯度的代码块用 with torch.no_grad() 包含起来。</li><li>model.eval() 和 torch.no_grad() 的区别在于，model.eval() 是将网络切换为测试状态，例如 BN 和dropout在训练和测试阶段使用不同的计算方法。torch.no_grad() 是关闭 PyTorch 张量的自动求导机制，以减少存储使用和加速计算，得到的结果无法进行 loss.backward()。</li><li>model.zero_grad()会把整个模型的参数的梯度都归零, 而optimizer.zero_grad()只会把传入其中的参数的梯度归零.</li><li>torch.nn.CrossEntropyLoss 的输入不需要经过 Softmax。torch.nn.CrossEntropyLoss 等价于 torch.nn.functional.log_softmax + torch.nn.NLLLoss。</li><li>loss.backward() 前用 optimizer.zero_grad() 清除累积梯度。</li><li>torch.utils.data.DataLoader 中尽量设置 pin_memory=True，对特别小的数据集如 MNIST 设置 pin_memory=False 反而更快一些。num_workers 的设置需要在实验中找到最快的取值。</li><li>用 del 及时删除不用的中间变量，节约 GPU 存储。</li><li>使用 inplace 操作可节约 GPU 存储，如</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.nn.functional.relu(x, inplace=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><ul><li>减少 CPU 和 GPU 之间的数据传输。例如如果你想知道一个 epoch 中每个 mini-batch 的 loss 和准确率，先将它们累积在 GPU 中等一个 epoch 结束之后一起传输回 CPU 会比每个 mini-batch 都进行一次 GPU 到 CPU 的传输更快。</li><li>使用半精度浮点数 half() 会有一定的速度提升，具体效率依赖于 GPU 型号。需要小心数值精度过低带来的稳定性问题。</li><li>时常使用 assert tensor.size() == (N, D, H, W) 作为调试手段，确保张量维度和你设想中一致。</li><li>除了标记 y 外，尽量少使用一维张量，使用 n*1 的二维张量代替，可以避免一些意想不到的一维张量计算结果。</li><li>统计代码各部分耗时</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> torch.autograd.profiler.profile(enabled=<span class="hljs-literal">True</span>, use_cuda=<span class="hljs-literal">False</span>) <span class="hljs-keyword">as</span> profile:<br>    ...<br><span class="hljs-built_in">print</span>(profile)<br><br><span class="hljs-comment"># 或者在命令行运行</span><br>python -m torch.utils.bottleneck main.py<br></code></pre></td></tr></table></figure><ul><li>使用TorchSnooper来调试PyTorch代码，程序在执行的时候，就会自动 print 出来每一行的执行结果的 tensor 的形状、数据类型、设备、是否需要梯度的信息。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># pip install torchsnooper</span><br><span class="hljs-keyword">import</span> torchsnooper<br><br><span class="hljs-comment"># 对于函数，使用修饰器</span><br><span class="hljs-meta">@torchsnooper.snoop()</span><br><br><span class="hljs-comment"># 如果不是函数，使用 with 语句来激活 TorchSnooper，把训练的那个循环装进 with 语句中去。</span><br><span class="hljs-keyword">with</span> torchsnooper.snoop():<br></code></pre></td></tr></table></figure><ul><li>原本的代码</li></ul><p><a href="https://github.com/zasdfgbnm/TorchSnooper">https://github.com/zasdfgbnm/TorchSnooper</a></p><ul><li>模型可解释性，使用captum库</li></ul><p><a href="https://captum.ai/">https://captum.ai/</a></p><p><strong>参考资料：</strong></p><p>1.<a href="https://zhuanlan.zhihu.com/p/59205847">https://zhuanlan.zhihu.com/p/59205847</a></p><p>2.<a href="https://pytorch.org/tutorials/">PyTorch官方文档和示例</a></p><p>3.<a href="https://pytorch.org/docs/stable/notes/faq.html">https://pytorch.org/docs/stable/notes/faq.html</a></p><p>4.<a href="https://github.com/szagoruyko/pytorchviz">https://github.com/szagoruyko/pytorchviz</a></p><p>5.<a href="https://github.com/sksq96/pytorch-summary">https://github.com/sksq96/pytorch-summary</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PID 控制在医学麻醉过程血压控制中的应用</title>
    <link href="/2022/05/04/pid/"/>
    <url>/2022/05/04/pid/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>自动控制原理大作业——pid控制器</p><span id="more"></span><p><img src="https://img.enderfga.cn/img/image-20220504214948390.png" alt=""></p><embed src="./PID.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    
    <tags>
      
      <tag>自动控制原理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>网络抓包与协议分析</title>
    <link href="/2022/05/01/net3/"/>
    <url>/2022/05/01/net3/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>计网作业：网络抓包与协议分析实验</p><span id="more"></span><p><img src="https://img.enderfga.cn/img/wall.png" alt=""></p><embed src="./problem.pdf" width="100%" height="750" type="application/pdf"><embed src="./wireshark.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    
    <tags>
      
      <tag>计算机网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数据库原理 Exercises 3&amp;4&amp;5</title>
    <link href="/2022/04/29/data3/"/>
    <url>/2022/04/29/data3/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Database System Concepts Exercises of Chapter 5&amp;6&amp;8</p><span id="more"></span><p><img src="https://img.enderfga.cn/img/image-20220429003143556.png" alt=""></p><h1 id="Database-System-Concepts-Exercises-of-Chapter-5-amp-6"><a href="#Database-System-Concepts-Exercises-of-Chapter-5-amp-6" class="headerlink" title="Database System Concepts Exercises of Chapter 5&amp;6"></a>Database System Concepts Exercises of Chapter 5&amp;6</h1><p><strong>Exercise 5.8</strong> Consider the bank database of Figure <strong>5.25</strong>. Write an sQL trigger to carryout the following action: On <strong>delete</strong> of an account, for each owner of theaccount, check if the owner has any remaining accounts, and if she doesnot, delete her from the <em>depositor</em> relation.</p><p>branch(branch_name, branch_city, assets)</p><p>customer ( customer_name, customer_street, customer_city )</p><p>loan( loan_number, branch_name, amount)</p><p>borrower ( customer_name, loan_number )</p><p>account ( account_number, branch_name, balance )</p><p>depositor ( customer_name, account_number )</p><p><strong>Figure 5.25</strong></p><p><strong>My answer:</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">create</span> <span class="hljs-keyword">trigger</span> <span class="hljs-keyword">check</span><span class="hljs-operator">-</span><span class="hljs-keyword">delete</span><span class="hljs-operator">-</span><span class="hljs-keyword">trigger</span> after <span class="hljs-keyword">delete</span> <span class="hljs-keyword">on</span> account <br><span class="hljs-keyword">referencing</span> <span class="hljs-keyword">old</span> <span class="hljs-type">row</span> <span class="hljs-keyword">as</span> orow<br><span class="hljs-keyword">for</span> <span class="hljs-keyword">each</span> <span class="hljs-type">row</span><br><span class="hljs-keyword">delete</span> <span class="hljs-keyword">from</span> depositor<br><span class="hljs-keyword">where</span> depositor.customer_name <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span><br>   ( <span class="hljs-keyword">select</span> customer_name <span class="hljs-keyword">from</span> depositor <br>    <span class="hljs-keyword">where</span> account_number <span class="hljs-operator">&lt;&gt;</span> orow.account_number ) <br><span class="hljs-keyword">end</span><br></code></pre></td></tr></table></figure><p><strong>Exercise</strong> <strong>5.15</strong> Consider an employee database with two relations<br>employee ($\underline{employee_name}$, street, city)<br>works ($\underline{employee_name}$, company_name, salary)<br>where the primary keys are underlined. Write a query to find companies whose employees earn a higher salary, on average, than the average salary at “First Bank Corporation”.<br>a. Using SQL functions as appropriate.<br>b. Without using SQL functions.</p><p><strong>My answer:</strong></p><p>a)</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">create</span> <span class="hljs-keyword">function</span> avg_salary(cname <span class="hljs-type">varchar</span>(<span class="hljs-number">15</span>))<br><span class="hljs-keyword">return</span> <span class="hljs-type">integer</span><br>     <span class="hljs-keyword">declare</span> <span class="hljs-keyword">result</span> <span class="hljs-type">integer</span>;<br><span class="hljs-keyword">select</span> <span class="hljs-built_in">avg</span>(salary) <span class="hljs-keyword">into</span> <span class="hljs-keyword">result</span><br><span class="hljs-keyword">from</span> works<br><span class="hljs-keyword">where</span> works.company.name <span class="hljs-operator">=</span> cname<br><span class="hljs-keyword">return</span> <span class="hljs-keyword">result</span>;<br><span class="hljs-keyword">end</span><br><span class="hljs-keyword">select</span> company_name<br><span class="hljs-keyword">from</span> works<br><span class="hljs-keyword">where</span> avg_salary(company_name) <span class="hljs-operator">&gt;</span> avg_salary(“<span class="hljs-keyword">First</span> Bank Corporation”)<br></code></pre></td></tr></table></figure><p>b)</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> company_name<br><span class="hljs-keyword">from</span> works<br><span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> company_name<br><span class="hljs-keyword">having</span> <span class="hljs-built_in">avg</span>(salary) <span class="hljs-operator">&gt;</span> (<span class="hljs-keyword">select</span> <span class="hljs-built_in">avg</span>(salary)<br><span class="hljs-keyword">from</span> works<br><span class="hljs-keyword">where</span> company_name<span class="hljs-operator">=</span>”<span class="hljs-keyword">First</span> Bank Corporation”)<br></code></pre></td></tr></table></figure><hr><p><strong>Exercise 6.1</strong> Write the following queries in relational algebra, using the university schema.<br> <strong>a</strong>. Find the titles of courses in the Comp. Sci. department that have 3 credits.<br> <strong>b</strong>. Find the IDs of all students who were taught by an instructor named Einstein; make sure there are no duplicates in the result.<br> <strong>c</strong>. Find the highest salary of any instructor.<br> <strong>d</strong>. Find all instructors earning the highest salary (there may be more than one with the same salary).</p><p><strong>My answer:</strong></p><p><img src="https://img.enderfga.cn/img/image-20220429003507835.png" alt=""></p><embed src="./data.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    
    <tags>
      
      <tag>数据库</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>conda/linux/git/正则表达式常用命令“笔记”</title>
    <link href="/2022/04/28/code/"/>
    <url>/2022/04/28/code/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>一些常用的命令，每次忘了都得搜，记录一下</p><p><img src="https://img.enderfga.cn/img/image-20220428094832340.png" alt=""></p><span id="more"></span><h1 id="Anaconda-amp-python"><a href="#Anaconda-amp-python" class="headerlink" title="Anaconda&amp;python"></a>Anaconda&amp;python</h1><p><img src="https://img.enderfga.cn/img/image-20220428093957780.png" alt=""></p><p><strong>pip安装（tensorflow-gpu为例）</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">pip  install tensorflow-gpu<br></code></pre></td></tr></table></figure><p><strong>conda安装</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">conda  install tensorflow-gpu<br></code></pre></td></tr></table></figure><p><strong>pip3安装（指定版本号只需在命令末尾添加==1.12.0版本号）</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">pip3  install tensorflow-gpu==1.12.0<br></code></pre></td></tr></table></figure><p><strong>使用清华镜像下载</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">pip install tensorflow-gpu==1.10 -i https://pypi.tuna.tsinghua.edu.cn/simple<br></code></pre></td></tr></table></figure><p><strong>指定目录安装</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">pip install -t D:\ProgramData\Anaconda3\Lib\site-packages torch-1.0.1-cp36-cp36m-win_amd64.whl<br></code></pre></td></tr></table></figure><p><strong>卸载安装（pip\pip3只需将conda换成pip\pip3）</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">conda  uninstall tensorflow-gpu<br></code></pre></td></tr></table></figure><p><strong>创建虚拟环境（conda为例）</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">conda create -n py36 python=3.6  #py36虚拟环境的名字  python=3.6  python版本<br></code></pre></td></tr></table></figure><p><strong>删除虚拟环境</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">conda remove -n py36 --all<br></code></pre></td></tr></table></figure><p><strong>激活虚拟环境</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">conda activate py36<br></code></pre></td></tr></table></figure><p><strong>退出虚拟环境</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">conda deactivate<br></code></pre></td></tr></table></figure><p><strong>查看所有创建的虚拟环境</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">conda env list<br></code></pre></td></tr></table></figure><p><strong>用virtualenv创建虚拟环境</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">VENV_DIR=venv<br>pip install virtualenv<br>virtualenv <span class="hljs-variable">$VENV_DIR</span><br><span class="hljs-built_in">source</span> <span class="hljs-variable">$VENV_DIR</span>/bin/activate<br>deactivate<br></code></pre></td></tr></table></figure><p><strong>nohup送入后台运行</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">nohup python train.py &gt;nohup 2&gt;&amp;1 &amp;      #train.py运行的文件  nohup生成的日志文件<br></code></pre></td></tr></table></figure><p><strong>CUDA指定GPU</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">CUDA_VISIBLE_DEVICES=0 nohup python train.py  &gt; nohup.log 2&gt;&amp;1 &amp;<br></code></pre></td></tr></table></figure><p>导出requirements.txt</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">python3 -m pip freeze &gt; requirements.txt<br></code></pre></td></tr></table></figure><p><strong>查看GPU使用情况</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">nvidia-smi<br></code></pre></td></tr></table></figure><p><strong>查看进程号</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ps aux<br></code></pre></td></tr></table></figure><p><strong>根据进程号杀死进程</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">kill -9 进程号<br></code></pre></td></tr></table></figure><p><img src="https://img.enderfga.cn/img/200301122312741.jpg" alt=""></p><h1 id="linux"><a href="#linux" class="headerlink" title="linux"></a>linux</h1><p>linux不像Windows 分了盘，它根目录下有如下常用文件夹:</p><p><em>home</em>         —————        用户的家</p><p><em>root</em>            —————        超级管理员root的家</p><p><em>etc</em>              —————        存放配置文件</p><p><em>usr</em>              —————        存放共享资源</p><h2 id="1、cd命令"><a href="#1、cd命令" class="headerlink" title="1、cd命令:"></a>1、cd命令:</h2><p><strong>①、进入某一个目录</strong> <code>cd 目录名</code></p><p><strong>②、进入多级目录</strong>  <code>cd 目录名/目录名</code></p><p><strong>③、返回上一级目录</strong> <code>cd ..</code></p><p><strong>④、返回根目录</strong> <code>cd /</code></p><p><strong>⑤、返回根目录下的某一个目录</strong> <code>cd /目录名</code></p><p><strong>⑥、回家</strong> <code>cd ~</code></p><h2 id="2、创建、删除目录"><a href="#2、创建、删除目录" class="headerlink" title="2、创建、删除目录:"></a>2、创建、删除目录:</h2><p><strong>①、创建目录</strong> <code>mkdir 目录名</code></p><p><strong>②、创建多级目录</strong> <code>mkdir -p a/b/c</code></p><p><strong>③、删除目录(只能删除空目录)</strong> <code>rmdir 目录名</code></p><p><strong>④、删除目录(可删除非空目录，带询问)</strong> <code>rm -r</code></p><p><strong>⑤、删除目录(不带询问，谨慎使用)</strong> <code>rm -rf</code></p><h2 id="3、对文件的操作"><a href="#3、对文件的操作" class="headerlink" title="3、对文件的操作:"></a>3、对文件的操作:</h2><p><strong>①、创建空白文件</strong> <code>touch 文件名</code></p><p><strong>②、复制文件</strong></p><p> <code>cp a.txt b.txt</code> <em>表示复制a文件并重命名为b。</em></p><p><code>cp a.txt dir/b.txt</code> <em>表示把a复制到dir文件夹下并重命名为b。</em></p><p><strong>③、移动文件</strong> <code>mv a.txt dir/b.txt</code> <em>把a.txt移动到dir目录下并重命名为b.txt。</em></p><p><strong>④、重命名文件</strong> <code>mv a.txt b.txt</code> <em>把a.txt重命名为b.txt。</em></p><p><strong>⑤、删除文件</strong></p><p><code>rm 文件名</code> <em>带询问的删除</em></p><p><code>rm -f 文件名</code> <em>不带询问的删除。</em></p><p><strong>⑥、浏览文件</strong></p><p> <code>cat 文件名</code> <em>显示文件所有内容</em></p><p><code>more 文件名</code> <em>分页显示，空格键下一页，回车键下一行。</em></p><p><code>less 文件名</code> <em>分页显示，pgup上一页，pgdn下一页。</em></p><p><code>tail -5 a.txt</code> <em>显示a.txt文件的最后5行。</em></p><p><code>tail -f 文件名</code> <em>动态的查看。</em></p><h2 id="4、查看目录下的文件"><a href="#4、查看目录下的文件" class="headerlink" title="4、查看目录下的文件:"></a>4、查看目录下的文件:</h2><p><strong>①、查看所有文件和目录名称</strong> <code>ls</code></p><p><strong>②、查看所有文件和目录名称(包括隐藏的)</strong> <code>ls -a</code></p><p><strong>③、查看文件并显示详细信息(最常用)</strong> <code>ll</code></p><p><strong>④、友好的显示</strong> <code>ll -h</code> <em>比如显示的文件大小是kb而不是字节。</em></p><h2 id="5、tar打包命令"><a href="#5、tar打包命令" class="headerlink" title="5、tar打包命令:"></a>5、tar打包命令:</h2><p><strong>①、将当前目录所有文件打包成haha.tar</strong> <code>tar -cvf haha.tar ./*</code></p><p><strong>②、将当前目录下所有文件打包并压缩成haha.tar</strong> <code>tar -zcvf haha.tar.gz ./*</code></p><p><strong>③、将haha.tar解压到当前目录</strong> <code>tar -xvf haha.tar</code></p><p><strong>④、将haha.tar解压到b目录</strong> <code>tar -xvf haha.tar -C b</code> <em>注意C是大写的！</em></p><h2 id="6、其他常用命令"><a href="#6、其他常用命令" class="headerlink" title="6、其他常用命令:"></a>6、其他常用命令:</h2><p><strong>①、grep命令</strong></p><p><code>grep category a.txt</code> <em>表示在a.txt中查找category字符串所在的行，前提是打开了a.txt文件。</em></p><p><code>grep category a.txt -A2</code> <em>在a.txt中查找category字符串的前两行。</em></p><p><code>grep category a.txt -B2</code> <em>在a.txt中查找category字符串的后两行。</em></p><p><strong>②、查看当前目录</strong> <code>pwd</code></p><p><strong>③、wget下载命令</strong> <code>wget www.baidu.com</code> <em>下载百度首页</em></p><h2 id="7、vi-vim编辑器"><a href="#7、vi-vim编辑器" class="headerlink" title="7、vi/vim编辑器:"></a>7、vi/vim编辑器:</h2><p><strong>①、编辑器有三种模式，分别是:</strong> <strong>命令行模式:</strong> 此模式无法编辑文件，<code>yy</code>复制行，<code>p</code>粘贴，<code>dd</code>删除行，按如下键都可以进入插入模式:</p><p><code>i</code>       当前位置前插入;</p><p><code>I</code>        当前行行首插入;</p><p><code>a</code>       当前位置后插入;</p><p><code>A</code>      当前行行尾插入;</p><p><code>o</code>       当前行之后插入一行;</p><p> <code>O</code>      当前的之前插入一行</p><p><strong>插入模式:</strong>此模式下可以对文件进行编辑。按 <code>esc</code>退出插入模式，回到命令行模式。 <strong>底行模式:</strong>命令行模式下按 <code>:</code>，即可进入底行模式。底行模式有如下常用命令:</p><p> <code>q</code>        不保存退出;</p><p> <code>q！</code>    不保存强制退出;</p><p><code>wq</code>     保存退出</p><h2 id="8、管道"><a href="#8、管道" class="headerlink" title="8、管道:"></a>8、管道:</h2><p><strong>管道:<code>|</code>，将一个命令的输出作为另一个命令的输入。例如:</strong> <strong>在 <code>ip addr</code>的输出结果中查找 <code>192.168</code>字符串:</strong> <code>ip addr | grep 192.168</code></p><h2 id="9、系统管理命令"><a href="#9、系统管理命令" class="headerlink" title="9、系统管理命令:"></a>9、系统管理命令:</h2><p><strong>①、查看系统时间</strong> <code>date</code>  查看系统时间 <code>date -s &quot;2018-05-15 22:22:22&quot;</code>将系统时间设置为引号里面的时间</p><p><strong>②、查看磁盘信息</strong> <code>df</code> 查看磁盘信息 <code>df -h</code>  友好地展示磁盘信息</p><p><strong>③、清屏</strong> <code>clear</code>或者按 <code>ctr L</code></p><p><strong>④、进程</strong> <code>ps -ef</code>查看所有进程 <code>ps -ef | grep ssh</code>查找ssh进程</p><p><strong>⑤、杀掉进程</strong> <code>kill 9527</code>杀掉9527号进程 <code>kill -9 9527</code> 强制杀掉9527号进程</p><p><strong>⑥、查看网络端口</strong> <code>netstat -an | grep 3306</code>查看3306端口占用情况</p><p><strong>⑦、ping命令</strong> <code>ping xx.xx.xxx</code>测试网络连通性</p><p><img src="https://img.enderfga.cn/img/bg2015120901.png" alt=""></p><h1 id="GIT"><a href="#GIT" class="headerlink" title="GIT"></a>GIT</h1><p>下面是常用 Git 命令清单。几个专用名词的译名如下。</p><blockquote><ul><li>Workspace：工作区</li><li>Index / Stage：暂存区</li><li>Repository：仓库区（或本地仓库）</li><li>Remote：远程仓库</li></ul></blockquote><h2 id="一、新建代码库"><a href="#一、新建代码库" class="headerlink" title="一、新建代码库"></a>一、新建代码库</h2><blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 在当前目录新建一个Git代码库</span><br>$ git init<br><br><span class="hljs-comment"># 新建一个目录，将其初始化为Git代码库</span><br>$ git init [project-name]<br><br><span class="hljs-comment"># 下载一个项目和它的整个代码历史</span><br>$ git <span class="hljs-built_in">clone</span> [url]<br></code></pre></td></tr></table></figure></blockquote><h2 id="二、配置"><a href="#二、配置" class="headerlink" title="二、配置"></a>二、配置</h2><p>Git的设置文件为 <code>.gitconfig</code>，它可以在用户主目录下（全局配置），也可以在项目目录下（项目配置）。</p><blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 显示当前的Git配置</span><br>$ git config --list<br><br><span class="hljs-comment"># 编辑Git配置文件</span><br>$ git config -e [--global]<br><br><span class="hljs-comment"># 设置提交代码时的用户信息</span><br>$ git config [--global] user.name <span class="hljs-string">&quot;[name]&quot;</span><br>$ git config [--global] user.email <span class="hljs-string">&quot;[email address]&quot;</span><br></code></pre></td></tr></table></figure></blockquote><h2 id="三、增加-删除文件"><a href="#三、增加-删除文件" class="headerlink" title="三、增加/删除文件"></a>三、增加/删除文件</h2><blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 添加指定文件到暂存区</span><br>$ git add [file1] [file2] ...<br><br><span class="hljs-comment"># 添加指定目录到暂存区，包括子目录</span><br>$ git add [dir]<br><br><span class="hljs-comment"># 添加当前目录的所有文件到暂存区</span><br>$ git add .<br><br><span class="hljs-comment"># 添加每个变化前，都会要求确认</span><br><span class="hljs-comment"># 对于同一个文件的多处变化，可以实现分次提交</span><br>$ git add -p<br><br><span class="hljs-comment"># 删除工作区文件，并且将这次删除放入暂存区</span><br>$ git rm [file1] [file2] ...<br><br><span class="hljs-comment"># 停止追踪指定文件，但该文件会保留在工作区</span><br>$ git rm --cached [file]<br><br><span class="hljs-comment"># 改名文件，并且将这个改名放入暂存区</span><br>$ git mv [file-original] [file-renamed]<br></code></pre></td></tr></table></figure></blockquote><h2 id="四、代码提交"><a href="#四、代码提交" class="headerlink" title="四、代码提交"></a>四、代码提交</h2><blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 提交暂存区到仓库区</span><br>$ git commit -m [message]<br><br><span class="hljs-comment"># 提交暂存区的指定文件到仓库区</span><br>$ git commit [file1] [file2] ... -m [message]<br><br><span class="hljs-comment"># 提交工作区自上次commit之后的变化，直接到仓库区</span><br>$ git commit -a<br><br><span class="hljs-comment"># 提交时显示所有diff信息</span><br>$ git commit -v<br><br><span class="hljs-comment"># 使用一次新的commit，替代上一次提交</span><br><span class="hljs-comment"># 如果代码没有任何新变化，则用来改写上一次commit的提交信息</span><br>$ git commit --amend -m [message]<br><br><span class="hljs-comment"># 重做上一次commit，并包括指定文件的新变化</span><br>$ git commit --amend [file1] [file2] ...<br></code></pre></td></tr></table></figure></blockquote><h2 id="五、分支"><a href="#五、分支" class="headerlink" title="五、分支"></a>五、分支</h2><blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 列出所有本地分支</span><br>$ git branch<br><br><span class="hljs-comment"># 列出所有远程分支</span><br>$ git branch -r<br><br><span class="hljs-comment"># 列出所有本地分支和远程分支</span><br>$ git branch -a<br><br><span class="hljs-comment"># 新建一个分支，但依然停留在当前分支</span><br>$ git branch [branch-name]<br><br><span class="hljs-comment"># 新建一个分支，并切换到该分支</span><br>$ git checkout -b [branch]<br><br><span class="hljs-comment"># 新建一个分支，指向指定commit</span><br>$ git branch [branch] [commit]<br><br><span class="hljs-comment"># 新建一个分支，与指定的远程分支建立追踪关系</span><br>$ git branch --track [branch] [remote-branch]<br><br><span class="hljs-comment"># 切换到指定分支，并更新工作区</span><br>$ git checkout [branch-name]<br><br><span class="hljs-comment"># 切换到上一个分支</span><br>$ git checkout -<br><br><span class="hljs-comment"># 建立追踪关系，在现有分支与指定的远程分支之间</span><br>$ git branch --set-upstream [branch] [remote-branch]<br><br><span class="hljs-comment"># 合并指定分支到当前分支</span><br>$ git merge [branch]<br><br><span class="hljs-comment"># 选择一个commit，合并进当前分支</span><br>$ git cherry-pick [commit]<br><br><span class="hljs-comment"># 删除分支</span><br>$ git branch -d [branch-name]<br><br><span class="hljs-comment"># 删除远程分支</span><br>$ git push origin --delete [branch-name]<br>$ git branch -dr [remote/branch]<br></code></pre></td></tr></table></figure></blockquote><h2 id="六、标签"><a href="#六、标签" class="headerlink" title="六、标签"></a>六、标签</h2><blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 列出所有tag</span><br>$ git tag<br><br><span class="hljs-comment"># 新建一个tag在当前commit</span><br>$ git tag [tag]<br><br><span class="hljs-comment"># 新建一个tag在指定commit</span><br>$ git tag [tag] [commit]<br><br><span class="hljs-comment"># 删除本地tag</span><br>$ git tag -d [tag]<br><br><span class="hljs-comment"># 删除远程tag</span><br>$ git push origin :refs/tags/[tagName]<br><br><span class="hljs-comment"># 查看tag信息</span><br>$ git show [tag]<br><br><span class="hljs-comment"># 提交指定tag</span><br>$ git push [remote] [tag]<br><br><span class="hljs-comment"># 提交所有tag</span><br>$ git push [remote] --tags<br><br><span class="hljs-comment"># 新建一个分支，指向某个tag</span><br>$ git checkout -b [branch] [tag]<br></code></pre></td></tr></table></figure></blockquote><h2 id="七、查看信息"><a href="#七、查看信息" class="headerlink" title="七、查看信息"></a>七、查看信息</h2><blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 显示有变更的文件</span><br>$ git status<br><br><span class="hljs-comment"># 显示当前分支的版本历史</span><br>$ git <span class="hljs-built_in">log</span><br><br><span class="hljs-comment"># 显示commit历史，以及每次commit发生变更的文件</span><br>$ git <span class="hljs-built_in">log</span> --<span class="hljs-built_in">stat</span><br><br><span class="hljs-comment"># 搜索提交历史，根据关键词</span><br>$ git <span class="hljs-built_in">log</span> -S [keyword]<br><br><span class="hljs-comment"># 显示某个commit之后的所有变动，每个commit占据一行</span><br>$ git <span class="hljs-built_in">log</span> [tag] HEAD --pretty=format:%s<br><br><span class="hljs-comment"># 显示某个commit之后的所有变动，其&quot;提交说明&quot;必须符合搜索条件</span><br>$ git <span class="hljs-built_in">log</span> [tag] HEAD --grep feature<br><br><span class="hljs-comment"># 显示某个文件的版本历史，包括文件改名</span><br>$ git <span class="hljs-built_in">log</span> --follow [file]<br>$ git whatchanged [file]<br><br><span class="hljs-comment"># 显示指定文件相关的每一次diff</span><br>$ git <span class="hljs-built_in">log</span> -p [file]<br><br><span class="hljs-comment"># 显示过去5次提交</span><br>$ git <span class="hljs-built_in">log</span> -5 --pretty --oneline<br><br><span class="hljs-comment"># 显示所有提交过的用户，按提交次数排序</span><br>$ git shortlog -sn<br><br><span class="hljs-comment"># 显示指定文件是什么人在什么时间修改过</span><br>$ git blame [file]<br><br><span class="hljs-comment"># 显示暂存区和工作区的差异</span><br>$ git diff<br><br><span class="hljs-comment"># 显示暂存区和上一个commit的差异</span><br>$ git diff --cached [file]<br><br><span class="hljs-comment"># 显示工作区与当前分支最新commit之间的差异</span><br>$ git diff HEAD<br><br><span class="hljs-comment"># 显示两次提交之间的差异</span><br>$ git diff [first-branch]...[second-branch]<br><br><span class="hljs-comment"># 显示今天你写了多少行代码</span><br>$ git diff --shortstat <span class="hljs-string">&quot;@&#123;0 day ago&#125;&quot;</span><br><br><span class="hljs-comment"># 显示某次提交的元数据和内容变化</span><br>$ git show [commit]<br><br><span class="hljs-comment"># 显示某次提交发生变化的文件</span><br>$ git show --name-only [commit]<br><br><span class="hljs-comment"># 显示某次提交时，某个文件的内容</span><br>$ git show [commit]:[filename]<br><br><span class="hljs-comment"># 显示当前分支的最近几次提交</span><br>$ git reflog<br></code></pre></td></tr></table></figure></blockquote><h2 id="八、远程同步"><a href="#八、远程同步" class="headerlink" title="八、远程同步"></a>八、远程同步</h2><blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 下载远程仓库的所有变动</span><br>$ git fetch [remote]<br><br><span class="hljs-comment"># 显示所有远程仓库</span><br>$ git remote -v<br><br><span class="hljs-comment"># 显示某个远程仓库的信息</span><br>$ git remote show [remote]<br><br><span class="hljs-comment"># 增加一个新的远程仓库，并命名</span><br>$ git remote add [shortname] [url]<br><br><span class="hljs-comment"># 取回远程仓库的变化，并与本地分支合并</span><br>$ git pull [remote] [branch]<br><br><span class="hljs-comment"># 上传本地指定分支到远程仓库</span><br>$ git push [remote] [branch]<br><br><span class="hljs-comment"># 强行推送当前分支到远程仓库，即使有冲突</span><br>$ git push [remote] --force<br><br><span class="hljs-comment"># 推送所有分支到远程仓库</span><br>$ git push [remote] --all<br></code></pre></td></tr></table></figure></blockquote><h2 id="九、撤销"><a href="#九、撤销" class="headerlink" title="九、撤销"></a>九、撤销</h2><blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 恢复暂存区的指定文件到工作区</span><br>$ git checkout [file]<br><br><span class="hljs-comment"># 恢复某个commit的指定文件到暂存区和工作区</span><br>$ git checkout [commit] [file]<br><br><span class="hljs-comment"># 恢复暂存区的所有文件到工作区</span><br>$ git checkout .<br><br><span class="hljs-comment"># 重置暂存区的指定文件，与上一次commit保持一致，但工作区不变</span><br>$ git reset [file]<br><br><span class="hljs-comment"># 重置暂存区与工作区，与上一次commit保持一致</span><br>$ git reset --hard<br><br><span class="hljs-comment"># 重置当前分支的指针为指定commit，同时重置暂存区，但工作区不变</span><br>$ git reset [commit]<br><br><span class="hljs-comment"># 重置当前分支的HEAD为指定commit，同时重置暂存区和工作区，与指定commit一致</span><br>$ git reset --hard [commit]<br><br><span class="hljs-comment"># 重置当前HEAD为指定commit，但保持暂存区和工作区不变</span><br>$ git reset --keep [commit]<br><br><span class="hljs-comment"># 新建一个commit，用来撤销指定commit</span><br><span class="hljs-comment"># 后者的所有变化都将被前者抵消，并且应用到当前分支</span><br>$ git revert [commit]<br><br><span class="hljs-comment"># 暂时将未提交的变化移除，稍后再移入</span><br>$ git stash<br>$ git stash pop<br></code></pre></td></tr></table></figure></blockquote><h2 id="十、其他"><a href="#十、其他" class="headerlink" title="十、其他"></a>十、其他</h2><blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 生成一个可供发布的压缩包</span><br>$ git archive<br></code></pre></td></tr></table></figure></blockquote><p>一个常用的实例</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">git remote add origin xxx(复制的SSH链接)<br>git branch -m master main<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">git add . <br>git commit -m <span class="hljs-string">&quot;注释&quot;</span> <br>git pull --rebase origin main<br>git push origin main<br></code></pre></td></tr></table></figure><h1 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h1><embed src="./code.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    
    <tags>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>自动驾驶技术基础之建模与控制</title>
    <link href="/2022/04/25/auto1/"/>
    <url>/2022/04/25/auto1/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>自动驾驶技术基础的建模与控制作业</p><span id="more"></span><p>概念题（18分）</p><ol><li>自动驾驶为了出色地完成驾驶任务，可分为哪四大模块？（4分）</li><li>系统建模一般分哪两种建模方式？（2分）</li><li>请写出高速转向车辆模型的简化横向误差模型（即四个状态为误差）（4分）</li><li>二次型性能指标函数一般包含哪三项优化项？（3分）</li><li>线性二次问题三种重要形式分别是？（3分）</li><li>Kalman Filter（LQE）如何通过LQR求得，请写出matlab关键代码，即：xxx=lqr(xxx) （2分）</li></ol><p>编程实践题（12分）</p><p><img src="https://img.enderfga.cn/img/clip_image002.png" alt=""></p><p>给定一个双质系统:  m~1~ =2, m~2~=1, 弹簧系数 k=5, 阻尼σ=0.1, 质量块与地面的滑动阻尼 δ=0.1 (与速度有成正比)。初始时刻 m~1~ 质量块处于 x=0 的位置, 两质量块距离为 0 。现在 m~2~ 处作用一外力 F 拖动系统使 m~1~ 与 m~2~ 质量块均处于 x=5 的位置。</p><ol><li>对系统建模（系统可以直接测量两个物体的位置）</li><li>判断系统可控性与可观</li><li>结合给定的simulink和脚本文件，设计实现上述系统的LQG控制器并绘制闭环控制性能曲线</li></ol><p><img src="https://img.enderfga.cn/img/clip_image020.jpg" alt=""></p><embed src="./auto1.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    
    <tags>
      
      <tag>自动驾驶</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Cognitive Mapping and Planning for Visual Navigation</title>
    <link href="/2022/04/24/CMP/"/>
    <url>/2022/04/24/CMP/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>认知科学基础课程设计报告</p><span id="more"></span><p>开放性题目，结合视语言、语音识别、机器人平台，采用Webots\ROS等不同开源仿真环境，构建一个单/多智能体的认知导航、认知规划、认知控制仿真算例，里面可以用已有的各种传感器组件、机器人模型，基于前期所讲简单的认知智能知识点做仿真试验，算法可以从github上开源下载使用。鼓励大家选择此题目开展一些小试验，可以提问实现的方式方法。同时提交仿真实现的设计和试验研习报告。突出认知智能应用的关键要点，进行详细阐述说明。</p><embed src="./CMP.pdf" width="100%" height="750" type="application/pdf"><p><img src="https://img.enderfga.cn/img/image-20220424225934056.png" alt=""></p><ol><li>论文地址： <a href="https://arxiv.org/abs/1702.03920">https://arxiv.org/abs/1702.03920</a></li><li>代码： <a href="https://github.com/tensorflow/models/tree/archive/research/cognitive_mapping_and_planning">https://github.com/tensorflow/models/tree/archive/research/cognitive_mapping_and_planning</a></li><li>论文Slide： <a href="https://sites.google.com/view/cognitive-mapping-and-planning/?authuser=1">https://sites.google.com/view/cognitive-mapping-and-planning/?authuser=1</a></li><li>作者主页： <a href="https://people.eecs.berkeley.edu/~sgupta/">https://people.eecs.berkeley.edu/~sgupta/</a></li><li>作者主页： <a href="https://people.eecs.berkeley.edu/~svlevine/">https://people.eecs.berkeley.edu/~svlevine/</a></li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>无人系统</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于套接字的网络程序设计</title>
    <link href="/2022/04/19/net2/"/>
    <url>/2022/04/19/net2/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>计网作业：套接字编程实验</p><span id="more"></span><p><img src="https://img.enderfga.cn/img/image-20220419191104235.png" alt=""></p><h1 id="套接字编程作业1：Web服务器"><a href="#套接字编程作业1：Web服务器" class="headerlink" title="套接字编程作业1：Web服务器"></a>套接字编程作业1：Web服务器</h1><p>在本实验中，您将学习Python中TCP连接的套接字编程的基础知识：如何创建套接字，将其绑定到特定的地址和端口，以及发送和接收HTTP数据包。您还将学习一些HTTP首部格式的基础知识。</p><p>您将开发一个处理一个HTTP请求的Web服务器。您的Web服务器应该接受并解析HTTP请求，然后从服务器的文件系统获取所请求的文件，创建一个由响应文件组成的HTTP响应消息，前面是首部行，然后将响应直接发送给客户端。如果请求的文件不存在于服务器中，则服务器应该向客户端发送“404 Not Found”差错报文。</p><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p>在文件下面你会找到Web服务器的代码框架。您需要填写这个代码。而且需要在标有#Fill in start 和 # Fill in end的地方填写代码。另外，每个地方都可能需要不止一行代码。</p><h3 id="运行服务器"><a href="#运行服务器" class="headerlink" title="运行服务器"></a>运行服务器</h3><p>将HTML文件（例如HelloWorld.html）放在服务器所在的目录中。运行服务器程序。确认运行服务器的主机的IP地址（例如128.238.251.26）。从另一个主机，打开浏览器并提供相应的URL。例如：</p><p><a href="http://128.238.251.26:6789/HelloWorld.html">http://128.238.251.26:6789/HelloWorld.html</a></p><p>“HelloWorld.html”是您放在服务器目录中的文件。还要注意使用冒号后的端口号。您需要使用服务器代码中使用的端口号来替换此端口号。在上面的例子中，我们使用了端口号6789. 浏览器应该显示HelloWorld.html的内容。如果省略“:6789”，浏览器将使用默认端口80，只有当您的服务器正在端口80监听时，才会从服务器获取网页。</p><p>然后用客户端尝试获取服务器上不存在的文件。你应该会得到一个“404 Not Found”消息。</p><h3 id="需要上交的内容"><a href="#需要上交的内容" class="headerlink" title="需要上交的内容"></a>需要上交的内容</h3><p>您需要上交完整的服务器代码，以及客户端浏览器的屏幕截图，用于验证您是否从服务器实际接收到HTML文件内容。</p><h3 id="Web服务器的Python代码框架"><a href="#Web服务器的Python代码框架" class="headerlink" title="Web服务器的Python代码框架"></a>Web服务器的Python代码框架</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#import socket module</span><br><span class="hljs-keyword">from</span> socket <span class="hljs-keyword">import</span> *<br>serverSocket = socket(AF_INET, SOCK_STREAM) <br><span class="hljs-comment">#Prepare a sever socket </span><br><span class="hljs-comment">#Fill in start </span><br><span class="hljs-comment">#Fill in end </span><br><span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:   <br>    <span class="hljs-comment">#Establish the connection  </span><br>    <span class="hljs-built_in">print</span> <span class="hljs-string">&#x27;Ready to serve...&#x27;</span>   <br>    connectionSocket, addr =   <span class="hljs-comment">#Fill in start  #Fill in end</span><br>    <span class="hljs-keyword">try</span>:     <br>        message =   <span class="hljs-comment">#Fill in start  #Fill in end</span><br>        filename = message.split()[<span class="hljs-number">1</span>]                      <br>        f = <span class="hljs-built_in">open</span>(filename[<span class="hljs-number">1</span>:])<br>        outputdata = <span class="hljs-comment">#Fill in start  #Fill in end</span><br>        <span class="hljs-comment">#Send one HTTP header line into socket     </span><br>        <span class="hljs-comment">#Fill in start     </span><br>        <span class="hljs-comment">#Fill in end  </span><br><br>        <span class="hljs-comment">#Send the content of the requested file to the client</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(outputdata)):<br>            connectionSocket.send(outputdata[i])<br>        connectionSocket.close()<br>    <span class="hljs-keyword">except</span> IOError:<br>        <span class="hljs-comment">#Send response message for file not found</span><br>        <span class="hljs-comment">#Fill in start</span><br>        <span class="hljs-comment">#Fill in end</span><br><br>        <span class="hljs-comment">#Close client socket</span><br>        <span class="hljs-comment">#Fill in start</span><br>        <span class="hljs-comment">#Fill in end         </span><br>    serverSocket.close()<br></code></pre></td></tr></table></figure><h3 id="可选练习"><a href="#可选练习" class="headerlink" title="可选练习"></a>可选练习</h3><ol><li>目前，这个Web服务器一次只处理一个HTTP请求。请实现一个能够同时处理多个请求的多线程服务器。使用线程，首先创建一个主线程，在固定端口监听客户端请求。当从客户端收到TCP连接请求时，它将通过另一个端口建立TCP连接，并在另外的单独线程中为客户端请求提供服务。这样在每个请求/响应对的独立线程中将有一个独立的TCP连接。</li><li>不使用浏览器，编写自己的HTTP客户端来测试你的服务器。您的客户端将使用一个TCP连接用于连接到服务器，向服务器发送HTTP请求，并将服务器响应显示出来。您可以假定发送的HTTP请求将使用GET方法。<br>客户端应使用命令行参数指定服务器IP地址或主机名，服务器正在监听的端口，以及被请求对象在服务器上的路径。</li></ol><h1 id="套接字编程作业2：UDPping程序"><a href="#套接字编程作业2：UDPping程序" class="headerlink" title="套接字编程作业2：UDPping程序"></a>套接字编程作业2：UDPping程序</h1><p>在本实验中，您将学习使用Python进行UDP套接字编程的基础知识。您将学习如何使用UDP套接字发送和接收数据报，以及如何设置适当的套接字超时。在实验中，您将熟悉Ping应用程序及其在计算统计信息（如丢包率）中的作用。</p><p>您首先需要研究一个用Python编写的简单的ping服务器程序，并实现对应的客户端程序。这些程序提供的功能类似于现代操作系统中可用的标准ping程序功能。然而，我们的程序使用更简单的UDP协议，而不是标准互联网控制消息协议（ICMP）来进行通信。 ping协议允许客户端机器发送一个数据包到远程机器，并使远程机器将数据包返回到客户（称为回显）的操作。另外，ping协议允许主机计算它到其他机器的往返时间。</p><p>以下是Ping服务器程序的完整代码。你的任务是写出Ping客户端程序。</p><h3 id="服务器代码"><a href="#服务器代码" class="headerlink" title="服务器代码"></a>服务器代码</h3><p>以下代码完整实现了一个ping服务器。您需要在运行客户端程序之前编译并运行此代码。<em>而且您不需要修改此代码。</em></p><p>在这个服务器代码中，30％的客户端的数据包会被模拟丢失。你应该仔细研究这个代码，它将帮助你编写ping客户端。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># UDPPingerServer.py </span><br><span class="hljs-comment"># We will need the following module to generate randomized lost packets import random </span><br><span class="hljs-keyword">from</span> socket <span class="hljs-keyword">import</span> * <br><span class="hljs-keyword">import</span> random<br><br><span class="hljs-comment"># Create a UDP socket  </span><br><span class="hljs-comment"># Notice the use of SOCK_DGRAM for UDP packets </span><br>serverSocket = socket(AF_INET, SOCK_DGRAM) <br><span class="hljs-comment"># Assign IP address and port number to socket </span><br>serverSocket.bind((<span class="hljs-string">&#x27;&#x27;</span>, <span class="hljs-number">12000</span>)) <br><br><span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:   <br><span class="hljs-comment"># Generate random number in the range of 0 to 10 </span><br>rand = random.randint(<span class="hljs-number">0</span>, <span class="hljs-number">10</span>)   <br><span class="hljs-comment"># Receive the client packet along with the address it is coming from  </span><br>message, address = serverSocket.recvfrom(<span class="hljs-number">1024</span>) <br><span class="hljs-comment"># Capitalize the message from the client   </span><br>message = message.upper() <br><span class="hljs-comment"># If rand is less is than 4, we consider the packet lost and do not respond   </span><br><span class="hljs-keyword">if</span> rand &lt; <span class="hljs-number">4</span>:     <br><span class="hljs-keyword">continue</span>   <br><span class="hljs-comment"># Otherwise, the server responds     </span><br>serverSocket.sendto(message, address) <br></code></pre></td></tr></table></figure><p>服务器程序在一个无限循环中监听到来的UDP数据包。当数据包到达时，如果生成一个随机整数大于或等于4，则服务器将数字转为大写并将其发送回客户端。</p><h3 id="数据包丢失"><a href="#数据包丢失" class="headerlink" title="数据包丢失"></a>数据包丢失</h3><p>UDP为应用程序提供了不可靠的传输服务。消息可能因为路由器队列溢出，硬件错误或其他原因，而在网络中丢失。但由于在内网中很丢包甚至不丢包，所以在本实验室的服务器程序添加人为损失来模拟网络丢包的影响。服务器创建一个随机整数，由它确定传入的数据包是否丢失。</p><h3 id="客户端代码"><a href="#客户端代码" class="headerlink" title="客户端代码"></a>客户端代码</h3><p>您需要实现以下客户端程序。</p><p>客户端向服务器发送10次ping。因为UDP是不可靠的协议，所以从客户端发送到服务器的数据包可能在网络中丢失。因此，客户端不能无限期地等待ping消息的回复。客户等待服务器回答的时间至多为一秒，如果在一秒内没有收到回复，您的客户端程序应该假定数据包在网络传输期间丢失。您需要查找Python文档，以了解如何在数据报套接字上设置超时值。</p><p>具体来说，您的客户端程序应该</p><ol><li>使用UDP发送ping消息（注意：不同于TCP，您不需要首先建立连接，因为UDP是无连接协议。）</li><li>从服务器输出响应消息</li><li>如果从服务器受到响应，则计算并输出每个数据包的往返时延（RTT）（以秒为单位），</li><li>否则输出“请求超时”</li></ol><p>在开发过程中，您应该先在计算机上运行 <code>UDPPingerServer.py</code>，并通过向 <code>localhost</code>（或127.0.0.1）发送数据包来测试客户端。调试完成代码后，您应该能看到ping服务器和ping客户端在不同机器上通过网络进行通信。</p><h3 id="消息格式"><a href="#消息格式" class="headerlink" title="消息格式"></a>消息格式</h3><p>本实验中的ping消息格式使用最简单的方式。客户端消息只有一行，由以下格式的ASCII字符组成：</p><blockquote><p>Ping <em>sequence_number time</em></p></blockquote><p>其中<em>sequence_number</em>从1开始，一直到10，共10条消息，而<em>time</em>则是客户端发送消息时的时间。</p><h3 id="需要上交的内容-1"><a href="#需要上交的内容-1" class="headerlink" title="需要上交的内容"></a>需要上交的内容</h3><p>您需要上交完整的客户端代码和屏幕截图，以验证您的ping程序是否按需求运行。</p><h3 id="可选练习-1"><a href="#可选练习-1" class="headerlink" title="可选练习"></a>可选练习</h3><ol><li>目前，程序计算每个数据包的往返时间（RTT），并单独打印出来。请按照标准ping程序的模式修改。您需要在客户端每次ping后显示最小，最大和平均RTT。另外，还需计算丢包率（百分比）。</li><li>UDP Ping的另一个类似的应用是UDP Heartbeat。心跳可用于检查应用程序是否已启动并运行，并报告单向丢包。客户端在UDP数据包中将一个序列号和当前时间戳发送给正在监听客户端心跳的服务器。服务器收到数据包后，计算时差，报告丢包（若发生）。如果心跳数据包在指定的一段时间内丢失，我们可以假设客户端应用程序已经停止。实现UDP Heartbeat（客户端和服务器端）。您需要修改给定的UDPPingerServer.py和您自己的UDP ping客户端。</li></ol><p><strong>以上内容来自《计算机网络：自顶向下方法》官方文档，与我的作业有部分不同，详见pdf。</strong></p><p><strong>个人实验报告</strong></p><embed src="./Socket.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    
    <tags>
      
      <tag>计算机网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>智能机器人技术——一阶运动学与静力学</title>
    <link href="/2022/03/30/robot4/"/>
    <url>/2022/03/30/robot4/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>智能机器人技术作业记录</p><span id="more"></span><p><img src="https://img.enderfga.cn/img/gVyodA.jpg" alt=""></p><h1 id="智能机器人技术——一阶运动学与静力学"><a href="#智能机器人技术——一阶运动学与静力学" class="headerlink" title="智能机器人技术——一阶运动学与静力学"></a>智能机器人技术——一阶运动学与静力学</h1><p>一、计算/解答题, 请写出解题过程 (30 分)。</p><ol><li>如下图所示有一处于初始位形的 RRP 机器人 (即讲义例 3 ), 求 (10 分):</li></ol><p>a) 写出各关节相对空间坐标系 ${s}$ 的旋量坐标。求解当 $\theta=\left(90^{\circ}, 90^{\circ}, 1\right)$ 时的正向运动学; 手绘此时的机器人, 标注 ${s}$ 系和 ${b}$ 系, 求解此时的空间雅克比 $J_{s}$;</p><p>b) 写出各关节相对末端坐标系 ${b}$ 的旋量坐标。求解当 $\theta=\left(90^{\circ}, 90^{\circ}, 1\right)$ 时的正向运动学, 确认与 a) 中结果相同; 求解此时的物体雅克比 $J_{b}$ 。</p><p><img src="https://img.enderfga.cn/img/image-20220330002641939.png" alt=""></p><p>a)</p><p>此时的机器人如图所示：</p><p><img src="https://img.enderfga.cn/img/image-20220330002652483.png" alt=""></p><p>初始位形<strong>M</strong>：</p><script type="math/tex; mode=display">M=\left[\begin{array}{cccc}-1 & 0 & 0 & 0  \\0 & 0 & 1 & 3  \\0 & 1 & 0 & 2 \\0 & 0 & 0 & 1\end{array}\right]</script><p>相对空间坐标系 ${s}$ 的旋量坐标为：</p><script type="math/tex; mode=display">\mathcal{S}_{i}：~\begin{array}{|c||c|c|}\hline i & \omega_{i}  & v_{i} \\\hline \hline 1 & (0,0,1) & (0, 0,0) \\\hline 2 & (1,0,0) & (0, 2,0) \\\hline 3 & (0,0,0)  & (0, 1,0) \\\hline\end{array}\\</script><p>故当$\theta=\left(90^{\circ}, 90^{\circ}, 1\right)$ 时的正向运动学<strong>PoE</strong>与空间雅克比 $J_{s}$：</p><script type="math/tex; mode=display">T=\left[\begin{array}{cccc}0 & 0 & -1 & -3 \\-1 & 0 & 0 & 0\\0 & 1 & 0 & 2\\0 & 0 & 0 & 1\\\end{array}\right]</script><script type="math/tex; mode=display">J_{s}(\theta)=\left[\begin{array}{ccc}0 & 0 & 0 \\0 & 1 & 0 \\1 & 0 & 0 \\0 & -2 & 0 \\0 & 0 & 0 \\0 & 0 & 1\end{array}\right]</script><p>使用到的函数如下：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs matlab"><span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">T</span> = <span class="hljs-title">FKinSpace</span><span class="hljs-params">(M, Slist, thetalist)</span></span><br>T = M;<br><span class="hljs-keyword">for</span> <span class="hljs-built_in">i</span> = <span class="hljs-built_in">size</span>(thetalist): <span class="hljs-number">-1</span>: <span class="hljs-number">1</span><br>    T = MatrixExp6(VecTose3(Slist(:, <span class="hljs-built_in">i</span>) * thetalist(<span class="hljs-built_in">i</span>))) * T;<br><span class="hljs-keyword">end</span><br><span class="hljs-keyword">end</span><br></code></pre></td></tr></table></figure><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs matlab"><span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">Js</span> = <span class="hljs-title">JacobianSpace</span><span class="hljs-params">(Slist, thetalist)</span></span><br>Js = Slist;<br>T = <span class="hljs-built_in">eye</span>(<span class="hljs-number">4</span>);<br><span class="hljs-keyword">for</span> <span class="hljs-built_in">i</span> = <span class="hljs-number">2</span>: <span class="hljs-built_in">length</span>(thetalist)<br>    T = T * MatrixExp6(VecTose3(Slist(:, <span class="hljs-built_in">i</span> - <span class="hljs-number">1</span>) * thetalist(<span class="hljs-built_in">i</span> - <span class="hljs-number">1</span>)));<br>Js(:, <span class="hljs-built_in">i</span>) = Adjoint(T) * Slist(:, <span class="hljs-built_in">i</span>);<br><span class="hljs-keyword">end</span><br><span class="hljs-keyword">end</span><br></code></pre></td></tr></table></figure><p>b)</p><p>初始位形<strong>M</strong>：</p><script type="math/tex; mode=display">M=\left[\begin{array}{cccc}-1 & 0 & 0 & 0  \\0 & 0 & 1 & 3  \\0 & 1 & 0 & 2 \\0 & 0 & 0 & 1\end{array}\right]</script><p>相对空间坐标系 ${b}$ 的旋量坐标为：</p><script type="math/tex; mode=display">\mathcal{B}_{i}：~\begin{array}{|c||c|c|}\hline i & \omega_{i}  & v_{i} \\\hline \hline 1 & (0,1,0) & (3, 0,0) \\\hline 2 & (-1,0,0) & (0, 3,0) \\\hline 3 & (0,0,0)  & (0, 0,1) \\\hline\end{array}\\</script><p>故当$\theta=\left(90^{\circ}, 90^{\circ}, 1\right)$ 时的正向运动学<strong>PoE</strong>与物体雅克比 $J_{b}$：</p><script type="math/tex; mode=display">T=\left[\begin{array}{cccc}0 & 0 & -1 & -3 \\-1 & 0 & 0 & 0\\0 & 1 & 0 & 2\\0 & 0 & 0 & 1\\\end{array}\right]\\(与a中结果相同)\\J_{b}(\theta)=\left[\begin{array}{ccc}0 & -1 & 0 \\0 & 0 & 0 \\1 & 0 & 0 \\0 & 0 & 0 \\0 & 4 & 0 \\0 & 0 & 1\end{array}\right]</script><p>使用到的函数如下：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs matlab"><span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">T</span> = <span class="hljs-title">FKinBody</span><span class="hljs-params">(M, Blist, thetalist)</span></span><br>T = M;<br><span class="hljs-keyword">for</span> <span class="hljs-built_in">i</span> = <span class="hljs-number">1</span>: <span class="hljs-built_in">size</span>(thetalist)<br>    T = T * MatrixExp6(VecTose3(Blist(:, <span class="hljs-built_in">i</span>) * thetalist(<span class="hljs-built_in">i</span>)));<br><span class="hljs-keyword">end</span><br><span class="hljs-keyword">end</span><br></code></pre></td></tr></table></figure><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs matlab"><span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">Jb</span> = <span class="hljs-title">JacobianBody</span><span class="hljs-params">(Blist, thetalist)</span></span><br>Jb = Blist;<br>T = <span class="hljs-built_in">eye</span>(<span class="hljs-number">4</span>);<br><span class="hljs-keyword">for</span> <span class="hljs-built_in">i</span> = <span class="hljs-built_in">length</span>(thetalist) - <span class="hljs-number">1</span>: <span class="hljs-number">-1</span>: <span class="hljs-number">1</span>   <br>    T = T * MatrixExp6(VecTose3(<span class="hljs-number">-1</span> * Blist(:, <span class="hljs-built_in">i</span> + <span class="hljs-number">1</span>) * thetalist(<span class="hljs-built_in">i</span> + <span class="hljs-number">1</span>)));<br>Jb(:, <span class="hljs-built_in">i</span>) = Adjoint(T) * Blist(:, <span class="hljs-built_in">i</span>);<br><span class="hljs-keyword">end</span><br><span class="hljs-keyword">end</span><br></code></pre></td></tr></table></figure><ol><li>如下图所示有一处于初始位形的 RRRP 空间开链机器人, $p$ 为${b}$ 系原点相对于 ${s}$ 系的坐标, 求 (10 分):</li></ol><p>a) 试求当 $\theta=(0,0, \pi / 2, L)$ 时的物体雅克比 $J_{b}(\theta)$;<br>b) 试求当 $\theta=(0,0, \pi / 2, L)$ 且 $\dot{\theta}=(1,1,1,1)$ 时的 $\dot{p}$。</p><p><img src="https://img.enderfga.cn/img/image-20220330002705450.png" alt=""></p><p>a)</p><p>相对空间坐标系 ${b}$ 的旋量坐标为：</p><script type="math/tex; mode=display">\mathcal{B}_{i}：~\begin{array}{|c||c|c|}\hline i & \omega_{i}  & v_{i} \\\hline \hline 1 & (0,0,1) & (-L, 0,0) \\\hline 2 & (1,0,0) & (0, 0,L) \\\hline 3 & (0,0,1)  & (0, 0,0) \\\hline 4 & (0,0,0)  & (0, 1,0) \\\hline\end{array}\\</script><p>使用JacobianBody(Blist, thetalist)计算得，当 $\theta=(0,0, \pi / 2, L)$ 时的物体雅克比 $J_{b}(\theta)$：</p><script type="math/tex; mode=display">J_{b}(\theta)=\left[\begin{array}{cccC}0 & 0 & 0 &0\\0 & -1 & 0 &0\\1 & 0 & 1 &0\\-L & 0 & -L&0 \\L & 0 & 0 &1\\0 & L & 0&0\end{array}\right]</script><p>b)</p><p>当 $\dot{\theta}=(1,1,1,1)$时：</p><script type="math/tex; mode=display">\mathcal{V}_{b}=J_{b}(\theta) \dot{\theta}=\left[\begin{array}{c}\omega_{b} \\v_{b}\end{array}\right] =\left[\begin{array}{c}0\\-1\\2\\-2L\\L+1\\L\end{array}\right]\\\because R_{s b}=\left[\begin{array}{rrc}0 & -1 & 0 \\1 & 0 & 0 \\0 & 0 & 1\end{array}\right] \\\therefore\dot{p}=R_{s b} v_{b}=\left[\begin{array}{c}-L-1 \\-2 L \\L\end{array}\right]</script><ol><li>如下图所示有一处于初始位形的 PRPRRR 空间开链机器人, 此时基坐标系原点与末端坐标系原点之间距离为 $L$, 求 (10 分):</li></ol><p>a) 空间雅克比 $J_{S}$ 的前三列;</p><p>b) 物体雅克比 $J_{b}$ 的后两列;</p><p>c) 初始位形时的 $J_{S}(0)$;</p><p>d) 初始位形时, 若在末端坐标系的 $-\hat{z}_{b}$ 方向产生 $100 \mathrm{~N}$ 的力, 需要关节提供多少力或力矩?<br><img src="https://img.enderfga.cn/img/image-20220330002722057.png" alt=""></p><p>a)</p><p>空间雅克比 $J_{S}$ 的前三列：</p><script type="math/tex; mode=display">\begin{aligned}&\mathcal{V}_{s 1}(\theta): \quad \omega_{s 1}=\left[\begin{array}{l}0 \\0 \\0\end{array}\right], v_{s 1}=\left[\begin{array}{l}0 \\0 \\1\end{array}\right] \\&\mathcal{V}_{s 2}(\theta): \quad \omega_{s 2}=\left[\begin{array}{l}0 \\0 \\1\end{array}\right], q_{2}=\left[\begin{array}{l}0 \\0 \\0\end{array}\right], v_{s 2}=\left[\begin{array}{l}0 \\0 \\0\end{array}\right] \\&\mathcal{V}_{s 3}(\theta): \quad \omega_{s 3}=\left[\begin{array}{l}0 \\0 \\0\end{array}\right], v_{s 3}=e^{\hat{z} \theta_{2}}\left[\begin{array}{l}0 \\1 \\0\end{array}\right]=\left[\begin{array}{c}-\sin \theta_{2} \\\cos \theta_{2} \\0\end{array}\right]\end{aligned}</script><p>b)</p><p>物体雅克比 $J_{b}$ 的后两列：</p><script type="math/tex; mode=display">\begin{aligned}&\mathcal{V}_{b 6}(\theta): \quad \omega_{b 6}=\left[\begin{array}{l}1 \\0 \\0\end{array}\right], q_{6}=\left[\begin{array}{l}0 \\0 \\0\end{array}\right], v_{b 6}=\left[\begin{array}{l}0 \\0 \\0\end{array}\right] \\&\mathcal{V}_{b 5}(\theta): \quad \omega_{b 5}=e^{\hat{x}\left(-\theta_{6}\right)}\left[\begin{array}{l}0 \\0 \\1\end{array}\right]=\left[\begin{array}{c}0 \\\sin \theta_{6} \\\cos \theta_{6}\end{array}\right], q_{5}=\left[\begin{array}{l}0 \\0 \\0\end{array}\right], v_{b 5}=\left[\begin{array}{l}0 \\0 \\0\end{array}\right]\end{aligned}</script><p>c)</p><p>初始位形时θ=0，故$ J_{S}(0)$：</p><script type="math/tex; mode=display">\because \mathcal{S}_{i}：~\begin{array}{|c||c|c|}\hline i & \omega_{i}  & v_{i} \\\hline \hline 1 & (0,0,0) & (0, 0,1) \\\hline 2 & (0,0,1) & (0,0,0) \\\hline 3 & (0,0,0)  & (0, 1,0) \\\hline 4 & (0,1,0) & (0, 0,0) \\\hline 5 & (0,0,1)  & (L, 0,0) \\\hline 6 & (1,0,0) & (0, 0,-L) \\\hline\end{array}\\\therefore J_{s}(0)=\left[\begin{array}{cccccc}0 & 0 & 0 & 0 & 0 & 1 \\0 & 0 & 0 & 1 & 0 & 0 \\0 & 1 & 0 & 0 & 1 & 0 \\0 & 0 & 0 & 0 & L & 0 \\0 & 0 & 1 & 0 & 0 & 0 \\1 & 0 & 0 & 0 & 0 & -L\end{array}\right]</script><p>d)</p><script type="math/tex; mode=display">\begin{aligned}&\because r_{s b}=\left[\begin{array}{l}0 \\L \\0\end{array}\right], f_{b}=\left[\begin{array}{c}0 \\0 \\-100\end{array}\right]\\&\therefore\mathcal{F}_{s}=\left[\begin{array}{c}m_{s} \\f_{s}\end{array}\right]=\left[\begin{array}{c}r_{s b} \times f_{b} \\f_{b}\end{array}\right]=\left[\begin{array}{c}-100 L \\0 \\0 \\0 \\0 \\-100\end{array}\right]\\&\therefore \tau=J_{s}^{T}(0) \mathcal{F}_{s}\\&=\left[\begin{array}{cccccc}0 & 0 & 0 & 0 & 0 & 1 \\0 & 0 & 1 & 0 & 0 & 0 \\0 & 0 & 0 & 0 & 1 & 0 \\0 & 1 & 0 & 0 & 0 & 0 \\0 & 0 & 1 & L & 0 & 0 \\1 & 0 & 0 & 0 & 0 & -L\end{array}\right]\left[\begin{array}{c}-100 L \\0 \\0 \\0 \\0 \\-100\end{array}\right]=\left[\begin{array}{c}-100 \\0 \\0 \\0 \\0 \\0\end{array}\right]\end{aligned}</script>]]></content>
    
    
    
    <tags>
      
      <tag>机器人</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于强化学习的无人机自主导航</title>
    <link href="/2022/03/28/UAV/"/>
    <url>/2022/03/28/UAV/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>无人系统导论课程设计报告</p><span id="more"></span><embed src="./UAV.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    
    <tags>
      
      <tag>无人系统</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Visual Synthesis Pre-training for NUWA</title>
    <link href="/2022/03/28/NUWA/"/>
    <url>/2022/03/28/NUWA/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>基于深度生成模型的多模态视觉合成</p><span id="more"></span><embed src="./NUWA.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    
    <tags>
      
      <tag>人工智能</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度学习在新冠肺炎辅助诊断中的应用</title>
    <link href="/2022/03/28/AI/"/>
    <url>/2022/03/28/AI/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>综述论文</p><span id="more"></span><embed src="./COVID-19.pdf" width="100%" height="750" type="application/pdf">]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器&amp;深度学习代码速查表</title>
    <link href="/2022/03/24/graph/"/>
    <url>/2022/03/24/graph/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>搬运自github.com/OUCMachineLearning/OUCML</p><span id="more"></span><h1 id="机器-amp-深度学习代码速查表"><a href="#机器-amp-深度学习代码速查表" class="headerlink" title="机器&amp;深度学习代码速查表"></a>机器&amp;深度学习代码速查表</h1><h3 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h3><p><img src="https://img.enderfga.cn/img/image-20220324225648930.png" alt=""><br><img src="https://img.enderfga.cn/img/%E7%BD%91%E7%BB%9C.png" alt=""></p><h3 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h3><p><img src="https://img.enderfga.cn/img/liner.jpg" alt=""></p><h3 id="python基础"><a href="#python基础" class="headerlink" title="python基础"></a>python基础</h3><p><img src="https://img.enderfga.cn/img/sci.jpg" alt=""></p><h3 id="scipy科学计算"><a href="#scipy科学计算" class="headerlink" title="scipy科学计算"></a>scipy科学计算</h3><p><img src="https://img.enderfga.cn/img/sci.png" alt=""></p><h3 id="spark"><a href="#spark" class="headerlink" title="spark"></a>spark</h3><p><img src="https://img.enderfga.cn/img/spark.jpeg" alt=""></p><h2 id="数据保存及可视化"><a href="#数据保存及可视化" class="headerlink" title="数据保存及可视化"></a>数据保存及可视化</h2><h3 id="numpy"><a href="#numpy" class="headerlink" title="numpy"></a>numpy</h3><p><img src="https://img.enderfga.cn/img/np.png" alt=""></p><h3 id="pandas"><a href="#pandas" class="headerlink" title="pandas"></a>pandas</h3><p><img src="https://img.enderfga.cn/img/pd.png" alt=""><br><img src="https://img.enderfga.cn/img/df.jpeg" alt=""><br><img src="https://img.enderfga.cn/img/df2.jpeg" alt=""></p><h3 id="bokeh"><a href="#bokeh" class="headerlink" title="bokeh"></a>bokeh</h3><p><img src="https://img.enderfga.cn/img/bokeh.jpg" alt=""></p><h2 id="画图"><a href="#画图" class="headerlink" title="画图"></a>画图</h2><h3 id="matplotlib"><a href="#matplotlib" class="headerlink" title="matplotlib"></a>matplotlib</h3><p><img src="https://img.enderfga.cn/img/matplot.png" alt=""></p><h3 id="ggplot"><a href="#ggplot" class="headerlink" title="ggplot"></a>ggplot</h3><p><img src="https://img.enderfga.cn/img/data%20vis.jpeg" alt=""></p><p><img src="https://img.enderfga.cn/img/gg.jpeg" alt=""></p><h2 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h2><h3 id="sklearn"><a href="#sklearn" class="headerlink" title="sklearn"></a>sklearn</h3><p><img src="https://img.enderfga.cn/img/sk.jpg" alt=""></p><p><img src="https://img.enderfga.cn/img/scikit.png" alt=""></p><h3 id="keras"><a href="#keras" class="headerlink" title="keras"></a>keras</h3><p><img src="https://img.enderfga.cn/img/keras.jpeg" alt=""></p><h3 id="tensorflow"><a href="#tensorflow" class="headerlink" title="tensorflow"></a>tensorflow</h3><p><img src="https://img.enderfga.cn/img/TF.png" alt=""></p><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><h3 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h3><p><img src="https://img.enderfga.cn/img/datastruct.png" alt=""></p><h3 id="复杂度"><a href="#复杂度" class="headerlink" title="复杂度"></a>复杂度</h3><p><img src="https://img.enderfga.cn/img/O.png" alt=""></p><h3 id="排序算法"><a href="#排序算法" class="headerlink" title="排序算法"></a>排序算法</h3><p><img src="https://img.enderfga.cn/img/sort.png" alt=""></p>]]></content>
    
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>智能机器人技术——正向运动学</title>
    <link href="/2022/03/24/robot3/"/>
    <url>/2022/03/24/robot3/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>智能机器人技术作业记录</p><span id="more"></span><p><img src="https://img.enderfga.cn/img/image-20220324223524468.png" alt=""></p><h1 id="智能机器人技术——正向运动学"><a href="#智能机器人技术——正向运动学" class="headerlink" title="智能机器人技术——正向运动学"></a>智能机器人技术——正向运动学</h1><ol><li>如下图所示有一处于初始位形的 PRRRRR 空间开链机器人, 试确定末端初始位形 $M$ 、在 ${0}$ 系描述的螺旋轴 Si 、$ 在 ${b} 系描述的螺旋轴 Bi （如讲义那样列表即可)。</li></ol><p><img src="https://img.enderfga.cn/img/image-20220323164358687.png" alt=""></p><p><strong>解得：</strong></p><p>相对基坐标系的$\mathrm{PoE}$ :</p><script type="math/tex; mode=display">T(\theta)=e^{\left[\mathcal{S}_{1}\right] \theta_{1}} \cdots e^{\left[\mathcal{S}_{n-1}\right] \theta_{n-1}} e^{\left[\mathcal{S}_{n}\right] \theta_{n}} M</script><p>相对末端坐标系的$\mathrm{PoE}$ :</p><script type="math/tex; mode=display">T(\theta)=M e^{\left[\mathcal{B}_{1}\right] \theta_{1}} \cdots e^{\left[\mathcal{B}_{n-1}\right] \theta_{n-1}} e^{\left[\mathcal{B}_{n}\right] \theta_{n}}</script><p>末端初始位形 $M$：</p><script type="math/tex; mode=display">\left[\begin{array}{cccc}1 & 0 & 0 & 0 \\0 & 1 & 0 & L_1+L_2+L_3+L_4 \\0 & 0 & 1 & h \\0 & 0 & 0 & 1\end{array}\right]</script><p>在 ${0}$ 系描述的螺旋轴 $\mathcal{S}_{i} $：</p><script type="math/tex; mode=display">[\mathcal{S}]=\left[\begin{array}{cc}{[\omega]} & v \\0 & 0\end{array}\right] \in \operatorname{se}(3) \longrightarrow \mathcal{S}=\left[\begin{array}{c}\omega \\v\end{array}\right] \in \mathbb{R}^{6}</script><script type="math/tex; mode=display">\because v_i=-\omega_{i} \times q_i \\\begin{array}{|c||c|c|c|}\hline i & \omega_{i} & q_i & v_{i} \\\hline \hline 1 & (0,0,0) & NULL & (0, 1,0) \\\hline 2 & (0,0,1) & (0, L_1,0)& (L_1, 0,0) \\\hline 3 & (-1,0,0) & (0, L_1,h)& (0, -h,L_1) \\\hline 4 & (-1,0,0) & (0,L_1+L_2,h)& (0, -h,L_1+L_2) \\\hline 5 & (-1,0,0) & (0,L_1+L_2+L_3,h)& (0, -h,L_1+L_2+L_3) \\\hline 6 & (0,1,0) & (0,0,h) & (-h, 0,1)\\\hline\end{array}</script><p>在 ${b}$ 系描述的螺旋轴 $\mathcal{B}_{i}$：</p><script type="math/tex; mode=display">[\mathcal{B}]=\left[\begin{array}{cc}{[\omega]} & v \\0 & 0\end{array}\right] \in \operatorname{se}(3) \longrightarrow \mathcal{B}=\left[\begin{array}{c}\omega \\v\end{array}\right] \in \mathbb{R}^{6}</script><script type="math/tex; mode=display">\because v_i=-\omega_{i} \times q_i \\\begin{array}{|c||c|c|c|}\hline i & \omega_{i} & q_i & v_{i} \\\hline \hline 1 & (0,0,0) & NULL & (0, 1,0) \\\hline 2 & (0,0,1) & (0, -L_2-L_3-L_4,0)& (-L_2-L_3-L_4, 0,0) \\\hline 3 & (-1,0,0) & (0, -L_2-L_3-L_4,0)& (0, 0,-L_2-L_3-L_4) \\\hline 4 & (-1,0,0) & (0,-L_3-L_4,0)& (0, 0,-L_3-L_4) \\\hline 5 & (-1,0,0) & (0,-L_4,0)& (0, 0,-L_4) \\\hline 6 & (0,1,0) & (0,0,0) & (0, 0,0)\\\hline\end{array}</script><ol><li><p>如下图所示有一处于初始位形的 RRRRPR 空间开链机器人, 试确定末端初始位形 $M$ 、在 ${0}$ 系描述的螺旋轴 Si 、$ 在 ${b} 系描述的螺旋轴 Bi （如讲义那样列表即可)。<br><img src="https://img.enderfga.cn/img/image-20220323164443717.png" alt=""></p><p><strong>解得：</strong></p></li></ol><p>相对基坐标系的$\mathrm{PoE}$ :</p><script type="math/tex; mode=display">T(\theta)=e^{\left[\mathcal{S}_{1}\right] \theta_{1}} \cdots e^{\left[\mathcal{S}_{n-1}\right] \theta_{n-1}} e^{\left[\mathcal{S}_{n}\right] \theta_{n}} M</script><p>相对末端坐标系的$\mathrm{PoE}$ :</p><script type="math/tex; mode=display">T(\theta)=M e^{\left[\mathcal{B}_{1}\right] \theta_{1}} \cdots e^{\left[\mathcal{B}_{n-1}\right] \theta_{n-1}} e^{\left[\mathcal{B}_{n}\right] \theta_{n}}</script><p>末端初始位形 $M$：</p><script type="math/tex; mode=display">\left[\begin{array}{cccc}1 & 0 & 0 & L_1 \\0 & 1 & 0 & L_3+L_4 \\0 & 0 & 1 & -L_5-L_6 \\0 & 0 & 0 & 1\end{array}\right]</script><p>在 ${0}$ 系描述的螺旋轴 $\mathcal{S}_{i} $：</p><script type="math/tex; mode=display">[\mathcal{S}]=\left[\begin{array}{cc}{[\omega]} & v \\0 & 0\end{array}\right] \in \operatorname{se}(3) \longrightarrow \mathcal{S}=\left[\begin{array}{c}\omega \\v\end{array}\right] \in \mathbb{R}^{6}</script><script type="math/tex; mode=display">\because v_i=-\omega_{i} \times q_i \\\begin{array}{|c||c|c|c|}\hline i & \omega_{i} & q_i & v_{i} \\\hline \hline 1 & (1,0,0) & (0, 0,0)& (0, 0,0) \\\hline 2 & (0,0,-1) & (L_1,0,0)& (0, L_1,0) \\\hline 3 & (0,1,0) & (L_1,0,L_2)& (-L_2, 0,L_1) \\\hline 4 & (1,0,0) & (0,L_3,0)& (0, 0,-L_3) \\\hline 5 & (0,0,0) & NULL & (0, 1,0)\\\hline 6 & (0,1,0) & (L_1,0,-L_5) & (L_5, 0,-L_1) \\\hline\end{array}</script><p>在 ${b}$ 系描述的螺旋轴 $\mathcal{B}_{i}$：</p><script type="math/tex; mode=display">[\mathcal{B}]=\left[\begin{array}{cc}{[\omega]} & v \\0 & 0\end{array}\right] \in \operatorname{se}(3) \longrightarrow \mathcal{B}=\left[\begin{array}{c}\omega \\v\end{array}\right] \in \mathbb{R}^{6}</script><script type="math/tex; mode=display">\because v_i=-\omega_{i} \times q_i \\\begin{array}{|c||c|c|c|}\hline i & \omega_{i} & q_i & v_{i} \\\hline \hline 1 & (1,0,0) & (0, -L_3-L_4,L_5+L_6)& (0, L_5+L_6,L_3+L_4) \\\hline 2 & (0,0,-1) & (0, -L_3-L_4,0)& (L_3+L_4, 0,0) \\\hline 3 & (0,1,0) & (0,0,L_2+L_5+L_6)& (-L_2-L_5-L_6, 0,0) \\\hline 4 & (1,0,0) & (0,-L_4,L_5+L_6)& (0, L_5+L_6,L_4) \\\hline 5 & (0,0,0) & NULL & (0, 1,0)\\\hline 6 & (0,1,0) & (0,0,L_6) & (-L_6, 0,0) \\\hline\end{array}</script><ol><li>如下图所示有一处于初始位形的 RRRPRR 空间开链机器人, 试确定末端初始位形 $M$ 、在 ${0}$ 系描述的螺旋轴 Si 、$ 在 ${b} 系描述的螺旋轴 Bi （如讲义那样列表即可)。</li></ol><p><img src="https://img.enderfga.cn/img/image-20220323164454698.png" alt=""></p><p><strong>解得：</strong></p><p>相对基坐标系的$\mathrm{PoE}$ :</p><script type="math/tex; mode=display">T(\theta)=e^{\left[\mathcal{S}_{1}\right] \theta_{1}} \cdots e^{\left[\mathcal{S}_{n-1}\right] \theta_{n-1}} e^{\left[\mathcal{S}_{n}\right] \theta_{n}} M</script><p>相对末端坐标系的$\mathrm{PoE}$ :</p><script type="math/tex; mode=display">T(\theta)=M e^{\left[\mathcal{B}_{1}\right] \theta_{1}} \cdots e^{\left[\mathcal{B}_{n-1}\right] \theta_{n-1}} e^{\left[\mathcal{B}_{n}\right] \theta_{n}}</script><p>末端初始位形 $M$：</p><script type="math/tex; mode=display">\left[\begin{array}{cccc}-1 & 0 & 0 & 0 \\0 & 1 & 0 & 4 \\0 & 0 & -1 & 1 \\0 & 0 & 0 & 1\end{array}\right]</script><p>在 ${0}$ 系描述的螺旋轴 $\mathcal{S}_{i} $：</p><script type="math/tex; mode=display">[\mathcal{S}]=\left[\begin{array}{cc}{[\omega]} & v \\0 & 0\end{array}\right] \in \operatorname{se}(3) \longrightarrow \mathcal{S}=\left[\begin{array}{c}\omega \\v\end{array}\right] \in \mathbb{R}^{6}</script><script type="math/tex; mode=display">\because v_i=-\omega_{i} \times q_i \\\begin{array}{|c||c|c|c|}\hline i & \omega_{i} & q_i & v_{i} \\\hline \hline 1 & (0,0,1) & (0, 0,0)& (0, 0,0) \\\hline 2 & (1,0,0) & (0,0,2)& (0, 2,0) \\\hline 3 & (1,0,0) & (0,1,2)& (0, 2,-1) \\\hline 4 & (0,0,0) & NULL & (0, 1,0) \\\hline 5 & (0, \frac{\sqrt{2}}{2} , \frac{\sqrt{2}}{2} ) & (0,3,2) & (\frac{\sqrt{2}}{2}, 0,0)\\\hline 6 & (0,0,-1) & (0,4,0)& (-4, 0,0) \\\hline\end{array}</script><p>在 ${b}$ 系描述的螺旋轴 $\mathcal{B}_{i}$：</p><script type="math/tex; mode=display">[\mathcal{B}]=\left[\begin{array}{cc}{[\omega]} & v \\0 & 0\end{array}\right] \in \operatorname{se}(3) \longrightarrow \mathcal{B}=\left[\begin{array}{c}\omega \\v\end{array}\right] \in \mathbb{R}^{6}</script><script type="math/tex; mode=display">\because v_i=-\omega_{i} \times q_i \\\begin{array}{|c||c|c|c|}\hline i & \omega_{i} & q_i & v_{i} \\\hline \hline 1 & (0,0,-1) & (0, -4,0)& (4, 0,0) \\\hline 2 & (-1,0,0) & (0,-4,-1)& (0, 1,-4) \\\hline 3 & (-1,0,0) & (0,-3,-1)& (0, 1,-3) \\\hline 4 & (0,0,0) & NULL & (0, 1,0) \\\hline 5 & (0, \frac{\sqrt{2}}{2} , -\frac{\sqrt{2}}{2} ) & (0,-1,-1) & (\sqrt{2}, 0,0)\\\hline 6 & (0,0,1) & (0,0,0)& (0, 0,0) \\\hline\end{array}</script>]]></content>
    
    
    
    <tags>
      
      <tag>机器人</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数据库原理 Exercises 2</title>
    <link href="/2022/03/23/data2/"/>
    <url>/2022/03/23/data2/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Database System Concepts Exercises of Chapter 3</p><span id="more"></span><p><img src="https://img.enderfga.cn/img/image-20220323230708879.png" alt=""></p><h1 id="Database-System-Concepts-Exercises-of-Chapter-3"><a href="#Database-System-Concepts-Exercises-of-Chapter-3" class="headerlink" title="Database System Concepts Exercises of Chapter 3"></a>Database System Concepts Exercises of Chapter 3</h1><p><strong>Exercises 3.9</strong> Consider the employee database of Figure $3.20$, where the primary keys are underlined. Give an expression in SQL for each of the following queries.</p><p>a. Find the names and cities of residence of all employees who work for “First Bank Corporation”.</p><p>b. Find the names, street addresses, and cities of residence of all employees who work for “First Bank Corporation” and earn more than $\$ 10,000$.</p><p>c. Find all employees in the database who do not work for “First Bank Corporation”.</p><p>d. Find all employees in the database who earn more than each employee of “Small Bank Corporation”.</p><p>e. Assume that the companies may be located in several cities. Find all companies located in every city in which “Small Bank Corporation” is located.</p><p>f. Find the company that has the most employees.</p><p>g. Find those companies whose employees earn a higher salary, on average, than the average salary at “First Bank Corporation”.</p><script type="math/tex; mode=display">employee (\underline{employee\_name}, street, city)\\works (\underline{employee\_name}, company\_name, salary)\\company (\underline{company\_name}, city)\\manages ( \underline{ employee\_name, } manager\_name )\\Figure~~3.20~~Employee~~database~~for~~Exercises~~3.9</script><p><strong>My answer：</strong></p><p>a.</p><p><strong>select</strong> e.employee_name, city<br><strong>from</strong> employee <strong>as</strong> e, works <strong>as</strong> w<br><strong>where</strong> w.company_name $=$ ‘First Bank Corporation’ <strong>and</strong> w.employee_name $=$ e.employee_name</p><p>b.</p><p><strong>select</strong> <em><br><strong>from</strong> employee<br><strong>where</strong> employee_name <strong>in</strong><br>(<strong>select</strong> employee_name<br><strong>from</strong> works<br><strong>where</strong> company_name = ‘First Bank Corporation’ <em>*and</em></em> salary $&gt;10000$ )</p><p>c.</p><p><strong>select</strong> employee_name<br><strong>from</strong> works<br><strong>where</strong> company_name $\neq$ ‘First Bank Corporation’</p><p>d.</p><p><strong>select</strong> employee_name<br><strong>from</strong> works<br><strong>where</strong> salary $&gt;$ <strong>all</strong><br>(<strong>select</strong> salary<br><strong>from</strong> works<br><strong>where</strong> company_name $=$ ‘Small Bank Corporation’)</p><p>e.</p><p><strong>select</strong> S.company_name<br><strong>from</strong> company <strong>as</strong> $S$<br><strong>where</strong> <strong>not exists</strong> ((<strong>select</strong> city<br><strong>from</strong> company<br><strong>where</strong> company_name $=$ ‘Small Bank Corporation’)<br><strong>except</strong><br>(<strong>select</strong> city<br><strong>from</strong> company <strong>as</strong> $T$<br><strong>where</strong> S.company_name $=$ T.company_name ) )</p><p>f.</p><p><strong>select</strong> company_name<br><strong>from</strong> works<br><strong>group by</strong> company_name<br><strong>having count</strong> (<strong>distinct</strong> employee_name) $&gt;=$ <strong>all</strong><br>(<strong>select</strong> <strong>count</strong> (<strong>distinct</strong> employee_name)<br><strong>from</strong> works<br><strong>group by</strong> company_name)</p><p>g.</p><p><strong>select</strong> company_name<br><strong>from</strong> works<br><strong>group by</strong> company_name<br><strong>having</strong> <strong>avg</strong> (salary) $&gt;$ (<strong>select</strong> <strong>avg</strong> (salary)<br><strong>from</strong> works<br><strong>where</strong> company_name $=$ ‘First Bank Corporation’)</p><p><strong>Exercises 3.8</strong> Consider the bank database of Figure $3.19$, where the primary keys are underlined. Construct the following SQL queries for this relational database.</p><p>a. Find all customers of the bank who have an account but not a loan.</p><p>b. Find the names of all customers who live on the same street and in the same city as “Smith”.</p><p>c. Find the names of all branches with customers who have an account in the bank and who live in “Harrison”.</p><script type="math/tex; mode=display">branch(\underline{branch\_name}, branch\_city, assets)\\customer (\underline{customer\_name}, customer\_street, customer\_city)\\ loan (\underline{loan\_numbe}r, branch\_name, amount)\\borrower (\underline{customer\_name}, \underline{loan\_number})\\account (\underline{account\_number}, branch\_name, balance)\\depositor (\underline{customer\_name}, \underline{account\_number})\\Figure~~3.19~~Banking~~database~~for~~Exercises~~3.8~~and~~3.15</script><p><strong>My answer:</strong></p><p>a.</p><p><strong>select</strong> customer_name<br><strong>from</strong> depositor<br><strong>except</strong><br>(<strong>select</strong> customer_name<br><strong>from</strong> borrower)</p><p>b.</p><p><strong>select</strong> F.customer_name<br><strong>from</strong> customer <strong>as</strong> $F$ join customer <strong>as</strong> $S$ using(customer_street, customer_city)<br><strong>where</strong> S.customer_name $=$ ‘Smith’</p><p>c.</p><p><strong>select</strong> <strong>distinct</strong> branch_name<br><strong>from</strong> account <strong>natural join</strong> depositor <strong>natural join</strong> customer<br><strong>where</strong> customer_city = ‘Harrison’</p><p><strong>Exercises 3.15</strong> Consider the bank database of Figure 3.19, where the primary keys are underlined. Construct the following SQL queries for this relational database.<br>a. Find all customers who have an account at all the branches located in “Brooklyn”.<br>b. Find out the total sum of all loan amounts in the bank.<br>c. Find the names of all branches that have assets greater than those of at least one branch located in “Brooklyn”.</p><p><strong>My answer:</strong></p><p>a.</p><p><strong>select</strong> <strong>distinct</strong> S.customer_name</p><p><strong>from</strong> depositor <strong>as</strong> S</p><p><strong>where</strong> <strong>not exists(</strong></p><p>(<strong>select</strong> branch_name</p><p><strong>from</strong> branch</p><p><strong>where</strong> branch_city = ‘Brooklyn’)</p><p><strong>except</strong></p><p>(<strong>select</strong> R.branch_name</p><p><strong>from</strong> depositor <strong>as</strong> T, account <strong>as</strong> R</p><p><strong>where</strong> T.account_number = R.account_number</p><p><strong>and</strong> S.customer_name = T.customer_name))</p><p>b.</p><p><strong>select</strong> <strong>sum</strong>(amount) <strong>as</strong> sum_loan</p><p><strong>from</strong> loan</p><p>c.</p><p><strong>select</strong> branch_name</p><p><strong>from</strong> branch</p><p><strong>where</strong> assets ＞<strong>some</strong></p><p>(<strong>select</strong> assets</p><p><strong>from</strong> branch</p><p><strong>where</strong> branch_city = ‘Brooklyn’)</p>]]></content>
    
    
    
    <tags>
      
      <tag>数据库</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>智能机器人技术——刚体运动</title>
    <link href="/2022/03/21/robot2/"/>
    <url>/2022/03/21/robot2/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>智能机器人技术作业记录</p><span id="more"></span><p><img src="https://img.enderfga.cn/img/A2.jpg" alt=""></p><h1 id="智能机器人技术第三章——刚体运动"><a href="#智能机器人技术第三章——刚体运动" class="headerlink" title="智能机器人技术第三章——刚体运动"></a>智能机器人技术第三章——刚体运动</h1><ol><li><script type="math/tex; mode=display">已知一固定的空间坐标系 \{s\} 及其 \hat{x}_{s} 、 \hat{y}_{s} 、 \hat{z}_{s}轴坐标, 坐标系 \{a\} 的 \hat{x}_{a}轴沿 (0,0,1) 方向,\\ \hat{y}_{a} 轴沿 (-1,0,0) 方向; 坐标系 \{b\} 的 \hat{x}_{b} 轴沿 (1,0,0) 方向, \hat{y}_{b} 轴沿 (0,0,-1)方向。</script></li></ol><p><strong>a)</strong> 手绘 3 个坐标系, 注意画在不同位置以便区分。</p><script type="math/tex; mode=display">由右手定则可知，坐标系 \{a\} 的 \hat{z}_{a} 轴沿 (0,-1,0) 方向，坐标系 \{b\} 的 \hat{z}_{b} 轴沿 (0,1,0) 方向。结果如图所示：</script><p><img src="https://img.enderfga.cn/img/image-20220320155656111.png" alt=""></p><p><strong>b)</strong> </p><script type="math/tex; mode=display">计算旋转矩阵 R_{s a} 和 R_{s b}</script><p> 。</p><script type="math/tex; mode=display">\begin{aligned}R_{sa} &=\left[\begin{array}{lll}0 & -1 & 0 \\0 & 0 & -1 \\1 & 0 & 0\end{array}\right] \\R_{sb} &=\left[\begin{array}{ccc}1 & 0 & 0 \\0 & 0 & 1 \\0 & -1 & 0\end{array}\right]\end{aligned}</script><p><strong>c)</strong> </p><script type="math/tex; mode=display">已知 R_{s b}, 在不使用逆矩阵的情况下计算 R_{s b}^{-1}, 并验证坐标系画的是否正确。</script><p>由旋转矩阵的性质得$R^{-1}=R^T$</p><script type="math/tex; mode=display">\begin{aligned}R_{sb}^{-1}=R^T_{sb} &=\left[\begin{array}{ccc}1 & 0 & 0 \\0 & 0 & -1 \\0 & 1 & 0\end{array}\right]\end{aligned}</script><p>可知坐标系绘画正确。</p><p><strong>d)</strong> </p><script type="math/tex; mode=display">已知 R_{s a} 和 R_{s b}, 计算 R_{a b}, 并验证坐标系画的是否正确。</script><script type="math/tex; mode=display">\begin{aligned}R_{as}=R_{sa}^{-1}=R^T_{sa} &=\left[\begin{array}{ccc}0 & 0 & 1 \\-1 & 0 & 0 \\0 & -1 & 0\end{array}\right]\end{aligned}</script><script type="math/tex; mode=display">\begin{aligned}R_{ab}=R_{as}R_{sb} &=\left[\begin{array}{ccc}0 & -1 & 0 \\-1 & 0 & 0 \\0 & 0 & -1\end{array}\right]\end{aligned}</script><p>可知坐标系绘画正确。</p><p><strong>e)</strong></p><script type="math/tex; mode=display">将 R=R_{s b} 作为变换算子, 表示绕 \hat{x} 轴转动 -90^{\circ} 。计算 R_{1}=R_{s a} R 与 R_{2}=R R_{s a}, 并回答新姿态 R_{1} 与 R_{2} 分别对应的是 R_{s a} 绕哪个坐标系的 \hat{x} 轴转动得到的结果?</script><script type="math/tex; mode=display">\begin{aligned}R_{1}=R_{sa}R &=\left[\begin{array}{ccc}0 & 0 & -1 \\0 & 1 & 0 \\1 & 0 & 0\end{array}\right]\end{aligned}</script><script type="math/tex; mode=display">\begin{aligned}R_{2}=RR_{sa} &=\left[\begin{array}{ccc}0 & -1 & 0 \\1 & 0 & 0 \\0 & 0 & 1\end{array}\right]\end{aligned}</script><script type="math/tex; mode=display">R_1表示绕坐标系 \{a\} 的 \hat{x}_{a} 轴，R_2表示绕坐标系 \{s\} 的 \hat{x}_{s} 轴。</script><p><strong>f)</strong> </p><script type="math/tex; mode=display">利用 R_{s b} 将点 p_{b}=(1,2,3) 从 \{b\} 系变换到 \{s\} 系。</script><script type="math/tex; mode=display">p_{s}=R_{sb}p_{b}=(1,3,-2)</script><p><strong>g)</strong> </p><script type="math/tex; mode=display">已知 \{s\} 系中一点 p_{s}=(1,2,3), 计算 p^{\prime}=R_{s b} p_{s} 和 p^{\prime \prime}=R_{s b}^{T} p_{s} 。每一推导过程均可以解释为坐标变换 (无须移动点的位置) 或移动点的位置 (无须改变坐标系)。</script><script type="math/tex; mode=display">p^{\prime}=R_{s b} p_{s}=(1,3,-2)：其几何意义为移动点的位置，将向量p_s绕 \hat{x} 轴转动 -90^{\circ}  (无须改变坐标系)。</script><script type="math/tex; mode=display">p^{\prime \prime}=R_{s b}^{T} p_{s}=(1,-3,2)：其几何意义为坐标变换，将 \{s\} 系中一点 p_{s}变换到 \{b\} 系 (无须移动点的位置) 。</script><p><strong>h)</strong>已知 ${s}$ 系中的角速度 $\omega_{s}=(3,2,1)$, 计算其在 ${a}$ 系中的表示。</p><script type="math/tex; mode=display">\omega_{a}=R_{as}\omega_s=(1,-3,-2)</script><p><strong>i)</strong> 计算 $R_{s a}$ 的矩阵对数 $[\widehat{\omega}] \theta$, 并提取其中的元素: 单位角速度 $\widehat{\omega}$ 和转动量 $\theta$ (可以利用编程手段)。</p><p>使用到的函数：</p><p>MatrixLog3：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs matlab"><span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">so3mat</span> = <span class="hljs-title">MatrixLog3</span><span class="hljs-params">(R)</span></span><br>acosinput = (trace(R) - <span class="hljs-number">1</span>) / <span class="hljs-number">2</span>;<br><span class="hljs-keyword">if</span> acosinput &gt;= <span class="hljs-number">1</span><br>    so3mat = <span class="hljs-built_in">zeros</span>(<span class="hljs-number">3</span>);<br><span class="hljs-keyword">elseif</span> acosinput &lt;= <span class="hljs-number">-1</span><br>    <span class="hljs-keyword">if</span> ~NearZero(<span class="hljs-number">1</span> + R(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>))<br>        omg = (<span class="hljs-number">1</span> / <span class="hljs-built_in">sqrt</span>(<span class="hljs-number">2</span> * (<span class="hljs-number">1</span> + R(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>)))) ...<br>              * [R(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>); R(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>); <span class="hljs-number">1</span> + R(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>)];<br>    <span class="hljs-keyword">elseif</span> ~NearZero(<span class="hljs-number">1</span> + R(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>))<br>        omg = (<span class="hljs-number">1</span> / <span class="hljs-built_in">sqrt</span>(<span class="hljs-number">2</span> * (<span class="hljs-number">1</span> + R(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))) ...<br>              * [R(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>); <span class="hljs-number">1</span> + R(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>); R(<span class="hljs-number">3</span>, <span class="hljs-number">2</span>)];<br>    <span class="hljs-keyword">else</span><br>        omg = (<span class="hljs-number">1</span> / <span class="hljs-built_in">sqrt</span>(<span class="hljs-number">2</span> * (<span class="hljs-number">1</span> + R(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)))) ...<br>              * [<span class="hljs-number">1</span> + R(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>); R(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>); R(<span class="hljs-number">3</span>, <span class="hljs-number">1</span>)];<br>    <span class="hljs-keyword">end</span><br>    so3mat = VecToso3(<span class="hljs-built_in">pi</span> * omg);<br><span class="hljs-keyword">else</span><br>theta = <span class="hljs-built_in">acos</span>(acosinput);   <br>    so3mat = theta * (<span class="hljs-number">1</span> / (<span class="hljs-number">2</span> * <span class="hljs-built_in">sin</span>(theta))) * (R - R&#x27;);<br><span class="hljs-keyword">end</span><br><span class="hljs-keyword">end</span><br></code></pre></td></tr></table></figure><p>so3ToVec：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs matlab"><span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">omg</span> = <span class="hljs-title">so3ToVec</span><span class="hljs-params">(so3mat)</span></span><br>omg = [so3mat(<span class="hljs-number">3</span>, <span class="hljs-number">2</span>); so3mat(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>); so3mat(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>)]; <br><span class="hljs-keyword">end</span><br></code></pre></td></tr></table></figure><p>AxisAng3：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs matlab"><span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-params">[omghat, theta]</span> = <span class="hljs-title">AxisAng3</span><span class="hljs-params">(expc3)</span></span><br>theta = norm(expc3);<br>omghat = expc3 / theta;<br><span class="hljs-keyword">end</span><br></code></pre></td></tr></table></figure><p>解得：</p><script type="math/tex; mode=display">[\widehat{\omega}] \theta = \left[\begin{array}{ccc}0 & -1.2092 & -1.2092 \\1.2092 & 0 & -1.2092 \\1.2092 & 1.2092 & 0\end{array}\right]</script><p>单位角速度 $\widehat{\omega}=(0.5774,-0.5774,0.5774)$ 和转动量 $\theta=2.0944$</p><p><strong>j)</strong> 计算与转动 $\widehat{\omega} \theta=(1,2,0)$ 的指数坐标对应的矩阵指数。</p><p>使用到的函数：</p><p>VecToso3：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs matlab"><span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">so3mat</span> = <span class="hljs-title">VecToso3</span><span class="hljs-params">(omg)</span></span><br>so3mat = [<span class="hljs-number">0</span>, -omg(<span class="hljs-number">3</span>), omg(<span class="hljs-number">2</span>); omg(<span class="hljs-number">3</span>), <span class="hljs-number">0</span>, -omg(<span class="hljs-number">1</span>); -omg(<span class="hljs-number">2</span>), omg(<span class="hljs-number">1</span>), <span class="hljs-number">0</span>];<br><span class="hljs-keyword">end</span><br></code></pre></td></tr></table></figure><p>MatrixExp3：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs matlab"><span class="hljs-function"><span class="hljs-keyword">function</span>  <span class="hljs-title">R</span> = <span class="hljs-title">MatrixExp3</span><span class="hljs-params">(so3mat)</span></span><br>omgtheta = so3ToVec(so3mat);<br><span class="hljs-keyword">if</span> NearZero(norm(omgtheta))<br>    R = <span class="hljs-built_in">eye</span>(<span class="hljs-number">3</span>);<br><span class="hljs-keyword">else</span><br>    [omghat, theta] = AxisAng3(omgtheta);<br>    omgmat = so3mat / theta;<br>    R = <span class="hljs-built_in">eye</span>(<span class="hljs-number">3</span>) + <span class="hljs-built_in">sin</span>(theta) * omgmat + (<span class="hljs-number">1</span> - <span class="hljs-built_in">cos</span>(theta)) * omgmat * omgmat;<br><span class="hljs-keyword">end</span><br><span class="hljs-keyword">end</span><br></code></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{aligned}R &=\left[\begin{array}{ccc}-0.2938 & 0.6469 & 0.7037 \\0.6469 & 0.6765 & -0.3518 \\-0.7037 & 0.3518 & -0.6173\end{array}\right]\end{aligned}</script><p>2.题干如 1 , 并且 ${a}$ 系原点相对 ${s}$ 系的坐标为 $(3,0,0),{b}$ 系原点相对 ${s}$ 系的坐标为 $(0,2,0)$。</p><p><strong>a)</strong> 手绘 3 个坐标系, 注意它们之间的相对位置关系。</p><p>图中${s}$系的三个坐标轴长度均为单位长度，以此绘得结果如下：</p><p><img src="https://img.enderfga.cn/img/image-20220320160304205.png" alt=""></p><p><strong>b)</strong> </p><script type="math/tex; mode=display">计算齐次变换矩阵 T_{s a} 和 T_{s b} 。</script><script type="math/tex; mode=display">\because T=\left[\begin{array}{cc}R & p \\0 & 1\end{array}\right]\\\therefore T_{sa}=\left[\begin{array}{cccc}0 & -1 & 0 & 3 \\0 & 0 & -1 & 0 \\1 & 0 & 0 & 0 \\0 & 0 & 0 & 1\end{array}\right]\\T_{sb}=\left[\begin{array}{cccc}1 & 0 & 0 & 0 \\0 & 0 & 1 & 2 \\0 & -1 & 0 & 0 \\0 & 0 & 0 & 1\end{array}\right]</script><p><strong>c)</strong> </p><script type="math/tex; mode=display">已知 T_{s b}, 在不使用逆矩阵的情况下计算 T_{s b}^{-1}, 并验证坐标系画的是否正确。</script><script type="math/tex; mode=display">\because T^{-1}=\left[\begin{array}{cc}R & p \\0 & 1\end{array}\right]^{-1}=\left[\begin{array}{cc}R^{\mathrm{T}} & -R^{\mathrm{T}} p \\0 & 1\end{array}\right]\\\therefore T_{sb}^{-1}=\left[\begin{array}{cccc}1 & 0 & 0 & 0 \\0 & 0 & -1 & 0 \\0 & 1 & 0 & -2 \\0 & 0 & 0 & 1\end{array}\right]</script><p>可知坐标系绘画正确。</p><p><strong>d)</strong> </p><script type="math/tex; mode=display">已知 T_{s a} 和 T_{s b}, 计算 T_{a b}, 并验证坐标系画的是否正确。</script><script type="math/tex; mode=display">\because T_{as}=T_{sa}^{-1}=\left[\begin{array}{cccc}0 & 0 & 1 & 0 \\-1 & 0 & 0 & 3 \\0 & -1 & 0 & 0 \\0 & 0 & 0 & 1\end{array}\right]\\\therefore T_{ab}=T_{as}T_{sb}=\left[\begin{array}{cccc}0 & -1 & 0 & 0 \\-1 & 0 & 0 & 3 \\0 & 0 & -1 & -2 \\0 & 0 & 0 & 1\end{array}\right]</script><p>可知坐标系绘画正确。</p><p><strong>e)</strong> </p><script type="math/tex; mode=display">将 T=T_{s b} 作为变换算子, 表示绕\hat{x}轴转动-90^{\circ} 与沿 \hat{y} 移动 2 个单位。计算T_{1}= T_{s a} T 与 T_{2}=T T_{s a}, 并回答新姿态 T_{1} 与 T_{2} 分别对应的是 T_{s a} 绕哪个坐标系的变换得到的结果?</script><script type="math/tex; mode=display">T_{1}=T_{sa}T=\left[\begin{array}{cccc}0 & 0 & -1 & 1 \\0 & 1 & 0 & 0 \\1 & 0 & 0 & 0 \\0 & 0 & 0 & 1\end{array}\right]，相对 \{a\} 系变换得到的结果，表示绕\hat{x}_{a}轴转动-90^{\circ} 与沿 \hat{x} 移动 2 个单位。</script><script type="math/tex; mode=display">T_{2}=TT_{sa}=\left[\begin{array}{cccc}0 & -1 & 0 & 3 \\1 & 0 & 0 & 2 \\0 & 0 & 1 & 0 \\0 & 0 & 0 & 1\end{array}\right]，相对 \{s\} 系变换得到的结果，表示绕\hat{x}_{s}轴转动-90^{\circ} 与沿 \hat{x} 移动 3 个单位，沿 \hat{y} 移动 2 个单位。</script><p><strong>f)</strong> </p><script type="math/tex; mode=display">利用 T_{s b} 将点 p_{b}=(1,2,3) 从 \{b\} 系变换到 \{s\} 系。</script><script type="math/tex; mode=display">p_{s}=T_{sb}p_{b}=(1,5,-2)</script><p><strong>k)</strong> </p><script type="math/tex; mode=display">已知 \{s\} 系中一点 p_{s}=(1,2,3), 计算 p^{\prime}=T_{s b} p_{s} 和 p^{\prime \prime}=T_{s b}^{-1} p_{s} 。每一推导过程均可以解释为坐标变换 (无须移动点的位置) 或移动点的位置 (无须改变坐标系)。</script><script type="math/tex; mode=display">p^{\prime}=T_{s b} p_{s}=(1,5,-2)：其几何意义为移动点的位置；</script><script type="math/tex; mode=display">p^{\prime \prime}=T_{s b}^{-1} p_{s}=(1,-3,0)：其几何意义为坐标变换。</script><p><strong>g)</strong>已知 ${s}$ 系中的旋量 $\mathcal{V}=(3,2,1,-1,-2,-3)$, 计算其在 ${a}$ 系中的表示。</p><script type="math/tex; mode=display">\mathcal{V}_{a}=\left[\begin{array}{c}\omega_{a} \\v_{a}\end{array}\right]=\left[\begin{array}{cc}R^{\mathrm{T}} & 0 \\-R^{\mathrm{T}}[p] & R^{\mathrm{T}}\end{array}\right]\left[\begin{array}{c}\omega_{s} \\v_{s}\end{array}\right]=\left[\mathrm{Ad}_{T_{a s}}\right] \mathcal{V}_{s}\\\because \left[\mathrm{Ad}_{T}\right]=\left[\begin{array}{cc}R & 0 \\{[p] R} & R\end{array}\right] \in \mathbb{R}^{6 \times 6} \\\therefore \mathcal{V}_{a}=(1,-3,-2,-9,1,-1)</script><p><strong>h)</strong> 计算 $T_{s a}$ 的矩阵对数 $[\delta] \theta$, 并提取其中的元素: 单位螺旋轴 $\mathcal{S}$ 和转动量 $\theta$ 。</p><p>使用到的函数：</p><p>MatrixLog6：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs matlab"><span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">expmat</span> = <span class="hljs-title">MatrixLog6</span><span class="hljs-params">(T)</span></span><br>[R, p] = TransToRp(T);<br>omgmat = MatrixLog3(R);<br><span class="hljs-keyword">if</span> <span class="hljs-built_in">isequal</span>(omgmat, <span class="hljs-built_in">zeros</span>(<span class="hljs-number">3</span>))<br>    expmat = [<span class="hljs-built_in">zeros</span>(<span class="hljs-number">3</span>), T(<span class="hljs-number">1</span>: <span class="hljs-number">3</span>, <span class="hljs-number">4</span>); <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>];<br><span class="hljs-keyword">else</span><br>    theta = <span class="hljs-built_in">acos</span>((trace(R) - <span class="hljs-number">1</span>) / <span class="hljs-number">2</span>); <br>    expmat = [omgmat, (<span class="hljs-built_in">eye</span>(<span class="hljs-number">3</span>) - omgmat / <span class="hljs-number">2</span> ...<br>                      + (<span class="hljs-number">1</span> / theta - <span class="hljs-built_in">cot</span>(theta / <span class="hljs-number">2</span>) / <span class="hljs-number">2</span>) ...<br>                        * omgmat * omgmat / theta) * p;<br>              <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>];  <br><span class="hljs-keyword">end</span><br><span class="hljs-keyword">end</span><br></code></pre></td></tr></table></figure><p>se3ToVec：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs matlab"><span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">V</span> = <span class="hljs-title">se3ToVec</span><span class="hljs-params">(se3mat)</span></span><br>V = [se3mat(<span class="hljs-number">3</span>, <span class="hljs-number">2</span>); se3mat(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>); se3mat(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>); se3mat(<span class="hljs-number">1</span>: <span class="hljs-number">3</span>, <span class="hljs-number">4</span>)]; <br><span class="hljs-keyword">end</span><br></code></pre></td></tr></table></figure><p>AxisAng6：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs matlab"><span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-params">[S, theta]</span> = <span class="hljs-title">AxisAng6</span><span class="hljs-params">(expc6)</span></span><br>theta = norm(expc6(<span class="hljs-number">1</span>: <span class="hljs-number">3</span>));<br><span class="hljs-keyword">if</span> NearZero(theta)<br>    theta = norm(expc6(<span class="hljs-number">4</span>: <span class="hljs-number">6</span>));<br><span class="hljs-keyword">end</span><br>S = expc6 / theta;    <br><span class="hljs-keyword">end</span><br></code></pre></td></tr></table></figure><p>解得：</p><script type="math/tex; mode=display">[\delta] \theta=\left[\begin{array}{cccc}0 & -1.2092 & -1.2092 & 2.2092 \\1.2092 & 0 & -1.2092 & -2.2092 \\1.2092 & 1.2092 & 0 & -1.4184 \\0 & 0 & 0 & 0\end{array}\right]</script><p>单位螺旋轴   $\mathcal{S}=(0.5774,-0.5774,0.5774,1.0548,-1.0548,-0.6772)$ 和转动量 $\theta=2.0944$</p><p><strong>i)</strong> 计算与转动 $\mathcal{S} \theta=(0,1,2,3,0,0)$ 的指数坐标对应的矩阵指数。</p><p>使用到的函数：</p><p>VecTose3：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs matlab"><span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">se3mat</span> = <span class="hljs-title">VecTose3</span><span class="hljs-params">(V)</span></span><br>se3mat = [VecToso3(V(<span class="hljs-number">1</span>: <span class="hljs-number">3</span>)), V(<span class="hljs-number">4</span>: <span class="hljs-number">6</span>); <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>];<br><span class="hljs-keyword">end</span><br></code></pre></td></tr></table></figure><p>MatrixExp6：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs matlab"><span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">T</span> = <span class="hljs-title">MatrixExp6</span><span class="hljs-params">(se3mat)</span><span class="hljs-title">omgtheta</span> = <span class="hljs-title">so3ToVec</span><span class="hljs-params">(se3mat(1: 3, 1: 3)</span>);</span><br><span class="hljs-keyword">if</span> NearZero(norm(omgtheta))<br>    T = [<span class="hljs-built_in">eye</span>(<span class="hljs-number">3</span>), se3mat(<span class="hljs-number">1</span>: <span class="hljs-number">3</span>, <span class="hljs-number">4</span>); <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>];<br><span class="hljs-keyword">else</span><br>    [omghat, theta] = AxisAng3(omgtheta);<br>    omgmat = se3mat(<span class="hljs-number">1</span>: <span class="hljs-number">3</span>, <span class="hljs-number">1</span>: <span class="hljs-number">3</span>) / theta; <br>    T = [MatrixExp3(se3mat(<span class="hljs-number">1</span>: <span class="hljs-number">3</span>, <span class="hljs-number">1</span>: <span class="hljs-number">3</span>)), ...<br>         (<span class="hljs-built_in">eye</span>(<span class="hljs-number">3</span>) * theta + (<span class="hljs-number">1</span> - <span class="hljs-built_in">cos</span>(theta)) * omgmat ...<br>          + (theta - <span class="hljs-built_in">sin</span>(theta)) * omgmat * omgmat) ...<br>            * se3mat(<span class="hljs-number">1</span>: <span class="hljs-number">3</span>, <span class="hljs-number">4</span>) / theta;<br>         <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>];<br><span class="hljs-keyword">end</span><br><span class="hljs-keyword">end</span><br></code></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{aligned}T &=\left[\begin{array}{cccc}-0.6173 & -0.7037 & 0.3518 & 1.0555 \\0.7037 & -0.2938 & 0.6469 & 1.9407 \\-0.3518 & 0.6469 & 0.6765 & -0.9704 \\0 & 0 & 0 & 1\end{array}\right]\end{aligned}</script><p>3.目前工业机器人领域经常需要定义 4 种坐标系: 参考坐标系 ${a}$, 末端或工具坐标系 ${b}$ 、图像坐标系 ${c}$ 、件坐标系 ${d}$, 如下所示。</p><p><img src="https://img.enderfga.cn/img/image-20220314230530164.png" alt=""></p><p><strong>a)</strong> </p><script type="math/tex; mode=display">基于图中所给尺寸, 确定 T_{a d} 和 T_{c d} 。</script><script type="math/tex; mode=display">T_{ad}=\left[\begin{array}{cccc}1 & 0 & 0 & -1 \\0 & 1 & 0 & 1 \\0 & 0 & 1 & 0 \\0 & 0 & 0 & 1\end{array}\right]</script><script type="math/tex; mode=display">T_{cd}=\left[\begin{array}{cccc}0 & 1 & 0 & 0 \\1 & 0 & 0 & 0 \\0 & 0 & -1 & 2 \\0 & 0 & 0 & 1\end{array}\right]</script><p><strong>b)</strong> </p><script type="math/tex; mode=display">若T_{bc}=[1~~0~~0~~4；0~~1 ~~0~~0；0~~0~~1~~0；0~~0~~0~~1],求T_{ab}。</script><script type="math/tex; mode=display">T_{ab}=T_{ac}T_{cb}=T_{ad}T_{dc}T_{bc}^{-1}=T_{ad}T_{cd}^{-1}T_{bc}^{-1}\\=\left[\begin{array}{cccc}0 & 1 & 0 & -1 \\1 & 0 & 0 & -3 \\0 & 0 & -1 & 2 \\0 & 0 & 0 & 1\end{array}\right]</script><p><img src="https://cdn.mathpix.com/cropped/2022_03_20_515d600b91de346f2453g-1.jpg?height=1510&amp;width=1069&amp;top_left_y=122&amp;top_left_x=108" alt=""></p><p><img src="https://cdn.mathpix.com/cropped/2022_03_20_515d600b91de346f2453g-2.jpg?height=556&amp;width=999&amp;top_left_y=598&amp;top_left_x=116" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20220322133139136.png" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20220322133103015.png" alt=""></p>]]></content>
    
    
    
    <tags>
      
      <tag>机器人</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数据库原理 Exercises 1</title>
    <link href="/2022/03/17/data1/"/>
    <url>/2022/03/17/data1/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Database System Concepts Exercises of Chapter 2</p><span id="more"></span><p><img src="https://img.enderfga.cn/img/photo-1613068687893-5e85b4638b56" alt=""></p><h1 id="Database-System-Concepts-Exercises-of-Chapter-2"><a href="#Database-System-Concepts-Exercises-of-Chapter-2" class="headerlink" title="Database System Concepts Exercises of Chapter 2"></a>Database System Concepts Exercises of Chapter 2</h1><p><strong>Exercise 2.1</strong> Consider the relational database of Figure 2.14,</p><p>Employee( person_name, street, city)<br>Works (person_name, company_name, salary)<br>Company(company_name, city)<br><strong>Figure 2.14</strong></p><p>What are the appropriate primary keys?</p><p><strong>My answer:</strong></p><p>$employee (\underline{person-name}, street, city)$</p><p>$works (\underline{person-name}, company-name, salary) $</p><p>$company (\underline{company-name}, city)$</p><p><strong>Exercise</strong> <strong>2.7</strong> Consider the relational database of Figure 2.14. Given an expression in the relational algebra to express each of the following queries:</p><p>a.Find the names of all employees who live in city “Miami”</p><p>b.Find the names of all employees whose salary is greater than $100,000.</p><p>c.Find the names of all employees who live in “Miami” and whose salary is greater than $100,000.</p><p><strong>My answer:</strong></p><p><img src="https://img.enderfga.cn/img/image-20220318132151932.png" alt=""></p><p><strong>Exercise</strong> <strong>2.9</strong> Consider the bank database of Figure 2.15.</p><p>branch(branch_name, branch_city, assets)<br>customer(customer_name, customer_street, customer_city)<br>loan(loan_number, branch_name, amount)<br>borrower(customer_name, loan_number)<br>account(account_number, loan_number)<br>depositor(customer_name, account_number)<br><strong>Figure 2.15</strong></p><p>a.  What are the appropriate primary keys?</p><p>b. Given your choice of primary keys, identify appropriate foreign keys. Assume that branch names and customer names uniquely identify branches and customers, but loans and accounts can be associated with more than one customer.</p><p><strong>My answer:</strong></p><p>The primary keys are marked with an $\underline{underline}$, and the foreign keys are marked with a $\overline{overline}$.</p><p>$branch(\underline{branch-name}, branch-city, assets)$</p><p>$customer(\underline{customer-name}, customer-street, customer-city)$</p><p>$loan(\underline{loan-number}, \overline{branch-name}, amount)$</p><p>$borrower(\overline{\underline{customer-name}}, \overline{loan-number})$</p><p>$account(\underline{account-number}, loan-number)$</p><p>$depositor(\overline{\underline{customer-name}}, \overline{account-number})$</p>]]></content>
    
    
    
    <tags>
      
      <tag>数据库</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>计算机网络和因特网</title>
    <link href="/2022/03/14/net1/"/>
    <url>/2022/03/14/net1/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>第一章计算机网络和因特网小结</p><span id="more"></span><h2 id="1-简述面向连接和无连接两种服务的特点。"><a href="#1-简述面向连接和无连接两种服务的特点。" class="headerlink" title="1.简述面向连接和无连接两种服务的特点。"></a>1.<strong>简述面向连接和无连接两种服务的特点。</strong></h2><p>面向连接服务：质量可靠，确保从发送方发出的数据最终按顺序完整地交付给接收方。</p><p>无连接服务：质量不可靠，不能对最终交付作任何保证。</p><p><img src="https://img.enderfga.cn/img/image-20220313164646441.png" alt=""></p><p><strong>面向连接服务</strong>(connection-oriented)</p><p>面向连接服务具有<strong>可靠的数据传送，流控制，拥塞控制</strong>三个特性。</p><p>特点：</p><p>1、面向连接的数据传输过程必须经过建立连接、连接维护和释放连接的3个过程；</p><p>2、数据传输过程中，各分组不需要携带目的地址。</p><p>面向连接数据传输的收发数据顺序不变，因此传输的可靠性好，但协议复杂，通信效率不高。</p><p><strong>无连接服务</strong>(connectionless)</p><p>两个实体之间的通信<strong>不需要先建立好连接</strong>。是一种不可靠的服务。这种服务常被描述为“尽最大努力交付”(best effort delivery)或“尽力而为”。</p><p>特点：</p><p>1、数据传输过程中，每个分组都携带完整的目的地址，各分组在系统中是独立传送的。因此，无连接中的数据传输过程不需要经过3个过程；</p><p>2、由于无连接发送的不同的分组，可能经历不同路径到目的主机，所以先发送的不一定先到，因此无连接的数据分组传输过程中，目的主机接收的数据分组可能出现乱序、重复与丢失的现象。</p><p>无连接的可靠性不是很好，但因其省去许多保征机制，因此通信协议相对简单，通信效率较高。</p><h2 id="2-什么是多路复用？常分为哪两种类型。"><a href="#2-什么是多路复用？常分为哪两种类型。" class="headerlink" title="2.什么是多路复用？常分为哪两种类型。"></a>2.<strong>什么是多路复用？常分为哪两种类型。</strong></h2><p>多路复用是指在一条传输链路上同时建立多条连接，分别传输数据。</p><p>常分为以下两种类型：</p><p><strong>频分多路复用FDM(frequency-division multiplexing)</strong>：链路的频谱由跨越链路创建的连接所共享，按频率划分若干频段，每个频段专用于一个连接。</p><p><strong>时分多路复用TDM (time-division multiplexing)</strong> ：时间划分为固定区间的帧，每帧再划分为固定数量的时隙，每一个时隙专用于一个连接，用于传输数据。</p><p><img src="https://img.enderfga.cn/img/image-20220314223649714.png" alt=""></p><h2 id="3-简述电路交换和分组交换特点及工作过程。"><a href="#3-简述电路交换和分组交换特点及工作过程。" class="headerlink" title="3.简述电路交换和分组交换特点及工作过程。"></a>3.<strong>简述电路交换和分组交换特点及工作过程。</strong></h2><p><strong>特点：</strong></p><ol><li><strong>电路交换 (circuit switching)</strong><br>预留端到端资源：端系统之间通信路径上所需要的资源 (缓存，链路带宽)，建立连接；<br>发送方以恒定速率向接收方传送数据。<br>如，电话网络。</li><li><strong>分组交换(packet switching)</strong><br>不需要资源预留；<br>按需使用资源，可能要排队等待，同时有其它分组发送。<br>如，因特网。</li></ol><p><strong>工作过程：</strong></p><ol><li><p>电路交换：</p><p>在两台主机A、B之间创建一条专用的端到端连接，分别占用每条链路中的一条电路；</p><p>该连接获得链路带宽的1/n，进行通信。</p></li><li><p>分组交换：</p><p>源端将报文划分为较小的数据块（分组packet）；</p><p>每个分组通过一系列链路和分组交换机传送，直到目的端</p><p>目的端恢复原报文。</p></li></ol><p><img src="https://img.enderfga.cn/img/image-20220314220812885.png" alt=""></p><h2 id="4-什么是协议？分层的服务模型？"><a href="#4-什么是协议？分层的服务模型？" class="headerlink" title="4.什么是协议？分层的服务模型？"></a>4.<strong>什么是协议？分层的服务模型？</strong></h2><p><strong>协议</strong>：控制网络中信息的发送和接收。定义了通信实体之间交换报文的格式和次序，以及在报文传输和/或接收或其他事件所采取的动作。</p><p><img src="https://img.enderfga.cn/img/image-20220314223735346.png" alt=""></p><p><strong>分层的服务模型</strong>：上层调用下层的服务，下层为上层提供服务。</p><p>通过第n层本身执行某些动作，或再使用其相邻下层（第n-1层）的服务，来完成向其上层（第n+1层）提供的服务。</p><h2 id="5-简述分层的特点和分层后数据的传递过程。"><a href="#5-简述分层的特点和分层后数据的传递过程。" class="headerlink" title="5.简述分层的特点和分层后数据的传递过程。"></a>5.<strong>简述分层的特点和分层后数据的传递过程。</strong></h2><p><strong>分层特点：</strong></p><ul><li>每层功能独立；</li><li>每两个相邻层之间有一逻辑接口，可交换信息；</li><li>上一层建立在下一层基础上，上一层可调用下一层的服务，下一层为上一层提供服务。</li></ul><p><strong>分层后数据的传递过程</strong>：主机（端系统）间数据传送实际上并不是在对等层间直接进行，而是通过相邻层间的传递合作完成。</p><p>发送方：将用户数据由高层向低层逐层传递，每经过一层，加上该层的控制信息，直到最低层（物理层），然后直接通过物理媒体传输到目的方。（逐层封装）</p><p>接收方：将收到的数据由低层向高层逐层传递，每经过一层，去掉该层的控制信息，直到最高层，恢复为用户数据。（逐层解封）</p><p><img src="https://img.enderfga.cn/img/image-20220314222653398.png" alt=""></p><h2 id="6-因特网分层模型及各层功能。"><a href="#6-因特网分层模型及各层功能。" class="headerlink" title="6.因特网分层模型及各层功能。"></a>6.<strong>因特网分层模型及各层功能。</strong></h2><p>因特网的协议栈由5个层次组成：物理层、链路层、网络层、运输层和应用层。</p><p><img src="https://img.enderfga.cn/img/image-20220314222941459.png" alt=""></p><ol><li>应用层：提供各种网络应用。传输应用报文。<br>FTP、 SMTP、 HTTP</li><li>运输层：在应用程序的客户机和服务器之间提供传输应用层报文服务。传输报文段。<br>TCP、 UDP</li><li>网络层：主机和主机之间传输网络层分组（数据报）。<br>IP协议、 选路协议</li><li>链路层： 在邻近单元之间传输数据（帧 ）。<br>PPP、以太网</li><li>物理层：在节点之间传输比特流。<br>传输媒体</li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>计算机网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>智能机器人技术——位形空间</title>
    <link href="/2022/03/02/robot1/"/>
    <url>/2022/03/02/robot1/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>智能机器人技术作业记录</p><span id="more"></span><p><img src="https://img.enderfga.cn/img/image-20220302214614790.png" alt=""></p><h1 id="智能机器人技术——位形空间"><a href="#智能机器人技术——位形空间" class="headerlink" title="智能机器人技术——位形空间"></a>智能机器人技术——位形空间</h1><p>一、选择/填空题（10 分）。</p><ol><li>机器人的自由度是 (<strong>D</strong>) ?<br>A. 机器人上点的数量<br>B. 机器人关节数量<br>C. 组成机器人的刚体的数量<br>D. 组成机器人的刚体的数量, 减去刚体间独立约束的数量</li><li>二维平面刚体的自由度为 (<strong>3</strong>) ； 三维空间刚体的自由度为 (<strong>6</strong>) 。</li><li>根据课堂上推算三维空间内刚体自由度的方法, 推算出四维空间中刚体的自由度 (<strong>10</strong>)、有关角度的自由度 (<strong>6</strong>)、有关平移位置的自由度 (<strong>4</strong>)。(如, 三维空间中分别为 $6,3,3$ )</li><li>假设你的手臂（从肩膀到手掌）, 有 7 个自由度。你如同一位服务生一样水平端着餐盘, 防止洒出酒水。你的手臂此时有几个自由度：<strong>5</strong>， 这个任务空间的自由度是：<strong>4</strong>。</li></ol><p>二、简答题, 请写出解题过程 (10 分)。</p><ol><li><p>考虑两个刚体之间的一个关节。每个刚体有 $\mathrm{m}$ 个自由度（二维空间刚体 $m=3$, 三维空间刚体 $m=6$ ), 并且没有任何约束。关节有 $f$ 个自由度（旋转关节 $\mathrm{f}=1$, 球形关节 $\mathrm{f}=3$ 等)。试问, 用这个关节关联两个刚体, 那么引入了多少个约束（一个刚体相对于另一个, 用字母表示）?</p><p><img src="https://img.enderfga.cn/img/image-20220302212205280.png" alt=""></p><p><strong>解：$自由度f+平面约束c=平面自由度3，自由度f＋空间约束c=空间自由度6$</strong></p><p><strong>故$\pmb{c=-f+m}$</strong></p></li><li><p>考虑一个机构包含了 3 个三维空间刚体 (记住, 包括地面, 所以 $\mathrm{N}=4$ ), 和 4 个关节: 1 个转动, 1 个平移, 一个万向, 一个球形。使用 Grubler 公式, 计算机构的自由度。</p><p><strong>解：转动副$f_1=1$，移动副$f_2=1$，万向铰$f_3=2$，球形铰$f_4=3$</strong></p><p><strong>又$\because N=4$，$J=4$，$m=6$</strong></p><p><strong>$\therefore \pmb{dof=m(N-1-j)+\sum_{i=1}^Jf_i=6\times\left(4-1-4\right)+\left(1+1+2+3\right)=1}$</strong></p></li><li>如下图的 SRS (球形-转动-球形) 机构, 正在抓取一个物体。试问, 当机构紧握物体时 (物体与机构中的机械臂最后一段没有相对运动时), 自由度是多少?</li></ol><p><img src="https://img.enderfga.cn/img/2022_02_28_40f28045fe8caa2a438eg-1.jpg" alt=""></p><p><strong>解：Spherical Joint即为球形铰$f=3$，共8个；Revolute Joint即为转动副$f=1$，共4个；</strong></p><p><strong>又$\because N=14$，$J=16$，$m=6$</strong></p><p>$\therefore \pmb{dof=m(N-1-j)+\sum_{i=1}^Jf_i=6\times\left(14-1-16\right)+\left(3\times8+1\times4\right)=10}$</p><ol><li><p>同上题, 如果现在有 $n$ 条这样的机械臂 (题 3 中 $n=4$ ), 机构的自由度是?</p><p><strong>同理：$\pmb{dof=m(N-1-j)+\sum_{i=1}^Jf_i=6\times\left(3n+2-1-4n\right)+\left(3\times2n+1\times n\right)=n+6}$</strong></p></li><li><p>同上题, 假设 $n$ 条机械臂的转动关节, 被替换成了万向关节, 机构的自由度是?</p><p><strong>同理：$\pmb{dof=m(N-1-j)+\sum_{i=1}^Jf_i=6\times\left(3n+2-1-4n\right)+\left(3\times2n+2\times n\right)=2n+6}$</strong></p></li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>机器人</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2022’summary</title>
    <link href="/2021/12/31/2022/"/>
    <url>/2021/12/31/2022/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="https://img.enderfga.cn/img/image-20211231192304439.png" alt=""></p><p>这是2021年发的第一条朋友圈配图，顺手拿来总结吧。</p><span id="more"></span><p>没想到曾经一个因为不怎么使用微信，不怎么看朋友圈而饱受困扰的人会用看朋友圈来回顾自己的一整年。</p><p>毕竟有感而发，这次就不纠结排版美不美观、结构严不严谨了。</p><h1 id="天时人事日相催，冬至阳生春又来"><a href="#天时人事日相催，冬至阳生春又来" class="headerlink" title="天时人事日相催，冬至阳生春又来"></a>天时人事日相催，冬至阳生春又来</h1><p>21的一月初我应该有在努力复习吧，因为一整个学期的怠惰，因为对一个优秀女生的追逐。大学跟高中差别真大，没有人逼我写必刷题了，没有人逼我额外补课了，没有所谓火箭班了。大一上虽然大部分都在20年这个区间，但是给我的21年真是奠定了乱七八糟的基础。</p><p>1月6号那天开始听YOASOBI的歌。</p><p><img src="https://img.enderfga.cn/img/image-20211231193810905.png" alt=""></p><p>我的“好友”为我做的图，可惜这位好友不再联系了，倒是图里的内容实现了。</p><p>放假回家了，回了一趟华美，可惜看到华美帝国回忆录的共享文档的时候我还是写不出东西来。</p><h1 id="鸿雁长飞光不度，鱼龙潜跃水成文"><a href="#鸿雁长飞光不度，鱼龙潜跃水成文" class="headerlink" title="鸿雁长飞光不度，鱼龙潜跃水成文"></a>鸿雁长飞光不度，鱼龙潜跃水成文</h1><p>在二月终于和大鸟转转转酒吧的兄弟们会师了，22年的美食王国的勇者传说继续辉煌！可能这是目前我唯一期待的年度团建了吧，一群我可以无条件信任的人。</p><p>这个月里跟了好多人出去玩，多年未见的童年好友，帮我很多的同乡师兄，关系超好的高中同学···（词穷了，就这样吧）</p><p>反正过了个年，又开学了！</p><p>甚至不记得为什么留下这段感慨：</p><p>凡事都有定期。天下万物都有定时，生有时，死有时，哭有时，笑有时，寻找有时，放手有时。</p><p>好消息是1204棋牌中心初具雏形，为后来的金碧辉煌作了铺垫。</p><h1 id="空里流霜不觉飞，汀上白沙看不见"><a href="#空里流霜不觉飞，汀上白沙看不见" class="headerlink" title="空里流霜不觉飞，汀上白沙看不见"></a>空里流霜不觉飞，汀上白沙看不见</h1><p>三月里买了我的宝贝surface，那我肯定开始家教了，虽然我天天骂它玩4399都卡，但确实给我带来了极大便利，不管去哪都只带着平板可比背着游戏本的习武之人好多了。</p><p><img src="https://img.enderfga.cn/img/image-20211231195659935.png" alt=""></p><p>在这个时候参加了大创，虽然可能自己完全没有做出什么东西的能力来，但那个星期还是挺震撼的，原来科研离我并不远。毕竟我也梦想成为拉格多科学院的“疯子”。</p><p>这个月里还参加了几次团建，文体组、智协、微软俱乐部（虽然改名了）···大一参加的每一个社团或组织都让我收获了好多好多，知识也好，朋友也好，不禁感慨，还好我参加了。</p><p>三月十号那天还认识了一个special的人，感谢大物，缘分真奇妙。</p><p>虽然我貌似有很多的朋友，但事实上我是一个很害怕交友的人。害怕我的所作所为会对其他人的人生轨迹造成影响（确实怪中二的）。</p><p>不过好在大部分时候都是正面影响，希望我的朋友毕业后，就业后，10年后，50年后还是我的朋友，在我的轨迹留多点痕迹。</p><h1 id="江水流春去欲尽，江潭落月复西斜"><a href="#江水流春去欲尽，江潭落月复西斜" class="headerlink" title="江水流春去欲尽，江潭落月复西斜"></a>江水流春去欲尽，江潭落月复西斜</h1><p>四月里我开始读诗了！语文老师她看了肯定很惊讶，为什么语文成绩这么差的我居然还会对这些感兴趣，感谢老师的滕王阁序。</p><p>这个月里我应该有好好在写python，matlab，c++和数据结构与算法吧，学基础的编程语言让人充满了成就感。</p><p>这个月最特殊的事应该是认识了another special people，刚好前后差了一个月。</p><p>虽然目前是我生活里最重要的人们，但是真的给我带来了许多的”痛苦“！烦死了！希望来年你们可以不要那么傻，但是请继续傻乐下去。</p><h1 id="谁家今夜扁舟子，何处相思明月楼"><a href="#谁家今夜扁舟子，何处相思明月楼" class="headerlink" title="谁家今夜扁舟子，何处相思明月楼"></a>谁家今夜扁舟子，何处相思明月楼</h1><p>五月Enderfga’s blog 建站了！虽然产生了没几篇高质量内容，但是反正是我的，我说了算！</p><p>看起来这个月里我的朋友圈大部分都是吃吃喝喝，参加了一些小比赛，</p><p>总是会产生一些，我学会了好多东西~然后过了一段时间就发现，原来的自己好傻，连时间复杂度都不知道是什么，连flag都没有听说过。</p><h1 id="玉户帘中卷不去，捣衣砧上拂还来"><a href="#玉户帘中卷不去，捣衣砧上拂还来" class="headerlink" title="玉户帘中卷不去，捣衣砧上拂还来"></a>玉户帘中卷不去，捣衣砧上拂还来</h1><p>这个月我生日了，收到了好多好多礼物呀，虽然我还是会觉得，某一天不会和它的昨天与明天有所区别，但是被大家重视的一天就另当别论了。</p><p>这个月还停止前面说的追逐，不过即使这样还是要告诉自己：人间一趟，积极向上！</p><p>学弟学妹参加了高考，他们信心满满；而我还在为不挂科努力。</p><h1 id="春江潮水连海平，海上明月共潮生"><a href="#春江潮水连海平，海上明月共潮生" class="headerlink" title="春江潮水连海平，海上明月共潮生"></a>春江潮水连海平，海上明月共潮生</h1><p>暑假没有回家，认真打工的七月。</p><p>一天12小时的家教，给我未来的体重激增奠定了经济基础。</p><p>总是会回忆一下人，即使物是人非。</p><p>b站前一段时间是不是火了随机挑战来着？我的七月十七号早知道应该拍成视频，全靠掷骰子随机出发，没想到最后还能到关山月美术馆，莲花山和深圳湾。</p><h1 id="斜月沉沉藏海雾，碣石潇湘无限路"><a href="#斜月沉沉藏海雾，碣石潇湘无限路" class="headerlink" title="斜月沉沉藏海雾，碣石潇湘无限路"></a>斜月沉沉藏海雾，碣石潇湘无限路</h1><p>整个八月估计都在为奥运热血沸腾，16年12年08年的我都没有感受到奥运的魅力。</p><p><img src="https://img.enderfga.cn/img/image-20211231202939847.png" alt=""></p><p>当然还有大吃大喝跟兄弟们的沙雕你画我猜。</p><h1 id="不知江月待何人，但见长江送流水"><a href="#不知江月待何人，但见长江送流水" class="headerlink" title="不知江月待何人，但见长江送流水"></a>不知江月待何人，但见长江送流水</h1><p>又开学啦····</p><p>我变成学长了，迎新了好多学弟学妹。</p><p>这个月我估计有点想恋爱了，转了很多有的没的。</p><p>暑假赚钱的心愿：请我的好朋友们吃大餐</p><p>开始在这个月一步步实现了！（钱包肉疼）</p><h1 id="禁街箫鼓，寒轻夜永"><a href="#禁街箫鼓，寒轻夜永" class="headerlink" title="禁街箫鼓，寒轻夜永"></a>禁街箫鼓，寒轻夜永</h1><p>国庆爱hanser五周年纪念日！</p><p>第一次跟朋友们出去旅游的国庆假期，有美女美食相伴的日子比在自习室打代码快乐多了。</p><p>越来越胖了，有钱导致我吃的太好，我应该破产的。</p><p>怎么我乐于助人，你们就乐于送我吃蛋糕奶茶呀，这不好！</p><h1 id="凝霜夜下拂罗衣，浮云中断开明月"><a href="#凝霜夜下拂罗衣，浮云中断开明月" class="headerlink" title="凝霜夜下拂罗衣，浮云中断开明月"></a>凝霜夜下拂罗衣，浮云中断开明月</h1><p>十一月不就是上个月吗？我在干嘛？</p><p>我在陪重要的人吃吃喝喝还有傻乐。</p><p>希望我来年总结的时候我还是在陪重要的人吃喝傻乐，不过得多点努力学习。怎么会有这么懒的人，就知道玩动森···</p><p>十二号那天在大教室里演讲了，感觉自己从小就很有表现欲，奈何没有任何技能，每一次上台前都手抖流汗，</p><p>但是我记得那天我讲的很满意，我自己很满意，有进步就好。</p><p><img src="https://img.enderfga.cn/img/image-20211231204209953.png" alt=""></p><h1 id="愿我如星君如月，夜夜流光相皎洁"><a href="#愿我如星君如月，夜夜流光相皎洁" class="headerlink" title="愿我如星君如月，夜夜流光相皎洁"></a>愿我如星君如月，夜夜流光相皎洁</h1><p><img src="https://img.enderfga.cn/img/image-20211231204818624.png" alt=""></p><p>12月就不单独总结了，总结全年吧！</p><p>本人2021年年度成就总结:</p><ol><li>学术方面，凭借个人努力，在核酸检测领域产出多份数据真实详尽的报告。</li><li>健康方面，保证膳食纤维摄入，具体表现为每日坚持吃瓜，吃好瓜，吃大瓜。</li><li>商业方面，与各大平台合作，全面参与投资618双11、双12等千亿级重大项目。</li><li>环保方面，股票基金一片绿，绿水青山就是金山银山。在废物利用领域更是成绩斐然:自己作为废物，常常被别人利用。</li><li>运动方面，专注于水上项目，在摸鱼、划水等小项上有突出表现。</li><li>信仰方面，全心全意坚持转发锦鲤不动摇。</li></ol><p>算了不玩梗了。</p><p>这一年有什么值得回顾的吗？</p><p>也许应该是友谊吧，从高中到大学，相识6，7年的朋友们不在了，跳出舒适圈，结识了另外一批可爱的人。</p><p>本来不想花时间写这种东西的，会觉得“你的总结关别人什么事，写出来谁会看呀”。</p><p>后来想了想，真有道理，我应该写给明年31号的自己。</p><p>喂，桂安，你有没有记得刷LeetCode，今天学习强国了吗？和朋友们的关系好不，有没有给他们带来快乐？有给学弟学妹做个好榜样没？</p><p>linux用惯没，c++/python/matlab还记得怎么用吗？绩点到5没？体重下来没？······</p><p><img src="https://v2.jinrishici.com/one.svg" alt=""></p><p>2021最大的收获是结识了读到这里的你，谢谢~</p>]]></content>
    
    
    
    <tags>
      
      <tag>闲谈</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>wsl安装与开发环境搭建</title>
    <link href="/2021/11/26/wsl/"/>
    <url>/2021/11/26/wsl/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>wsl2，vscode，windows terminal，zsh，docker····</p><span id="more"></span><h1 id="Windows-Subsystem-for-Linux"><a href="#Windows-Subsystem-for-Linux" class="headerlink" title="Windows Subsystem for Linux"></a>Windows Subsystem for Linux</h1><p>首先贴一个<a href="https://docs.microsoft.com/zh-cn/windows/wsl/install">官方文档</a></p><h2 id="什么是适用于-Linux-的-Windows-子系统"><a href="#什么是适用于-Linux-的-Windows-子系统" class="headerlink" title="什么是适用于 Linux 的 Windows 子系统"></a>什么是适用于 Linux 的 Windows 子系统</h2><p>适用于 Linux 的 Windows 子系统可让开发人员按原样运行 GNU/Linux 环境 - 包括大多数命令行工具、实用工具和应用程序 - 且不会产生传统虚拟机或双启动设置开销。</p><ul><li><a href="https://aka.ms/wslstore">在 Microsoft Store</a> 中选择你偏好的 GNU/Linux 分发版。</li><li>运行常用的命令行软件工具（例如 <code>grep</code>、<code>sed</code>、<code>awk</code>）或其他 ELF-64 二进制文件。</li><li>运行 Bash shell 脚本和 GNU/Linux 命令行应用程序，包括：<ul><li>工具：vim、emacs、tmux</li><li>语言：<a href="https://docs.microsoft.com/zh-cn/windows/nodejs/setup-on-wsl2">NodeJS</a>、Javascript、<a href="https://docs.microsoft.com/zh-cn/windows/python/web-frameworks">Python</a>、Ruby、C/ C++、C# 与 F#、Rust、Go 等。</li><li>服务：SSHD、<a href="https://docs.microsoft.com/zh-cn/windows/wsl/tutorials/wsl-database">MySQL</a>、Apache、lighttpd、<a href="https://docs.microsoft.com/zh-cn/windows/wsl/tutorials/wsl-database">MongoDB</a>、<a href="https://docs.microsoft.com/zh-cn/windows/wsl/tutorials/wsl-database">PostgreSQL</a>。</li></ul></li><li>使用自己的 GNU/Linux 分发包管理器安装其他软件。</li><li>使用类似于 Unix 的命令行 shell 调用 Windows 应用程序。</li><li>在 Windows 上调用 GNU/Linux 应用程序。</li></ul><h2 id="什么是-WSL-2？"><a href="#什么是-WSL-2？" class="headerlink" title="什么是 WSL 2？"></a>什么是 WSL 2？</h2><p>WSL 2 是适用于 Linux 的 Windows 子系统体系结构的一个新版本，它支持适用于 Linux 的 Windows 子系统在 Windows 上运行 ELF64 Linux 二进制文件。 它的主要目标是 <strong>提高文件系统性能</strong>，以及添加 <strong>完全的系统调用兼容性</strong>。</p><p>这一新的体系结构改变了这些 Linux 二进制文件与Windows 和计算机硬件进行交互的方式，但仍然提供与 WSL 1（当前广泛可用的版本）中相同的用户体验。</p><p>单个 Linux 分发版可以在 WSL 1 或 WSL 2 体系结构中运行。 每个分发版可随时升级或降级，并且你可以并行运行 WSL 1 和 WSL 2 分发版。 WSL 2 使用全新的体系结构，该体系结构受益于运行<strong>真正</strong>的 Linux 内核。</p><h2 id="WSL-2安装"><a href="#WSL-2安装" class="headerlink" title="WSL 2安装"></a>WSL 2安装</h2><p>需要CPU开启VT（Virtualization Technology），这一步根据CPU不同操作方式不同就不细说了。</p><p>需要先启用“适用于 Linux 的 Windows 子系统”可选功能，然后才能在 Windows 上安装 Linux 分发。</p><p>以管理员身份打开 PowerShell 并运行：</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs powershell">dism.exe /online /<span class="hljs-built_in">enable-feature</span> /featurename:Microsoft<span class="hljs-literal">-Windows</span><span class="hljs-literal">-Subsystem</span><span class="hljs-literal">-Linux</span> /all /norestart<br></code></pre></td></tr></table></figure><p>若要更新到 WSL 2，需要运行 Windows 10。</p><ul><li>对于 x64 系统：<strong>版本 1903</strong> 或更高版本，采用 <strong>内部版本 18362</strong> 或更高版本。</li><li>对于 ARM64 系统：<strong>版本 2004</strong> 或更高版本，采用 <strong>内部版本 19041</strong> 或更高版本。</li><li>低于 18362 的版本不支持 WSL 2。 使用 <a href="https://www.microsoft.com/software-download/windows10">Windows Update 助手</a>更新 Windows 版本。</li></ul><p>若要检查 Windows 版本及内部版本号，选择 Windows 徽标键 + R，然后键入“winver”，选择“确定”。 更新到“设置”菜单中的<a href="ms-settings:windowsupdate">最新 Windows 版本</a>。</p><p><img src="https://img.enderfga.cn/img/image-20211126110320999.png" alt=""></p><p>安装 WSL 2 之前，必须启用“虚拟机平台”可选功能。 计算机需要<a href="https://docs.microsoft.com/zh-cn/windows/wsl/troubleshooting#error-0x80370102-the-virtual-machine-could-not-be-started-because-a-required-feature-is-not-installed">虚拟化功能</a>才能使用此功能。</p><p>以管理员身份打开 PowerShell 并运行：</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs powershell">dism.exe /online /<span class="hljs-built_in">enable-feature</span> /featurename:VirtualMachinePlatform /all /norestart<br></code></pre></td></tr></table></figure><p><img src="https://img.enderfga.cn/img/image-20211126110900611.png" alt=""></p><p><strong>重新启动</strong> 计算机，以完成 WSL 安装并更新到 WSL 2。</p><p>重启之后<a href="https://wslstorestorage.blob.core.windows.net/wslblob/wsl_update_x64.msi">下载安装</a> Linux 内核更新包并将 WSL 2 设置为默认版本。</p><p><img src="https://img.enderfga.cn/img/image-20211126111436728.png" alt=""></p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs powershell">wsl -<span class="hljs-literal">-set</span><span class="hljs-literal">-default</span><span class="hljs-literal">-version</span> <span class="hljs-number">2</span><br></code></pre></td></tr></table></figure><p><img src="https://img.enderfga.cn/img/image-20211126111557676.png" alt=""></p><p>最后，打开 <a href="https://aka.ms/wslstore">Microsoft Store</a>，并选择你偏好的 Linux 分发版。</p><p>单击以下链接会打开每个分发版的 Microsoft Store 页面：</p><ul><li><a href="https://www.microsoft.com/store/apps/9N9TNGVNDL3Q">Ubuntu 18.04 LTS</a></li><li><a href="https://www.microsoft.com/store/apps/9n6svws3rx71">Ubuntu 20.04 LTS</a></li><li><a href="https://www.microsoft.com/store/apps/9NJFZK00FGKV">openSUSE Leap 15.1</a></li><li><a href="https://www.microsoft.com/store/apps/9MZ3D1TRP8T1">SUSE Linux Enterprise Server 12 SP5</a></li><li><a href="https://www.microsoft.com/store/apps/9PN498VPMF3Z">SUSE Linux Enterprise Server 15 SP1</a></li><li><a href="https://www.microsoft.com/store/apps/9PKR34TNCV07">Kali Linux</a></li><li><a href="https://www.microsoft.com/store/apps/9MSVKQC78PK6">Debian GNU/Linux</a></li><li><a href="https://www.microsoft.com/store/apps/9n6gdm4k2hnc">Fedora Remix for WSL</a></li><li><a href="https://www.microsoft.com/store/apps/9NV1GV1PXZ6P">Pengwin</a></li><li><a href="https://www.microsoft.com/store/apps/9N8LP0X93VCP">Pengwin Enterprise</a></li><li><a href="https://www.microsoft.com/store/apps/9p804crf0395">Alpine WSL</a></li><li><a href="https://www.microsoft.com/store/apps/9msmjqd017x7">Raft（免费试用版）</a></li></ul><p>首次启动新安装的 Linux 分发版时，将打开一个控制台窗口，系统会要求你等待一分钟或两分钟，以便文件解压缩并存储到电脑上。 未来的所有启动时间应不到一秒。</p><p>然后，需要为新的 Linux 分发版创建用户帐户和密码。</p><p>另外，以上内容貌似（？）可以使用以下一行命令解决</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">wsl --install -d Ubuntu-20.04<br></code></pre></td></tr></table></figure><p><img src="https://img.enderfga.cn/img/image-20211126142919333.png" alt=""></p><p>另外我发现用户名不能用大写，并且密码是不会显示出来的。</p><p><img src="https://img.enderfga.cn/img/image-20211126143140954.png" alt=""></p><h2 id="换源"><a href="#换源" class="headerlink" title="换源"></a>换源</h2><p>如果遇到下载速度较慢，或者下载失败等问题，我们还可以把官方源换成国内源。</p><p><strong>备份list文件</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">cd /etc/apt/<br>sudo cp sources.list sources.list.bak<br></code></pre></td></tr></table></figure><p>备份了，如果下面的哪一步出错了好恢复。</p><p><strong>修改list文件</strong></p><p>管理员权限，使用 vim 进行修改：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo vim sources.list<br></code></pre></td></tr></table></figure><p>把想要更换的源复制在剪切板，这里以阿里源为例：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs shell">deb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse<br>deb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse<br>deb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse<br>deb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse<br>deb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse<br>deb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse<br>deb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse<br>deb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse<br>deb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse<br>deb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse<br></code></pre></td></tr></table></figure><p>我们先通过按<strong>ggdG</strong>这几个字母将里面的内容全部删除，</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plain">通过按下gg后发现光标移动到文件首行了。其中，gg为跳转到文件首行；dG为删除光标所在行以及其下所有行的内容。d为删除，G为跳转到文件末尾行。<br></code></pre></td></tr></table></figure><p>按鼠标右键会进行粘贴，然后输入    :wq!    进行退出与保存。</p><p><img src="https://img.enderfga.cn/img/image-20211126135607014.png" alt=""></p><p>最后复制这两条命令进行更新镜像源列表。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo apt-get update<br>sudo apt-get upgrade<br></code></pre></td></tr></table></figure><h2 id="WSL-迁移"><a href="#WSL-迁移" class="headerlink" title="WSL 迁移"></a>WSL 迁移</h2><p>开发用久了c盘爆满，所以建议大家安装好之后立即进行迁移</p><ol><li>查看Ubuntu 版本号</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">wsl -l<br></code></pre></td></tr></table></figure><ol><li><p>在D盘新建一个文件夹，我的路径是D:\wslubuntu</p></li><li><p>一步一步执行以下命令行。其中，Ubuntu-18.04是我的Ubuntu版本，大家可以根据wsl-l的版本号自行修改</p></li></ol><p>Step1: 导出</p><p>Step2: 注销原来的</p><p>Step3: 导入</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">wsl --export Ubuntu-18.04 d://wslubuntu//ubuntu-18.04.tar<br>wsl --unregister Ubuntu-18.04<br>wsl --import Ubuntu-18.04 d://wslubuntu d://wslubuntu//ubuntu-18.04.tar<br></code></pre></td></tr></table></figure><p>不过在系统设置里有更简洁的移动方法，这两种我都试过且都失败/成功过，一次不行就再来一遍</p><p><img src="https://img.enderfga.cn/img/image-20220530133102447.png" alt=""></p><h1 id="Windows-terminal-vscode"><a href="#Windows-terminal-vscode" class="headerlink" title="Windows terminal+vscode"></a>Windows terminal+vscode</h1><p><a href="https://www.microsoft.com/en-us/p/windows-terminal/9n0dx20hk701?rtc=1&amp;activetab=pivot:overviewtab">Windows Terminal</a> <a href="https://docs.microsoft.com/en-us/windows/terminal/get-started">https://docs.microsoft.com/en-us/windows/terminal/get-started</a></p><p>VS Code <a href="https://code.visualstudio.com">https://code.visualstudio.com</a></p><p>这两样软件可以大幅提高效率（还很装逼很好看）</p><p>windows terminal各项设置可以实现许多使用功能，比如我设置了默认启动Ubuntu，添加了git bash，透明亚克力效果等（本次不介绍，感兴趣自行研究）</p><p><img src="https://img.enderfga.cn/img/image-20211126112846563.png" alt=""></p><p>可以在工作区文件夹内右键然后在windows终端打开</p><p><img src="https://img.enderfga.cn/img/image-20211126112958904.png" alt=""></p><p>也可以直接打开window终端，cd到对应文件夹，然后输入</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs powershell">code .<br></code></pre></td></tr></table></figure><p><img src="https://img.enderfga.cn/img/image-20211126113402308.png" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20211126113507900.png" alt=""></p><p>至此就可以实现在windows环境下编程，在linux环境下测试。</p><h1 id="Docker"><a href="#Docker" class="headerlink" title="Docker"></a>Docker</h1><p>使用以下脚本可以实现自动安装。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> install docker</span><br>curl -fsSL get.docker.com -o get-docker.sh<br>sh get-docker.sh<br><br>if [ ! $(getent group docker) ];<br>then <br>    sudo groupadd docker;<br>else<br>    echo &quot;docker user group already exists&quot;<br>fi<br><br>sudo gpasswd -a $USER docker<br>sudo service docker restart<br><br>rm -rf get-docker.sh<br></code></pre></td></tr></table></figure><p><img src="https://img.enderfga.cn/img/image-20211126133921891.png" alt=""></p><p>可以直接复制到txt中，然后修改文件类型跟文件名为install-docker.sh（后缀是sh，名字任意）</p><p>例如我在桌面放置了该脚本，右键打开终端之后输入以下命令即可完成安装。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">sh install-docker.sh<br></code></pre></td></tr></table></figure><p><img src="https://img.enderfga.cn/img/image-20211126140126640.png" alt=""></p><p>接下来输入：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo service docker start<br>docker version<br></code></pre></td></tr></table></figure><p>就可以启动服务，查看版本，证明我们已经安装成功。</p><p><img src="https://img.enderfga.cn/img/image-20211126140532065.png" alt=""></p><p>如果不希望每次都特地启动docker的服务可以使用以下命令设置自启动：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo systemctl enable docker<br></code></pre></td></tr></table></figure><p>关于docker的使用就大家自己研究啦。</p><h1 id="On-my-zsh"><a href="#On-my-zsh" class="headerlink" title="On-my-zsh"></a>On-my-zsh</h1><p>一个美观且功能强大的终端</p><p><img src="https://img.enderfga.cn/img/image-20211126145704636.png" alt=""></p><p>用windows terminal启动Ubuntu，输入以下命令安装：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo apt-get install -y zsh<br></code></pre></td></tr></table></figure><p>oh-my-zsh中整理了常用的zsh<a href="https://github.com/ohmyzsh/ohmyzsh/tree/master/plugins">扩展</a>和<a href="https://github.com/ohmyzsh/ohmyzsh/wiki/Themes">主题</a>,所以先安装oh-my-zsh</p><p>使用curl安装 :</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">sh -c &quot;$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)&quot;<br></code></pre></td></tr></table></figure><p>使用wget安装：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">sh -c &quot;$(wget https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh -O -)&quot;<br></code></pre></td></tr></table></figure><p>虽然我列出了上面两条命令，但最好还是看看<a href="https://ohmyz.sh/#install">官网</a>的安装页面，确保命令的最新版本。</p><p><img src="https://img.enderfga.cn/img/image-20211126150308574.png" alt=""></p><p>接下来输入：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">cd<br>code .zshrc<br></code></pre></td></tr></table></figure><p>就可以cd到根目录文件夹，用vscode的可视化界面编辑配置文件，添加需要的扩展与主题了。</p><p>扩展与主题根据个人需求添加，本文不再赘述。</p><h1 id="Python开发"><a href="#Python开发" class="headerlink" title="Python开发"></a>Python开发</h1><p>花里胡哨那么多东西了，讲一点实战（假的）。</p><p>默认Ubuntu已经安装了python，我们只需要安装pip，就可以进行一些简单的编程。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo apt-get install python3-pip<br></code></pre></td></tr></table></figure><p><img src="https://img.enderfga.cn/img/image-20211126151809809.png" alt=""></p><p>当然我们不能局限于简单的编程，我们需要<strong>创建虚拟环境</strong>，确保各个环境互相隔离，互不影响。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo apt-get install python3-venv<br></code></pre></td></tr></table></figure><p>创建虚拟环境：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo python3 -m venv env<br></code></pre></td></tr></table></figure><p>激活与退出：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">source env/bin/activate<br><br>deactivate<br></code></pre></td></tr></table></figure><p><img src="https://img.enderfga.cn/img/image-20211126181729703.png" alt=""></p><p>接下来就可以尽情地pip install了，在工作环境下新建文件夹，再用vscode打开，即可开始编程。</p><h1 id="WSL-Ubuntu安装中文语言"><a href="#WSL-Ubuntu安装中文语言" class="headerlink" title="WSL-Ubuntu安装中文语言"></a>WSL-Ubuntu安装中文语言</h1><ol><li>安装中文语言包</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo apt install language-pack-zh-hans<br></code></pre></td></tr></table></figure><ol><li>运行 <code>dpkg-reconfigure locales</code></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo dpkg-reconfigure locales<br></code></pre></td></tr></table></figure><ol><li>选择<strong>en_US.UTF-8</strong>和 <strong>zh_CN.UTF-8</strong> , 选择<strong>zh_CN.UTF-8</strong>为默认语言</li><li>安装字体管理工具<strong>fontconfig</strong></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo apt install fontconfig<br></code></pre></td></tr></table></figure><p>   5.安装Windows字体</p><p>复制windows的字体到 <code>/usr/share/fonts/</code>下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo cp -r /mnt/c/Windows/Fonts /usr/share/fonts/windows<br></code></pre></td></tr></table></figure><ol><li>刷新字体缓存</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">fc-cache -f -v<br></code></pre></td></tr></table></figure><ol><li><p>退出重进wsl</p><h1 id="安装ROS系统（记得版本对应！例如我的Ubuntu18-04-对应melodic）"><a href="#安装ROS系统（记得版本对应！例如我的Ubuntu18-04-对应melodic）" class="headerlink" title="安装ROS系统（记得版本对应！例如我的Ubuntu18.04 对应melodic）"></a>安装ROS系统（记得版本对应！例如我的Ubuntu18.04 对应melodic）</h1></li></ol><p>使用鱼香ros大佬的一键安装脚本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">wget http://fishros.com/install -O fishros &amp;&amp; . fishros<br></code></pre></td></tr></table></figure><p>上面那行是官方的，我运行不了，我根据内容用下面这种方式可以运行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">wget http://fishros.com/install -O fishros.sh &amp;&amp; sh fishros.sh<br></code></pre></td></tr></table></figure><p>如果上面那个打开失败，可以用下面这种方式</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs shell">mkdir -p /tmp/fishinstall/tools<br>wget http://fishros.com/install/install1s/install.py -O /tmp/fishinstall/install.py 2&gt;&gt;/dev/null <br>source /etc/profile<br>sudo apt install python3-distro python3-yaml -y<br>if [ $UID -eq 0 ];then<br>    apt-get install sudo <br>fi<br>sudo python3 /tmp/fishinstall/install.py<br>sudo rm -rf /tmp/fishinstall/<br>sudo rm fishros<br>. ~/.bashrc<br><br></code></pre></td></tr></table></figure><p>自己对照着<a href="https://wiki.ros.org/melodic/Installation/Ubuntu#Installation.2BAC8-Ubuntu.2BAC8-Sources.Configure_your_Ubuntu_repositories">官方文档</a>的顺序安装</p><p><img src="https://img.enderfga.cn/img/image-20220530134026598.png" alt=""></p><p>我使用的顺序是5（换源）→1（ROS）→3（rosdep）→4（环境配置）</p><p>出现了一些小bug都是因为zsh</p><p>如果你跟我一样用的zsh 补上一个</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">echo &quot;source /opt/ros/melodic/setup.zsh&quot; &gt;&gt; ~/.zshrc<br>source ~/.zshrc<br></code></pre></td></tr></table></figure><h1 id="安装图形界面"><a href="#安装图形界面" class="headerlink" title="安装图形界面"></a>安装图形界面</h1><p>1.安装 <a href="https://sourceforge.net/projects/vcxsrv/">VcXsrv</a></p><p>2.安装好后运行 “XLaunch”，在界面里选择 “one large window” ，“Display number” 设置为 0 ，然后其它选项默认，点击下一步，等出现这个界面，将Disable access control打上勾，之后点击下一页。</p><p><img src="https://img.enderfga.cn/img/image-20220529115936249.png" alt=""></p><p>点击 “Save Configuration” 保存设置，会让你保存一个 “config.xlaunch” 的配置文件快捷方式，下次运行这个快捷方式就不用重新设置了。点击完成会出现一个纯黑的窗口，把它挂在后台先不管。</p><p>3.安装xfce4</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo apt install -y xfce4<br></code></pre></td></tr></table></figure><ol><li><p>配置变量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 首先需要查看Windows系统和WSL2通信使用的虚拟网卡地址</span><br><span class="hljs-meta">$</span><span class="bash"> sudo vim /etc/resolv.conf</span><br><span class="hljs-meta">#</span><span class="bash"> nameserver后面的地址就是Windows系统虚拟网卡的地址,记一下</span><br><span class="hljs-meta">#</span><span class="bash"> 同时需要取消下面两行内容的注释,禁用自动重新生成配置文件,否则重启后这个地址会变</span><br>[network]<br>generateResolvConf = false<br> <br> <br><span class="hljs-meta">$</span><span class="bash"> vim ~/.bashrc <span class="hljs-comment">#如果你跟我一样用的zsh就是zshrc</span></span><br><span class="hljs-meta">#</span><span class="bash"> 在文件最后追加下面内容,地址使用上面查看到的</span><br>export DISPLAY=192.168.112.1:0 #改ip 端口不动<br> <br><span class="hljs-meta">$</span><span class="bash"><span class="hljs-built_in">source</span> ~/.bashrc</span><br><span class="hljs-meta">#</span><span class="bash">执行刚修改的初始化文件，使之立即生效</span><br></code></pre></td></tr></table></figure><p>5.启动xfce4 </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">startxfce4<br></code></pre></td></tr></table></figure></li></ol><p><img src="https://img.enderfga.cn/img/image-20220529120240951.png" alt="大功告成！"></p>]]></content>
    
    
    
    <tags>
      
      <tag>操作系统</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>One-Hot Encoding 独热编码</title>
    <link href="/2021/11/23/onehot/"/>
    <url>/2021/11/23/onehot/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>One-Hot Encoding学习记录</p><span id="more"></span><h1 id="一、One-Hot-Encoding"><a href="#一、One-Hot-Encoding" class="headerlink" title="一、One-Hot Encoding"></a>一、One-Hot Encoding</h1><p>One-Hot 编码，又称为一位有效编码，主要是采用位状态寄存器来对个状态进行编码，每个状态都由他独立的寄存器位，并且在任意时候只有一位有效。<br>在实际的机器学习的应用任务中，特征有时候并不总是连续值，有可能是一些分类值，如性别可分为“ male ”和“ female ”。在机器学习任务中，对于这样的特征，</p><p>One-hot在数位电路中被用来表示一种特殊的位元组合，该字节里，仅容许单一位元为1，其他位元都必须为0。之所以称为one-hot就是因为只能有一个1（hot）。若情况相反，只有一个0，其余为1，则称为one-cold。在机器学习里，也有one-hot向量（英语：one-hot vector）的概念。在一任意维度的向量中，仅有一个维度的值是1，其余为0。譬如向量 ${\displaystyle [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]}{\displaystyle [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]}$，即为15维空间中的一组one-hot向量。将类别性资料转换成one-hot向量的过程则称one-hot编码（英语：one-hot encoding）。在统计学中，虚拟变数代表了类似的概念。</p><p>One-hot目前并无公认或被广泛使用的中文译名。目前可见的one-hot encoding译名有独热编码以及一位有效编码。</p><p>通常我们需要对其进行特征数字化，如下面的例子，这些特征值并不是连续的，而是离散的，无序的。</p><p>有如下三个特征属性：</p><ul><li>性别：[“male”，”female”]</li><li>地区：[“Europe”，”US”，”Asia”]</li><li>浏览器：[“Firefox”，”Chrome”，”Safari”，”Internet Explorer”]</li></ul><p>对于某一个样本，如[“male”，”US”，”Internet Explorer”]，我们需要将这个分类值的特征数字化，最直接的方法，我们可以采用序列化的方式：[0,1,3]。但是这样的特征处理并不能直接放入机器学习算法中。</p><p><img src="https://img.enderfga.cn/img/20190514150006715.jpg" alt=""></p><h1 id="二、One-Hot-Encoding的处理方法"><a href="#二、One-Hot-Encoding的处理方法" class="headerlink" title="二、One-Hot Encoding的处理方法"></a>二、One-Hot Encoding的处理方法</h1><p>One-Hot 编码是分类变量作为二进制向量的表示。</p><ol><li>将分类值映射到整数值。</li><li>每个整数值被表示为二进制向量，除了整数的索引之外，它都是零值，它被标记为1。</li></ol><p><img src="https://img.enderfga.cn/img/image-20211123195029323.png" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20211123195041636.png" alt=""></p><p>对于上述的问题，性别的属性是二维的，同理，地区是三维的，浏览器则是四维的，这样，我们可以采用One-Hot编码的方式对上述的样本“[“male”，”US”，”Internet Explorer”]”编码，“male”则对应着[1，0]，同理“US”对应着[0，1，0]，“Internet Explorer”对应着[0，0，0，1]。</p><p>则完整的特征数字化的结果为：[1,0,0,1,0,0,0,0,1]。这样导致的一个结果就是数据会变得非常的稀疏。</p><h1 id="三、Python代码举例"><a href="#三、Python代码举例" class="headerlink" title="三、Python代码举例"></a>三、Python代码举例</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> preprocessing<br> <br>enc = preprocessing.OneHotEncoder()<br>enc.fit([[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">3</span>],[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>],[<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>],[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">2</span>]])<br> <br>array = enc.transform([[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">3</span>]]).toarray()<br> <br><span class="hljs-built_in">print</span>(array)<br></code></pre></td></tr></table></figure><p>结果：[[ 1.  0.  0.  1.  0.  0.  0.  0.  1.]]</p><h1 id="四、优缺点分析"><a href="#四、优缺点分析" class="headerlink" title="四、优缺点分析"></a>四、优缺点分析</h1><p><strong>优点：</strong></p><p>(1) 解决了分类器不好处理离散数据的问题。</p><pre><code class="hljs">a. 欧式空间。在回归，分类，聚类等机器学习算法中，特征之间 距离计算 或 相似度计算 是非常重要的，而我们常用的距离或相似度的计算都是在欧式空间的相似度计算，计算余弦相似性，基于的就是欧式空间。b. one-hot 编码。使用 one-hot 编码，将离散特征的取值扩展到了欧式空间，离散特征的某个取值就对应欧式空间的某个点。将离散型特征使用 one-hot 编码，确实会让 特征之间的距离计算更加合理。</code></pre><p>(2) 在一定程度上也起到了扩充特征的作用。</p><p><strong>缺点：</strong></p><p>(1) 它是一个词袋模型，不考虑词与词之间的<strong>顺序</strong>（文本中词的顺序信息也是很重要的）；</p><p>(2) 它<strong>假设词与词相互独立</strong>（在大多数情况下，词与词是相互影响的）；</p><p>(3) 它得到的特征是<strong>离散稀疏</strong>的 （这个问题最严重）。</p><p><strong>与其他编码的差异：</strong></p><ul><li>决定状态机目前状态的时间成本低，因为读取一个正反器的时间成本固定。</li><li>改变机器的状态所需时间成本也是固定，因为每次只需要改变两个正反器的值。</li><li>设计及设计变更容易。</li><li>容易侦测出非法状态。</li><li>可以有效率地使用FPGA的大量正反器。</li><li>相较于其他编码，使用one-hot来实现状态机通常可以达到更高的时脉频率。</li><li>比起其他编码，需要更多的正反器，使得其在PAL装置上不切实际。</li><li>会有很多非法状态存在[7]。这是由于${\displaystyle N}$个正反器构成的计数器总共有${\displaystyle 2^{N}}$个状态（每个正反器可以是0或1，所以总共${\displaystyle 2^{N}}$种可能状态），但是合法状态却只有${\displaystyle N}$个（即同一时间只允许一个正反器是1,其他必须为0），所以总共会有${\displaystyle 2^{N}-N}$个可能的非法状态。</li></ul><h1 id="五、应用"><a href="#五、应用" class="headerlink" title="五、应用"></a>五、应用</h1><p><strong>自然语言处理</strong><br>在自然语言处理中，若有个字典或字库里有${\displaystyle N}$个单字，则每个单字可以被一个${\displaystyle N}$维的one-hot向量代表。譬如若字库里仅有apple（苹果），banana（香蕉），以及pineapple（凤梨）这三个单字，则他们各自的one-hot向量可以为：</p><script type="math/tex; mode=display">{\displaystyle {\begin{array}{ll}apple&=[1\ 0\ 0]\\banana&=[0\ 1\ 0]\\pineapple&=[0\ 0\ 1]\end{array}}}</script><p>由于电脑无法理解非数字类的数据，One-hot编码可以将类别性数据转换成统一的数字格式，方便机器学习的算法进行处理及计算。而转换成固定维度的向量则方便机器学习算法进行线性代数上的计算。另外由于一个one-hot向量中，绝大部分的数字都是0，所以若使用稀疏矩阵的数据结构，则可以节省电脑内存的使用量。</p><p><strong>有限状态机</strong><br>One-hot编码常常被用来表示一个有限状态机的状态。如果使用二进制或格雷码来代表状态，则需要用到解码器才能得知该码代表的状态。使用one-hot来代表状态的话，则不需要解码器，因为若第${\displaystyle n}$个位元为1，就代表机器目前在第${\displaystyle n}$个状态。</p><p>一个有限状态机的例子是由15个状态构成的环状计数器。使用one-hot编码来实现此状态机的话，可以将15个正反器串联在一起，每个正反器的Q输出接到下一个正反器的D输入，而第一个正反器的D输入则是接到第15个的Q输出，形成一个环状。第一个正反器代表机器的第一个状态，第二个正反器代表第二个状态，依此类推。当机器被归零重设时，第一个正反器的值为1，其余为0。当一个时脉边缘抵达正反器时，会将1推进到下一个正反器。依照这种方式，1可一步步推进到第15个正反器，亦即第15个状态，再之后则重新回到第一个状态。</p><p>位址解码器可以将二进制或格雷码转换成one-shot表示形式。而优先编码器则是作用相反。</p><h1 id="六、资料来源"><a href="#六、资料来源" class="headerlink" title="六、资料来源"></a>六、资料来源</h1><ul><li><a href="https://zh.wikipedia.org/wiki/One-hot">https://zh.wikipedia.org/wiki/One-hot</a></li><li><a href="https://blog.csdn.net/google19890102/article/details/44039761">https://blog.csdn.net/google19890102/article/details/44039761</a></li><li><a href="https://blog.csdn.net/qq_15192373/article/details/89552498">https://blog.csdn.net/qq_15192373/article/details/89552498</a></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>神经网络回归与分类（波士顿房价与红酒分类）</title>
    <link href="/2021/11/16/middle/"/>
    <url>/2021/11/16/middle/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>机器学习期中大作业，神经网络回归与分类</p><span id="more"></span><h1 id="一、描述所使用的神经网络模型"><a href="#一、描述所使用的神经网络模型" class="headerlink" title="一、描述所使用的神经网络模型"></a>一、描述所使用的神经网络模型</h1><h2 id="1-1-神经元模型"><a href="#1-1-神经元模型" class="headerlink" title="1.1 神经元模型"></a>1.1 神经元模型</h2><h3 id="1-1-1-神经元模型的定义"><a href="#1-1-1-神经元模型的定义" class="headerlink" title="1.1.1 神经元模型的定义"></a>1.1.1 神经元模型的定义</h3><p><strong>神经网络是由具有适应性的简单单元组成的广泛并行互联的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互反应。神经网络中最基本的成分是神经元模型，即上述的“简单单元”。</strong></p><h3 id="1-1-2-M-P神经元模型"><a href="#1-1-2-M-P神经元模型" class="headerlink" title="1.1.2 M-P神经元模型"></a>1.1.2 M-P神经元模型</h3><p><img src="https://img.enderfga.cn/img/image-20211114180429830.png" alt=""></p><p><strong>输入</strong>：来自其他n个神经元传递过来的输入信号</p><p><strong>处理</strong>：输入信号通过带权重的连接进行传递, 神经元接受到总输入值将其与神经元的阈值进行比较</p><p><strong>输出</strong>：通过激活函数的处理以得到输出</p><h3 id="1-1-3-激活函数"><a href="#1-1-3-激活函数" class="headerlink" title="1.1.3 激活函数"></a>1.1.3 激活函数</h3><p><img src="https://img.enderfga.cn/img/image-20211114180032126.png" alt=""></p><h4 id="1-1-3-1-激活函数的介绍"><a href="#1-1-3-1-激活函数的介绍" class="headerlink" title="1.1.3.1 激活函数的介绍"></a>1.1.3.1 激活函数的介绍</h4><p><strong>如下图所示，神经网络中的每个神经元节点接受上一层神经元的输出值作为本神经元的输入值，并将输入值传递给下一层，输入层神经元节点会将输入属性值直接传递给下一层（隐层或输出层）。在多层神经网络中，上层节点的输出和下层节点的输入之间具有一个函数关系，这个函数称为激活函数（又称激励函数）。</strong></p><p><img src="https://img.enderfga.cn/img/image-20211113213541281.png" alt=""></p><h4 id="1-1-3-2-激活函数的用途"><a href="#1-1-3-2-激活函数的用途" class="headerlink" title="1.1.3.2 激活函数的用途"></a>1.1.3.2 激活函数的用途</h4><p><strong>如果不用激励函数（相当于激励函数是f(x) = x），每一层节点的输入都是上层输出的线性函数，这样无论神经网络有多少层，输出都是输入的线性组合，这种情况就是最原始的感知机，网络的逼近能力就相当有限。当我们引入非线性函数作为激励函数，深层神经网络表达能力就更加强大，不再是输入的线性组合，几乎可以逼近任意函数。</strong></p><h4 id="1-1-3-3-一些常见的激活函数及其性质"><a href="#1-1-3-3-一些常见的激活函数及其性质" class="headerlink" title="1.1.3.3 一些常见的激活函数及其性质"></a>1.1.3.3 一些常见的激活函数及其性质</h4><h5 id="1-1-3-3-1-Relu激活函数"><a href="#1-1-3-3-1-Relu激活函数" class="headerlink" title="1.1.3.3.1 Relu激活函数"></a>1.1.3.3.1 Relu激活函数</h5><p><strong>Relu函数的解析式：</strong></p><script type="math/tex; mode=display">f_{Relu}(x)=max(0,x)</script><p><strong>Relu函数及其导数的图像如下图所示：</strong></p><p><img src="https://img.enderfga.cn/img/image-20211113213557078.png" alt=""></p><p><strong>优点：</strong></p><p><strong>①计算效率较高</strong></p><p><strong>②兼具线性和非线性特性</strong></p><p><strong>缺点：</strong></p><p><strong>梯度消失问题：在x&lt;0时，神经元保持非激活状态，且在反向传导（backward pass）中梯度为零</strong></p><h5 id="1-1-3-3-2-Sigmoid激活函数"><a href="#1-1-3-3-2-Sigmoid激活函数" class="headerlink" title="1.1.3.3.2 Sigmoid激活函数"></a>1.1.3.3.2 Sigmoid激活函数</h5><p><strong>Sigmoid 函数的解析式：</strong></p><script type="math/tex; mode=display">f_{Sigmoid}(x)=\frac{1}{1+e^{-x}}</script><p><strong>Sigmoid函数及其导数的图像如下图所示：</strong></p><p><img src="https://img.enderfga.cn/img/image-20211114180235544.png" alt=""></p><p><strong>优点：</strong></p><p><strong>①梯度的“平滑性”</strong></p><p><strong>②输出在“0-1区间”</strong></p><p><strong>缺点：</strong></p><p><strong>①梯度消失问题：神经网络使用 Sigmoid 激活函数进行反向传播时，输出接近0或1的神经元其梯度趋近于0 </strong></p><p><strong>②计算成本问题：涉及指数计算</strong></p><p><strong>③不以零为中心：Sigmoid 输出不以零为中心</strong></p><h5 id="1-1-3-3-3-tanh-激活函数"><a href="#1-1-3-3-3-tanh-激活函数" class="headerlink" title="1.1.3.3.3  tanh 激活函数"></a>1.1.3.3.3  tanh 激活函数</h5><p><strong>tanh函数的解析式：</strong></p><script type="math/tex; mode=display">f_{tanh}(x)=\frac{1-e^{-2x}}{1+e^{-2x}}</script><p><strong>tanh函数及其导数的图像如下图所示：</strong></p><p><img src="https://img.enderfga.cn/img/image-20211113213650281.png" alt=""></p><p><strong>优点：</strong></p><p><strong>①梯度的“平滑性”</strong></p><p><strong>②输出以零为中心</strong></p><p><strong>缺点：</strong></p><p><strong>①梯度消失问题：神经网络使用 tanh 激活函数进行反向传播时，输出接近-1或1的神经元其梯度趋近于0</strong></p><p><strong>②计算成本问题：涉及指数计算</strong></p><h2 id="1-2-神经网络模型"><a href="#1-2-神经网络模型" class="headerlink" title="1.2 神经网络模型"></a>1.2 神经网络模型</h2><h3 id="1-2-1-神经网络模型的定义"><a href="#1-2-1-神经网络模型的定义" class="headerlink" title="1.2.1 神经网络模型的定义"></a>1.2.1 神经网络模型的定义</h3><p><strong>将若干神经元按一定的层次结构连接起来就得到了神经网络，可将神经网络视为包含了若干参数的数学模型，这个模型是由若干个函数相互（嵌套）代入而得。</strong></p><p><img src="https://img.enderfga.cn/img/image-20211113213743041.png" alt=""></p><h3 id="1-2-2-多层前馈神经网络"><a href="#1-2-2-多层前馈神经网络" class="headerlink" title="1.2.2 多层前馈神经网络"></a>1.2.2 多层前馈神经网络</h3><h4 id="1-2-2-1-定义"><a href="#1-2-2-1-定义" class="headerlink" title="1.2.2.1 定义"></a>1.2.2.1 定义</h4><p><strong>每层神经元与下一层神经元全互联，神经元之间不存在同层连接也不存在跨层连接。输入层接受外界输入，隐含层与输出层神经元对信号进行加工，最终结果由输出层神经元输出。根据训练数据来调整神经元之间的“连接权”以及每个功能神经元的“阈值”。</strong></p><p><img src="https://img.enderfga.cn/img/image-20211113213753140.png" alt=""></p><h4 id="1-2-2-2-模型训练"><a href="#1-2-2-2-模型训练" class="headerlink" title="1.2.2.2 模型训练"></a>1.2.2.2 模型训练</h4><p><strong>数据</strong>：</p><script type="math/tex; mode=display">D=\{(x_1,y_1),(x_2,y_2),……,(x_m,y_m)\},x_i∈R^d,y_i∈R^l</script><p><strong>模型</strong>：若干神经元按一定的层次结构连接起来，每层神经元与下一层神经元全互联，神经元之间不存在同层连接也不存在跨层连接，所形成的神经网络模型。</p><p><strong>策略</strong>：</p><p><strong>①平方损失（回归问题）</strong></p><script type="math/tex; mode=display">L(y_i,f(x_i))=(y_i-f(x_i)^2),ERM</script><p><strong> ②交叉熵损失（二分类问题）</strong></p><hr><script type="math/tex; mode=display">L(y_i,f(x_i))=-y_ilog(p(y_i=1|x_i))-(1-y_i)log(p(y_i=0|x_i))</script><p><strong>算法</strong>：误差逆传播算法（error Back Propagation）</p><h1 id="二、描述训练模型所使用的算法"><a href="#二、描述训练模型所使用的算法" class="headerlink" title="二、描述训练模型所使用的算法"></a>二、描述训练模型所使用的算法</h1><h1 id="2-1-误差逆传播算法"><a href="#2-1-误差逆传播算法" class="headerlink" title="2.1 误差逆传播算法"></a>2.1 误差逆传播算法</h1><h3 id="2-1-1-应用领域"><a href="#2-1-1-应用领域" class="headerlink" title="2.1.1 应用领域"></a>2.1.1 应用领域</h3><p><strong>反向传播算法应用较为广泛，从字面意思理解，与前向传播相互对应。在简单的神经网络中，反向传播算法，可以理解为最优化损失函数过程，求解每个参与运算的参数的梯度的方法。在前馈神经网中，反向传播从求解损失函数偏导过程中，步步向前求解每一层的参数梯度。在卷积神经网络中，反向传播可以求解全连接层的参数梯度。在循环神经网络中，反向传播算法可以求解每一个时刻t或者状态t的参数梯度（在RNN\LSTM\GRU中，反向传播更多是BPTT）。如今对于BP的理解，认为是在优化损失函数或者目标函数过程中，求解参与运算的参数的梯度方法，是一种比较普遍的说法。</strong></p><h3 id="2-1-2-网络结构"><a href="#2-1-2-网络结构" class="headerlink" title="2.1.2 网络结构"></a>2.1.2 网络结构</h3><p><img src="https://img.enderfga.cn/img/image-20211113192037294.png" alt=""></p><ol><li><strong>正向传播求损失，反向传播回传误差</strong></li><li><strong>根据误差信号修正每层的权重</strong></li><li><strong>f是激活函数；f(netj)是隐层的输出； f(netk）是输出层的输出O; d是target。</strong></li></ol><h3 id="2-1-2-基本参数结构"><a href="#2-1-2-基本参数结构" class="headerlink" title="2.1.2 基本参数结构"></a>2.1.2 基本参数结构</h3><p><strong>为了方便讨论，我们以一个隐层的神经网络结构进行推导。多隐层的神经网络推导思想与此类似，可推广。如下图为一个神经网络结构。</strong></p><p><img src="https://img.enderfga.cn/img/image-20211113192136112.png" alt=""></p><h4 id="2-1-2-1参数简述"><a href="#2-1-2-1参数简述" class="headerlink" title="2.1.2.1参数简述"></a>2.1.2.1参数简述</h4><script type="math/tex; mode=display">\begin{align}&输入参数：x_1^k,\dots ,x_i^k,\dots,x_d^k\\&输入层到第一隐层第h个神经元的权重：v_{1h},\dots,v_{ih},\dots,v_{dh}\\&第一层第h个神经元输入：\alpha _{h}= \sum_{i=1}^{d}v_{ih}x_i^k\\&第一隐层阙值：\gamma _{1},\dots,\gamma _{h},\dots,\gamma _{q}\\&第一隐层第h个输出：b_h=f_{sigmoid}(\alpha _h-\gamma _h) \\&第一隐层到第j个输出神经元的权重：w_{1j},\dots,w_{hj},\dots,w_{qj}\\&第j个输出神经元的输入：\beta _j=\sum_{h=1}^{q}w_{hj}b_h\\&输出层阙值：\theta _{1},\dots,\theta _{j},\dots,\theta_{l}\\&输出值：y_j^k=f_{sigmoid}(\alpha _h-\gamma _h)\end{align}</script><p><strong>所以前向传播计算误差为：</strong></p><script type="math/tex; mode=display">E_{k}=\frac{1}{2} \sum_{j=1}^{l}\left(y_{j}^{k}-\hat{y}_{j}^{k}\right)^{2}</script><h3 id="2-1-3-参数调整策略"><a href="#2-1-3-参数调整策略" class="headerlink" title="2.1.3 参数调整策略"></a>2.1.3 参数调整策略</h3><p><strong>BP算法的核心思想：使用梯度下降来搜索可能的权向量的假设空间，以找到最佳的拟合样例的权向量。具体而言，即利用损失函数，每次向损失函数负梯度方向移动，直到损失函数取得最小值。或者说，反向传播算法，是根据损失函数，求出损失函数关于每一层的权值及偏置项的偏导数，也称为梯度，用该值更新初始的权值和偏置项，一直更新到损失函数取得最小值或是设置的迭代次数完成为止。以此来计算神经网络中的最佳的参数。</strong></p><script type="math/tex; mode=display">损失函数：E_{k}=\frac{1}{2} \sum_{j=1}^{l}\left(y_{j}^{k}-\hat{y}_{j}^{k}\right)^{2}</script><h4 id="2-1-3-1-计算准备"><a href="#2-1-3-1-计算准备" class="headerlink" title="2.1.3.1 计算准备"></a>2.1.3.1 计算准备</h4><script type="math/tex; mode=display">\frac{\partial E_k}{\partial \hat{y_j^k} }=(y_j^k-\hat{y_j^k})(-1)\\f_{sigmoid}(x)^{(1)}=f_{sigmoid}(x)(1-f_{sigmoid}(x))\\\eta:学习率</script><p><strong>下面，我们讲分别讨论每个参数的更新：</strong></p><h4 id="2-1-3-2-w更新"><a href="#2-1-3-2-w更新" class="headerlink" title="2.1.3.2 w更新"></a>2.1.3.2 <strong>w</strong>更新</h4><script type="math/tex; mode=display">更新公式：w_{hj}=w_{hj}+\bigtriangleup w_{hj}</script><p>下面对<strong>w</strong>进行讨论:</p><script type="math/tex; mode=display">\begin{align}\bigtriangleup w_{hj}=&-\eta  \frac{\partial E_k}{\partial w_{hj}}\\=&-\eta (\frac{\partial E_k}{\partial \hat{y_j^k} }\cdot\frac{\partial \hat{y_j^k}}{\partial \beta _j}  \cdot \frac{\partial \beta _j}{\partial w_{hj}}  )\\=&-\eta (y_j^k-\hat{y_j^k})(-1)\cdot \hat{y_j^k} (1-\hat{y_j^k})\cdot b_h\\令g_j&=(y_j^k-\hat{y_j^k})\cdot \hat{y_j^k} (1-\hat{y_j^k})\\\\最终可得&\bigtriangleup w_{hj}=\eta g_jb_h\end{align}</script><h4 id="2-1-3-3-θ更新"><a href="#2-1-3-3-θ更新" class="headerlink" title="2.1.3.3 θ更新"></a>2.1.3.3 <strong>θ</strong>更新</h4><script type="math/tex; mode=display">更新公式：\theta_{j}=\theta_{j}+\bigtriangleup \theta_{j}</script><p><strong>下面对</strong>θ<strong>进行讨论：</strong></p><script type="math/tex; mode=display">\begin{align}\bigtriangleup \theta_j=&-\eta  \frac{\partial E_k}{\partial \theta_{j}}\\=&-\eta  (\frac{\partial E_k}{\partial \hat{y_j^k} }\cdot\frac{\partial \hat{y_j^k}}{\partial \theta _j}    )\\=&-\eta (y_j^k-\hat{y_j^k})(-1)\cdot \hat{y_j^k} (1-\hat{y_j^k})\cdot(-1)\\=&-\eta g_j\end{align}</script><h4 id="2-1-3-4-v更新"><a href="#2-1-3-4-v更新" class="headerlink" title="2.1.3.4 v更新"></a>2.1.3.4 <strong>v</strong>更新</h4><script type="math/tex; mode=display">更新公式：v_{ih}=v_{ih}+\bigtriangleup v_{ih}</script><p><strong>下面对</strong>v<strong>进行讨论：</strong></p><script type="math/tex; mode=display">\begin{align}\bigtriangleup v_{ih}=&-\eta \frac{\partial E_k}{\partial v_{ih}}\\ =&-\eta\frac{\partial E_k}{\partial b_h}\cdot  \frac{\partial b_h}{\partial \alpha _h}\cdot  \frac{\partial \alpha _h}{\partial v_{ih}} \\=&-\eta(\sum_{j=1}^{l}\frac{\partial E_k}{\partial \hat{y_j^k} }\cdot\frac{\partial \hat{y_j^k}}{\partial \beta _j}  \cdot \frac{\partial \beta _j}{\partial b_{h}})\cdot b_h(1-b_h)\cdot x_i^k\\=&\eta \sum_{j=1}^{l}g_jw_{hj}b_h(1-b_h)x_i^k \end{align}</script><h4 id="2-1-3-5γ更新"><a href="#2-1-3-5γ更新" class="headerlink" title="2.1.3.5γ更新"></a>2.1.3.5γ更新</h4><script type="math/tex; mode=display">更新公式：\gamma_{h}=\gamma_{h}+\bigtriangleup \gamma_{h}</script><p><strong>下面对</strong>γ<strong>进行讨论：</strong></p><script type="math/tex; mode=display">\begin{align}\bigtriangleup \gamma_{h}=&-\eta \frac{\partial E_k}{\partial \gamma_{h}}\\ =&-\eta\frac{\partial E_k}{\partial b_h}\cdot  \frac{\partial b_h}{\partial \gamma _h} \\=&-\eta(\sum_{j=1}^{l}\frac{\partial E_k}{\partial \hat{y_j^k} }\cdot\frac{\partial \hat{y_j^k}}{\partial \beta _j}  \cdot \frac{\partial \beta _j}{\partial b_{h}})\cdot b_h(1-b_h)\cdot (-1)\\=&\eta \sum_{j=1}^{l}g_jw_{hj}b_h(1-b_h)(-1)\end{align}</script><h3 id="2-1-3-算法伪代码"><a href="#2-1-3-算法伪代码" class="headerlink" title="2.1.3 算法伪代码"></a>2.1.3 算法伪代码</h3><p><img src="https://img.enderfga.cn/img/image-20211114180628211.png" alt=""></p><h1 id="三、描述模型超参数确定的过程，分析模型训练结果"><a href="#三、描述模型超参数确定的过程，分析模型训练结果" class="headerlink" title="三、描述模型超参数确定的过程，分析模型训练结果"></a>三、描述模型超参数确定的过程，分析模型训练结果</h1><h2 id="3-1-超参数的概念"><a href="#3-1-超参数的概念" class="headerlink" title="3.1 超参数的概念"></a>3.1 超参数的概念</h2><p>大部分机器学习算法都需要花费大量时间去训练，而在训练之前需要提前配置一些变量。这些变量对训练结果影响很大，但没有对任何数据集都适用的一组变量，需要根据具体应用具体配置，这些需要配置的变量称之为超参数（hyperparameters）。区分超参数和模型参数最大的一点就是是否通过数据来进行调整，模型参数通常是有数据来驱动调整，超参数则不需要数据来驱动，而是在训练前或者训练中人为的进行调整的参数。例如卷积核的具体核参数就是指模型参数，这是由数据驱动的。而学习率则是人为来进行调整的超参数。这里需要注意的是，通常情况下卷积核数量、卷积核尺寸这些也是超参数，注意与卷积核的核参数区分。</p><h2 id="3-2-神经网络包含的超参数"><a href="#3-2-神经网络包含的超参数" class="headerlink" title="3.2 神经网络包含的超参数"></a>3.2 神经网络包含的超参数</h2><h3 id="3-2-1-超参数种类"><a href="#3-2-1-超参数种类" class="headerlink" title="3.2.1 超参数种类"></a>3.2.1 超参数种类</h3><p>通常可以将超参数分为三类：网络参数、优化参数、正则化参数。</p><p>网络参数：可指网络层与层之间的交互方式（相加、相乘或者串接等）、卷积核数量和卷积核尺寸、网络层数（也称深度）和激活函数等。</p><p>优化参数：一般指学习率（learning rate）、批样本数量（batch size）、不同优化器的参数以及部分损失函数的可调参数。</p><p>正则化：权重衰减系数，丢弃法比率（dropout）</p><p><strong>神经网络包含的超参数具体为以下十一个：</strong></p><ol><li><strong>学习率 η</strong></li><li><strong>正则化参数 λ</strong></li><li><strong>神经网络的层数 L</strong></li><li><strong>每一个隐层中神经元的个数 j</strong></li><li><strong>学习的回合数Epoch</strong></li><li><strong>小批量数据 minibatch 的大小</strong></li><li><strong>输出神经元的编码方式</strong></li><li><strong>代价函数的选择</strong></li><li><strong>权重初始化的方法</strong></li><li><strong>神经元激活函数的种类</strong></li><li><strong>参加训练模型数据的规模</strong></li></ol><p><img src="https://img.enderfga.cn/img/image-20211112080119294.png" alt=""></p><p>在上图中可以看到超参数 2，3，4， 7 主要影响的时神经网络的分类正确率；9 主要影响代价函数曲线下降速度，同时有时也会影响正确率；1，8，10 主要影响学习速度，这点主要体现在训练数据代价函数曲线的下降速度上；5，6，11 主要影响模型分类正确率和训练用总体时间。这上面所提到的时某个超参数对于神经网络想到的首要影响，并不代表着该超参数只影响学习速度或者正确率。</p><h3 id="3-2-2-超参数重要性顺序"><a href="#3-2-2-超参数重要性顺序" class="headerlink" title="3.2.2 超参数重要性顺序"></a>3.2.2 超参数重要性顺序</h3><ul><li>首先， 学习率，损失函数上的可调参数。在网络参数、优化参数、正则化参数中最重要的超参数可能就是学习率了。学习率直接控制着训练中网络梯度更新的量级，直接影响着模型的有效容限能力；损失函数上的可调参数，这些参数通常情况下需要结合实际的损失函数来调整，大部分情况下这些参数也能很直接的影响到模型的的有效容限能力。这些损失一般可分成三类，第一类辅助损失结合常见的损失函数，起到辅助优化特征表达的作用。例如度量学习中的Center loss，通常结合交叉熵损失伴随一个权重完成一些特定的任务。这种情况下一般建议辅助损失值不高于或者不低于交叉熵损失值的两个数量级；第二类，多任务模型的多个损失函数，每个损失函数之间或独立或相关，用于各自任务，这种情况取决于任务之间本身的相关性，目前笔者并没有一个普适的经验由于提供参考；第三类，独立损失函数，这类损失通常会在特定的任务有显著性的效果。例如RetinaNet中的focal loss，其中的参数γ，α，对最终的效果会产生较大的影响。这类损失通常论文中会给出特定的建议值。</li><li>其次，批样本数量，动量优化器（Gradient Descent with Momentum）的动量参数β。批样本决定了数量梯度下降的方向。过小的批数量，极端情况下，例如batch size为1，即每个样本都去修正一次梯度方向，样本之间的差异越大越难以收敛。若网络中存在批归一化（batchnorm），batch size过小则更难以收敛，甚至垮掉。这是因为数据样本越少，统计量越不具有代表性，噪声也相应的增加。而过大的batch size，会使得梯度方向基本稳定，容易陷入局部最优解，降低精度。一般参考范围会取在[1:1024]之间，当然这个不是绝对的，需要结合具体场景和样本情况；动量衰减参数β是计算梯度的指数加权平均数，并利用该值来更新参数，设置为 0.9 是一个常见且效果不错的选择；</li><li>最后，Adam优化器的超参数、权重衰减系数、丢弃法比率（dropout）和网络参数。在这里说明下，这些参数重要性放在最后并不等价于这些参数不重要。而是表示这些参数在大部分实践中不建议过多尝试，例如Adam优化器中的β1，β2，ϵ，常设为 0.9、0.999、10−8就会有不错的表现。权重衰减系数通常会有个建议值，例如0.0005 ，使用建议值即可，不必过多尝试。dropout通常会在全连接层之间使用防止过拟合，建议比率控制在[0.2,0.5]之间。使用dropout时需要特别注意两点：一、在RNN中，如果直接放在memory cell中,循环会放大噪声，扰乱学习。一般会建议放在输入和输出层；二、不建议dropout后直接跟上batchnorm，dropout很可能影响batchnorm计算统计量，导致方差偏移，这种情况下会使得推理阶段出现模型完全垮掉的极端情况；网络参数通常也属于超参数的范围内，通常情况下增加网络层数能增加模型的容限能力，但模型真正有效的容限能力还和样本数量和质量、层之间的关系等有关，所以一般情况下会选择先固定网络层数，调优到一定阶段或者有大量的硬件资源支持可以在网络深度上进行进一步调整。</li></ul><h2 id="3-3-模型超参数确定"><a href="#3-3-模型超参数确定" class="headerlink" title="3.3 模型超参数确定"></a>3.3 模型超参数确定</h2><h3 id="3-3-1-超参数调优的原因"><a href="#3-3-1-超参数调优的原因" class="headerlink" title="3.3.1 超参数调优的原因"></a>3.3.1 超参数调优的原因</h3><p>本质上，这是模型优化寻找最优解和正则项之间的关系。网络模型优化调整的目的是为了寻找到全局最优解（或者相比更好的局部最优解），而正则项又希望模型尽量拟合到最优。两者通常情况下，存在一定的对立，但两者的目标是一致的，即最小化期望风险。模型优化希望最小化经验风险，而容易陷入过拟合，正则项用来约束模型复杂度。所以如何平衡两者之间的关系，得到最优或者较优的解就是超参数调整优化的目的。</p><h3 id="3-3-2-模型超参数的确定"><a href="#3-3-2-模型超参数的确定" class="headerlink" title="3.3.2 模型超参数的确定"></a>3.3.2 模型超参数的确定</h3><p><strong>四种主流超参数调优技术：</strong></p><ol><li><strong>传统或手动调参</strong></li><li><strong>网格搜索</strong></li><li><strong>随机搜索</strong></li><li><strong>贝叶斯搜索</strong></li></ol><p>在传统的调优中，我们通过手动检查随机超参数集来训练算法，并选择最适合我们目标的参数集。但这种方法不能保证得到最佳的参数组合，反复试验会消耗更多的时间。</p><h4 id="3-3-2-1-网格搜索"><a href="#3-3-2-1-网格搜索" class="headerlink" title="3.3.2.1 网格搜索"></a>3.3.2.1 网格搜索</h4><ul><li>网格搜索是一种基本的超参数调整技术。它类似于手动调优，为网格中指定的所有给定超参数值的每个排列建立模型，并评估和选择最佳模型。由于它尝试每一种超参数组合，并根据交叉验证分数选择最佳组合，这使得 GridsearchCV 极其缓慢。</li><li>这种启发式的搜索算法对超参数搜索算法，被称之为网格搜索。(如果人工处理所有可能的超参数组合，通常的办法是，根据超参数的维度，列成相应的表格，比如说k的取值有[2，3，4，5，6，7，8]，另一个系数比如λ取值有[0.01,0.03,0.1,0.3]等，这样就可以列出一个二维表格，组合出7*4种可能性的超参数组合，再对每一个格子中具体的超参数组合，通过交叉验证的方式进行模型性能的评估，然后通过验证性能的比较，最终筛选出最佳的超参数数据组合)</li><li>网格搜索采用交叉验证的方法，来寻找更好的超参数组合的过程非常耗时，由于各个新模型在执行交叉验证的过程中是相互独立的，那么我们可以充分利用多核处理器甚至是分布式的计算资源来从事并行搜索，从而成倍的节省运算时间。<img src="https://img.enderfga.cn/img/482493cc74974f85ba09b50697801c27.jpeg" alt=""></li></ul><h4 id="3-3-2-2-随机搜索"><a href="#3-3-2-2-随机搜索" class="headerlink" title="3.3.2.2 随机搜索"></a>3.3.2.2 随机搜索</h4><p>使用随机搜索代替网格搜索的动机是，在许多情况下，所有的超参数可能并非同等重要。随机搜索从超参数空间中随机选择参数组合，参数按 n_iter 给定的迭代次数进行选择。随机搜索已经被实践证明比网格搜索得到的结果更好，但随机搜索的问题是它不能保证给出最佳的参数组合。</p><p><img src="https://img.enderfga.cn/img/image-20211114180715729.png" alt=""></p><h4 id="3-3-2-3-贝叶斯优化"><a href="#3-3-2-3-贝叶斯优化" class="headerlink" title="3.3.2.3 贝叶斯优化"></a>3.3.2.3 贝叶斯优化</h4><p>贝叶斯优化属于一类被称为<strong><em>sequential model-based optimization</em>(SMBO)</strong>的优化算法。这些算法使用先前对损失 f 的观测，来确定下一个(最佳)点来取样 f。该算法大致可以概括如下：</p><ol><li><strong>使用先前计算过的点 X1: n，计算损失 f 的后验期望值。</strong></li><li><strong>在一个新的点 Xnew取样损失 f ，它最大化了 f 的期望的某些效用函数。该函数指定 f 域的哪些区域是最适合采样的。</strong></li></ol><p>重复这些步骤，直到达到某种收敛准则。</p><h5 id="3-3-2-3-1-高斯过程"><a href="#3-3-2-3-1-高斯过程" class="headerlink" title="3.3.2.3.1 高斯过程"></a>3.3.2.3.1 高斯过程</h5><p>在贝叶斯调参过程中，假设一组超参数组合是X=x1,x2,…,xn(xn表示某一个超参数的值)，而这组超参数与最后我们需要优化的损失函数存在一个函数关系，最终的评估结果为Y，通过什么样的X可以取得最优的Y，我们假设是f(X)， Y=F(X)</p><p>而目前机器学习其实是一个黑盒子(black box),即我们只知道input和output，所以上面的函数f(x)很难确定。所以我们需要将注意力转移到一个我们可以解决的函数上去。</p><p>于是可以假设这个寻找最优化参数的过程是一个高斯过程。高斯过程有个特点，就是当随机遍历一定的数据点并拿到结果之后，可以大致绘制出整个数据的分布曲线。</p><p><img src="https://img.enderfga.cn/img/998084-20180726204924171-1721363009.png" alt=""></p><h5 id="3-3-2-3-2-贝叶斯优化理论"><a href="#3-3-2-3-2-贝叶斯优化理论" class="headerlink" title="3.3.2.3.2 贝叶斯优化理论"></a>3.3.2.3.2 贝叶斯优化理论</h5><p>还是这张图，把横轴看作是参数组合X，纵轴看作是这个参数的结果Y。可以通过已经构建的曲线，找到曲线上升的方向，从而在这个方向上继续探索，这样就可以大概率拿到更好的结果。在生活的轨迹上，如果找到一条明确通往幸福的路，可以继续向前探索，因为大概率可以成功，但也许也有会错过更好的机会，陷入局部最优解。请看上图中的五角星，如果我们处于它的位置，继续向上走会迎来一个高峰，但是如果后退，在下降一段时间之后可能会迎来更高的波峰，你该如何选择。</p><p>于是，在参数的探索中要掌握一个平衡：</p><p>开发：在明确的曲线上扬方向继续走，大概率获得更好的结果，但是容易陷入局部最优。</p><p>探索：除了在曲线上扬的方向，在其它的区域也不忘寻找</p><p><img src="https://img.enderfga.cn/img/image-20211114102651780.png" alt=""></p><h2 id="3-4-结果分析"><a href="#3-4-结果分析" class="headerlink" title="3.4 结果分析"></a>3.4 结果分析</h2><h3 id="3-4-1-MAPE"><a href="#3-4-1-MAPE" class="headerlink" title="3.4.1 MAPE"></a>3.4.1 MAPE</h3><p><strong>平均绝对百分比误差（Mean Absolute Percentage Error）</strong></p><script type="math/tex; mode=display">M A P E=\frac{100 \%}{n} \sum_{i=1}^{n}\left|\frac{\hat{y}_{i}-y_{i}}{y_{i}}\right|</script><p>范围[0,+∞)，MAPE 为0%表示完美模型，MAPE 大于 100 %则表示劣质模型。</p><p>注意：当真实值有数据等于0时，存在分母0除问题，该公式不可用！</p><h3 id="3-4-2-调参前结果及分析"><a href="#3-4-2-调参前结果及分析" class="headerlink" title="3.4.2 调参前结果及分析"></a>3.4.2 调参前结果及分析</h3><h4 id="3-4-2-1-代码"><a href="#3-4-2-1-代码" class="headerlink" title="3.4.2.1 代码"></a>3.4.2.1 代码</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs routeros">mlp = MLPRegressor(hidden_layer_sizes=(100), <span class="hljs-attribute">activation</span>=<span class="hljs-string">&#x27;relu&#x27;</span>, <span class="hljs-attribute">solver</span>=<span class="hljs-string">&#x27;adam&#x27;</span>, <span class="hljs-attribute">alpha</span>=0.0001, <span class="hljs-attribute">batch_size</span>=<span class="hljs-string">&#x27;auto&#x27;</span>, <span class="hljs-attribute">learning_rate</span>=<span class="hljs-string">&#x27;constant&#x27;</span>, <span class="hljs-attribute">learning_rate_init</span>=0.001, <span class="hljs-attribute">power_t</span>=0.5, <span class="hljs-attribute">max_iter</span>=200, <span class="hljs-attribute">shuffle</span>=<span class="hljs-literal">True</span>, <span class="hljs-attribute">random_state</span>=None, <span class="hljs-attribute">tol</span>=0.0001, <span class="hljs-attribute">verbose</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">warm_start</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">momentum</span>=0.9, <span class="hljs-attribute">nesterovs_momentum</span>=<span class="hljs-literal">True</span>, <span class="hljs-attribute">early_stopping</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">validation_fraction</span>=0.1, <span class="hljs-attribute">beta_1</span>=0.9, <span class="hljs-attribute">beta_2</span>=0.999, <span class="hljs-attribute">epsilon</span>=1e-08, <span class="hljs-attribute">n_iter_no_change</span>=10, <span class="hljs-attribute">max_fun</span>=15000) #所有参数默认<br>mlp.fit(X_std, Y)<br>MAPE = -1<span class="hljs-number">*c</span>ross_val_score(mlp, X_std, Y, <span class="hljs-attribute">cv</span>=rkf,scoring=&#x27;neg_mean_absolute_percentage_error&#x27;).mean()<br><span class="hljs-builtin-name">print</span>(<span class="hljs-string">&#x27;MAPE:&#x27;</span>,MAPE)<br></code></pre></td></tr></table></figure><h4 id="3-4-2-2-结果及分析"><a href="#3-4-2-2-结果及分析" class="headerlink" title="3.4.2.2 结果及分析"></a>3.4.2.2 结果及分析</h4><p>首先我们使用<a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.neural_network"><code>sklearn.neural_network</code></a>.<a href="https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html?highlight=mlp#sklearn.neural_network.MLPRegressor">MLPRegressor</a>中的所有默认参数设置来训练模型，五次五折交叉验证的平均MAPE为：0.2036462204885882</p><p><img src="https://img.enderfga.cn/img/image-20211114171851292.png" alt=""></p><p>Fig.1  调参前残差图</p><p><img src="https://img.enderfga.cn/img/image-20211114171902074.png" alt=""></p><p>Fig.2  调参前预测误差图</p><p>残差是因变量未被自变量解释的部分，线性模型要求残差服从独立同分布，且分布类型为正态分布。通过一系列方法判断残差是否符合这一要求，可以达到检验模型是否符合相应假设的目的。从上图可以看出，我们的训练集和测试集的R^2^在0.75左右，说明我们的模型训练结果具有一定的可信度，但并不理想。下面我们进行调参，尝试提高准确率。</p><h3 id="3-4-3-网格搜索调参"><a href="#3-4-3-网格搜索调参" class="headerlink" title="3.4.3 网格搜索调参"></a>3.4.3 网格搜索调参</h3><p>接下来我们手动调试模型，将各个超参数逐一修改并查看MAPE的变化结果，最终得出’<strong>hidden_layer_sizes</strong>‘，’<strong>activation</strong>‘，’<strong>solver</strong>‘，’<strong>alpha</strong>‘，’<strong>learning_rate</strong>‘</p><p>这五个对结果影响较大的参数。</p><p>最后我们利用GridSearchCV结合一些“经验结论”来搜索出最优的超参数。</p><p><img src="https://img.enderfga.cn/img/image-20211114222524771.png" alt=""></p><h4 id="3-4-3-1-代码"><a href="#3-4-3-1-代码" class="headerlink" title="3.4.3.1 代码"></a>3.4.3.1 代码</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># 超参数调优</span><br><span class="hljs-keyword">from</span> sklearn.model_selection import GridSearchCV<br>parameters = &#123;<span class="hljs-string">&#x27;hidden_layer_sizes&#x27;</span>: [(10,10,10,10,10),(20,20,20,20,20),(30,30,30,30,30),(40,40,40,40,40),(50,50,50,50,50),(60,60,60,60,60),(70,70,70,70,70),(80,80,80,80,80),(90,90,90,90,90),(100,100,100,100,100)],<br>                <span class="hljs-string">&#x27;activation&#x27;</span>: [<span class="hljs-string">&#x27;identity&#x27;</span>, <span class="hljs-string">&#x27;logistic&#x27;</span>,<span class="hljs-string">&#x27;tanh&#x27;</span>, <span class="hljs-string">&#x27;relu&#x27;</span>],<br>                <span class="hljs-string">&#x27;solver&#x27;</span>: [<span class="hljs-string">&#x27;adam&#x27;</span>,<span class="hljs-string">&#x27;lbgfs&#x27;</span>,<span class="hljs-string">&#x27;sgd&#x27;</span>],<br>                <span class="hljs-string">&#x27;alpha&#x27;</span>: [0.0001, 0.001, 0.01, 0.1, 1,10,100],<br>                <span class="hljs-string">&#x27;learning_rate&#x27;</span>: [<span class="hljs-string">&#x27;constant&#x27;</span>, <span class="hljs-string">&#x27;invscaling&#x27;</span>, <span class="hljs-string">&#x27;adaptive&#x27;</span>]&#125;<br>grid = GridSearchCV(mlp, parameters, <span class="hljs-attribute">cv</span>=rkf, <span class="hljs-attribute">scoring</span>=<span class="hljs-string">&#x27;neg_mean_absolute_percentage_error&#x27;</span>,n_jobs=-1)<br>grid.fit(X_std, Y)<br><span class="hljs-builtin-name">print</span>(<span class="hljs-string">&#x27;最优参数：&#x27;</span>,grid.best_params_)<br><span class="hljs-builtin-name">print</span>(<span class="hljs-string">&#x27;最优模型得分：&#x27;</span>,grid.best_score_)<br></code></pre></td></tr></table></figure><h4 id="3-4-3-2-结果及分析"><a href="#3-4-3-2-结果及分析" class="headerlink" title="3.4.3.2 结果及分析"></a>3.4.3.2 结果及分析</h4><ul><li><strong>最优参数： {‘activation’: ‘relu’, ‘alpha’: 1, ‘hidden_layer_sizes’: (100, 100, 100, 100, 100), ‘learning_rate’: ‘adaptive’, ‘solver’: ‘sgd’} </strong></li><li><strong>最优模型得分： 0.10669120458358475</strong></li></ul><p><img src="https://img.enderfga.cn/img/image-20211114221915446.png" alt=""></p><p>Fig.3  调参后残差图</p><p><img src="https://img.enderfga.cn/img/image-20211114221934910.png" alt=""></p><p>Fig.4  调参后预测误差图</p><p>经过网格搜索最优参数后，我们的模型得到大幅度提升。从上可见，我们的训练和测试的R^2^均在0.98以上，说明模型对训练集的拟合效果和泛化能力都很强。</p><h1 id="四、总结模型训练过程中的收获"><a href="#四、总结模型训练过程中的收获" class="headerlink" title="四、总结模型训练过程中的收获"></a>四、总结模型训练过程中的收获</h1><h2 id="4-1-神经网络的训练过程"><a href="#4-1-神经网络的训练过程" class="headerlink" title="4.1 神经网络的训练过程"></a>4.1 神经网络的训练过程</h2><p>简单的神经网络的训练过程包括以下几个步骤：</p><ol><li><strong>定义一个包含多个可学习参数（权重）的神经网络；</strong></li><li><strong>对输入的数据集进行迭代计算；</strong></li><li><strong>通过多层网络结构来处理输入数据；</strong></li><li><strong>计算损失值（输出值与目标值的差值）；</strong></li><li><strong>反向传播梯度到神经网络的参数中；</strong></li><li><strong>根据更新规则来更新网络中的权重值。</strong></li></ol><h2 id="4-2-确定超参数"><a href="#4-2-确定超参数" class="headerlink" title="4.2 确定超参数"></a>4.2 确定超参数</h2><p>其中，如何定义一个包含多个可学习参数的神经网络（即如何确定模型的超参数）是重点，会影响神经网络学习速度和最后结果。我们确定超参数的步骤如下：</p><p>①我们根据经验结论手动调试模型，将各个超参数逐一修改并查看MAPE的变化结果，最终得出’<strong>hidden_layer_sizes</strong>‘，’<strong>activation</strong>‘，’<strong>solver</strong>‘，’<strong>alpha</strong>‘，</p><p>‘<strong>learning_rate</strong>‘这五个对结果影响较大的参数。</p><p>②搜集“经验总结”的资料后，我们用网格搜索法对下列超参数进行排列组合，得到10<strong>*</strong>4<strong>*</strong>3<strong>*</strong>7<strong>*</strong>3=2520种超参数的排列组合方式。</p><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs gcode">parameters = &#123;<span class="hljs-string">&#x27;hidden_layer_sizes&#x27;</span>: [<span class="hljs-comment">(10,10,10,10,10)</span>,<span class="hljs-comment">(20,20,20,20,20)</span>,<span class="hljs-comment">(30,30,30,30,30)</span>,<span class="hljs-comment">(40,40,40,40,40)</span>,<span class="hljs-comment">(50,50,50,50,50)</span>,<span class="hljs-comment">(60,60,60,60,60)</span>,<span class="hljs-comment">(70,70,70,70,70)</span>,<span class="hljs-comment">(80,80,80,80,80)</span>,<span class="hljs-comment">(90,90,90,90,90)</span>,<span class="hljs-comment">(100,100,100,100,100)</span>],<br>                <span class="hljs-string">&#x27;activation&#x27;</span>: [<span class="hljs-string">&#x27;identity&#x27;</span>, <span class="hljs-string">&#x27;logistic&#x27;</span>,<span class="hljs-string">&#x27;tanh&#x27;</span>, <span class="hljs-string">&#x27;relu&#x27;</span>],<br>                <span class="hljs-string">&#x27;solver&#x27;</span>: [<span class="hljs-string">&#x27;adam&#x27;</span>,<span class="hljs-string">&#x27;lbgfs&#x27;</span>,<span class="hljs-string">&#x27;sgd&#x27;</span>],<br>                <span class="hljs-string">&#x27;alpha&#x27;</span>: [<span class="hljs-number">0.0001</span>, <span class="hljs-number">0.001</span>, <span class="hljs-number">0.01</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">1</span>,<span class="hljs-number">10</span>,<span class="hljs-number">100</span>],<br>                <span class="hljs-string">&#x27;learning_rate&#x27;</span>: [<span class="hljs-string">&#x27;constant&#x27;</span>, <span class="hljs-string">&#x27;invscaling&#x27;</span>, <span class="hljs-string">&#x27;adaptive&#x27;</span>]&#125;<br></code></pre></td></tr></table></figure><p>③使用五次五折将数据划分为25份，把上述2520种超参数的组合都跑一遍数据（计算神经网络中的最佳的参数用的是误差逆传播算法），每一个组合都会得到25个MAPE值，取平均；之后2520份MAPE中的最小值对应的超参数组合即为我们选定的最优超参数组合。</p><h2 id="4-3-防止过拟合的方法"><a href="#4-3-防止过拟合的方法" class="headerlink" title="4.3 防止过拟合的方法"></a>4.3 防止过拟合的方法</h2><p>在机器学习模型（特别是深度学习模型）的训练过程中，模型是非常容易过拟合的。深度学习模型在不断的训练过程中训练误差会逐渐降低，但测试误差的走势则不一定。</p><p>①正则化方法。正则化方法包括L0正则、L1正则和L2正则，而正则一般是在目标函数之后加上对于的范数。但是在机器学习中一般使用L2正则。</p><p>②数据增强（Data augmentation），增大数据的训练量；还有一个原因就是我们用于训练的数据量太小导致的，训练数据占总数据的比例过小。</p><p>③重新清洗数据，导致过拟合的一个原因也有可能是数据不纯导致的，如果出现了过拟合就需要我们重新清洗数据。</p><p>④提前终止法（Early stopping），对模型进行训练的过程即是对模型的参数进行学习更新的过程，这个参数学习的过程往往会用到一些迭代方法，如梯度下降（Gradient descent）学习算法。提前终止法便是一种迭代次数截断的方法来防止过拟合的方法，即在模型对训练数据集迭代收敛之前停止迭代来防止过拟合。</p><p>⑤丢弃法（Dropout）。这个方法在神经网络里面很常用。丢弃法是ImageNet中提出的一种方法，通俗一点讲就是丢弃法在训练的时候让神经元以一定的概率不工作。</p><p>以下内容出自<strong>复旦大学邱锡鹏教授著作</strong>《神经网络与深度学习》</p><h1 id="一、描述所使用的神经网络模型-1"><a href="#一、描述所使用的神经网络模型-1" class="headerlink" title="一、描述所使用的神经网络模型"></a>一、描述所使用的神经网络模型</h1><h2 id="1-1-人脑神经网络"><a href="#1-1-人脑神经网络" class="headerlink" title="1.1 人脑神经网络"></a>1.1 人脑神经网络</h2><p>人类大脑是人体最复杂的器官，由神经元、神经胶质细胞、神经干细胞和血管组成．其中，神经元（Neuron），也叫神经细胞（NerveCell），是携带和传输信息的细胞，是人脑神经系统中最基本的单元．人脑神经系统是一个非常复杂的组织，包含近860亿个神经元，每个神经元有上千个突触和其他神经元相连接．这些神经元和它们之间的连接形成巨大的复杂网络，其中神经连接的总长度可达数千公里．我们人造的复杂网络，比如全球的计算机网络，和大脑神经网络相比要“简单”得多．</p><p><img src="https://img.enderfga.cn/img/image-20211119093130413.png" alt=""></p><h2 id="1-2-人工神经网络"><a href="#1-2-人工神经网络" class="headerlink" title="1.2 人工神经网络"></a>1.2 人工神经网络</h2><p>人工神经网络是为模拟人脑神经网络而设计的一种计算模型， 它从结构、实现机理和功能上模拟人脑神经网络． 人工神经网络与生物神经元类似， 由多个节点（ 人工神经元） 互相连接而成， 可以用来对数据之间的复杂关系进行建模． 不同节点之间的连接被赋予了不同的权重， 每个权重代表了一个节点对另一个节点的影响大小． 每个节点代表一种特定函数， 来自其他节点的信息经过其相应的权重综合计算， 输入到一个激活函数中并得到一个新的活性值（ 兴奋或抑制）．从系统观点看， 人工神经元网络是由大量神经元通过极其丰富和完善的连接而构成的自适应非线性动态系统．<br>虽然我们可以比较容易地构造一个人工神经网络， 但是如何让人工神经网络具有学习能力并不是一件容易的事情． 早期的神经网络模型并不具备学习能力． 首个可学习的人工神经网络是赫布网络， 采用一种基于赫布规则的无监督学习方法． 感知器是最早的具有机器学习思想的神经网络， 但其学习方法无法扩展到多层的神经网络上． 直到 1980 年左右， 反向传播算法才有效地解决了多层神 经网络的学习问题， 并成为最为流行的神经网络学习算法．</p><p>人工神经网络诞生之初并不是用来解决机器学习问题． 由于人工神经网络可以用作一个通用的函数逼近器（ 一个两层的神经网络可以逼近任意的函数），因此我们可以将人工神经网络看作一个可学习的函数， 并将其应用到机器学习中． 理论上， 只要有足够的训练数据和神经元数量， 人工神经网络就可以学到很多复杂的函数． 我们可以把一个人工神经网络塑造复杂函数的能力称为网络容量（ Network Capacity）， 这与可以被储存在网络中的信息的复杂度以及数量相关．</p><p><img src="https://img.enderfga.cn/img/image-20211119094549829.png" alt=""></p><h2 id="1-3-前馈神经网络"><a href="#1-3-前馈神经网络" class="headerlink" title="1.3 前馈神经网络"></a>1.3 前馈神经网络</h2><p>在本次作业中， 我们主要采用误差反向传播来进行学习的神经网络， 即作为一种机器学习模型的神经网络．从机器学习的角度来看， 神经网络一般可以看作一个非线性模型， 其基本组成单元为具有非线性激活函数的神经元， 通过大量神经元之间的连接， 使得神经网络成为一种高度非线性的模型． 神经元之间的连接权重就是需要学习的参数， 可以在机器学习的框架下通过梯度下降方法来进行学习．</p><h3 id="1-3-1-神经元"><a href="#1-3-1-神经元" class="headerlink" title="1.3.1 神经元"></a>1.3.1 神经元</h3><p>1943 年， 心理学家 McCulloch 和数学家 Pitts 根据生物神经元的结构， 提出了一种非常简单的神经元模型， MP神经元． 现代神经网络中的神经元和 MP 神经元的结构并无太多变化． 不同的是， MP 神经元中的激活函数𝑓 为0或1的阶跃函数， 而现代神经元中的激活函数通常要求是连续可导的函数．</p><p><img src="https://img.enderfga.cn/img/image-20211119205516708.png" alt=""></p><p>净输入  <strong>z</strong>  在经过一个非线性函数  $f(\cdot) $ 后, 得到神经元的活性值 ( Activation ) <strong>a</strong> ,</p><script type="math/tex; mode=display">a=f(z)</script><p>其中非线性函数  $f(\cdot) $ 称为激活函数 ( Activation Function ).</p><p><img src="https://img.enderfga.cn/img/image-20211119100021496.png" alt=""></p><h3 id="1-3-2-激活函数"><a href="#1-3-2-激活函数" class="headerlink" title="1.3.2 激活函数"></a>1.3.2 激活函数</h3><ol><li>激活函数在神经元中非常重要的。为了增强网络的表示能力和学习能力，激活函数需要具备以下几点性质:<br>连续并可导(允许少数点上不可导)的非线性函数.可导的激活函数可以直接利用数值优化的方法来学习网络参数.</li><li>激活函数及其导函数要尽可能的简单,有利于提高网络计算效率.</li><li>激活函数的导函数的值域要在一个合适的区间内,不能太大也不能太小,否则会影响训练的效率和稳定性.</li></ol><p>下面介绍几种在神经网络中常用的激活函数.</p><h4 id="1-3-2-1-Sigmoid型函数"><a href="#1-3-2-1-Sigmoid型函数" class="headerlink" title="1.3.2.1 Sigmoid型函数"></a>1.3.2.1 Sigmoid型函数</h4><p>Sigmoid型函数是指一类S型曲线函数,为两端饱和函数.常用的Sigmoid型函数有Logistic 函数和Tanh 函数.</p><h5 id="1-3-2-1-1-Logistic"><a href="#1-3-2-1-1-Logistic" class="headerlink" title="1.3.2.1.1 Logistic"></a>1.3.2.1.1 Logistic</h5><p>Logistic 函数定义为</p><script type="math/tex; mode=display">\sigma(x)=\frac{1}{1+\exp (-x)}</script><p>Logistic 函数可以看成是一个“挤压” 函数， 把一个实数域的输入“挤压” 到(0, 1)． 当输入值在0附近时， Sigmoid型函数近似为线性函数； 当输入值靠近两端时， 对输入进行抑制． 输入越小， 越接近于 0； 输入越大， 越接近于 1． 这样的特点也和生物神经元类似， 对一些输入会产生兴奋（ 输出为1）， 对另一些输入产生抑制（ 输出为0）． 和感知器使用的阶跃激活函数相比， Logistic函数是连续可导的，其数学性质更好．因为Logistic函数的性质， 使得装备了Logistic激活函数的神经元具有以下两点性质：</p><ol><li>其输出直接可以看作概率分布， 使得神经网络可以更好地和统计学习模型进行结合．</li><li>其可以看作一个软性门（ Soft Gate）， 用来控制其他神经元输出信息的数量.</li></ol><h5 id="1-3-2-1-2-Tanh"><a href="#1-3-2-1-2-Tanh" class="headerlink" title="1.3.2.1.2 Tanh"></a>1.3.2.1.2 Tanh</h5><p>Tanh 函数也是一种 Sigmoid 型函数. 其定义为</p><script type="math/tex; mode=display">\tanh (x)=\frac{\exp (x)-\exp (-x)}{\exp (x)+\exp (-x)}</script><p>Tanh 函数可以看作放大并平移的 Logistic 函数, 其值域是  (-1,1) .</p><script type="math/tex; mode=display">\tanh (x)=2 \sigma(2 x)-1</script><p>下图给出了 Logistic 函数和 Tanh 函数的形状． Tanh 函数的输出是零中心化的（ Zero-Centered）， 而 Logistic 函数的输出恒大于 0． 非零中心化的输出会使得其后一层的神经元的输入发生偏置偏移（ Bias Shift）， 并进一步使得梯度下降的收敛速度变慢 .</p><p><img src="https://img.enderfga.cn/img/image-20211119100752525.png" alt=""></p><p>Logistic函数和Tanh函数都是Sigmoid型函数， 具有饱和性， 但是计算开销较大． 因为这两个函数都是在中间（ 0附近） 近似线性， 两端饱和． 因此， 这两个函数可以通过分段函数来近似．</p><p>以 Logistic 函数 $ \sigma(x) $ 为例, 其导数为  $\sigma^{\prime}(x)=\sigma(x)(1-\sigma(x)) $. Logistic 函数 在 0 附近的一阶泰勒展开 ( Taylor expansion ) 为</p><script type="math/tex; mode=display">\begin{aligned}g_{l}(x) & \approx \sigma(0)+x \times \sigma^{\prime}(0) \\&=0.25 x+0.5\end{aligned}</script><p>这样 Logistic 函数可以用分段函数 hard-logistic  (x)  来近似.</p><script type="math/tex; mode=display">\begin{aligned}\operatorname{hard}-\operatorname{logistic}(x) &=\left\{\begin{array}{ll}1 & g_{l}(x) \geq 1 \\g_{l} & 0<g_{l}(x)<1 \\0 & g_{l}(x) \leq 0\end{array}\right.\\&=\max \left(\min \left(g_{l}(x), 1\right), 0\right) \\&=\max (\min (0.25 x+0.5,1), 0)\end{aligned}</script><p>同样, Tanh 函数在 0 附近的一阶泰勒展开为</p><script type="math/tex; mode=display">\begin{aligned}g_{t}(x) & \approx \tanh (0)+x \times \tanh ^{\prime}(0) \\&=x\end{aligned}</script><p>这样  $\operatorname{Tanh} $ 函数也可以用分段函数$  \operatorname{hard}-\tanh (x)  $来近似.</p><script type="math/tex; mode=display">\begin{aligned}\operatorname{hard}-\tanh (x) &=\max \left(\min \left(g_{t}(x), 1\right),-1\right) \\&=\max (\min (x, 1),-1)\end{aligned}</script><p><img src="https://img.enderfga.cn/img/image-20211119101318245.png" alt=""></p><h4 id="1-3-2-2-ReLU函数"><a href="#1-3-2-2-ReLU函数" class="headerlink" title="1.3.2.2 ReLU函数"></a>1.3.2.2 ReLU函数</h4><p>ReLU（ Rectified Linear Unit， 修正线性单元） [Nair et al., 2010]， 也叫Rectifier函数[Glorot et al., 2011]， 是目前深度神经网络中经常使用的激活函数．ReLU实际上是一个斜坡（ ramp） 函数， 定义为</p><script type="math/tex; mode=display">\begin{aligned}\operatorname{ReLU}(x) &=\left\{\begin{array}{ll}x & x \geq 0 \\0 & x<0\end{array}\right.\\&=\max (0, x)\end{aligned}</script><p><strong>优点</strong> 采用 ReLU 的神经元只需要进行加、 乘和比较的操作， 计算上更加高效．ReLU 函数也被认为具有生物学合理性（ Biological Plausibility）， 比如单侧抑制、宽兴奋边界（ 即兴奋程度可以非常高）． 在生物神经网络中， 同时处于兴奋状态的神经元非常稀疏． 人脑中在同一时刻大概只有 1% ∼ 4% 的神经元处于活跃状态． Sigmoid 型激活函数会导致一个非稀疏的神经网络， 而 ReLU 却具有很好的稀疏性， 大约50%的神经元会处于激活状态．<br>在优化方面， 相比于Sigmoid型函数的两端饱和， ReLU函数为左饱和函数，且在 𝑥 &gt; 0 时导数为 1， 在一定程度上缓解了神经网络的梯度消失问题， 加速梯度下降的收敛速度．</p><p><strong>缺点</strong> ReLU 函数的输出是非零中心化的， 给后一层的神经网络引入偏置偏移，会影响梯度下降的效率．此外， ReLU 神经元在训练时比较容易“死亡”． 在训练时， 如果参数在一次不恰当的更新后， 第一个隐藏层中的某个 ReLU 神经元在所有的训练数据上都不能被激活， 那么这个神经元自身参数的梯度永远都会是0， 在以后的训练过程中永远不能被激活． 这种现象称为死亡 ReLU 问题（ Dying ReLU Problem ),并且也有可能会发生在其他隐藏层.</p><p>在实际使用中,为了避免上述情况,有几种 ReLU的变种也会被广泛使用.</p><h5 id="1-3-2-2-1-带泄露的ReLU"><a href="#1-3-2-2-1-带泄露的ReLU" class="headerlink" title="1.3.2.2.1 带泄露的ReLU"></a>1.3.2.2.1 带泄露的ReLU</h5><p>带泄露的ReLU（ Leaky ReLU） 在输入 𝑥 &lt; 0时， 保持一个很小的梯度𝛾． 这样当神经元非激活时也能有一个非零的梯度可以更新参数， 避免永远不能被激活[Maas et al., 2013]． 带泄露的ReLU的定义如下：</p><script type="math/tex; mode=display">\begin{aligned}\operatorname{LeakyReLU}(x) &=\left\{\begin{array}{ll}x & \text { if } x>0 \\\gamma x & \text { if } x \leq 0\end{array}\right.\\&=\max (0, x)+\gamma \min (0, x)\end{aligned}</script><p>其中 𝛾是一个很小的常数， 比如0.01． 当𝛾 &lt; 1时， 带泄露的ReLU也可以写为</p><script type="math/tex; mode=display">\operatorname{LeakyReLU}(x)=\max (x, \gamma x)</script><p>相当于是一个比较简单的maxout单元 .</p><h5 id="1-3-2-2-2-带参数的ReLU"><a href="#1-3-2-2-2-带参数的ReLU" class="headerlink" title="1.3.2.2.2 带参数的ReLU"></a>1.3.2.2.2 带参数的ReLU</h5><p>带参数的 ReLU（ Parametric ReLU， PReLU） 引入一个可学习的参数， 不同神经元可以有不同的参数． 对于第 𝑖 个神经元， 其 PReLU 的定义为</p><script type="math/tex; mode=display">\begin{aligned}\operatorname{PReLU}_{i}(x) &=\left\{\begin{array}{ll}x & \text { if } x>0 \\\gamma_{i} x & \text { if } x \leq 0\end{array}\right.\\&=\max (0, x)+\gamma_{i} \min (0, x)\end{aligned}</script><p>其中 𝛾𝑖 为 𝑥 ≤ 0 时函数的斜率． 因此， PReLU 是非饱和函数． 如果 𝛾𝑖 = 0， 那么PReLU就退化为ReLU． 如果𝛾𝑖 为一个很小的常数， 则PReLU可以看作带泄露的ReLU． PReLU 可以允许不同神经元具有不同的参数， 也可以一组神经元共享一个参数．</p><h5 id="1-3-2-2-3-ELU函数"><a href="#1-3-2-2-3-ELU函数" class="headerlink" title="1.3.2.2.3 ELU函数"></a>1.3.2.2.3 ELU函数</h5><p>ELU（ Exponential Linear Unit， 指数线性单元）是一个近似的零中心化的非线性函数， 其定义为</p><script type="math/tex; mode=display">\begin{aligned}\operatorname{ELU}(x) &=\left\{\begin{array}{ll}x & \text { if } x>0 \\\gamma(\exp (x)-1) & \text { if } x \leq 0\end{array}\right.\\&=\max (0, x)+\min (0, \gamma(\exp (x)-1))\end{aligned}</script><p>其中 𝛾 ≥ 0是一个超参数， 决定𝑥 ≤ 0时的饱和曲线， 并调整输出均值在0附近．</p><h5 id="1-3-2-2-4-Softplus函数"><a href="#1-3-2-2-4-Softplus函数" class="headerlink" title="1.3.2.2.4 Softplus函数"></a>1.3.2.2.4 Softplus函数</h5><p>Softplus 函数 可以看作 Rectifier 函数的平滑版本， 其定义为</p><script type="math/tex; mode=display">\operatorname{Softplus}(x)=\log (1+\exp (x))</script><p>Softplus函数其导数刚好是Logistic函数． Softplus函数虽然也具有单侧抑制、宽兴奋边界的特性， 却没有稀疏激活性 .</p><p><img src="https://img.enderfga.cn/img/image-20211119103024771.png" alt=""></p><h3 id="1-3-3-多层前馈神经网络模型"><a href="#1-3-3-多层前馈神经网络模型" class="headerlink" title="1.3.3 多层前馈神经网络模型"></a>1.3.3 多层前馈神经网络模型</h3><p>给定一组神经元， 我们可以将神经元作为节点来构建一个网络． 不同的神经网络模型有着不同网络连接的拓扑结构． 一种比较直接的拓扑结构是前馈网络． 前馈神经网络（ Feedforward Neural Network， FNN） 是最早发明的简单人工神经网络． 前馈神经网络也经常称为多层感知器（ Multi-Layer Perceptron MLP）．但多层感知器的叫法并不是十分合理， 因为前馈神经网络其实是由多层的 Logistic 回归模型（ 连续的非线性函数） 组成， 而不是由多层的感知器（ 不连续的非线性函数） 组成 .</p><p>前馈神经网络中， 各神经元分别属于不同的层． 每一层的神经元可以接收前一层神经元的信号， 并产生信号输出到下一层． 第0层称为输入层， 最后一层称为输出层， 其他中间层称为隐藏层． 整个网络中无反馈， 信号从输入层向输出层单向传播， 可用一个有向无环图表示 .</p><p><img src="https://img.enderfga.cn/img/image-20211119103824459.png" alt=""></p><p>对于本次作业中的多分类问题 $ y \in{1, \cdots, C} $, 使用 Softmax 回归分类器, 相当于网络 最后一层设置  C  个神经元, 其激活函数为 Softmax 函数. 网络最后一层 (第  L  层) 的输出可以作为每个类的条件概率, 即</p><script type="math/tex; mode=display">\hat{\boldsymbol{y}}=\operatorname{softmax}\left(\boldsymbol{z}^{(L)}\right)</script><p>其中  $\boldsymbol{z}^{(L)} \in \mathbb{R}^{C}  $为第  L  层神经元的净输入;  $\hat{\boldsymbol{y}} \in \mathbb{R}^{C}  $为第  L  层神经元的活性值, 每 一维分别表示不同类别标签的预测条件概率.</p><p>故采用交叉熵损失函数, 对于样本 $ (\boldsymbol{x}, y) $, 其损失函数为</p><script type="math/tex; mode=display">\mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})=-\boldsymbol{y}^{\top} \log \hat{\boldsymbol{y}},</script><p>其中  $\boldsymbol{y} \in{0,1}^{C}  $为标签  y  对应的 one-hot 向量表示.<br><img src="https://img.enderfga.cn/img/image-20211119205657015.png" alt=""></p><script type="math/tex; mode=display">\|\boldsymbol{W}\|_{F}^{2}=\sum_{l=1}^{L} \sum_{i=1}^{M_{l}} \sum_{j=1}^{M_{l-1}}\left(w_{i j}^{(l)}\right)^{2}</script><p>有了学习准则和训练样本, 网络参数可以通过梯度下降法来进行学习. 在梯度下降方法的每次迭代中,第 l 层的参数  $\boldsymbol{W}^{(l)} $ 和  $\boldsymbol{b}^{(l)}  $参数更新方式为</p><script type="math/tex; mode=display">\begin{aligned}\boldsymbol{W}^{(l)} & \leftarrow \boldsymbol{W}^{(l)}-\alpha \frac{\partial \mathcal{R}(\boldsymbol{W}, \boldsymbol{b})}{\partial \boldsymbol{W}^{(l)}} \\&=\boldsymbol{W}^{(l)}-\alpha\left(\frac{1}{N} \sum_{n=1}^{N}\left(\frac{\partial \mathcal{L}\left(\boldsymbol{y}^{(n)}, \hat{\boldsymbol{y}}^{(n)}\right)}{\partial \boldsymbol{W}^{(l)}}\right)+\lambda \boldsymbol{W}^{(l)}\right) \\\boldsymbol{b}^{(l)} & \leftarrow \boldsymbol{b}^{(l)}-\alpha \frac{\partial \mathcal{R}(\boldsymbol{W}, \boldsymbol{b})}{\partial \boldsymbol{b}^{(l)}} \\&=\boldsymbol{b}^{(l)}-\alpha\left(\frac{1}{N} \sum_{n=1}^{N} \frac{\partial \mathcal{L}\left(\boldsymbol{y}^{(n)}, \hat{\boldsymbol{y}}^{(n)}\right)}{\partial \boldsymbol{b}^{(l)}}\right)\end{aligned}</script><p>其中  $\alpha $ 为学习率.</p><p>梯度下降法需要计算损失函数对参数的偏导数, 如果通过链式法则逐一对每个参数进行求偏导比较低效. 在神经网络的训练中经常使用<strong>反向传播算法</strong>来高效地计算梯度.</p><h1 id="二、描述训练模型所使用的算法-1"><a href="#二、描述训练模型所使用的算法-1" class="headerlink" title="二、描述训练模型所使用的算法"></a>二、描述训练模型所使用的算法</h1><p>假设采用随机梯度下降进行神经网络参数学习, 给定一个样本$  (\boldsymbol{x}, \boldsymbol{y})$ , 将其输入到神经网络模型中, 得到网络输出为  $\hat{\boldsymbol{y}} $. 假设损失函数为$  \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}}) $, 要进行参数学习就需要计算损失函数关于每个参数的导数.</p><p>不失一般性, 对第  l  层中的参数$  \boldsymbol{W}^{(l)}  $和 $ \boldsymbol{b}^{(l)}  $计算偏导数. 因为$  \frac{\partial \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})}{\partial \boldsymbol{W}^{(l)}}  $的计算 涉及向量对矩阵的微分, 十分繁琐, 因此我们先计算$  \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})  $关于参数矩阵中每个元素的偏导数  $\frac{\partial \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})}{\partial w_{i j}^{(l)}} $. 根据链式法则,</p><script type="math/tex; mode=display">\begin{array}{l}\frac{\partial \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})}{\partial w_{i j}^{(l)}}=\frac{\partial \boldsymbol{z}^{(l)}}{\partial w_{i j}^{(l)}} \frac{\partial \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})}{\partial \boldsymbol{z}^{(l)}} \\\frac{\partial \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})}{\partial \boldsymbol{b}^{(l)}}=\frac{\partial \boldsymbol{z}^{(l)}}{\partial \boldsymbol{b}^{(l)}} \frac{\partial \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})}{\partial \boldsymbol{z}^{(l)}}\end{array}</script><p>以上两个公式中的第二项都是目标函数关于第  l  层的神经元  $\boldsymbol{z}^{(l)}  $的偏导数,称为误差项, 可以一次计算得到. 这样我们只需要计算三个偏导数, 分别为  $\frac{\partial \boldsymbol{z}^{(l)}}{\partial w_{i j}^{(l)}}, \frac{\partial \boldsymbol{z}^{(l)}}{\partial \boldsymbol{b}^{(l)}}  和  \frac{\partial \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})}{\partial \boldsymbol{z}^{(l)}} .$</p><h2 id="2-1-偏导数计算"><a href="#2-1-偏导数计算" class="headerlink" title="2.1 偏导数计算"></a>2.1 偏导数计算</h2><p>下面分别来计算这三个偏导数：</p><p>(1) 计算偏导数  $\frac{\partial z^{(l)}}{\partial w_{i j}^{(l)}}  $因  $z^{(l)}=\boldsymbol{W}^{(l)} \boldsymbol{a}^{(l-1)}+\boldsymbol{b}^{(l)} ,$ 偏导数</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial z^{(l)}}{\partial w_{i j}^{(l)}} &=[\frac{\partial z_{1}^{(l)}}{\partial w_{i j}^{(l)}}, \cdots, \frac{\partial z_{i}^{(l)}}{\partial w_{i j}^{(l)}}\cdots, \frac{\partial z_{M_{l}}^{(l)}}{\partial w_{i j}^{(l)}}]\\&=[0, \cdots, {\frac{\partial\left(\boldsymbol{w}_{i:}^{(l)} \boldsymbol{a}^{(l-1)}+b_{i}^{(l)}\right)}{\partial w_{i j}^{(l)}}}, \cdots, 0] \\&=\left[0, \cdots, a_{j}^{(l-1)}, \cdots, 0\right] \\& \triangleq \mathbb{l}_{i}\left(a_{j}^{(l-1)}\right) \quad \in \mathbb{R}^{1 \times M_{l}}\end{aligned}</script><p><img src="https://img.enderfga.cn/img/image-20211119205757205.png" alt=""></p><h2 id="2-2-误差项"><a href="#2-2-误差项" class="headerlink" title="2.2 误差项"></a>2.2 误差项</h2><p>误差项  $\delta^{(l)}  $也间接反映了不同神经元对网络能力的贡献程度, 从而比较好地解决 了贡献度分配问题 ( Credit Assignment Problem,CAP ).<br>根据 $ \boldsymbol{z}^{(l+1)}=\boldsymbol{W}^{(l+1)} \boldsymbol{a}^{(l)}+\boldsymbol{b}^{(l+1)} $, 有</p><script type="math/tex; mode=display">\frac{\partial \boldsymbol{z}^{(l+1)}}{\partial \boldsymbol{\alpha}^{(l)}}=\left(\boldsymbol{W}^{(l+1)}\right)^{\top} \quad \in \mathbb{R}^{M_{l} \times M_{l+1}}</script><p><img src="https://img.enderfga.cn/img/image-20211119205825727.png" alt=""></p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial \boldsymbol{\alpha}^{(l)}}{\partial \boldsymbol{z}^{(l)}} &=\frac{\partial f_{l}\left(\boldsymbol{z}^{(l)}\right)}{\partial \boldsymbol{z}^{(l)}} \\&=\operatorname{diag}\left(f_{l}^{\prime}\left(\boldsymbol{z}^{(l)}\right)\right) \quad \in \mathbb{R}^{M_{l} \times M_{l}}\end{aligned}</script><p>因此,根据链式法则,第  l  层的误差项为</p><script type="math/tex; mode=display">\begin{aligned}\delta^{(l)} & \triangleq \frac{\partial \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})}{\partial \boldsymbol{z}^{(l)}} \\&=\frac{\partial \boldsymbol{a}^{(l)}}{\partial \boldsymbol{z}^{(l)}}  \cdot \frac{\partial \boldsymbol{z}^{(l+1)}}{\partial \boldsymbol{a}^{(l)}} \cdot \frac{\partial \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})}{\partial \boldsymbol{z}^{(l+1)}} \\&={\operatorname{diag}\left(f_{l}^{\prime}\left(\boldsymbol{z}^{(l)}\right)\right)} \cdot\left(\boldsymbol{W}^{(l+1)}\right)^{\top}  \cdot{\delta^{(l+1)}} \\&=f_{l}^{\prime}\left(\boldsymbol{z}^{(l)}\right) \odot\left(\left(\boldsymbol{W}^{(l+1)}\right)^{\top} \delta^{(l+1)}\right) \quad \in \mathbb{R}^{M_{l}},\end{aligned}</script><p>其中 $ \odot  $是向量的 Hadamard 积运算符, 表示每个元素相乘.<br>从上面的公式可以看出,第  l  层的误差项可以通过第  l+1  层的误差项计算得到, 这就是误差的反向传播 ( BackPropagation, BP ). 反向传播算法的含义是: 第  l  层的一个神经元的误差项 ( 或敏感性 ) 是所有与该神经元相连的第  l+1  层 的神经元的误差项的权重和. 然后, 再乘上该神经元激活函数的梯度.<br>在计算出上面三个偏导数之后,</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})}{\partial w_{i j}^{(l)}} &={l}_{i}\left(a_{j}^{(l-1)}\right) \delta^{(l)} \\&=\left[0, \cdots, a_{j}^{(l-1)}, \cdots, 0\right]\left[\delta_{1}^{(l)}, \cdots, \delta_{i}^{(l)}, \cdots, \delta_{M_{l}}^{(l)}\right]^{\top} \\&=\delta_{i}^{(l)} a_{j}^{(l-1)}\end{aligned}</script><p><img src="https://img.enderfga.cn/img/image-20211119205855415.png" alt=""></p><script type="math/tex; mode=display">\left[\frac{\partial \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})}{\partial \boldsymbol{W}^{(l)}}\right]_{i j}=\left[\delta^{(l)}\left(\boldsymbol{a}^{(l-1)}\right)^{\top}\right]_{i j}</script><p>因此, $ \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})  $关于第  l  层权重 $ \boldsymbol{W}^{(l)}  $的梯度为</p><script type="math/tex; mode=display">\frac{\partial \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})}{\partial \boldsymbol{W}^{(l)}}=\delta^{(l)}\left(\boldsymbol{a}^{(l-1)}\right)^{\top} \quad \in \mathbb{R}^{M_{l} \times M_{l-1}}</script><p>同理,  $\mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})  $关于第  l  层偏置 $ \boldsymbol{b}^{(l)}  $的梯度为</p><script type="math/tex; mode=display">\frac{\partial \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})}{\partial \boldsymbol{b}^{(l)}}=\delta^{(l)} \quad \in \mathbb{R}^{M_{l}}</script><h2 id="2-3-实现步骤与伪代码"><a href="#2-3-实现步骤与伪代码" class="headerlink" title="2.3 实现步骤与伪代码"></a>2.3 实现步骤与伪代码</h2><p>在计算出每一层的误差项之后, 我们就可以得到每一层参数的梯度. 因此, 使用误差反向传播算法的前馈神经网络训练过程可以分为以下三步:</p><ol><li>前馈计算每一层的净输入  $\boldsymbol{z}^{(l)}  $和激活值 $ \boldsymbol{a}^{(l)} $, 直到最后一层;</li><li>反向传播计算每一层的误差项  $\delta^{(l)}$ ;</li><li>计算每一层参数的偏导数, 并更新参数.</li></ol><p>使用反向传播算法的随机梯度下降训练过程：</p><p><img src="https://img.enderfga.cn/img/image-20211119113727799.png" alt=""></p><h1 id="三、描述模型超参数确定的过程，分析模型训练结果-1"><a href="#三、描述模型超参数确定的过程，分析模型训练结果-1" class="headerlink" title="三、描述模型超参数确定的过程，分析模型训练结果"></a>三、描述模型超参数确定的过程，分析模型训练结果</h1><p>在神经网络中， 除了可学习的参数之外， 还存在很多超参数． 这些超参数对网络性能的影响也很大． 不同的机器学习任务往往需要不同的超参数． 常见的超参数有以下三类：</p><ol><li>网络结构,包括神经元之间的连接关系、层数、每层的神经元数量、激活函数的类型等.</li><li>优化参数,包括优化方法、学习率、小批量的样本数量等.</li><li>正则化系数.</li></ol><p>超参数优化（ Hyperparameter Optimization） 主要存在两方面的困难：</p><ul><li>超参数优化是一个组合优化问题， 无法像一般参数那样通过梯度下降方法来优化， 也没有一种通用有效的优化方法；</li><li>评估一组超参数配置（ Configuration）的时间代价非常高， 从而导致一些优化方法（ 比如演化算法（ Evolution Algorithm）） 在超参数优化中难以应用</li></ul><p>对于超参数的配置， 比较简单的方法有网格搜索、随机搜索、贝叶斯优化、动态资源分配和神经架构搜索 ，这次我们介绍最后两种。</p><h2 id="3-1-动态资源分配"><a href="#3-1-动态资源分配" class="headerlink" title="3.1 动态资源分配"></a>3.1 动态资源分配</h2><p>在超参数优化中， 每组超参数配置的评估代价比较高． 如果我们可以在较早的阶段就估计出一组配置的效果会比较差， 那么我们就可以中止这组配置的评估， 将更多的资源留给其他配置．这个问题可以归结为多臂赌博机问题的一个泛化问题： <strong>最优臂问题（ Best-Arm Problem）</strong>， 即在给定有限的机会次数下， 如何玩这些赌博机并找到收益最大的臂． 和多臂赌博机问题类似， 最优臂问题也是在利用和探索之间找到最佳的平衡．</p><p>由于目前神经网络的优化方法一般都采取随机梯度下降， 因此我们可以通过一组超参数的学习曲线来预估这组超参数配置是否有希望得到比较好的结果． 如果一组超参数配置的学习曲线不收敛或者收敛比较差， 我们可以应用<strong>早期停止（ Early-Stopping）</strong> 策略来中止当前的训练．</p><p>动态资源分配的关键是将有限的资源分配给更有可能带来收益的超参数组合． 一种有效方法是<strong>逐次减半（ Successive Halving）</strong> 方法， 将超参数优化看作一种非随机的最优臂问题． 假设要尝试 𝑁 组超参数配置， 总共可利用的资源预算（ 摇臂的次数） 为𝐵， 我们可以通过𝑇 = ⌈log2(𝑁)⌉ - 1轮逐次减半的方法来选取最优的配置．</p><h3 id="3-1-1-实现步骤与伪代码"><a href="#3-1-1-实现步骤与伪代码" class="headerlink" title="3.1.1 实现步骤与伪代码"></a>3.1.1 实现步骤与伪代码</h3><p><img src="https://img.enderfga.cn/img/image-20211119165313867.png" alt=""></p><p>在逐次减半方法中， 尝试的超参数配置数量 𝑁 十分关键． 如果𝑁 越大， 得到最佳配置的机会也越大， 但每组配置分到的资源就越少， 这样早期的评估结果可能不准确． 反之， 如果 𝑁 越小， 每组超参数配置的评估会越准确， 但有可能无法得到最优的配置． 因此， 如何设置 𝑁 是平衡“利用-探索” 的一个关键因素． 一种改进的方法是<strong>HyperBand方法</strong>， 通过尝试不同的𝑁 来选取最优参数．</p><h2 id="3-2-神经架构搜索"><a href="#3-2-神经架构搜索" class="headerlink" title="3.2 神经架构搜索"></a>3.2 神经架构搜索</h2><p>上面介绍的超参数优化方法都是在固定（ 或变化比较小） 的超参数空间 𝒳中进行最优配置搜索， 而最重要的神经网络架构一般还是需要由有经验的专家来进行设计 . 神经架构搜索（ Neural Architecture Search， NAS） 是一个新的比较有前景的研究方向， 通过神经网络来自动实现网络架构的设计． 一个神经网络的架构可以用一个变长的字符串来描述． 利用元学习的思想， 神经架构搜索利用一个控制器来生成另一个子网络的架构描述． 控制器可以由一个循环神经网络来实现． 控制器的训练可以通过强化学习来完成， 其奖励信号为生成的子网络在开发集上的准确率  .</p><h2 id="3-3-结果分析"><a href="#3-3-结果分析" class="headerlink" title="3.3 结果分析"></a>3.3 结果分析</h2><p><img src="https://img.enderfga.cn/img/image-20211119191016077.png" alt=""></p><p>在两次报告中我们一共介绍了5种超参数调优方式，也都尝试过应用到实验中，但并没有获得很大的提升。且结合数据量较小，网络结构较简单的实际，我们依旧选择了根据一些“经验结论”来进行网格搜索。</p><p>默认参数下，五次五折交叉验证结果：</p><p>Accuracy: 0.95</p><p>使用代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.neural_network <span class="hljs-keyword">import</span> MLPClassifier<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> cross_val_score<br><span class="hljs-keyword">import</span> warnings<br>warnings.filterwarnings(<span class="hljs-string">&#x27;ignore&#x27;</span>)<br>mlp = MLPClassifier(hidden_layer_sizes=(<span class="hljs-number">100</span>), activation=<span class="hljs-string">&#x27;relu&#x27;</span>, solver=<span class="hljs-string">&#x27;adam&#x27;</span>, alpha=<span class="hljs-number">0.0001</span>, batch_size=<span class="hljs-string">&#x27;auto&#x27;</span>, learning_rate=<span class="hljs-string">&#x27;constant&#x27;</span>, learning_rate_init=<span class="hljs-number">0.001</span>, power_t=<span class="hljs-number">0.5</span>, max_iter=<span class="hljs-number">200</span>, shuffle=<span class="hljs-literal">True</span>, random_state=<span class="hljs-literal">None</span>, tol=<span class="hljs-number">0.0001</span>, verbose=<span class="hljs-literal">False</span>, warm_start=<span class="hljs-literal">False</span>, momentum=<span class="hljs-number">0.9</span>, nesterovs_momentum=<span class="hljs-literal">True</span>, early_stopping=<span class="hljs-literal">False</span>, validation_fraction=<span class="hljs-number">0.1</span>, beta_1=<span class="hljs-number">0.9</span>, beta_2=<span class="hljs-number">0.999</span>, epsilon=<span class="hljs-number">1e-08</span>, n_iter_no_change=<span class="hljs-number">10</span>, max_fun=<span class="hljs-number">15000</span>) <span class="hljs-comment">#均以官网默认参数设置</span><br>mlp.fit(X_std, Y)<br>ACC = cross_val_score(mlp, X_std, Y, cv=rkf,scoring=<span class="hljs-string">&#x27;accuracy&#x27;</span>).mean()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Accuracy:&#x27;</span>,ACC)<br></code></pre></td></tr></table></figure><p>网格搜索最优参数的结果为：</p><p>Accuracy: 0.96125</p><p>混淆矩阵如图：</p><p><img src="https://img.enderfga.cn/img/image-20211119193317068.png" alt=""></p><p>ROC曲线如图：</p><p><img src="https://img.enderfga.cn/img/image-20211119193347579.png" alt=""></p><p>评价指标（精确率，召回率和F1值）：</p><p><img src="https://img.enderfga.cn/img/image-20211119193422505.png" alt=""></p><p>从上面可以看出，我们的模型分类结果完全正确，甚至有可能出现了过拟合。为了避免这种情况，我们继续修改参数，发现即使不设置任何超参数也可以达到这样的准确率，也从另一方面证明了神经网络拟合能力的强悍。</p><p>网格搜索代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 超参数调优</span><br><span class="hljs-keyword">import</span> warnings<br>warnings.filterwarnings(<span class="hljs-string">&#x27;ignore&#x27;</span>)<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> GridSearchCV<br>parameters = &#123;<span class="hljs-string">&#x27;hidden_layer_sizes&#x27;</span>: [(<span class="hljs-number">10</span>,<span class="hljs-number">10</span>,<span class="hljs-number">10</span>,<span class="hljs-number">10</span>,<span class="hljs-number">10</span>),(<span class="hljs-number">20</span>,<span class="hljs-number">20</span>,<span class="hljs-number">20</span>,<span class="hljs-number">20</span>,<span class="hljs-number">20</span>),(<span class="hljs-number">30</span>,<span class="hljs-number">30</span>,<span class="hljs-number">30</span>,<span class="hljs-number">30</span>,<span class="hljs-number">30</span>),(<span class="hljs-number">40</span>,<span class="hljs-number">40</span>,<span class="hljs-number">40</span>,<span class="hljs-number">40</span>,<span class="hljs-number">40</span>),(<span class="hljs-number">50</span>,<span class="hljs-number">50</span>,<span class="hljs-number">50</span>,<span class="hljs-number">50</span>,<span class="hljs-number">50</span>),(<span class="hljs-number">60</span>,<span class="hljs-number">60</span>,<span class="hljs-number">60</span>,<span class="hljs-number">60</span>,<span class="hljs-number">60</span>),(<span class="hljs-number">70</span>,<span class="hljs-number">70</span>,<span class="hljs-number">70</span>,<span class="hljs-number">70</span>,<span class="hljs-number">70</span>),(<span class="hljs-number">80</span>,<span class="hljs-number">80</span>,<span class="hljs-number">80</span>,<span class="hljs-number">80</span>,<span class="hljs-number">80</span>),(<span class="hljs-number">90</span>,<span class="hljs-number">90</span>,<span class="hljs-number">90</span>,<span class="hljs-number">90</span>,<span class="hljs-number">90</span>),(<span class="hljs-number">100</span>,<span class="hljs-number">100</span>,<span class="hljs-number">100</span>,<span class="hljs-number">100</span>,<span class="hljs-number">100</span>)],<br>                <span class="hljs-string">&#x27;activation&#x27;</span>: [<span class="hljs-string">&#x27;identity&#x27;</span>, <span class="hljs-string">&#x27;logistic&#x27;</span>,<span class="hljs-string">&#x27;tanh&#x27;</span>, <span class="hljs-string">&#x27;relu&#x27;</span>],<br>                <span class="hljs-string">&#x27;solver&#x27;</span>: [<span class="hljs-string">&#x27;adam&#x27;</span>,<span class="hljs-string">&#x27;lbgfs&#x27;</span>,<span class="hljs-string">&#x27;sgd&#x27;</span>],<br>                <span class="hljs-string">&#x27;alpha&#x27;</span>: [<span class="hljs-number">0.0001</span>, <span class="hljs-number">0.001</span>, <span class="hljs-number">0.01</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">1</span>,<span class="hljs-number">10</span>,<span class="hljs-number">100</span>],<br>                <span class="hljs-string">&#x27;learning_rate&#x27;</span>: [<span class="hljs-string">&#x27;constant&#x27;</span>, <span class="hljs-string">&#x27;invscaling&#x27;</span>, <span class="hljs-string">&#x27;adaptive&#x27;</span>]&#125;<br>grid = GridSearchCV(mlp, parameters, cv=rkf, scoring=<span class="hljs-string">&#x27;neg_mean_absolute_percentage_error&#x27;</span>,n_jobs=-<span class="hljs-number">1</span>)<br>grid.fit(X_std, Y)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;最优参数：&#x27;</span>,grid.best_params_)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;最优模型得分：&#x27;</span>,grid.best_score_)<br></code></pre></td></tr></table></figure><h1 id="四、总结模型训练过程中的收获-1"><a href="#四、总结模型训练过程中的收获-1" class="headerlink" title="四、总结模型训练过程中的收获"></a>四、总结模型训练过程中的收获</h1><h2 id="4-1-优化与正则化"><a href="#4-1-优化与正则化" class="headerlink" title="4.1 优化与正则化"></a>4.1 优化与正则化</h2><p>神经网络的优化和正则化是既对立又统一的关系． 一方面我们希望优化算法能找到一个全局最优解（ 或较好的局部最优解）， 另一方面我们又不希望模型优化到最优解， 这可能陷入过拟合． 优化和正则化的统一目标是期望风险最小化．</p><h3 id="4-1-1-优化"><a href="#4-1-1-优化" class="headerlink" title="4.1.1 优化"></a>4.1.1 优化</h3><p>在优化方面， 训练神经网络时的主要难点是非凸优化以及梯度消失问题． 在深度学习技术发展的初期， 我们通常需要利用预训练和逐层训练等比较低效的<br>方法来辅助优化．随着深度学习技术的发展， 我们目前通常可以高效地、 端到端地训练一个深度神经网络． 这些提高训练效率的方法通常分为以下 3 个方面：</p><ol><li>修改网络模型来得到更好的优化地形， 比如使用逐层归一化、 残差连接以及ReLU激活函数等；</li><li>使用更有效的优化算法， 比如动态学习率以及梯度估计修正等；</li><li>使用更好的参数初始化方法</li></ol><h3 id="4-1-2-泛化"><a href="#4-1-2-泛化" class="headerlink" title="4.1.2 泛化"></a>4.1.2 泛化</h3><p>在泛化方面， 传统的机器学习中有一些很好的理论可以帮助我们在模型的表示能力、复杂度和泛化能力之间找到比较好的平衡， 但是这些理论无法解释深度神经网络在实际应用中的泛化能力表现． 根据通用近似定理，神经网络的表示能力十分强大． 从直觉上， 一个过度参数化的神经网络很容易产生过拟合现象， 因为它的容量足够记住所有训练数据． 但是实验表明， 神经网络在训练过程中依然优先记住训练数据中的一般模式（ Pattern）， 即具有高泛化能力的模式 ． 但目前， 神经网络的泛化能力还没有很好的理论支持． 在传统机器学习模型上比较有效的ℓ1 或ℓ2 正则化在深度神经网络中作用也比较有限， 而一些经验的做法（ 比如小的批量大小、大的学习率、提前停止、丢弃法、数据增强） 会更有效  。</p><h2 id="4-2-与回归的差异"><a href="#4-2-与回归的差异" class="headerlink" title="4.2 与回归的差异"></a>4.2 与回归的差异</h2><p>和回归问题不同， 分类问题中的目标标签 𝑦 是离散的类别标签， 因此分类问题中的决策函数需要输出离散值或是标签的后验概率． 线性分类模型一般是一个广义线性函数， 即一个或多个线性判别函数加上一个非线性激活函数． 所谓“线性”是指决策边界由一个或多个超平面组成．</p><p><img src="https://img.enderfga.cn/img/image-20211119171348116.png" alt=""></p><h2 id="4-3-对深度学习框架的初步了解"><a href="#4-3-对深度学习框架的初步了解" class="headerlink" title="4.3 对深度学习框架的初步了解"></a>4.3 对深度学习框架的初步了解</h2><p>在深度学习中， 一般通过误差反向传播算法来进行参数学习． 采用手工方式来计算梯度再写代码实现的方式会非常低效， 并且容易出错． 此外， 深度学习模型需要的计算机资源比较多， 一般需要在 CPU 和 GPU 之间不断进行切换， 开发难度也比较大． 因此， 一些支持自动梯度计算、无缝CPU和GPU切换等功能的深度学习框架就应运而生．比较有代表性的框架包括： Theano、Caffe、TensorFlow、Pytorch、飞桨（ PaddlePaddle）、Chainer和MXNet等。</p><p><img src="https://img.enderfga.cn/img/image-20211119185714226.png" alt=""></p><p>因此本次作业我们没有局限在scikit-learn中，我们了解并学习了TensorFlow，pytorch等框架的使用，搭建了简易的神经网络对本次作业的数据进行分析。虽然最终得到的效果不及sklearn，但是在体验的过程加深了对神经网络的理解。</p>]]></content>
    
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>岭回归，特征工程分析advertising.csv</title>
    <link href="/2021/10/28/ridge/"/>
    <url>/2021/10/28/ridge/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>机器学习第二次作业记录。小组成员：方桂安，刘玥，周敏。</p><span id="more"></span><h1 id="一、数据分析"><a href="#一、数据分析" class="headerlink" title="一、数据分析"></a>一、数据分析</h1><h2 id="1-1-数据缺失检查"><a href="#1-1-数据缺失检查" class="headerlink" title="1.1 数据缺失检查"></a>1.1 数据缺失检查</h2><p>首先，为了我们能正常进行数据分析，我们进行了数据缺失分布情况检查。代码及结果如下：</p><p><img src="https://img.enderfga.cn/img/image-20211026214742931.png" alt=""></p><p>缺失值总数：</p><p><img src="https://img.enderfga.cn/img/image-20211026214800042.png" alt=""></p><p>由上可知，我们的数据中没有缺失值，不需要进行插值处理。</p><h2 id="1-2-销售量与各媒体投入关系分析"><a href="#1-2-销售量与各媒体投入关系分析" class="headerlink" title="1.2 销售量与各媒体投入关系分析"></a>1.2 销售量与各媒体投入关系分析</h2><h3 id="1-2-1-散点图"><a href="#1-2-1-散点图" class="headerlink" title="1.2.1 散点图"></a>1.2.1 散点图</h3><p><img src="https://img.enderfga.cn/img/image-20211026214824135.png" alt=""></p><p>以上是销售量与各项媒体投入量的散点图。从上图我们可以看出，sales和TV投入量有明显的正相关关系，随着TV投入增多，sales大体上呈上升趋势。sales和radio投入量也有较弱的正相关趋势，但sales分布在以radio投入量为指标时，分布较零散，相关关系弱于sales与TV投入量。而sales和newspaper的相关性最弱，sales集中分布在newspaper低投入区域内。</p><h3 id="1-2-2-各项数据分析"><a href="#1-2-2-各项数据分析" class="headerlink" title="1.2.2 各项数据分析"></a>1.2.2 各项数据分析</h3><p><img src="https://img.enderfga.cn/img/image-20211026215012236.png" alt=""></p><p>由上图可知，TV类广告的平均投入量最大，其投入量的最小值，二分位数，中位数和四分位数，最大值均大于其他类型的广告，说明企业偏向于在TV类广告投入更多资金。</p><h3 id="1-2-3-相关系数"><a href="#1-2-3-相关系数" class="headerlink" title="1.2.3 相关系数"></a>1.2.3 相关系数</h3><p><img src="https://img.enderfga.cn/img/image-20211026215047222.png" alt=""></p><p>上图为四个变量的相关系数热力图，由此可以看出，销售量和TV，radio，newspaper的相关性依次减弱。</p><h3 id="1-2-4-散点图矩阵，多变量之间的关系可视化"><a href="#1-2-4-散点图矩阵，多变量之间的关系可视化" class="headerlink" title="1.2.4 散点图矩阵，多变量之间的关系可视化"></a>1.2.4 散点图矩阵，多变量之间的关系可视化</h3><p><img src="https://img.enderfga.cn/img/image-20211026215104208.png" alt=""></p><h2 id="1-3-得出结论"><a href="#1-3-得出结论" class="headerlink" title="1.3 得出结论"></a>1.3 得出结论</h2><p>由上面的分析可知，销售量和TV投入量相关性最大，其次是radio，newspaper，这也符合我们目前的社会情况，人们更多的是在电视等电子产品上获取信息。所以，加大上述三种广告方式的投入会对销售量有依次递减的增幅影响。</p><h1 id="二、描述10折交叉验证对数据集的处理"><a href="#二、描述10折交叉验证对数据集的处理" class="headerlink" title="二、描述10折交叉验证对数据集的处理"></a>二、描述10折交叉验证对数据集的处理</h1><h2 id="2-1-引入10折交叉验证的原因"><a href="#2-1-引入10折交叉验证的原因" class="headerlink" title="2.1 引入10折交叉验证的原因"></a>2.1 引入10折交叉验证的原因</h2><p>泛化能力是指模型在训练集上训练后,对新数据进行准确预测的能力。在机器学习的模型选择中，我们要对候选模型的泛化误差进行评估，然后选择泛化误差最小的那个模型。而实际应用中，我们无法直接获得泛化误差，而训练误差又由于过拟合现象的存在而不适合作为标准，所以我们随机将数据集切为三部分：</p><ul><li>训练集：用来训练模型，对应训练误差。</li><li>验证集：用来选择模型，对应测试误差。</li><li>测试集：用来最终对学习方法进行评估，对应泛化误差的近似。</li></ul><p>但是在实际应用中数据往往是不充足的，为了选择泛化能力更好的模型，我们可以对数据集D进行适当的处理，从中产生出训练集S和测试集T。几种常见的做法有：简单交叉验证(holdout cross-validation)、留一交叉验证(leave-one-out cross-validation,LOOCV)、<em>k</em>折交叉验证(<em>k</em>-fold cross-validation)、多重<em>k</em>折交叉验证、分层法(stratification-split cross-validation)、自助法(bootstraps)等。而综合考虑几种方法的特点后，本次我们选择的处理方法是10折交叉验证法。</p><h2 id="2-2-10折交叉验证的基本原理"><a href="#2-2-10折交叉验证的基本原理" class="headerlink" title="2.2 10折交叉验证的基本原理"></a>2.2 10折交叉验证的基本原理</h2><p>10折交叉验证是指将原始数据集随机划分为样本数量近乎相等的10个子集，轮流将其中的9个合并作为训练集，其余1个作为测试集。在每次试验中计算正确率等评价指标，最终通过k次试验后取评价指标的平均值来评估该模型的泛化能力。</p><p>10折交叉验证的基本步骤如下:</p><ol><li>原始数据集划分为10个样本量尽可能均衡的子集；</li><li>使用第1个子集作为测试集，第2～9个子集合并作为训练集；</li><li>使用训练集对模型进行训练,计算多种评价指标在测试集下的结果；</li><li>重复2-3步骤,轮流将第2-10个子集作为测试集；</li><li>计算各评价指标的平均值作为最终结果，最终选出10次测评中平均测试误差最小的模型。</li></ol><p>10折交叉验证的原理示意见下图。</p><p><img src="https://img.enderfga.cn/img/image-20211026214227086.png" alt=""></p><p>由于将数据集D划分为k个子集同样存在多种划分方式，为了减小因样本划分不同而引入的差别，k折交叉验证通常要随机使用不同的划分重复p次。故我们可以采用“10次10折交叉验证”。</p><h2 id="2-3-10折交叉验证函数python代码"><a href="#2-3-10折交叉验证函数python代码" class="headerlink" title="2.3 10折交叉验证函数python代码"></a>2.3 10折交叉验证函数python代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> KFold  <span class="hljs-comment"># 从sklearn导入KFold包</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">Ten_Flod_spilt</span>(<span class="hljs-params">fold,data,label</span>):</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    param fold: 要取第几折的数据。</span><br><span class="hljs-string">    param data: 需要分块的数据</span><br><span class="hljs-string">    param label: 对应的需要分块标签</span><br><span class="hljs-string">    return: 对应折的训练集、测试集和对应的标签</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    split_list = []<br>    kf = KFold(n_splits=<span class="hljs-number">10</span>)<br>    <span class="hljs-keyword">for</span> train, test <span class="hljs-keyword">in</span> kf.split(data):<br>        split_list.append(train.tolist())<br>        split_list.append(test.tolist())<br>    train,test=split_list[<span class="hljs-number">2</span> * fold],split_list[<span class="hljs-number">2</span> * fold + <span class="hljs-number">1</span>]<br>    <span class="hljs-keyword">return</span>  data[train], data[test], label[train], label[test]  <span class="hljs-comment">#已经分好块的数据集</span><br></code></pre></td></tr></table></figure><p>在后续使用中只需循环调用该函数即可达到10折交叉验证的目的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>        x_train, x_test, y_train, y_test = Ten_Flod_spilt(i,X_s,y_s)<br></code></pre></td></tr></table></figure><h1 id="三、描述所使用的线性模型"><a href="#三、描述所使用的线性模型" class="headerlink" title="三、描述所使用的线性模型"></a>三、描述所使用的线性模型</h1><h2 id="3-1-基本形式"><a href="#3-1-基本形式" class="headerlink" title="3.1 基本形式"></a>3.1 基本形式</h2><p>给定由d个属性描述的示例<strong>x</strong>=(x~1~;x~2~;… ; x~d~)，其中x~i~是<strong>x</strong>在第i个属性上的取值，线性回归(linear regression)试图学得一个通过属性的线性组合来进行预测的函数，即</p><script type="math/tex; mode=display">f(\boldsymbol{x})=w_{1} x_{1}+w_{2} x_{2}+\ldots+w_{d} x_{d}+b</script><p>一般用向量形式写成</p><script type="math/tex; mode=display">f(\boldsymbol{x})=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b</script><p>其中<strong>w</strong>=(w~1~;w~2~;… ; w~d~)。<strong>w</strong>和b学得之后，模型就得以确定。</p><p>故本题中的模型应该为</p><script type="math/tex; mode=display">\widehat{Sales}=w_{1} ·TV+w_{2} ·radio+w_{3} ·newspaper+b，使得\widehat{Sales}\cong Sales</script><p>此处有三个属性描述样本，故又称为多元线性回归(multivariate linear regression)。</p><p>其基本形式为</p><script type="math/tex; mode=display">\hat{f}\left(\hat{x}_{N+1}\right)=\hat{x}_{N+1}^{T} \widehat{\omega}^{*}\\其中  \hat{x}_{N+1}=\left(x_{N+1} ; 1\right) \in \mathbb{R}^{n+1}, \widehat{\omega}^{*}=\left(\omega^{*} ; b^{*}\right) \in \mathbb{R}^{n+1}</script><h2 id="3-2-岭回归"><a href="#3-2-岭回归" class="headerlink" title="3.2 岭回归"></a>3.2 岭回归</h2><p>吉洪诺夫正则化以安德烈·尼古拉耶维奇·吉洪诺夫命名，为非适定性问题的正则化中最常见的方法。在统计学中，本方法被称为脊回归或岭回归（ridge regression）；在机器学习领域则称为权重衰减或权值衰减（weight decay）。因为有不同的数学家独立发现此方法，此方法又称做吉洪诺夫－米勒法（Tikhonov–Miller method）、菲利浦斯－图米法（Phillips–Twomey method）、受限线性反演（constrained linear inversion method），或线性正规化（linear regularization）。</p><script type="math/tex; mode=display">min\ L(W)=\frac{1}{2}(XW-y)^T(XW-y)+\frac{1}{2}\alpha||W||^2_2</script><script type="math/tex; mode=display">W=(X^TX+\alpha I)^{-1}X^Ty</script><p>根据4.2、4.3的分析，我们最终决定在最小二乘法的基础上采取L2正则化，即岭回归。相应地，为了使用岭回归和缩减技术，首先需要对特征做标准化处理。因为，我们需要使每个维度特征具有相同的重要性，故采用了z-score标准化。随着模型复杂度的提升，在训练集上的效果就越好，即模型的偏差就越小；但是同时模型的方差就越大。对于岭回归的α而言，随着α的增大，$|X^TX+\alpha I|$就越大，$(X^TX+\alpha I)^{-1}$ 就越小，模型的方差就越小；而α越大使得<strong>W</strong>的估计值更加偏离真实值，模型的偏差就越大。所以岭回归的关键是找到一个合理的α值来平衡模型的方差和偏差。</p><p>本次使用10折交叉验证法来确定α值，每一种训练集和测试集下都会有对应的一个模型及模型评分（如均方误差），进而可以得到一个平均评分。对于α值则选择平均评分最优的α值。</p><h2 id="3-3-特征工程"><a href="#3-3-特征工程" class="headerlink" title="3.3 特征工程"></a>3.3 特征工程</h2><p><img src="https://img.enderfga.cn/img/image-20211027233323219.png" alt=""></p><p>如图所示为梯度下降法，最小二乘法和sklearn调用所得结果与真实值的对比折线图。从中可以看出，三种折线都已经接近重合，但又与真实值存在差异。查阅资料后，我们了解了特征工程的相关知识。</p><p><img src="https://img.enderfga.cn/img/967090-20170116151505067-1134887580.png" alt=""></p><p>“数据决定了机器学习的上限，而算法只是尽可能逼近这个上限”，这里的数据指的就是经过特征工程得到的数据。特征工程指的是把原始数据转变为模型的训练数据的过程，它的目的就是获取更好的训练数据特征，使得机器学习模型逼近这个上限。特征工程能使得模型的性能得到提升，有时甚至在简单的模型上也能取得不错的效果。特征工程在机器学习中占有非常重要的作用，一般认为括特征构建、特征提取、特征选择三个部分。特征构建比较麻烦，需要一定的经验。 特征提取与特征选择都是为了从原始特征中找出最有效的特征。它们之间的区别是特征提取强调通过特征转换的方式得到一组具有明显物理或统计意义的特征；而特征选择是从特征集合中挑选一组具有明显物理或统计意义的特征子集。两者都能帮助减少特征的维度、数据冗余，特征提取有时能发现更有意义的特征属性，特征选择的过程经常能表示出每个特征的重要性对于模型构建的重要性。</p><p>本次作业中主要使用了特征构建、特征选择、特征缩放，具体结果将在第五部分讨论。</p><h1 id="四、描述训练模型所使用的算法"><a href="#四、描述训练模型所使用的算法" class="headerlink" title="四、描述训练模型所使用的算法"></a>四、描述训练模型所使用的算法</h1><h2 id="4-1-数据预处理"><a href="#4-1-数据预处理" class="headerlink" title="4.1 数据预处理"></a>4.1 数据预处理</h2><p><img src="https://img.enderfga.cn/img/image-20211027090542621.png" alt=""></p><p>本次数据处理使用的是z-score标准化，转换公式为：</p><script type="math/tex; mode=display">z=\frac{x-\mu}{\sigma}</script><p>使用python具体实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fit_transform</span>(<span class="hljs-params">x</span>):</span><br>    x = np.asarray(x)<br>    std_ = np.std(x, axis=<span class="hljs-number">0</span>)  <span class="hljs-comment"># 标准差</span><br>    mean_ = np.mean(x, axis=<span class="hljs-number">0</span>)  <span class="hljs-comment"># 均值</span><br>    <span class="hljs-keyword">return</span> (x - mean_) / std_<br></code></pre></td></tr></table></figure><h2 id="4-2-策略"><a href="#4-2-策略" class="headerlink" title="4.2 策略"></a>4.2 策略</h2><h3 id="4-2-1-经验风险最小化"><a href="#4-2-1-经验风险最小化" class="headerlink" title="4.2.1 经验风险最小化"></a>4.2.1 经验风险最小化</h3><p>均方误差是回归任务中最常用的性能度量，因此我们可试图让均方误差最小化，即</p><script type="math/tex; mode=display">\begin{aligned}\left(w^{*}, b^{*}\right) &=\underset{(w, b)}{\arg \min } \sum_{i=1}^{m}\left(f\left(x_{i}\right)-y_{i}\right)^{2} \\&=\underset{(w, b)}{\arg \min } \sum_{i=1}^{m}\left(y_{i}-w x_{i}-b\right)^{2}\end{aligned}</script><p>均方误差有非常好的几何意义，它对应了常用的欧几里得距离或简称“欧氏距离”(Euclidean distance)。基于均方误差最小化来进行模型求解的方法称为“最小二乘法”(least square method)。具体求解过程在4.3中进行介绍。</p><h3 id="4-2-2-结构风险最小化"><a href="#4-2-2-结构风险最小化" class="headerlink" title="4.2.2 结构风险最小化"></a>4.2.2 结构风险最小化</h3><h4 id="4-2-2-1-正则化"><a href="#4-2-2-1-正则化" class="headerlink" title="4.2.2.1 正则化"></a>4.2.2.1 正则化</h4><p>当模型的复杂度增大时，训练误差会逐渐减小并趋于0；而测试误差会先减小，达到最大值后又增大。当选择的模型复杂度过大时，就会发生过拟合，如下图所示。</p><p><img src="https://img.enderfga.cn/img/image-20211027095448459.png" alt=""></p><p>为了避免因为过拟合问题而导致拟合效果不佳，我们在经验风险上加一个正则化项或罚项，使结构风险最小，这种方法叫做正则化，一般具有如下形式：</p><script type="math/tex; mode=display">\min _{f \in \mathcal{F}} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)+\lambda J(f)</script><h4 id="4-2-2-2-L1与L2正则化"><a href="#4-2-2-2-L1与L2正则化" class="headerlink" title="4.2.2.2 L1与L2正则化"></a>4.2.2.2 L1与L2正则化</h4><p>使用L1范数（也称曼哈顿距离或Taxicab范数，只允许在与空间轴平行行径的距离）又叫<strong>lasso</strong>回归，损失函数变为：</p><script type="math/tex; mode=display">J(\mathbf{W})=\frac{1}{2 n}(\mathbf{X} \mathbf{W}-\mathbf{Y})^{T}(\mathbf{X} \mathbf{W}-\mathbf{Y})+\alpha\|W\|_{1}</script><p>使用L2范数（也称欧几里德距离，是向量到原点的最短距离）又叫<strong>ridge</strong>回归，损失函数变为：</p><script type="math/tex; mode=display">J(\mathbf{W})=\frac{1}{2}(\mathbf{X} \mathbf{W}-\mathbf{Y})^{T}(\mathbf{X} \mathbf{W}-\mathbf{Y})+\frac{1}{2} \alpha\|W\|_{2}^{2}</script><p>L1能使得一些特征的系数变小，甚至还使一些绝对值较小的系数直接变为0，产生稀疏解，起到特征选择的作用，增强模型的泛化能力。</p><p>L2的优点是可以限制|w|的大小，从而使模型更简单，更稳定，即使加入一些干扰样本也不会对模型产生较大的影响，而且还能解决非正定的问题，强制使XTX可逆有解。</p><script type="math/tex; mode=display">\theta=\left(X X^{T}+\alpha I\right)^{-1} X Y</script><p><img src="https://img.enderfga.cn/img/1335117-20180708193526314-357302334.png" alt=""></p><p><img src="https://img.enderfga.cn/img/1335117-20180708194712773-1094778410.png" alt=""></p><p>在上图中，两个坐标分别是要学习到的两个参数ω1和ω2；彩色线是损失函数J的等高线即损失值相等线；方形和圆形就分别是L1和L2所产生的额外误差（约束空间）；最后的目标要是两者最小，即要得到能使两者相加最小的点，也就是图中的黑色交点。在画等差图时，L1的效果就很容易与坐标轴相交了，这就是会产生很多0，即造成参数稀疏的原因。而且同时如果给一个微小的偏移，L2移动不会很大，而L1可能会移动到方形边上产生很多的交点，所以L1比较不稳定。</p><p>L2倾向于使ω的分量取值更均衡，即非零分量个数更稠密，而L1倾向ω的分量取值更稀疏，即非零分量个数更少。所以从图可以看出L1的边缘比较尖锐，与目标函数的等高线相交时，交点会常在那些尖锐的地方，所以很多的参数就是0，即L1能产生稀疏解。所以在调参时如果我们主要的目的只是为了解决过拟合，一般选择L2正则化就够了。但是如果选择L2正则化发现还是过拟合，即预测效果差的时候，就可以考虑L1正则化。另外，如果模型的特征非常多，我们希望一些不重要的特征系数归零，从而让模型系数稀疏化的话，也可以使用L1正则化。</p><p>综合考虑，我们在本次的损失函数中引入的是L2正则化。</p><h2 id="4-3-算法"><a href="#4-3-算法" class="headerlink" title="4.3 算法"></a>4.3 算法</h2><h3 id="4-3-1-最小二乘法"><a href="#4-3-1-最小二乘法" class="headerlink" title="4.3.1 最小二乘法"></a>4.3.1 最小二乘法</h3><h4 id="4-3-1-1-问题分析"><a href="#4-3-1-1-问题分析" class="headerlink" title="4.3.1.1 问题分析"></a>4.3.1.1 问题分析</h4><p>我们的策略是</p><p><img src="https://img.enderfga.cn/img/image-20211028184420327.png" alt=""></p><p>我们进行展开</p><p><img src="https://img.enderfga.cn/img/image-20211028184438472.png" alt=""></p><p>下面，我们进行梯度推导</p><p><img src="https://img.enderfga.cn/img/image-20211028184450724.png" alt=""></p><p>由于$L(W)$是关于$W$的凸函数，所以我们在梯度为零的点，即是我们要求的最优解。</p><script type="math/tex; mode=display">令\frac{\partial L}{\partial W}=0\\得(X^TX+\alpha I)W=X^Ty</script><p>我们要通过此方法求得$W$，需要的条件是$X^TX+\alpha I$可逆，若其可逆，则$W$的解是</p><script type="math/tex; mode=display">W=(X^TX+\alpha I)^{-1}X^Ty</script><p>因为最小二乘法要求$X^TX+\alpha I$必须存在可逆矩阵，在实际问题中可能不满足，于是我们下面采用梯度下降法进行迭代求解。</p><h4 id="4-3-1-2-代码实现"><a href="#4-3-1-2-代码实现" class="headerlink" title="4.3.1.2 代码实现"></a>4.3.1.2 代码实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">lms</span>(<span class="hljs-params">x_train, x_test, y_train, y_test</span>):</span><br>    x_train_=np.c_[np.ones([x_train.shape[<span class="hljs-number">0</span>],<span class="hljs-number">1</span>]),x_train]<br>    x_test_=np.c_[np.ones([x_test.shape[<span class="hljs-number">0</span>],<span class="hljs-number">1</span>]),x_test]<br>    theta_n = np.dot(np.dot(inv(np.dot(x_train_.T, x_train_)+<span class="hljs-number">0.1</span>*np.eye(x_train_.shape[<span class="hljs-number">1</span>])), x_train_.T), y_train)  <span class="hljs-comment"># theta = (X`X)^(-1)X`Y，其中X`表示X的转置，使用L2范数正则化</span><br>    y_pre = np.dot(x_test_, theta_n)<br>    mse = np.average((y_test - y_pre) ** <span class="hljs-number">2</span>)<br>    <span class="hljs-keyword">return</span> theta_n, y_pre, mse<br></code></pre></td></tr></table></figure><h3 id="4-3-2-梯度下降法"><a href="#4-3-2-梯度下降法" class="headerlink" title="4.3.2 梯度下降法"></a>4.3.2 梯度下降法</h3><h4 id="4-3-2-1-问题分析"><a href="#4-3-2-1-问题分析" class="headerlink" title="4.3.2.1 问题分析"></a>4.3.2.1 问题分析</h4><p>首先，我们的目标是下式</p><script type="math/tex; mode=display">令\hat{\omega}=W\\E(\hat{\omega})=\frac{1}{2}(X\hat{\omega}-y)^T(X\hat{\omega}-y)+\frac{1}{2}\alpha||\hat{\omega}||^2_2\quad,\hat{\omega}=\mathop{argmin}_\hat{\omega}\ E(\hat{\omega})</script><p>梯度下降法是一种迭代算法：我们选取适当的初始值$\hat{\omega}^{(0)}$，不断迭代，更新$\hat{\omega}$的值，进行目标函数的极小化，直到收敛。由于负梯度方向是使得函数值下降最快的方向，在迭代的每一步，以负梯度方向更新$\hat{\omega}$的值，从而达到减小函数值的目的。如下图形象化表示：</p><p><img src="https://img.enderfga.cn/img/image-20211027100605390.png" alt=""></p><h4 id="4-3-2-2-核心思想"><a href="#4-3-2-2-核心思想" class="headerlink" title="4.3.2.2 核心思想"></a>4.3.2.2 核心思想</h4><ol><li>$E(\hat{\omega})$是具有一阶连续偏导数的凸函数，其极值点在一阶导数为零的地方取得</li><li>一阶泰勒展开：$E(\hat{\omega})\thickapprox E(\hat{\omega}^{(k)})+\nabla E(\hat{\omega}^{(k)})(\hat{\omega}-\hat{\omega}^{(k)})$，其中，$\nabla E(\hat{\omega}^{(k)})$是$E(\hat{\omega})$在$\hat{\omega}^{(k)}$的梯度：</li></ol><script type="math/tex; mode=display">\nabla E(\hat{\omega}^{(k)})=\frac{\partial E(\hat{\omega})}{\partial \hat{\omega}}|_{\hat{\omega}=\hat{\omega}^{(k)}}</script><ol><li>求取第k+1次迭代值：$\hat{\omega}^{k+1}=\hat{\omega}^{(k)}+\eta_k*(-\nabla E(\hat{\omega}^{(k)}))$，其中$\eta_k$是步长，有我们最初指定。梯度如下（推导在上面部分）：<script type="math/tex; mode=display">\frac{\partial E}{\partial \hat{\omega}}=X^TX\hat{\omega}-X^Ty+\alpha I\hat{\omega}\quad(I是n\times n的单位矩阵)</script></li></ol><h4 id="4-3-2-3-求解步骤"><a href="#4-3-2-3-求解步骤" class="headerlink" title="4.3.2.3 求解步骤"></a>4.3.2.3 求解步骤</h4><p>输入：目标函数$E(\hat{\omega})$，梯度函数$\nabla E(\hat{\omega})$，计算精度ε，步长$\eta_k$；</p><p>输出： $E(\hat{\omega})$的极小点$\hat{\omega}^*$。</p><p>（1）取初始值$\hat{\omega}^{(0)}\in \mathbb{R}^{d+1}$，置k=0；</p><p>（2）计算$E(\hat{\omega}^{(k)})$；</p><p>（3）计算梯度$\nabla E(\hat{\omega}^{(k)})$，当$||\nabla E(\hat{\omega}^{(k)})||&lt;\varepsilon$时，令$\hat{\omega}^*=\hat{\omega}^{(k)}$，</p><p>停止迭代；</p><p>（4）置$\hat{\omega}^{(k+1)}=\hat{\omega}^{(k)}+\eta_k(-\nabla E(\hat{\omega}^{(k)}))$，计算$E(\hat{\omega}^{(k+1)})$，</p><p>当$||E(\hat{\omega}^{(k+1)})-E(\hat{\omega}^{(k)})||&lt;\varepsilon$或$||\hat{\omega}^{(k+1)}-\hat{\omega}^{(k)}||&lt;\varepsilon$时，</p><p>令$\hat{\omega}^*=\hat{\omega}^{(k)}$，停止迭代；</p><p>（5）否则，置k=k+1，转步骤（3）。</p><h4 id="4-3-2-4-代码实现"><a href="#4-3-2-4-代码实现" class="headerlink" title="4.3.2.4 代码实现"></a>4.3.2.4 代码实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">GradientDescent_MultiLine</span>:</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, lr, epochs</span>):</span><br>        self.lr = lr  <span class="hljs-comment"># 学习率，用来控制步长（权重调整幅度）</span><br>        self.epochs = epochs  <span class="hljs-comment"># 循环迭代的次数</span><br>        self.lose = []  <span class="hljs-comment"># 损失值计算（损失函数）：均方误差</span><br><br>    <span class="hljs-string">&#x27;&#x27;&#x27;根据提供的训练数据对模型进行训练&#x27;&#x27;&#x27;</span><br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fit</span>(<span class="hljs-params">self, x, y</span>):</span><br>        x = np.asarray(x)<br>        y = np.asarray(y)<br>        y = np.squeeze(y)  <span class="hljs-comment"># 去掉冗余的维度</span><br><br>        self.w = np.zeros(<span class="hljs-number">1</span> + x.shape[<span class="hljs-number">1</span>])  <span class="hljs-comment"># 初始权重，权重向量初始值为0（或任何其他值），长度比X的特征数量多1（多出来的为截距）</span><br><br>        <span class="hljs-comment"># 开始训练</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.epochs):<br>            y_hat = np.dot(x, self.w[<span class="hljs-number">1</span>:]) + self.w[<span class="hljs-number">0</span>]  <span class="hljs-comment"># 计算预测值</span><br>            error = y - y_hat  <span class="hljs-comment"># 计算真实值与预测值之间的差距</span><br>            self.lose.append(np.<span class="hljs-built_in">sum</span>(error ** <span class="hljs-number">2</span>) / <span class="hljs-number">2</span> + <span class="hljs-number">0.1</span>* np.dot(self.w.T, self.w))  <span class="hljs-comment"># 将损失加入到损失列表中，使用L2范数正则化</span><br>            <span class="hljs-comment">#print(&quot;迭代次数:&#123;0&#125;,进度：&#123;1&#125;%&quot;.format(i + 1, 100.0 * (i + 1) / self.epochs), &quot;  loss:&quot;, np.sum(error ** 2) / 2)</span><br>            <span class="hljs-comment"># j &lt;- j + α * sum((y - y_hat) * x(j))</span><br>            x_=np.c_[np.ones([x.shape[<span class="hljs-number">0</span>],<span class="hljs-number">1</span>]),x]<br>            <span class="hljs-comment">#self.w[0] += self.lr * np.sum(error)</span><br>            <span class="hljs-comment">#self.w[1:] += self.lr * np.dot(x.T, error)</span><br>            I=np.identity(x_.shape[<span class="hljs-number">1</span>])<br>            self.w=self.w-self.lr*(np.dot((np.dot(x_.T, x_)+<span class="hljs-number">0.2</span>*I), self.w)-np.dot(x_.T, y))<br></code></pre></td></tr></table></figure><h1 id="五、分析模型训练结果，包括训练误差和测试误差"><a href="#五、分析模型训练结果，包括训练误差和测试误差" class="headerlink" title="五、分析模型训练结果，包括训练误差和测试误差"></a>五、分析模型训练结果，包括训练误差和测试误差</h1><h2 id="5-1-评估指标计算公式"><a href="#5-1-评估指标计算公式" class="headerlink" title="5.1 评估指标计算公式"></a>5.1 评估指标计算公式</h2><p>训练误差是模型关于训练数据集的平均损失；测试误差是模型关于测试数据集的平均损失。计算公式如下：</p><script type="math/tex; mode=display">R_{e m p}(\hat{f})=\frac{1}{N} \sum_{i=1}^{N} L\left(y, \hat{f}\left(x_{i}\right)\right)</script><script type="math/tex; mode=display">e_{t e s t}=\frac{1}{N^{\prime}} \sum_{i=1}^{N^{\prime}} L\left(y_{i}, \hat{f}\left(x_{i}\right)\right)</script><p>其中N为训练样本容量，<em>N</em>′为测试样本容量。由于我们在线性回归中使用的是平方损失函数，故上述计算结果又叫均方误差 MSE（Mean Squared Error）：</p><script type="math/tex; mode=display">M S E=\frac{1}{m} \sum_{i=1}^{m}\left(y_{\text {test }}^{(i)}-\hat{y}_{\text {test }}^{(i)}\right)^{2}</script><p>但是，MSE 公式有一个问题是会改变量纲。因为公式平方了，我们可以对这个MSE开方，得到第二个评价指标：均方根误差 RMSE（Root Mean Squared Error）：</p><script type="math/tex; mode=display">R M S E = \sqrt{M S E}=\sqrt{\frac{1}{m} \sum_{i=1}^{m}\left(y_{\text {test }}^{(i)}-\hat{y}_{\text {test }}^{(i)}\right)^{2}}</script><p>但是MSE不甚全面，某些情况下决定系数 R2（coefficient of determination）显得尤为有用，它可以看作是MSE的标准化版本，用于更好地解释模型的性能。R2值的定义如下：</p><script type="math/tex; mode=display">R^{2}=1-\frac{\left(\sum_{i=1}^{m}\left(\hat{y}^{(i)}-y^{(i)}\right)^{2}\right) / m}{\left(\sum_{i=1}^{m}\left(y^{(i)}-\bar{y}\right)^{2}\right) / m}=1-\frac{M S E(\hat{y}, y)}{\operatorname{Var}(y)}</script><h2 id="5-2-误差分析思路"><a href="#5-2-误差分析思路" class="headerlink" title="5.2 误差分析思路"></a>5.2 误差分析思路</h2><p>结合前文的推导分析，我们最终采用的是线性最小二乘法与L2正则化，即alpha取值为1.0的<strong>Ridge回归</strong>，并结合特征工程中特征构建（将<strong>TV*radio</strong>，<strong>radio*newspaper</strong>作为新的特征），特征选择（加入新的特征，舍弃相关系数较小的newspaper），特征缩放（将TV，radio，newspaper进行开方、平方、三次方等）的思路进行了14种情况的实验，并得出了每一种情况的MSE，RMSE，R^2^。</p><p>所使用的代码为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">X = np.asarray(data_s.get([<span class="hljs-string">&#x27;TV&#x27;</span>,<span class="hljs-string">&#x27;radio&#x27;</span>,<span class="hljs-string">&#x27;newspaper&#x27;</span>]))<br>y = np.asarray(data_s.get(<span class="hljs-string">&#x27;sales&#x27;</span>))<br>clf.fit(X,y)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;MSE=%f&#x27;</span>%(-<span class="hljs-number">0.1</span>*cross_val_score(clf, X, y, cv=<span class="hljs-number">10</span>, scoring=<span class="hljs-string">&#x27;neg_mean_squared_error&#x27;</span>).<span class="hljs-built_in">sum</span>()))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;RMSE=%f&#x27;</span>%(-<span class="hljs-number">0.1</span>*cross_val_score(clf, X, y, cv=<span class="hljs-number">10</span>, scoring=<span class="hljs-string">&#x27;neg_root_mean_squared_error&#x27;</span>).<span class="hljs-built_in">sum</span>()))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;R2=%f&#x27;</span>%(<span class="hljs-number">10</span>*cross_val_score(clf, X, y, cv=<span class="hljs-number">10</span>, scoring=<span class="hljs-string">&#x27;r2&#x27;</span>).<span class="hljs-built_in">sum</span>())+<span class="hljs-string">&#x27;%&#x27;</span>)<br><span class="hljs-built_in">print</span>(clf.coef_)<br><span class="hljs-built_in">print</span>(clf.intercept_)<br></code></pre></td></tr></table></figure><h2 id="5-3-训练结果"><a href="#5-3-训练结果" class="headerlink" title="5.3 训练结果"></a>5.3 训练结果</h2><p>14种情况的训练误差及测试误差记录在jupyter notebook的ipynb文件中，此处展示其中三种。</p><p><img src="https://img.enderfga.cn/img/image-20211028143446583.png" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20211028143509634.png" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20211028144137987.png" alt=""></p><p>上图所示的第13种情况训练所得模型的各项评估指标最优，故将其model文件保存，助教老师可以使用test.ipynb自动载入模型，并计算出测试误差MSE。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> joblib<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> mean_squared_error <br>model = joblib.load(<span class="hljs-string">&#x27;model.pickle&#x27;</span>) <span class="hljs-comment">#载入模型</span><br>data=pd.read_csv(<span class="hljs-string">&#x27;5_test.csv&#x27;</span>) <span class="hljs-comment">#读入数据</span><br>data_s = (data-data.<span class="hljs-built_in">min</span>())/(data.<span class="hljs-built_in">max</span>()-data.<span class="hljs-built_in">min</span>()) <span class="hljs-comment">#归一化</span><br>data_s[<span class="hljs-string">&#x27;TV_min&#x27;</span>] = data_s[<span class="hljs-string">&#x27;TV&#x27;</span>].apply(<span class="hljs-keyword">lambda</span> x:x**<span class="hljs-number">0.2</span>)<br>data_s[<span class="hljs-string">&#x27;TV_radio&#x27;</span>]=data_s[<span class="hljs-string">&#x27;TV&#x27;</span>]*data_s[<span class="hljs-string">&#x27;radio&#x27;</span>]<br>X = np.asarray(data_s.get([<span class="hljs-string">&#x27;TV_radio&#x27;</span>,<span class="hljs-string">&#x27;TV_min&#x27;</span>,<span class="hljs-string">&#x27;radio&#x27;</span>,<span class="hljs-string">&#x27;newspaper&#x27;</span>]))<br>y = np.asarray(data_s.get(<span class="hljs-string">&#x27;sales&#x27;</span>))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;测试误差MSE=%f&#x27;</span>%mean_squared_error(y,model.predict(X)))<br></code></pre></td></tr></table></figure><h1 id="六、总结模型训练过程中的收获"><a href="#六、总结模型训练过程中的收获" class="headerlink" title="六、总结模型训练过程中的收获"></a>六、总结模型训练过程中的收获</h1><h2 id="6-1-学习数据分析处理"><a href="#6-1-学习数据分析处理" class="headerlink" title="6.1 学习数据分析处理"></a>6.1 学习数据分析处理</h2><p>在进行计算之前，我们首先对数据进行了预处理和分析。首先，我们检查了数据是否缺失。然后，我们画出了散点图，散点图矩阵，相关系数热力图等，分析了销售量和各项广告投入量之间的数据关系，以便于对数据的进一步处理。在对数据的处理中，我们首先进行了数据标准化，将不同量级的数据统一转化为同一量级，以保证数据之间的可比性。而后，我们查阅资料，为了获取更好的训练数据特征，了解了特征工程相关内容，再根据之前对数据的分析，我们对数据进行了特征构建、特征选择等，具体结果上面已经展示。</p><h2 id="6-2-加深对十折交叉验证的理解"><a href="#6-2-加深对十折交叉验证的理解" class="headerlink" title="6.2 加深对十折交叉验证的理解"></a>6.2 加深对十折交叉验证的理解</h2><p>在机器学习的模型选择中，我们要对候选模型的泛化误差进行评估，然后选择泛化误差最小的那个模型。而实际应用中，我们无法直接获得泛化误差，而训练误差又由于过拟合现象的存在而不适合作为标准，所以我们随机将数据集切为三部分：训练集，验证集和测试集。在十折交叉验证中，我们通过某种特定的划分，将所有数据划分为十个，并依次选取作为测试集，剩下的作为训练集。在这个过程中，我们加深了对十折交叉验证的理解。</p><h2 id="6-3-对于正则化的理解加深"><a href="#6-3-对于正则化的理解加深" class="headerlink" title="6.3 对于正则化的理解加深"></a>6.3 对于正则化的理解加深</h2><p>正则化的目的：防止过拟合。过拟合指的是给定一堆数据，这堆数据带有噪声，利用模型去拟合这堆数据，可能会把噪声数据也给拟合了，这一方面会造成模型比较复杂，比如，原本一次函数能够拟合的数据，由于数据带有噪声，导致需要用五次函数来拟合；另一方面，同时会导致模型的泛化性能很差，在测试集上的结果准确率非常高，但测试新数据时，因为得到的是过拟合的模型，正确率会很低。</p><p>正则化的本质：约束（限制）要优化的参数。本来<strong>解空间</strong>是全部区域，但通过正则化添加了一些约束，使得解空间变小了，甚至在个别正则化方式下，解变得稀疏了。正如下图所示：</p><p><img src="https://img.enderfga.cn/img/1335117-20180708193526314-357302334.png" alt=""><img src="https://img.enderfga.cn/img/1335117-20180708194712773-1094778410.png" alt=""></p><p>彩色线就是优化过程中遇到的等高线，一圈代表一个目标函数值，圆心就是样本观测值（假设一个样本），半径就是误差值，受限条件就是黑色边界（就是正则化的部分），二者相交处，才是最优参数。</p><p>可以看到，L1 与L2 的不同就在于L1在和每个坐标轴相交的地方都有“角”出现，而目标函数的测地线除非位置摆得非常好，大部分时候都会在角的地方相交。注意到在角的位置就会产生稀疏性，例如图中的相交点就有w1=0，而更高维的时候除了角点以外，还有很多边的轮廓也是既有很大的概率成为第一次相交的地方，又会产生稀疏性。相比之下，L2就没有这样的性质，因为没有角，所以第一次相交的地方出现在具有稀疏性的位置的概率就变得非常小了。这就从直观上来解释了为什么L1-regularization 能产生稀疏性，而L2-regularization 不行的原因了。</p><h2 id="6-4-关于算法的择优"><a href="#6-4-关于算法的择优" class="headerlink" title="6.4 关于算法的择优"></a>6.4 关于算法的择优</h2><p>最开始我们分析结构风险最小化的策略，最小二乘法可能不可逆，同时为了增加模型的泛化能力，我们在损失函数中加入了惩罚项，由于对L1，L2正则化的分析，我们选择L2正则化，经过推导，发现最小二乘法可以直接得到解析解，解决了W系数矩阵非正定问题。由于梯度下降法是通过迭代逼近结果，所以只能得到近似解，所以我们选择最小二乘法来进行计算。</p>]]></content>
    
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>逻辑回归模型及lbfgs算法公式推导</title>
    <link href="/2021/10/23/logistic/"/>
    <url>/2021/10/23/logistic/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>机器学习第一次作业记录。小组成员：方桂安，刘玥，周敏。</p><span id="more"></span><h2 id="一、描述逻辑回归模型"><a href="#一、描述逻辑回归模型" class="headerlink" title="一、描述逻辑回归模型"></a>一、描述逻辑回归模型</h2><h3 id="1-1数据"><a href="#1-1数据" class="headerlink" title="1.1数据"></a>1.1数据</h3><script type="math/tex; mode=display">D=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\},x_i\in \mathbb{R}^n,y_i\in\{0,1\}</script><h3 id="1-2模型"><a href="#1-2模型" class="headerlink" title="1.2模型"></a>1.2模型</h3><p><img src="https://img.enderfga.cn/img/image-20211023235517425.png" alt=""></p><p>最初模型：</p><script type="math/tex; mode=display">f(x_i)=\omega^Tx_i+b,使得f(x_i)\simeq g(y_i)</script><p>我们的标记变量y的范围是0或1，所以我们需要一个函数能够将上述x的线性组合转化为0或1，最理想的是阶跃函数。</p><script type="math/tex; mode=display">阶跃函数：y=g^{-1}(\omega^Tx+b)= \begin{cases}0, & \omega^Tx+b<0\\0.5, & \omega^Tx+b=0\\1, & \omega^Tx+b>0\end{cases}</script><p>但由于阶跃函数不连续，不满足单调可微的条件。所以我们希望通过一个一定程度上近似阶跃函数的“替代函数”，并且希望它单调可微。由此，我们想到了逻辑斯蒂函数。</p><p><img src="https://img.enderfga.cn/img/image-20211023235558559.png" alt=""></p><p>它的图像如下：</p><p><img src="https://img.enderfga.cn/img/1990595-20200922165029694-1712738583.png" alt=""></p><p>因为Logistic 回归主要用于分类问题，以二分类为例，对于所给数据集假设存在这样的一条直线可以将数据完成线性可分。                <img src="https://img.enderfga.cn/img/tW5koNMJrG193KEtuAH7cQ.png" alt=""></p><p>当我们要找到分类概率 P(Y=1) 与输入向量 x 的直接关系时，我们引入Sigmoid函数，然后通过比较概率值来判断类别。</p><p>引入sigmoid函数具体实现如下：</p><p>但因为逻辑斯蒂函数的值域在[0,1]之间，无法直接输出0或1。在此基础上，考虑到$\omega^Tx+b$取值是连续的，因此它不能拟合离散变量。可以考虑用它来拟合条件概率$p(y=1|x)$，因为概率的取值也是连续的,我们将逻辑斯蒂函数的输出作为输入x能预测到y为1的概率，并利用对数几率函数，得到下面三个式子。通过此方法，我们将线性模型转换为概率模型。</p><p><img src="https://img.enderfga.cn/img/image-20211023235625407.png" alt=""></p><h3 id="1-3策略"><a href="#1-3策略" class="headerlink" title="1.3策略"></a>1.3策略</h3><p>在策略上，我们采用极大似然法。即选择最优的w，b使得我们输入x得到的正确的y的概率最大，即下式：</p><script type="math/tex; mode=display">(w^*,b^*)=\mathop{argmax}\limits_{(w,b)}\prod_{i=1}^Np(y_i|x_i;\omega,b)</script><p>我们这里做一点变换：</p><p><img src="https://img.enderfga.cn/img/image-20211023235646944.png" alt=""></p><p>因为上式是连乘的函数，我们通过对数似然函数将之转化为求和，即下式：</p><p><img src="https://img.enderfga.cn/img/image-20211023235715659.png" alt=""></p><p>为了方便计算，我们做以下处理</p><script type="math/tex; mode=display">assume \ that\  \hat{\omega}=(\omega;b),\hat{x}=(x;1)</script><p>则上式可化为</p><script type="math/tex; mode=display">\hat{\omega^*}=\mathop{argmin}\limits_{\hat{\omega}}\sum_{i=1}^N(-y_i\hat{\omega }x_i+ln(1+e^{\hat{\omega}^T\hat{x}_i}))</script><p>这是一个凸函数，可用经典的数值优化算法，如梯度下降法、牛顿法求解。</p><p>最终，我们学得的逻辑斯蒂回归模型为</p><p><img src="https://img.enderfga.cn/img/image-20211023235742051.png" alt=""></p><h2 id="二、描述训练模型所使用的算法"><a href="#二、描述训练模型所使用的算法" class="headerlink" title="二、描述训练模型所使用的算法"></a>二、描述训练模型所使用的算法</h2><h3 id="2-1梯度下降法"><a href="#2-1梯度下降法" class="headerlink" title="2.1梯度下降法"></a>2.1梯度下降法</h3><h4 id="2-1-1问题分析"><a href="#2-1-1问题分析" class="headerlink" title="2.1.1问题分析"></a>2.1.1问题分析</h4><p>首先，我们的目标是下式</p><script type="math/tex; mode=display">E(\hat{\omega})=\sum_{i=1}^N(-y_i\hat{\omega}^T\hat{x}_i+ln(1+e^{\hat{\omega}^T\hat{x}_i})),\hat{\omega}^*=\mathop{argmin}_{\hat{\omega}}E(\hat{\omega})</script><p>梯度下降法是一种迭代算法：我们选取适当的初始值$\hat{\omega}^{(0)}$，不断迭代，更新$\hat{\omega}$的值，进行目标函数的极小化，直到收敛。由于负梯度方向是使得函数值下降最快的方向，在迭代的每一步，以负梯度方向更新$\hat{\omega}$的值，从而达到减小函数值的目的。如下图形象化表示：</p><p><img src="https://img.enderfga.cn/img/image-20211023152139696.png" alt=""></p><h4 id="2-1-2核心思想："><a href="#2-1-2核心思想：" class="headerlink" title="2.1.2核心思想："></a>2.1.2核心思想：</h4><ol><li>$E(\hat{\omega})$是具有一阶连续偏导数的凸函数，其极值点在一阶导数为零的地方取得</li><li>一阶泰勒展开：$E(\hat{\omega})\thickapprox E(\hat{\omega}^{(k)})+\nabla E(\hat{\omega}^{(k)})(\hat{\omega}-\hat{\omega}^{(k)})$，其中，$\nabla E(\hat{\omega}^{(k)})$是$E(\hat{\omega})$在$\hat{\omega}^{(k)}$的梯度：</li></ol><script type="math/tex; mode=display">\nabla E(\hat{\omega}^{(k)})=\frac{\partial E(\hat{\omega})}{\partial \hat{\omega}}|_{\hat{\omega}=\hat{\omega}^{(k)}}</script><ol><li>求取第k+1次迭代值：$\hat{\omega}^{k+1}=\hat{\omega}^{(k)}+\eta_k*(-\nabla E(\hat{\omega}^{(k)}))$，其中$\eta_k$是步长，由我们最初指定。梯度推导：</li></ol><script type="math/tex; mode=display">E(\hat{\omega})=\sum_{i=1}^N(-y_i\hat{\omega}^T\hat{x}_i+ln(1+e^{\hat{\omega}^T\hat{x}_i}))\\\nabla E(\hat{\omega}^{(k)})=\sum_{i=1}^N-y_i\hat{x}_i+\frac{1}{1+e^{\hat{\omega}^T\hat{x}_i}}*e^{\hat{\omega}^T\hat{x}_i}*\hat{x}_i\\=-\sum_{i=1}^Nx_i(y_i-\frac{e^{\hat{\omega}^T\hat{x}_i}}{1+e^{\hat{\omega}^T\hat{x}_i}})</script><h4 id="2-1-3伪代码："><a href="#2-1-3伪代码：" class="headerlink" title="2.1.3伪代码："></a>2.1.3伪代码：</h4><p>输入：目标函数$E(\hat{\omega})$，梯度函数$\nabla E(\hat{\omega})$，计算精度ε，步长$\eta_k$；</p><p>输出： $E(\hat{\omega})$的极小点$\hat{\omega}^*$。</p><p>（1）取初始值$\hat{\omega}^{(0)}\in \mathbb{R}^{d+1}$，置k=0；</p><p>（2）计算$E(\hat{\omega}^{(k)})$；</p><p>（3）计算梯度$\nabla E(\hat{\omega}^{(k)})$，当$||\nabla E(\hat{\omega}^{(k)})||&lt;\varepsilon$时，令$\hat{\omega}^*=\hat{\omega}^{(k)}$，</p><p>停止迭代；</p><p>（4）置$\hat{\omega}^{(k+1)}=\hat{\omega}^{(k)}+\eta_k(-\nabla E(\hat{\omega}^{(k)}))$，计算$E(\hat{\omega}^{(k+1)})$，</p><p>当$||E(\hat{\omega}^{(k+1)})-E(\hat{\omega}^{(k)})||&lt;\varepsilon$或$||\hat{\omega}^{(k+1)}-\hat{\omega}^{(k)}||&lt;\varepsilon$时，</p><p>令$\hat{\omega}^*=\hat{\omega}^{(k)}$，停止迭代；</p><p>（5）否则，置k=k+1，转步骤（3）。</p><h4 id="2-1-4分析"><a href="#2-1-4分析" class="headerlink" title="2.1.4分析"></a>2.1.4分析</h4><p>优点：方法简单，易理解</p><p>缺点：迭代次数多，下降速度慢，如下图，我们采用梯度下降法，迭代近50000次才收敛</p><p><img src="https://img.enderfga.cn/img/image-20211023152205584.png" alt=""></p><p>且准确率如下，可以看出准确率不高。</p><p><img src="https://img.enderfga.cn/img/image-20211023152217061.png" alt=""></p><h3 id="2-2牛顿法"><a href="#2-2牛顿法" class="headerlink" title="2.2牛顿法"></a>2.2牛顿法</h3><h4 id="2-2-1核心思想："><a href="#2-2-1核心思想：" class="headerlink" title="2.2.1核心思想："></a>2.2.1核心思想：</h4><p>$E(\hat{\omega})$是具有二阶连续偏导数的函数</p><p>二阶泰勒展开：$E(\hat{\omega})\thickapprox E(\hat{\omega}^{(k)})+\nabla E(\hat{\omega}^{(k)})(\hat{\omega}-\hat{\omega}^{(k)})+\frac{1}{2}(\hat{\omega}-\hat{\omega}^{(k)})^TH(\hat{\omega}^{(k)})(\hat{\omega}-\hat{\omega}^{(k)})$</p><script type="math/tex; mode=display">\nabla E(\hat{\omega})=\frac{\partial E(\hat{\omega})}{\partial \hat{\omega}}|_{(d+1)\times 1},H(\hat{\omega})=\frac{\partial ^2E(\hat{\omega})}{\partial \hat{\omega}_i \partial \hat{\omega}_j}|_{(d+1)\times 1}</script><p>利用二阶泰勒展开$E(\hat{\omega})$取极小点的必要条件$\nabla E(\hat{\omega})=0$，在第k次迭代$\hat{\omega}^{(k)}$，求$\nabla E(\hat{\omega}^{(k)})+H(\hat{\omega}^{(k)})(\hat{\omega}-\hat{\omega}^{(k)})=0$的点，作为第k+1次迭代值$\hat{\omega}^{(k+1)}$</p><h4 id="2-2-2伪代码"><a href="#2-2-2伪代码" class="headerlink" title="2.2.2伪代码"></a>2.2.2伪代码</h4><p>输入：目标函数$E(\hat{\omega})$，梯度函数$\nabla E(\hat{\omega})$，海森矩阵$H(\hat{\omega})$，精度ε；</p><p>输出：$E(\hat{\omega})$的极小点$\hat{\omega}^*$。</p><p>（1）取初始值$\hat{\omega}^{(0)}\in \mathbb{R}^{n+1}$，置k=0；</p><p>（2）计算梯度$\nabla E(\hat{\omega}^{(k)})$；</p><p>（3）当$||E(\hat{\omega}^{(k)})||&lt;\varepsilon$时，令$\hat{\omega}^*=\hat{\omega}^{(k)}$，停止迭代；</p><p>否则，计算海森矩阵$H(\hat{\omega}^{(k)})$ ；</p><p>（4）置$\hat{\omega}^{(k+1)}=\hat{\omega}^{(k)}-(H(\hat{\omega}))^{(-1)}\nabla E(\hat{\omega}^{(k)})$；</p><p>（5）置k=k+1，转步骤（2）。</p><h4 id="2-2-3分析"><a href="#2-2-3分析" class="headerlink" title="2.2.3分析"></a>2.2.3分析</h4><p>牛顿法优点：下降速度快，属于二次收敛</p><p>缺点：海森矩阵计算复杂度高，且要求可逆才能计算，所以我们查阅资料，将采用拟牛顿法。</p><h3 id="2-3-BFGS算法"><a href="#2-3-BFGS算法" class="headerlink" title="2.3 BFGS算法:"></a>2.3 BFGS算法:</h3><p>由于上述牛顿公式中可以看出，我们的海森矩阵不易得到，因此我们有以下迭代公式来逼近海森矩阵：</p><script type="math/tex; mode=display">H_{k+1}=H_k+\frac{y_ky_k^T}{y_k^Ts_k}-\frac{H_ks_ks_k^TH_k^T}{s_k^TH_k^Ts_k}</script><p>但计算量还是很大，矩阵相乘太多。所以我们最终采取$Sherman-Morrison$公式进行变换可得：</p><script type="math/tex; mode=display">H_{k+1}=\left(I-\frac{s_{k} y_{k}^{T}}{y_{k}^{T} s_{k}}\right) H_{k}\left(I-\frac{y_{k} s_{k}^{T}}{y_{k}^{T} s_{k}}\right)+\frac{s_{k} s_{k}^{T}}{y_{k}^{T} s_{k}} \quad(1)</script><p>公式推导如下：</p><script type="math/tex; mode=display">\begin{array}{l}\text { Sherman Morrison 公式: }\\\left(\mathrm{A}+\frac{u u^{T}}{t}\right)^{-1}=A^{-1}-\frac{A^{-1} u u^{T} A^{-1}}{t+u^{T} A^{-1} u}\\\left(\mathrm{H}+\frac{y y^{T}}{y^{T} \mathrm{~s}}-\frac{H s s^{T} \mathrm{H}}{s^{T} H s}\right)^{-1}\\=\left(\mathrm{H}+\frac{y y^{T}}{y^{T} \mathrm{~s}}\right)^{-1}+\left(\mathrm{H}+\frac{y y^{T}}{y^{T} \mathrm{~s}}\right)^{-1} \frac{H s s^{T} H}{s^{T} H^{T} s-s^{T} H\left(\mathrm{H}+\frac{y y^{T}}{y^{T} \mathrm{~S}}\right)^{-1} \mathrm{Hs}}\left(\mathrm{H}+\frac{y y^{T}}{y^{T} \mathrm{~S}}\right)^{-1}\\=\left(H^{-1}-\frac{H^{-1} y y^{T} H^{-1}}{y^{T} S+y^{T} H^{-1} y}\right)+\left(H^{-1}-\frac{H^{-1} y y^{T} H^{-1}}{y^{T} S+y^{T} H^{-1} y}\right) \frac{H s s^{T} H}{s^{T} H S-s^{T} H\left(H^{-1}-\frac{H^{-1} y y^{T}-1}{y^{T} s+y^{T} H^{-1} y}\right) H s}\left(H^{-1}-\frac{H^{-1} y y^{T} H^{-1}}{y^{T} S+y^{T} H^{-1} y}\right)\\=\left(H^{-1}-\frac{H^{-1} y y^{T} H^{-1}}{y^{T} \mathrm{~s}+y^{T} H^{-1} y}\right)+\left(H^{-1}-\frac{H^{-1} y y^{T} H^{-1}}{y^{T} \mathrm{~s}+y^{T} H^{-1} y}\right) \frac{H s s^{T} H}{\frac{s^{T} y y^{T} s}{y^{T} \mathrm{~s}+y^{T} H^{-1} y}}\left(H^{-1}-\frac{H^{-1} y y^{T} H^{-1}}{y^{T} \mathrm{~S}+y^{T} H^{-1} y}\right)\\=\left(H^{-1}-\frac{H^{-1} y y^{T} H^{-1}}{y^{T} \mathrm{~s}+y^{T} H^{-1} y}\right)+\frac{H^{-1} H s s^{T} H H^{-1}}{\frac{s^{T} y y^{T} S}{y^{T} s+y^{T} H^{-1} y}}-\frac{H^{-1} H s s^{T} H}{\frac{s^{T} y y^{T} s}{}}{y^{T} \mathrm{~s}+y^{T} H^{-1} y} H^{-1} \frac{y y^{T}}{y^{T} \mathrm{~s}+y^{T} H^{-1} y} H^{-1}\\-\frac{H^{-1} y y^{T} H^{-1}}{y^{T} \mathrm{~s}+y^{T} H^{-1} y} \frac{H s s^{T} H}{\frac{s^{T} y y^{T} S}{y^{T} \mathrm{~s}+y^{T} H^{-1} y}} H^{-1}\\+H^{-1} \frac{y y^{T}}{y^{T} \mathrm{~s}+y^{T} H^{-1} y} H^{-1} \frac{H s s^{T} H}{\frac{s^{T} y y^{T} S}{y^{T} \mathrm{~s}+y^{T} H^{-1} y}} H^{-1} \frac{y y^{T}}{y^{T} \mathrm{~s}+y^{T} H^{-1} y} H^{-1}\\=\left(H^{-1}-\frac{H^{-1} y y^{T} H^{-1}}{y^{T} \mathrm{~s}+y^{T} H^{-1} y}\right)+\frac{s s^{T}\left(y^{T} \mathrm{~s}+y^{T} H^{-1} y\right)}{s^{T} y y^{T} s}-\frac{s s^{T} y y^{T} H^{-1}}{s^{T} y y^{T} S}-\frac{H^{-1} y y^{T} S S^{T}}{s^{T} y y^{T} S}\\+\frac{H^{-1} y y^{T} S s^{T} y y^{T} H^{-1}}{\left(y^{T} \mathrm{~s}+y^{T} H^{-1} y\right) s^{T} y y^{T} S}\\=\left(H^{-1}-\frac{H^{-1} y y^{T} H^{-1}}{y^{T} \mathrm{~s}+y^{T} H^{-1} y}\right)+\frac{s s^{T}\left(y^{T} \mathrm{~s}+y^{T} H^{-1} y\right)}{\left(s^{T} y\right)^{2}}-\frac{s\left(s^{T} y\right) y^{T} H^{-1}}{\left(s^{T} y\right)^{2}}-\frac{H^{-1} y\left(y^{T} s\right) s^{T}}{\left(s^{T} y\right)^{2}}\\+\frac{H^{-1} y\left(y^{T} S s^{T} y\right) y^{T} H^{-1}}{\left(y^{T} \mathrm{~s}+y^{T} H^{-1} y\right) s^{T} y y^{T} S}\\=\left(H^{-1}-\frac{H^{-1} y y^{T} H^{-1}}{y^{T} \mathrm{~s}+y^{T} H^{-1} y}\right)+\frac{s s^{T}\left(y^{T} \mathrm{~s}+y^{T} H^{-1} y\right)}{\left(s^{T} y\right)^{2}}-\frac{s y^{T} H^{-1}}{s^{T} y}-\frac{H^{-1} y s^{T}}{s^{T} y}+\frac{H^{-1} y y^{T} H^{-1}}{\left(y^{T} \mathrm{~s}+y^{T} H^{-1} y\right)}\\=H^{-1}+\frac{s s^{T}\left(y^{T} \mathrm{~s}+y^{T} H^{-1} y\right)}{\left(s^{T} y\right)^{2}}-\frac{s y^{T} H^{-1}}{s^{T} y}-\frac{H^{-1} y s^{T}}{s^{T} y}\\=H^{-1}+\frac{s s^{T} y^{T} \mathrm{~s}}{\left(s^{T} y\right)^{2}}+\frac{s s^{T} y^{T} H^{-1} y}{\left(s^{T} y\right)^{2}}-\frac{s y^{T} H^{-1}}{s^{T} y}-\frac{H^{-1} y s^{T}}{s^{T} y}\\=H^{-1}\left(I-\frac{y s^{T}}{s^{T} y}\right)-\frac{s y^{T} H^{-1}}{s^{T} y}\left(I-\frac{y s^{T}}{s^{T} y}\right)+\frac{s s^{T}}{s^{T} y}\\=\left(I-\frac{s y^{T}}{s^{T} y}\right) H^{-1}\left(I-\frac{y s^{T}}{s^{T} y}\right)+\frac{s s^{T}}{s^{T} y}\end{array}</script><h3 id="2-4-L-BFGS算法"><a href="#2-4-L-BFGS算法" class="headerlink" title="2.4 L-BFGS算法"></a>2.4 L-BFGS算法</h3><p><img src="https://img.enderfga.cn/img/image-20211023235811630.png" alt=""></p><script type="math/tex; mode=display">\begin{array}{c}H_{k+1}=V_{k}^{T} H_{k} V_{k}+\rho_{k} s_{k} s_{k}^{T} \\\end{array}</script><p>给定初始矩阵$H_0=I$，利用上式，可得：</p><script type="math/tex; mode=display">\begin{aligned}H_{1}&=V_{0}^{T} H_{0} V_{0}+\rho_{0} s_{0} s_{0}^{T}\\H_{2} &=V_{1}^{T} H_{1} V_{1}+\rho_{1} s_{1} s_{1}^{T} \\&=V_{1}^{T}\left(V_{0}^{T} H_{0} V_{0}+\rho_{0} s_{0} s_{0}^{T}\right) V_{1}+\rho_{1} s_{1} s_{1}^{T} \\&\left.=V_{1}^{T} V_{0}^{T} H_{0} V_{0} V_{1}+V_{1}^{T} \rho_{0} s_{0} s_{0}^{T}\right) V_{1}+\rho_{1} s_{1} s_{1}^{T} \\& \\\quad H_{k+1} &=\left(V_{k}^{T} V_{k-1}^{T} \ldots V_{1}^{T} V_{0}^{T}\right) H_{0}\left(V_{0} V_{1} \ldots V_{k-1} V_{k}\right) \\&+\left(V_{k}^{T} V_{k-1}^{T} \ldots V_{1}^{T}\right) \rho_{1} s_{1} s_{1}^{T}\left(V_{1} \ldots V_{k-1} V_{k}\right) \\&+\ldots \\&+\left(V_{k}^{T}\right) \rho_{k-1} s_{k-1} s_{k-1}^{T}\left(V_{k}\right) \\&+\rho_{k} s_{k} s_{k}^{T}\end{aligned}</script><p>只保留最近的m步后，上式的迭代公式变为：</p><script type="math/tex; mode=display">\begin{aligned}H_{k+1} &=\left(V_{k}^{T} V_{k-1}^{T} \ldots V_{k-m}^{T}\right) H_{0}\left(V_{k-m} \ldots V_{k-1} V_{k}\right) \\&+\left(V_{k}^{T} V_{k-1}^{T} \ldots V_{k-m+1}^{T}\right) \rho_{k-m} s_{k-m} s_{k-m}^{T}\left(V_{k-m+1} \ldots V_{k-1} V_{k}\right) \\+& \ldots \\&+\left(V_{k}^{T}\right) \rho_{k-1} s_{k-1} s_{k-1}^{T}\left(V_{k}\right) \\&+\rho_{k} s_{k} s_{k}^{T}\end{aligned}</script><p>所求方向为：</p><script type="math/tex; mode=display">\begin{aligned}H_{k} \nabla f &=\left(V_{K-1}^{T} V_{K-2}^{T} \ldots V_{K-m}^{T}\right) H_{0}\left(V_{K-m} V_{K-m+1} \ldots V_{K-1}\right) \nabla f \\&+\left(V_{K-1}^{T} \ldots V_{K-m+1}^{T}\right)\rho_{k-m} s_{k-m} s_{k-m}^T(V_{k-m+1}\dots V_{k-1}V_{k}) \nabla f\\&+\ldots \\&+V_{k-1} \rho_{k-1} s_{k-1}s_{k-1}^TV_k\nabla f \\&+\rho_{k} s_{k}s_{k}^T\nabla f\end{aligned}</script><p>Two-Loop 算法：</p><script type="math/tex; mode=display">\begin{array}{l}q_{k} \leftarrow \nabla f_{k} \\\text { for } i=k-1 \text { to } k-m \text { do } \\\quad \alpha_{i}=\rho_{i} s_{i}^{T} q_{i+1} \\q_{i}=q_{i+1}-\alpha_{i} y_{i} \\\text { end for } \\r_{k-m-1}=H_{0} q_{k-m} \\\text { for } i=k-m, k-m+1 \text { to } k-1 \text { do } \\\quad \beta_{i}=\rho_{i} y_{i}^{T} r_{i-1} \\r_{i}=r_{i-1}+s_{i} \alpha_{i}-\beta_{i} \\\text { end for } \\\text { End, The result is } H_{k+1} \nabla f=r\end{array}</script><p>Two-Loop算法解析—-第一个循环：</p><p><img src="https://img.enderfga.cn/img/image-20211023235846434.png" alt=""></p><p>重写公式：</p><script type="math/tex; mode=display">\begin{aligned}H_{k} \nabla f &=\left(V_{K-1}^{T} V_{K-2}^{T} \ldots V_{K-m}^{T}\right) H_{0}\left(V_{K-m} V_{K-m+1} \ldots V_{K-1}\right) \nabla f \\&+\left(V_{K-1}^{T} \ldots V_{K-m+1}^{T}\right) s_{k-m} \alpha_{k-m} \\&+\ldots \\&+V_{k-1} s_{k-1} \alpha_{k-2} \\&+s_{k-1} \alpha_{k-1}\end{aligned}</script><p>Two-Loop算法解析—-第二个循环：</p><p><img src="https://img.enderfga.cn/img/image-20211023235910775.png" alt=""></p><p>初始:</p><script type="math/tex; mode=display">r_{k-\mathrm{m}}=V_{k-\mathrm{m}} H_{0} V_{k-\mathrm{m}} V_{k-\mathrm{m}+1} \ldots V_{k-1} \nabla \mathrm{f}+\mathrm{s}_{k-\mathrm{m}} \alpha_{k-m}</script><p>得：</p><script type="math/tex; mode=display">\begin{aligned}r_{k-\mathrm{m}+\mathrm{i}} &=V_{k-\mathrm{m}+\mathrm{i}} \ldots V_{k-\mathrm{m}} H_{0} V_{k-\mathrm{m}} \ldots V_{k-\mathrm{m}+\mathrm{i}} \nabla \mathrm{f} \\&+\left(V_{k-\mathrm{m}+\mathrm{i}} \ldots V_{k-\mathrm{m}+1}\right) \mathrm{s}_{k-\mathrm{m}} \alpha_{k-m} \\&+\left(V_{k-\mathrm{m}+\mathrm{i}} \ldots V_{k-\mathrm{m}+2}\right) \mathrm{s}_{k-\mathrm{m}+1} \alpha_{k-m+1}\\&+\dots\\&s_{k-m+1}\alpha_{k-m+i}\end{aligned}</script><p>$r_{k-1}$即是所求的搜索方向d。</p><p>使用LBFGS求解逻辑回归模型代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score<br><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-comment"># 进一步处理数据集和测试集，将输入和输出分割</span><br>train.columns=<span class="hljs-built_in">list</span>([<span class="hljs-string">&#x27;x1&#x27;</span>,<span class="hljs-string">&#x27;x2&#x27;</span>,<span class="hljs-string">&#x27;y&#x27;</span>])<br>test.columns=<span class="hljs-built_in">list</span>([<span class="hljs-string">&#x27;x1&#x27;</span>,<span class="hljs-string">&#x27;x2&#x27;</span>,<span class="hljs-string">&#x27;y&#x27;</span>])<br>X_train = np.asarray(train.get([<span class="hljs-string">&#x27;x1&#x27;</span>, <span class="hljs-string">&#x27;x2&#x27;</span>]))<br>y_train = np.asarray(train.get(<span class="hljs-string">&#x27;y&#x27;</span>))<br>X_test = np.asarray(test.get([<span class="hljs-string">&#x27;x1&#x27;</span>, <span class="hljs-string">&#x27;x2&#x27;</span>]))<br>y_test = np.asarray(test.get(<span class="hljs-string">&#x27;y&#x27;</span>))<br><span class="hljs-comment"># 使用 sklearn 的 LogisticRegression 作为模型</span><br><span class="hljs-comment"># 其中有 penalty，solver，multi_class 几个比较重要的参数，不同的参数有不同的准确率</span><br>model = LogisticRegression(solver=<span class="hljs-string">&#x27;newton-cg&#x27;</span>)<br><span class="hljs-comment"># newton-cg sag lbfgs liblinear</span><br><br><br><span class="hljs-comment"># 对数据进行标准化</span><br>ss = StandardScaler()<br>X_train = ss.fit_transform(X_train) <br>X_test = ss.fit_transform(X_test)<br><span class="hljs-comment"># 拟合</span><br>model.fit(X_train, y_train)<br><br><span class="hljs-comment"># 预测测试集</span><br>predictions = model.predict(X_test)<br><br><span class="hljs-comment"># 打印准确率</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;测试集准确率：&#x27;</span>, accuracy_score(y_test, predictions))<br><br>weights = np.column_stack((model.intercept_, model.coef_)).transpose()<br><span class="hljs-comment">#print(weights)</span><br></code></pre></td></tr></table></figure><h2 id="三、绘制ROC曲线和PR曲线"><a href="#三、绘制ROC曲线和PR曲线" class="headerlink" title="三、绘制ROC曲线和PR曲线"></a>三、绘制ROC曲线和PR曲线</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs markdown">该部分出现的英语缩写：<br>TP: True Positive<br>FP: False Positive<br>FN: False Negative<br>TN: True Negative<br>P: Precision<br>R: Recall<br>TPR: True Positive Rate<br>FPR: False Positive Rate<br></code></pre></td></tr></table></figure><h3 id="3-1-ROC曲线"><a href="#3-1-ROC曲线" class="headerlink" title="3.1 ROC曲线"></a>3.1 ROC曲线</h3><h4 id="3-1-1介绍"><a href="#3-1-1介绍" class="headerlink" title="3.1.1介绍"></a>3.1.1介绍</h4><p>ROC全称是“受试者工作特征”(Receiver Operating Characteristic)曲线，它源于“二战”中用于敌机检测的雷达信号分析技术，二十世纪六七十年代开始被用于一些心理学、医学检测应用中，此后被引入机器学习领域，用来评判分类、检测结果的好坏。因此，ROC曲线是非常重要和常见的统计分析方法。</p><p>为了绘制ROC曲线，我们需要计算出两个重要量的值（</p><p><strong>TPR</strong>、<strong>FPR</strong>），分别以它们为横、纵坐标作图。其中的TP、FP、TN、FN来自于<strong>混淆矩阵</strong>，且TP+FP+TN+FN=样本总数。</p><script type="math/tex; mode=display">TPR=\frac{TP}{TP+FN}</script><script type="math/tex; mode=display">FPR=\frac{FP}{FP + TN}</script><p><img src="https://img.enderfga.cn/img/image-20211023004557339.png" alt=""></p><h4 id="3-1-2画图流程"><a href="#3-1-2画图流程" class="headerlink" title="3.1.2画图流程"></a>3.1.2画图流程</h4><ol><li>给定m+个正例和m-个负例，根据学习器预测结果对样例进行排序</li><li>然后把分类阈值设为最大，即把所有样例均预测为反例，此时真正例率和假正例率均为0，在坐标(0,0)处标记一个点</li><li>将分类阈值依次设为每个样例的预测值，即依次将每个样例划分为正例，设前一个标记点坐标为(x,y)，当前若为真正例，则对应标记点的坐标为$\left ( x,y+\frac{1}{m^{+}} \right )$；当前若为假正例，则对应标记点的坐标为$\left ( x+\frac{1}{m^{-}},y \right )$</li><li>最后用线段连接相邻点</li></ol><h4 id="3-1-3-AUC分析"><a href="#3-1-3-AUC分析" class="headerlink" title="3.1.3 AUC分析"></a>3.1.3 AUC分析</h4><p>ROC曲线下方的面积也有着重要意义（英语：Area under the Curve of ROC (AUC ROC)），其意义是：</p><ul><li>因为是在1x1的方格里求面积，AUC必在0~1之间。</li><li>假设阈值以上是正例，以下是反例；</li><li>简单说：<strong>AUC值越大的分类器，正确率越高。</strong></li></ul><p>从AUC判断分类器（预测模型）优劣的标准：</p><ul><li>AUC = 1，是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数预测的场合，不存在完美分类器。</li><li>0.5 &lt; AUC &lt; 1，优于随机猜测。这个分类器（模型）妥善设置阈值的话，能有预测价值。</li><li>AUC = 0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。</li><li>AUC &lt; 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。</li></ul><p>假设ROC曲线由为{ ( x1,y1 ),⋯,( xN′,yN′ ) }的点按需连接而成且有x~1~=0,x~N’~=1，则AUC可估算为：</p><script type="math/tex; mode=display">AUC=\frac{1}{2} \sum_{j=1}^{N{}'-1} \left ( x_{j+1}-x_{j}  \right ) \left ( y_{j+1}+y_{j}  \right )</script><p><img src="https://img.enderfga.cn/img/image-20211023143342594.png" alt=""></p><p>如图即为使用本次作业所提供数据绘制的ROC曲线。由于测试样例有限，所以仅能获得有限个（真正例率，假正例率）坐标对，无法产生光滑的ROC曲线；由此计算得到的AUC的值为0.9648，可以得知该模型的性能较优。</p><p>完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">draw_roc</span>(<span class="hljs-params">confidence_scores, data_labels</span>):</span><br>    <span class="hljs-comment">#真正率，假正率</span><br>    fpr, tpr, thresholds = roc_curve(data_labels, confidence_scores)<br>    plt.figure()<br>    plt.grid()<br>    plt.title(<span class="hljs-string">&#x27;ROC Curve&#x27;</span>)<br>    plt.xlabel(<span class="hljs-string">&#x27;FPR&#x27;</span>)<br>    plt.ylabel(<span class="hljs-string">&#x27;TPR&#x27;</span>)<br> <br>    <span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> auc<br>    auc=auc(fpr, tpr) <span class="hljs-comment">#AUC计算</span><br>    plt.plot(fpr,tpr,<span class="hljs-string">&#x27;k--&#x27;</span>, label = <span class="hljs-string">&#x27;roc_curve(AUC=%0.4f)&#x27;</span> % auc)<br>    plt.legend()<br>    plt.show()<br></code></pre></td></tr></table></figure><h3 id="3-2-PR曲线"><a href="#3-2-PR曲线" class="headerlink" title="3.2 PR曲线"></a>3.2 PR曲线</h3><h4 id="3-2-1介绍"><a href="#3-2-1介绍" class="headerlink" title="3.2.1介绍"></a>3.2.1介绍</h4><p>PR曲线全称为查准率-查全率曲线，查准率P与查全率R分别定义为：</p><script type="math/tex; mode=display">P=\frac{TP}{TP+FP}，R=\frac{TP}{TP+FN}</script><p>查准率和查全率是一对矛盾的度量。一般来说，查准率高时，查全率往往偏低；而查全率高时，查准率往往偏低。</p><h4 id="3-2-2画图流程"><a href="#3-2-2画图流程" class="headerlink" title="3.2.2画图流程"></a>3.2.2画图流程</h4><p>绘制PR曲线的流程与ROC曲线类似，我们需要根据学习器的预测结果按正例可能性大小对样例进行排序，再逐个样本的选择阈值，在该样本之前的都属于正例，该样本之后的都属于负例。每一个样本作为划分阈值时，都可以计算对应的precision和recall，那么就可以以此绘制曲线。</p><h4 id="3-2-3-AP分析"><a href="#3-2-3-AP分析" class="headerlink" title="3.2.3 AP分析"></a>3.2.3 AP分析</h4><p>其中平衡点是曲线上“查准率=查全率”时的取值，可用于度量PR曲线有交叉的分类器性能高低。与AUC类似，PR曲线下方面积也有重要意义。PR曲线下的面积称之为AP(Average Precision)，通常来说一个越好的分类器，AP值越高。</p><p>对于连续的PR曲线，有：</p><script type="math/tex; mode=display">AP=\int_{0}^{1} p\left ( r \right ) \mathrm{d}r</script><p>但由于曲线可能出现不可导的部分，故我们常常求其近似值：</p><script type="math/tex; mode=display">p_{\text {interp }}(r)=\max _{\tilde{r} \geq r} p(\tilde{r})</script><p>对于离散的PR曲线，有：</p><script type="math/tex; mode=display">\mathrm{AP}=\sum_{k=1}^{n} p(k) \Delta r(k)</script><p>另外PR曲线平衡点更用常用的是F1度量：</p><script type="math/tex; mode=display">F 1=\frac{2 \times P \times R}{P+R}=\frac{2 \times T P}{\text { 样例总数 }+T P-T N}</script><p>比F1度量更一般的形式是F~β~：</p><script type="math/tex; mode=display">F_{\beta}=\frac{\left(1+\beta^{2}\right) \times P \times R}{\left(\beta^{2} \times P\right)+R}</script><ul><li>β=1：标准F1</li><li>β&gt;1：偏重查全率（逃犯信息检索）</li><li>β&lt;1：偏重查准率（商品推荐系统）</li></ul><p><img src="https://img.enderfga.cn/img/image-20211023150341520.png" alt=""></p><p>如图即为使用本次作业所提供数据绘制的PR曲线。在现实任务中，PR曲线是非单调、不平滑的，在很多局部有上下波动；由此计算得到的AP的值为0.9751，可以得知该模型的性能较优。</p><p>完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">draw_pr</span>(<span class="hljs-params">confidence_scores, data_labels</span>):</span><br>    plt.figure()<br>    plt.title(<span class="hljs-string">&#x27;PR Curve&#x27;</span>)<br>    plt.xlabel(<span class="hljs-string">&#x27;Recall&#x27;</span>)<br>    plt.ylabel(<span class="hljs-string">&#x27;Precision&#x27;</span>)<br>    plt.grid()<br> <br>    <span class="hljs-comment">#精确率，召回率，阈值</span><br>    precision,recall,thresholds = precision_recall_curve(data_labels,confidence_scores)<br> <br>    <span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> average_precision_score<br>    AP = average_precision_score(data_labels, confidence_scores) <span class="hljs-comment"># 计算AP</span><br>    plt.plot(recall, precision,<span class="hljs-string">&#x27;k--&#x27;</span>, label = <span class="hljs-string">&#x27;pr_curve(AP=%0.4f)&#x27;</span> % AP)<br>    plt.legend()<br>    plt.show()<br></code></pre></td></tr></table></figure><h2 id="四、总结模型训练过程中的收获"><a href="#四、总结模型训练过程中的收获" class="headerlink" title="四、总结模型训练过程中的收获"></a>四、总结模型训练过程中的收获</h2><h3 id="4-1加深了对逻辑斯蒂回归的理解"><a href="#4-1加深了对逻辑斯蒂回归的理解" class="headerlink" title="4.1加深了对逻辑斯蒂回归的理解"></a>4.1加深了对逻辑斯蒂回归的理解</h3><h4 id="4-1-1简述对模型的理解："><a href="#4-1-1简述对模型的理解：" class="headerlink" title="4.1.1简述对模型的理解："></a>4.1.1简述对模型的理解：</h4><p>因为线性回归模型产生的预测值是一系列实值。为了使得输出的预测结果变成分类所需的0和1，我们需要在线性回归的基础式子外再套一个函数将其输出变成0和1，又要求该函数单调可微，所以我们引入logistic函数，将输出的预测结果成功转为概率值。这样，逻辑斯蒂回归模型被成功应用于解决分类模型。</p><h4 id="4-1-2关于算法的择优："><a href="#4-1-2关于算法的择优：" class="headerlink" title="4.1.2关于算法的择优："></a>4.1.2关于算法的择优：</h4><p>在代码实现过程中，我们最开始使用的是梯度下降法，但是迭代速度较慢，拟合效果不是很好；之后我们选择了牛顿法，但是因为计算海森矩阵的复杂度太高，我们选择用一种拟牛顿法——‘L-BFGS’来逼近海森矩阵，最终达到了我们理想的效果。</p><p>梯度下降法和牛顿法/拟牛顿法相比，两者都是迭代求解，不过梯度下降法是梯度求解，而牛顿法/拟牛顿法是用二阶的海森矩阵的逆矩阵或伪逆矩阵求解。相对而言，使用牛顿法/拟牛顿法收敛更快。</p><h3 id="4-2实现了代码技能的提升"><a href="#4-2实现了代码技能的提升" class="headerlink" title="4.2实现了代码技能的提升"></a>4.2实现了代码技能的提升</h3><p>在代码实现过程中，我们调用了机器学习工具包sklearn中的重要函数——LogisticRegression函数，熟悉了它的常用参数及意义，下面以表格形式列出我们在此次模型训练中使用到的参数。</p><div class="table-container"><table><thead><tr><th>参数</th><th>意义</th><th>备注</th></tr></thead><tbody><tr><td>penalty</td><td>str类型，可选项有{‘L1’,‘L2’}，用来确定惩罚项的规范。‘newton-cg’，‘sag’和‘lbfgs’仅支持‘L2’惩罚项。</td><td>该参数是为了添加惩罚项，避免过拟合，用以提高函数的泛化能力。我们在本次模型训练中使用的是‘L2’。</td></tr><tr><td>solver</td><td>可选的优化算法有{‘newton-cg’，‘lbfgs’,‘liblinear’,‘sag’}</td><td>小数据集中，liblinear是一个好选择，sag和saga对大数据更快； 多分类问题中，除了liblinear其它四种算法都可以使用；newton-cg，lbfgs和sag仅能使用L2惩罚项；  我们经过对比，选择的算法是lbfgs。</td></tr><tr><td>multi_class</td><td>str类型，可选参数有{‘ovr’，‘multinomial’}  如果是二元分类问题则两个选项一样，如果是多元分类则ovr将进行多次二分类，分别为一类别和剩余其它所有类别;  multinomial则分别进行两两分类，需要T(T-1)/2次分类。</td><td>在多分类中，ovr快，精度低; multinomial慢，精度高。</td></tr></tbody></table></div><h3 id="4-3提高了公式推导和文章排版能力"><a href="#4-3提高了公式推导和文章排版能力" class="headerlink" title="4.3提高了公式推导和文章排版能力"></a>4.3提高了公式推导和文章排版能力</h3><p>报告中的所有公式，我们都脚踏实地，一步步手动推导，并学习使用latex将其手动输入并排版。在这个过程中，我们对算法中公式的来源更加清楚，对其原理理解更加深透。这提高了我们的公式推导能力和文章排版能力。</p><h3 id="4-4锻炼了小组合作精神，提高了小组合作能力"><a href="#4-4锻炼了小组合作精神，提高了小组合作能力" class="headerlink" title="4.4锻炼了小组合作精神，提高了小组合作能力"></a>4.4锻炼了小组合作精神，提高了小组合作能力</h3><p>在正式写报告之前，我们对本次作业任务以及对逻辑斯蒂回归模型的理解进行了讨论；然后为了加深彼此对知识的掌握程度，每个人都对代码进行了独立编写，在实现的过程中探讨互助；最后，我们根据彼此的优势项对任务进行了分工合作，齐心协力创作出了这份尽可能完善的报告。</p>]]></content>
    
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>typora+picgo+coding高效写作</title>
    <link href="/2021/10/20/picgo/"/>
    <url>/2021/10/20/picgo/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>解决hexo博客图片上传问题</p><span id="more"></span><p>Markdown 是一种轻量级标记语言，它允许人们使用易读易写的纯文本格式编写文档。</p><p>Markdown 语言在 2004 由约翰·格鲁伯（英语：John Gruber）创建。</p><p>Markdown 编写的文档可以导出 HTML 、Word、图像、PDF、Epub 等多种格式的文档。</p><p>Markdown 编写的文档后缀为 <strong>.md</strong>, <strong>.markdown</strong>。</p><hr><p><a href="https://typora.io/">Typora</a> gives you a seamless experience as both a reader and a writer.</p><p>It removes the preview window, mode switcher, syntax symbols of</p><p>markdown source code, and all other unnecessary distractions. Instead,</p><p>it provides a real live preview feature to help you concentrate on the content itself.</p><hr><p>懒得解释了，总之markdown真好用，typora yyds！（正式版开始收费了，但还是很良心）</p><p>但是一直以来我都觉得写blog是个很痛苦的过程，除了因为我懒，就是每次都得一张一张上传图片，所以迟迟未更新。</p><p>直到我使用了typora+picgo+gitee这一组合。</p><hr><p><a href="https://github.com/PicGo/">Picgo</a>的GitHub页面上包含了<a href="https://github.com/PicGo/PicGo-Core">core</a>，<a href="https://github.com/PicGo/vs-picgo">vscode的扩展版</a>和各种各样<a href="https://github.com/PicGo/Awesome-PicGo">awesome的插件</a>。</p><p>不过我选择的是编译好的<a href="https://github.com/Molunerfinn/PicGo/releases">发行版程序</a>。</p><p><img src="https://img.enderfga.cn/img/image-20211020213735806.png" alt=""></p><p>安装过程中没什么需要注意的，总之就是一路点下去。</p><p><img src="https://img.enderfga.cn/img/image-20211020213944319.png" alt=""></p><p>打开后应该是一个这样的界面，我首先排除GitHub和Imgur了，毕竟不能保证每一位用户都科学上网。</p><p>接下来我根据网上的教程按顺序尝试了SMMS，七牛云，又拍云···</p><p>第一个配置好上传不了，后面俩配置得也很完美，什么域名绑定，cdn加速···结果本地图片显示，上传到博客上就显示不了了，原因不得而知。</p><p>最后我选择了免费，访问速度还快的Gitee。</p><p>首先在插件设置中搜索并下载：</p><p><img src="https://img.enderfga.cn/img/image-20211020215002022.png" alt=""></p><p>如果没有Gitee账户，先进行<a href="https://gitee.com/">注册</a>。</p><p><img src="https://img.enderfga.cn/img/image-20211020220550100.png" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20211020220758789.png" alt=""></p><p>记好我红框出来的内容，然后到个人设置里生成私人令牌。</p><p><img src="https://img.enderfga.cn/img/image-20211020221019061.png" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20211020221038756.png" alt=""></p><p>描述由你决定，只需要选择图中两项，然后把令牌<strong>复制</strong>好备用。（关闭之后就不再明文显示了！）</p><p>最后在picgo中填好各项参数：</p><p><img src="https://img.enderfga.cn/img/image-20211020221334908.png" alt=""></p><p>把gitee设置为默认图床之后至此已经可以使用Ctrl+C复制图片，Ctrl＋Shift＋P上传至图床，Ctrl+V粘贴到typora中。</p><p>为了简化操作，可以在typora的偏好设置中进行如下修改：</p><p><img src="https://img.enderfga.cn/img/image-20211020222002514.png" alt=""></p><p>（顺便验证一下成功与否）</p><p>大功告成，从此写作可以完全抛弃图片的苦恼，一键上传，so easy~</p><p>另外推荐两个我常常使用的图片<a href="https://bigjpg.com/">无损放大</a>和<a href="https://www.picdiet.com/zh-cn">无损压缩</a>的网站。</p><p>虽然自动化上传之后我就不会去调整图片的大小了······</p><hr><h1 id="2022-3-30更新"><a href="#2022-3-30更新" class="headerlink" title="2022.3.30更新"></a>2022.3.30更新</h1><p><img src="https://img.enderfga.cn/img/image-20220330110138852.png" alt=""></p><p>图片不能显示几天了，还以为是我网络环境不好。上了知乎才知道，原来gitee直接把外链屏蔽了，甚至把所有图片替换成他们的logo，气抖冷！</p><p>只能连夜把我200多张图片下载然后上传到GitHub，重新配置好picgo，再把以往所有文章的图片链接前缀替换了。</p><p>GitHub在picgo里自带，不需要插件，配置方法跟上面大同小异，我就不重新写一遍了。</p><p><img src="https://img.enderfga.cn/img/image-20220330110738648.png" alt=""></p><p>设定仓库名：用户名/新建的仓库名</p><p>设定分支名：master或者main均可</p><p>设定Token：类似于gitee</p><p>存储路径：仓库里的文件夹名，可任取，上传后会自动生成目录</p><p><strong>设定自定义域名</strong>：虽然可以用GitHub的前缀，但由于考虑到访问速度问题，故采取国内cdn加速的方法解决，格式如下：</p><p><a href="https://cdn.jsdelivr.net/gh/用户名/新建的仓库名@分支名">https://cdn.jsdelivr.net/gh/用户名/新建的仓库名@分支名</a></p><hr><h1 id="2022-5-29更新"><a href="#2022-5-29更新" class="headerlink" title="2022.5.29更新"></a>2022.5.29更新</h1><p>懒得写了</p><p>参考<a href="https://www.myql.xyz/post/92e90c46/">https://www.myql.xyz/post/92e90c46/</a></p><p>现在我站所有图片替换成了coding</p><p>作者写的非常详细，需要注意的点是分支，搞清楚是main还是master还是什么，他的代码这部分要改；</p><p>团队名不太准确，应该是你设置团队名的那个域名。</p><h1 id="结局"><a href="#结局" class="headerlink" title="结局"></a>结局</h1><p>现在用的七牛云，钱能解决的问题果然就是少折腾。</p>]]></content>
    
    
    
    <tags>
      
      <tag>写作</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用搜索算法解决八数码问题</title>
    <link href="/2021/10/19/8puzzle/"/>
    <url>/2021/10/19/8puzzle/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>人工智能导论作业记录。</p><span id="more"></span><h2 id="一、问题描述与分析"><a href="#一、问题描述与分析" class="headerlink" title="一、问题描述与分析"></a>一、问题描述与分析</h2><p>八数码问题就是在一个大小为3×3的九宫格上,放置8块编号为1-8的木块，九宫格中有一个空格，周围(上下左右)的木块可以和空格交换位置。对于问题，给定一个初始状态，目标状态是期望达到1-8顺序排列的序列，并且空格在右下角，问题的实质就是寻找一个合法的移动序列。</p><p><img src="https://img.enderfga.cn/img/20211020204607.png" alt=""></p><p>不是每一个给定的初始状态都存在解，在分析之前，引入线性代数中的几个概念：</p><ul><li>逆序数：在一个排列中，如果一对数字的前后位置与大小顺序相反，即前面的数大于后面的数，那么它们就称为一个逆序。一个排列中序的总数就称为这个排列的逆序数。</li><li>奇排列：逆序数为奇数的排列称为奇排列</li><li><p>偶排列：逆序数为偶数的排列称为偶排列</p><p>使用线性代数理论可以得知，对于任意目标状态，只有初始状态的逆序数和目标状态的逆序数的奇偶性相同才有解(逆序数计算不包括0的逆序数)。</p><p>证明：</p><p>∵八数码问题每一个步骤都可以视作 0 的移动， 0 的移动至多有四个可能的方向<br>又∵ 0 是序列中最小的数，序列的奇偶性不会跟随 0 的移动而改变<br>且对于其余数字而言，要么与 0 互换，要么跨过两个数字和 0 互换<br>∴逆序数的改变只有变化为 0、 -2、 +2 这三种情况<br>又∵奇数±偶数=奇数，偶数±偶数=偶数<br>∴序列在变换过程中，它的奇偶性不会发生改变<br>∴如果初始序列和目标序列不是同为奇排列或者偶排列，那么这个八数码问题就是无解的。</p></li></ul><p>以图中所给状态为例，初始状态的逆序数t=0+6+5+1+2+1+1=16，目标状态的逆序数t’=0，故有解。</p><h2 id="二、深度优先遍历搜索-DFS"><a href="#二、深度优先遍历搜索-DFS" class="headerlink" title="二、深度优先遍历搜索(DFS)"></a>二、深度优先遍历搜索(DFS)</h2><h3 id="2-1算法介绍"><a href="#2-1算法介绍" class="headerlink" title="2.1算法介绍"></a>2.1算法介绍</h3><p><strong>深度优先搜索算法</strong>（英语：Depth-First-Search，DFS）是一种用于遍历或搜索树或图的算法。这个算法会尽可能深的搜索树的分支。当节点v的所在边都己被探寻过，搜索将回溯到发现节点v的那条边的起始节点。这一过程一直进行到已发现从源节点可达的所有节点为止。如果还存在未被发现的节点，则选择其中一个作为源节点并重复以上过程，整个进程反复进行直到所有节点都被访问为止，属于盲目搜索。以下图为例，DFS方法首先从根节点1开始，其最终得到的遍历顺序是“1-2-3-4-5-6-7-8-9-10-11-12”。（假定左分枝和右分枝中优先选择左分枝）</p><p><img src="https://img.enderfga.cn/img/20211020204640.png" alt=""></p><p>我们将其应用于八数码问题的解决。解八数码问题实际上就是找出从初始状态到达目标状态所经过的一系列中间过渡状态。前文提到DFS遍历的树是已经存在的，我们只需要按照规定的遍历方法就能完成遍历，而对于八数码问题，没有已经存在的路径供我们遍历，需要我们从初始状态向下延伸（也就是上下左右移动）才能构造出类似的树。</p><p><img src="https://img.enderfga.cn/img/20211020204711.png" alt=""></p><p>以上图为例。在使用DFS进行搜索时，每个状态都会按照一定的顺序进行上下左右移动（在上图中是下、左、右、上的顺序），一次移动后会产生一个新的状态，然后以新状态为起点继续按约定的顺序（例如先向下）移动。终止的条件是找到解或者达到深度界限。那么如果按照图中下、左、右、上的顺序搜索后的结果将会是最左边的一条路一直是优先向下移动，如果不能向下则依次会是左、右、上的一种。</p><h3 id="2-2实验代码"><a href="#2-2实验代码" class="headerlink" title="2.2实验代码"></a>2.2实验代码</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//初始状态压入队列</span><br>    D_open.<span class="hljs-built_in">push</span>(<span class="hljs-keyword">new</span> <span class="hljs-built_in">borad</span>(<span class="hljs-literal">NULL</span>, start, <span class="hljs-number">0</span>, INT_MAX - <span class="hljs-number">1</span>));<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;DFS：\n&quot;</span>);<br>    <span class="hljs-keyword">while</span> (!D_open.<span class="hljs-built_in">empty</span>()) &#123;<br>        <span class="hljs-comment">//弹出一个状态</span><br>        borad *cur = D_open.<span class="hljs-built_in">top</span>();<br>           D_open.<span class="hljs-built_in">pop</span>();<br>        <span class="hljs-comment">//if (cur-&gt;depth == 5) &#123;</span><br>        <span class="hljs-comment">//    break;</span><br>        <span class="hljs-comment">//&#125;</span><br>        <span class="hljs-comment">//与目标状态的距离，为0即到达目标状态</span><br>        <span class="hljs-keyword">if</span> (<span class="hljs-built_in">hn</span>(cur-&gt;status, target) == <span class="hljs-number">0</span>) &#123;<br>            <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;到达目标状态\nclose表大小为%d\n目标状态深度为%d\n\n&quot;</span>, close.<span class="hljs-built_in">size</span>(), cur-&gt;depth);<br>            <span class="hljs-comment">//printans(cur);</span><br>            <span class="hljs-keyword">break</span>;<br>        &#125;<br>        <span class="hljs-comment">//存放int格式的状态</span><br>        <span class="hljs-keyword">int</span> intstatus = <span class="hljs-built_in">status2int</span>(cur-&gt;status);<br>        <span class="hljs-comment">//出现重复状态</span><br>        <span class="hljs-keyword">if</span> (close.<span class="hljs-built_in">count</span>(intstatus)) &#123;<br>            <span class="hljs-keyword">continue</span>;<br>        &#125;<br>        <span class="hljs-comment">//加入close表，表示已访问过</span><br>        close.<span class="hljs-built_in">insert</span>(intstatus);<br><br>        <span class="hljs-comment">//获得0的坐标</span><br>        <span class="hljs-keyword">int</span> zeroindex = <span class="hljs-built_in">getindex</span>(cur-&gt;status, <span class="hljs-number">0</span>);<br>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">4</span>; i++) &#123;<br>            <span class="hljs-comment">//新建节点，复制当前棋盘状态，深度+1</span><br>            borad *temp = <span class="hljs-keyword">new</span> <span class="hljs-built_in">borad</span>(cur, cur-&gt;status, cur-&gt;depth + <span class="hljs-number">1</span>, INT_MAX - <span class="hljs-number">1</span>);<br>            <span class="hljs-comment">//0向四个方向移动</span><br>            <span class="hljs-keyword">if</span> (<span class="hljs-built_in">swapnum</span>(zeroindex, zeroindex + go[i], temp-&gt;status)) &#123;<br>                <span class="hljs-comment">//移动成功</span><br>                D_open.<span class="hljs-built_in">push</span>(temp);<br>            &#125;<br>            <span class="hljs-keyword">else</span> &#123;<br>                <span class="hljs-comment">//移动失败</span><br>                <span class="hljs-built_in"><span class="hljs-keyword">delete</span></span>(temp);<br>            &#125;<br>        &#125;<br>    &#125;<br></code></pre></td></tr></table></figure><h3 id="2-3实验结果"><a href="#2-3实验结果" class="headerlink" title="2.3实验结果"></a>2.3实验结果</h3><p><img src="https://img.enderfga.cn/img/20211020204805.png" alt=""></p><p>如图所示，深度优先算法在解决八数码问题时有一个致命缺点，就是必须设置一个深度界限，否则，搜索会一直沿着纵深方向发展，会一直无法搜索到解路径。即使加了限制条件，搜索到了解路径，解路径也不一定是最优解路径。</p><h3 id="2-4实验总结"><a href="#2-4实验总结" class="headerlink" title="2.4实验总结"></a>2.4实验总结</h3><ul><li>缺点：如果目标节点不在搜索进入的分支上，而该分支又是一个无穷分支,就得不到解,因此该算法是不完备的。</li><li>优点：如果目标节点在搜索进入的分支上，则可以较快得到解。</li></ul><h2 id="三、广度优先遍历搜索-BFS"><a href="#三、广度优先遍历搜索-BFS" class="headerlink" title="三、广度优先遍历搜索(BFS)"></a>三、广度优先遍历搜索(BFS)</h2><h3 id="3-1算法介绍"><a href="#3-1算法介绍" class="headerlink" title="3.1算法介绍"></a>3.1算法介绍</h3><p><strong>广度优先搜索算法</strong>（英语：Breadth-First-Search，缩写为BFS），又译作宽度优先搜索，或横向优先搜索，是一种图形搜索算法。简单的说，BFS是从根节点开始，沿着树的宽度遍历树的节点。如果所有节点均被访问，则算法中止。BFS是一种盲目搜索法，目的是系统地展开并检查图中的所有节点，以找寻结果。</p><p>BFS会先访问根节点的所有邻居节点，然后再依次访问邻居节点的邻居节点，直到所有节点都访问完毕。在具体的实现中，使用open和closed两个表，open是一个队列，每次对open进行一次出队操作（并放入closed中），并将其邻居节点进行入队操作。直到队列为空时即完成了所有节点的遍历。closed表在遍历树时其实没有用，因为子节点只能从父节点到达。但在进行图的遍历时，一个节点可能会由多个节点到达，所以此时为了防止重复遍历应该每次都检查下一个节点是否已经在closed中了。 依旧以下图为例，BFS方法首先从根节点1开始，其最终得到的遍历顺序是“1-2-7-8-3-6-9-12-4-5-10-11”。可以看出来BFS进行遍历时是一层一层的搜索的。</p><p><img src="https://img.enderfga.cn/img/20211020204640.png" alt=""></p><p><img src="https://img.enderfga.cn/img/20211020204722.png" alt=""></p><p>在应用BFS算法进行八数码问题搜索时需要open和closed两个表。首先将初始状态加入open队列，然后进行出队操作并放入closed中，对出队的状态进行扩展（所谓扩展也就是找出其上下左右移动后的状态），将扩展出的状态加入队列，然后继续循环出队-扩展-入队的操作，直到找到解为止。在上图这个例子中，红圈里的数字是遍历顺序。当找到解时一直往前找父节点即可找出求解的移动路线。</p><h3 id="3-2实验代码"><a href="#3-2实验代码" class="headerlink" title="3.2实验代码"></a>3.2实验代码</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//初始状态压入队列</span><br>    B_open.<span class="hljs-built_in">push</span>(<span class="hljs-keyword">new</span> <span class="hljs-built_in">borad</span>(<span class="hljs-literal">NULL</span>, start, <span class="hljs-number">0</span>, INT_MAX - <span class="hljs-number">1</span>));<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;BFS：\n&quot;</span>);<br>    <span class="hljs-keyword">while</span> (!B_open.<span class="hljs-built_in">empty</span>()) &#123;<br>        <span class="hljs-comment">//弹出一个状态</span><br>        borad* cur = B_open.<span class="hljs-built_in">front</span>();<br>        B_open.<span class="hljs-built_in">pop</span>();<br>        <span class="hljs-comment">//与目标状态的距离，为0即到达目标状态</span><br>        <span class="hljs-keyword">if</span> (<span class="hljs-built_in">hn</span>(cur-&gt;status, target) == <span class="hljs-number">0</span>) &#123;<br>            <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;到达目标状态\nclose表大小为%d\n目标状态深度为%d\n\n&quot;</span>, close.<span class="hljs-built_in">size</span>(), cur-&gt;depth);<br>            <span class="hljs-comment">//printans(cur);</span><br>            <span class="hljs-keyword">break</span>;<br>        &#125;<br>        <span class="hljs-comment">//存放int格式的状态</span><br>        <span class="hljs-keyword">int</span> intstatus = <span class="hljs-built_in">status2int</span>(cur-&gt;status);<br>        <span class="hljs-comment">//出现重复状态</span><br>        <span class="hljs-keyword">if</span> (close.<span class="hljs-built_in">count</span>(intstatus)) &#123;<br>            <span class="hljs-keyword">continue</span>;<br>        &#125;<br>        <span class="hljs-comment">//加入close表，表示已访问过</span><br>        close.<span class="hljs-built_in">insert</span>(intstatus);<br><br>        <span class="hljs-comment">//获得0的坐标</span><br>        <span class="hljs-keyword">int</span> zeroindex = <span class="hljs-built_in">getindex</span>(cur-&gt;status, <span class="hljs-number">0</span>);<br>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">4</span>; i++) &#123;<br>            <span class="hljs-comment">//新建节点，复制当前棋盘状态，深度+1</span><br>            borad* temp = <span class="hljs-keyword">new</span> <span class="hljs-built_in">borad</span>(cur, cur-&gt;status, cur-&gt;depth + <span class="hljs-number">1</span>, INT_MAX - <span class="hljs-number">1</span>);<br>            <span class="hljs-comment">//0向四个方向移动</span><br>            <span class="hljs-keyword">if</span> (<span class="hljs-built_in">swapnum</span>(zeroindex, zeroindex + go[i], temp-&gt;status)) &#123;<br>                <span class="hljs-comment">//移动成功</span><br>                B_open.<span class="hljs-built_in">push</span>(temp);<br>            &#125;<br>            <span class="hljs-keyword">else</span> &#123;<br>                <span class="hljs-comment">//移动失败</span><br>                <span class="hljs-built_in"><span class="hljs-keyword">delete</span></span>(temp);<br>            &#125;<br>        &#125;<br>    &#125;<br>    <span class="hljs-comment">//清空close表</span><br>    close.<span class="hljs-built_in">clear</span>();<br></code></pre></td></tr></table></figure><h3 id="3-3实验结果"><a href="#3-3实验结果" class="headerlink" title="3.3实验结果"></a>3.3实验结果</h3><p><img src="https://img.enderfga.cn/img/20211020204754.png" alt=""></p><p>如图所示，广度优先算法成功找到了深度为22的最优解，但是close表是DFS中深度46312产生的大小为47788的close表的两倍多，由于𝐵𝐹𝑆算法进行的是盲目的搜索，没有考虑代价，而且利用了空间换取时间的策略，故空间也相对会有更大的复杂度。</p><h3 id="3-4实验总结"><a href="#3-4实验总结" class="headerlink" title="3.4实验总结"></a>3.4实验总结</h3><ul><li>缺点：当目标节点距离初始节点较远时会产生许多无用的节点，搜索效率低，只能适用于到达目标结点步数较少的情况。</li><li>优点：只要问题有解，则总可以得到解，而且是最短路径的解。</li></ul><h2 id="四、A-算法实现八数码问题"><a href="#四、A-算法实现八数码问题" class="headerlink" title="四、A*算法实现八数码问题"></a>四、A*算法实现八数码问题</h2><h3 id="4-1算法介绍"><a href="#4-1算法介绍" class="headerlink" title="4.1算法介绍"></a>4.1算法介绍</h3><p><strong>A*搜索算法</strong>（A* search algorithm）是一种在图形平面上，有多个节点的路径，求出最低通过成本的算法，也是许多其他问题的常用启发式算法。该算法综合了最良优先搜索和Dijkstra算法的优点：在进行启发式搜索提高算法效率的同时，可以保证找到一条最优路径（基于评估函数）。</p><p>在A*算法中，一个结点位置的好坏用估价函数来对它进行评估：</p><script type="math/tex; mode=display">f{}'\left ( n \right )=g{}'\left ( n \right )+h{}'\left ( n \right )</script><p>这里，f’(n)是估价函数，g’(n)是起点到终点的最短路径值(也称为最小耗费或最小代价)，h’(n)是n到目标的最短路经的启发值。由于这个f’(n)其实是无法预先知道的，因而实际上使用的是如下估价函数：</p><script type="math/tex; mode=display">f\left ( n \right )=g\left ( n \right )+h\left ( n \right )</script><p>这个公式遵循以下特性：</p><ul><li>如果g(n)为0，即只计算任意顶点n到目标的评估函数h(n)，而不计算起点到顶点n的距离，则算法转化为使用贪心策略的最良优先搜索，速度最快，但可能得不出最优解；</li><li>如果h(n)不大于顶点n到目标顶点的实际距离，则一定可以求出最优解，而且h(n)越小，需要计算的节点越多，算法效率越低，常见的评估函数有——欧几里得距离、曼哈顿距离、切比雪夫距离；</li><li>如果h(n)为0，即只需求出起点到任意顶点n的最短路径g(n)，而不计算任何评估函数h(n)，则转化为单源最短路径问题，即Dijkstra算法，此时需要计算最多的顶点；</li></ul><p>其中，g(n)是从初始结点到节点n的实际代价，h(n)是从结点n到目标结点的最佳路径的估计代价。在这里主要是h(n)体现了搜索的启发信息，因为g(n)是已知的。用f(n)作为f’(n)的近似，也即用g(n)代替g’(n)，h(n)代替h’(n)。这样必须满足两个条件：</p><ol><li>g(n)≥g’(n)(大多数情况下都是满足的，可以不用考虑)，且f必须保持单调递增；</li><li>h必须小于等于实际的从当前节点到达目标节点的最小耗费h(n)≤h’(n)。（可以证明应用这样的估价函数可以找到最短路径）</li></ol><h3 id="4-2实验代码"><a href="#4-2实验代码" class="headerlink" title="4.2实验代码"></a>4.2实验代码</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//初始状态压入队列</span><br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;A* Fn=Gn+Hn：\n&quot;</span>);<br>    <span class="hljs-keyword">while</span> (!A_open.<span class="hljs-built_in">empty</span>()) &#123;<br>        <span class="hljs-comment">//弹出一个状态</span><br>        borad* cur = A_open.<span class="hljs-built_in">top</span>();<br>        A_open.<span class="hljs-built_in">pop</span>();<br>        <span class="hljs-comment">//hn=Fn-depth为与目标状态的曼哈顿距离，为0即到达目标状态</span><br>        <span class="hljs-keyword">if</span> (cur-&gt;Fn - cur-&gt;depth == <span class="hljs-number">0</span>) &#123;<br>            <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;到达目标状态\nclose表大小为%d\n目标状态深度为%d\n\n&quot;</span>, close.<span class="hljs-built_in">size</span>(), cur-&gt;depth);<br>            <span class="hljs-comment">//printans(cur);</span><br>            <span class="hljs-keyword">break</span>;<br>        &#125;<br>        <span class="hljs-comment">//存放int格式的状态</span><br>        <span class="hljs-keyword">int</span> intstatus = <span class="hljs-built_in">status2int</span>(cur-&gt;status);<br>        <span class="hljs-comment">//出现重复状态</span><br>        <span class="hljs-keyword">if</span> (close.<span class="hljs-built_in">count</span>(intstatus)) &#123;<br>            <span class="hljs-keyword">continue</span>;<br>        &#125;<br>        <span class="hljs-comment">//加入close表，表示已访问过</span><br>        close.<span class="hljs-built_in">insert</span>(intstatus);<br>        <span class="hljs-comment">//获得0的坐标</span><br>        <span class="hljs-keyword">int</span> zeroindex = <span class="hljs-built_in">getindex</span>(cur-&gt;status, <span class="hljs-number">0</span>);<br>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">4</span>; i++) &#123;<br>            <span class="hljs-comment">//新建节点，复制当前棋盘状态，深度+1</span><br>            borad* temp = <span class="hljs-keyword">new</span> <span class="hljs-built_in">borad</span>(cur, cur-&gt;status, cur-&gt;depth + <span class="hljs-number">1</span>, <span class="hljs-number">0</span>);<br>            <span class="hljs-comment">//0向四个方向移动</span><br>            <span class="hljs-keyword">if</span> (<span class="hljs-built_in">swapnum</span>(zeroindex, zeroindex + go[i], temp-&gt;status)) &#123;<br>                <span class="hljs-comment">//移动成功</span><br>                <span class="hljs-comment">//计算启发函数值，并更新节点</span><br>                temp-&gt;Fn = temp-&gt;depth + <span class="hljs-built_in">hn</span>(temp-&gt;status, target);<br>                <span class="hljs-comment">//加入A_open表</span><br>                A_open.<span class="hljs-built_in">push</span>(temp);<br>            &#125;<br>            <span class="hljs-keyword">else</span> &#123;<br>                <span class="hljs-comment">//移动失败</span><br>                <span class="hljs-built_in"><span class="hljs-keyword">delete</span></span>(temp);<br>            &#125;<br>        &#125;<br>    &#125;<br>    <span class="hljs-comment">//清空close表</span><br>    close.<span class="hljs-built_in">clear</span>();<br></code></pre></td></tr></table></figure><h3 id="4-3实验结果"><a href="#4-3实验结果" class="headerlink" title="4.3实验结果"></a>4.3实验结果</h3><p><img src="https://img.enderfga.cn/img/20211020204738.png" alt=""></p><p>如图所示，A*搜索算法在解决八数码问题时取得了最优的结果，无论是时间复杂度还是空间复杂度都得到了极大的优化。但是𝐴∗算法作为一种预测算法，不能保证解为最优解。</p><h3 id="4-4实验总结"><a href="#4-4实验总结" class="headerlink" title="4.4实验总结"></a>4.4实验总结</h3><ul><li>优点：A*算法在绝大多数的情况下，在性能方面都远远优与DFS和BFS。算法的主要运行性能，取决于估价函数f的选取。</li><li>缺点：由于算法本身的特点，因此根据估价函数找到的解路径不一定是最优解路径。</li></ul><h2 id="五、效率比较及优缺点"><a href="#五、效率比较及优缺点" class="headerlink" title="五、效率比较及优缺点"></a>五、效率比较及优缺点</h2><h3 id="5-1概念"><a href="#5-1概念" class="headerlink" title="5.1概念"></a>5.1概念</h3><p>首先给出几个用来进行效率比价的变量：</p><ol><li>深度(D)：从初始节点到达目标的路径深度；</li><li>时间(T)：搜索程序运行的时间,单位毫秒(ms)；</li><li>状态数(N)：整个过程中生成的状态总数；</li><li><p>外显率(P)：搜索工程中,从初始节点向目标节点进行搜索的区域的宽度。</p><p>其中时间使用C标准库函数 clock_t clock(void) 计算获得，返回三个算法程序执行起，处理器时钟所使用的时间。为了获取 CPU 所使用的秒数，必须除以 CLOCKS_PER_SEC。而外显率定义为以下公式计算获得：</p><script type="math/tex; mode=display">P=\frac{D}{N},P\in \left( 0,1\right]</script></li></ol><h3 id="5-2-实验数据分析"><a href="#5-2-实验数据分析" class="headerlink" title="5.2 实验数据分析"></a>5.2 实验数据分析</h3><p>数据说明：</p><ol><li>环境为Windows系统，语言为C++，使用clock()函数输出算法时间；</li><li>目标状态1 2 3 4 5 6 7 8 0；</li><li>由于运行时间受电脑影响很大，具有一定的随机性，因而每个状态执行3次,取平均数作为最终结果时间。</li></ol><p>以下为题目所给初始状态产生的数据：</p><div class="table-container"><table><thead><tr><th></th><th>深度D</th><th>时间T</th><th>状态数N</th><th>外显率P</th></tr></thead><tbody><tr><td>DFS</td><td>46312</td><td>0.295000</td><td>47788</td><td>0.969113</td></tr><tr><td>BFS</td><td>22</td><td>0.793000</td><td>102868</td><td>0.000213</td></tr><tr><td>A*</td><td>22</td><td>0.005000</td><td>503</td><td>0.043737</td></tr></tbody></table></div><p>以下为随机初始状态产生的数据：</p><div class="table-container"><table><thead><tr><th>状态数N</th><th>DFS</th><th>BFS</th><th>A*</th></tr></thead><tbody><tr><td>1</td><td>37809</td><td>60897</td><td>1114</td></tr><tr><td>2</td><td>13571</td><td>129921</td><td>1289</td></tr><tr><td>3</td><td>39006</td><td>36948</td><td>926</td></tr><tr><td>4</td><td>56982</td><td>38459</td><td>182</td></tr><tr><td>5</td><td>101524</td><td>23754</td><td>610</td></tr><tr><td>6</td><td>62529</td><td>85828</td><td>1175</td></tr><tr><td>7</td><td>119230</td><td>43684</td><td>750</td></tr><tr><td>8</td><td>72091</td><td>129811</td><td>2492</td></tr><tr><td>9</td><td>68716</td><td>40819</td><td>393</td></tr><tr><td>10</td><td>128887</td><td>159858</td><td>6852</td></tr></tbody></table></div><p><img src="https://img.enderfga.cn/img/20211020204834.png" alt=""></p><div class="table-container"><table><thead><tr><th>深度D</th><th>DFS</th><th>BFS</th><th>A*</th></tr></thead><tbody><tr><td>1</td><td>36756</td><td>20</td><td>20</td></tr><tr><td>2</td><td>13268</td><td>24</td><td>24</td></tr><tr><td>3</td><td>37943</td><td>19</td><td>19</td></tr><tr><td>4</td><td>55007</td><td>19</td><td>19</td></tr><tr><td>5</td><td>95102</td><td>18</td><td>20</td></tr><tr><td>6</td><td>60172</td><td>22</td><td>22</td></tr><tr><td>7</td><td>108378</td><td>20</td><td>20</td></tr><tr><td>8</td><td>69118</td><td>24</td><td>24</td></tr><tr><td>9</td><td>66096</td><td>20</td><td>20</td></tr><tr><td>10</td><td>113307</td><td>25</td><td>27</td></tr></tbody></table></div><p><img src="https://img.enderfga.cn/img/20211020204843.png" alt=""></p><div class="table-container"><table><thead><tr><th>时间T</th><th>DFS</th><th>BFS</th><th>A*</th></tr></thead><tbody><tr><td>1</td><td>0.226</td><td>0.444</td><td>0.011</td></tr><tr><td>2</td><td>0.079</td><td>1.026</td><td>0.012</td></tr><tr><td>3</td><td>0.231</td><td>0.277</td><td>0.01</td></tr><tr><td>4</td><td>0.347</td><td>0.291</td><td>0.002</td></tr><tr><td>5</td><td>0.675</td><td>0.176</td><td>0.006</td></tr><tr><td>6</td><td>0.372</td><td>0.665</td><td>0.011</td></tr><tr><td>7</td><td>0.749</td><td>0.321</td><td>0.007</td></tr><tr><td>8</td><td>0.429</td><td>0.997</td><td>0.024</td></tr><tr><td>9</td><td>0.439</td><td>0.302</td><td>0.003</td></tr><tr><td>10</td><td>0.796</td><td>1.367</td><td>0.068</td></tr></tbody></table></div><p><img src="https://img.enderfga.cn/img/20211020204853.png" alt=""></p><div class="table-container"><table><thead><tr><th>外显率P</th><th>DFS</th><th>BFS</th><th>A*</th></tr></thead><tbody><tr><td>1</td><td>0.000972</td><td>0.000328</td><td>0.017953</td></tr><tr><td>2</td><td>0.000978</td><td>0.000185</td><td>0.018619</td></tr><tr><td>3</td><td>0.000973</td><td>0.000514</td><td>0.020518</td></tr><tr><td>4</td><td>0.000965</td><td>0.000494</td><td>0.104396</td></tr><tr><td>5</td><td>0.000937</td><td>0.000758</td><td>0.032787</td></tr><tr><td>6</td><td>0.000962</td><td>0.000256</td><td>0.018723</td></tr><tr><td>7</td><td>0.000909</td><td>0.000458</td><td>0.026667</td></tr><tr><td>8</td><td>0.000959</td><td>0.000185</td><td>0.009631</td></tr><tr><td>9</td><td>0.000962</td><td>0.00049</td><td>0.050891</td></tr><tr><td>10</td><td>0.000879</td><td>0.000156</td><td>0.00394</td></tr></tbody></table></div><p><img src="https://img.enderfga.cn/img/20211020204901.png" alt=""></p><h3 id="5-3研究结论"><a href="#5-3研究结论" class="headerlink" title="5.3研究结论"></a>5.3研究结论</h3><p>通过研究，可得结论如下：</p><ol><li>DFS搜索效率受深度影响很大，由于深度界限设置得很大，故搜索结点冗余多、速度慢；</li><li>BFS找到的一定是最优解，但是在算法效率上，不一定比DFS好，且远远不如A*算法，同时BFS在搜索深度较深时，产生的冗余结点较多；</li><li>A*算法在效率上相对最优，时间和空间上都比DFS和BFS更优，但缺点是，找到的解不一定是最优解。</li></ol><h2 id="六、参考文献"><a href="#六、参考文献" class="headerlink" title="六、参考文献"></a>六、参考文献</h2><p>[1]付宏杰,王雪莹,周健,周孙静,朱珠,张俊余.八数码问题解法效率比较及改进研究[J].软件导刊,2016,15(09):41-45.</p><p>[2]StuartJ.Russell,PeterNorvig. 人工智能:一种现代的方法(第3版)[J]. 计算机教育, 2011(15):68-68.</p><p>[3]Thomas,H.Cormen,Charles,E.Leiserson,Ronald,L.Rivest,Clifford,Stein,殷建平,徐云,王刚,刘晓光,苏明,邹恒明,王宏志. 算法导论(原书第3版)[J]. 计算机教育(10期):51-51.</p><h2 id="七、完整代码"><a href="#七、完整代码" class="headerlink" title="七、完整代码"></a>七、完整代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;queue&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;stack&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;unordered_set&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;unordered_map&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;string&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;cstdlib&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;ctime&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;time.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;math.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;climits&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><br><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">borad</span> &#123;</span><br>    <span class="hljs-keyword">int</span> status[<span class="hljs-number">9</span>];<span class="hljs-comment">//status[0]到status[8]表示3X3的矩阵，0表示空格</span><br>    <span class="hljs-keyword">int</span> depth;<span class="hljs-comment">//深度</span><br>    <span class="hljs-keyword">int</span> Fn;<span class="hljs-comment">//启发函数值，Fn = depth + hn即深度加曼哈顿距离</span><br>    borad* pre;<span class="hljs-comment">//父指针，指向移动前的棋盘状态</span><br>    <span class="hljs-built_in">borad</span>() : <span class="hljs-built_in">pre</span>(<span class="hljs-number">0</span>), <span class="hljs-built_in">status</span>(), <span class="hljs-built_in">depth</span>(<span class="hljs-number">0</span>), <span class="hljs-built_in">Fn</span>(INT_MAX - <span class="hljs-number">1</span>) &#123;<br>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> j = <span class="hljs-number">0</span>; j &lt; <span class="hljs-number">9</span>; j++) &#123;<br>            status[j] = j;<br>        &#125;<br>    &#125;<br>    <span class="hljs-built_in">borad</span>(borad* x, <span class="hljs-keyword">int</span> i[<span class="hljs-number">9</span>], <span class="hljs-keyword">int</span> y, <span class="hljs-keyword">int</span> z) : <span class="hljs-built_in">pre</span>(x), <span class="hljs-built_in">depth</span>(y), <span class="hljs-built_in">Fn</span>(z) &#123;<br>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> j = <span class="hljs-number">0</span>; j &lt; <span class="hljs-number">9</span>; j++) &#123;<br>            status[j] = i[j];<br>        &#125;<br>    &#125;<br>&#125;;<br><br><span class="hljs-comment">//优先队列自定义排序规则，升序</span><br><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">cmp</span> &#123;</span><br>    <span class="hljs-function"><span class="hljs-keyword">bool</span> <span class="hljs-title">operator</span><span class="hljs-params">()</span> <span class="hljs-params">(<span class="hljs-keyword">const</span> borad* a, <span class="hljs-keyword">const</span> borad* b)</span> </span>&#123;<br>        <span class="hljs-keyword">return</span> a-&gt;Fn &gt; b-&gt;Fn;<br>    &#125;<br>&#125;;<br><br><span class="hljs-function"><span class="hljs-keyword">bool</span> <span class="hljs-title">swapnum</span><span class="hljs-params">(<span class="hljs-keyword">int</span> a, <span class="hljs-keyword">int</span> b, <span class="hljs-keyword">int</span>* status)</span></span>;<span class="hljs-comment">//交换元素</span><br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">getindex</span><span class="hljs-params">(<span class="hljs-keyword">int</span>* status, <span class="hljs-keyword">int</span> num)</span></span>;<span class="hljs-comment">//获得元素在棋盘上的一维坐标</span><br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-keyword">int</span>* status)</span></span>;<span class="hljs-comment">//打印棋盘</span><br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">hn</span><span class="hljs-params">(<span class="hljs-keyword">int</span>* status, <span class="hljs-keyword">int</span>* target)</span></span>;<span class="hljs-comment">//当前状态与目标状态的曼哈顿距离</span><br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">printans</span><span class="hljs-params">(borad* cur)</span></span>;<span class="hljs-comment">//打印解法，回溯</span><br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">status2int</span><span class="hljs-params">(<span class="hljs-keyword">int</span>* status)</span></span>;<span class="hljs-comment">//棋盘状态转为int格式</span><br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">reversesum</span><span class="hljs-params">(<span class="hljs-keyword">int</span>* status)</span></span>;<span class="hljs-comment">//计算逆序数之和</span><br><span class="hljs-function"><span class="hljs-keyword">int</span>* <span class="hljs-title">randstatus</span><span class="hljs-params">(<span class="hljs-keyword">int</span>* target)</span></span>;<span class="hljs-comment">//获得随机初始状态</span><br><br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>    <span class="hljs-keyword">clock_t</span> <span class="hljs-keyword">start_t</span>, <span class="hljs-keyword">end_t</span>;<br>    <span class="hljs-keyword">double</span> <span class="hljs-keyword">total_t</span>;<br>    <span class="hljs-keyword">int</span> go[<span class="hljs-number">4</span>] = &#123; <span class="hljs-number">-1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">-3</span>,<span class="hljs-number">3</span> &#125;;<span class="hljs-comment">//四个移动方向</span><br>    <span class="hljs-keyword">int</span> start[<span class="hljs-number">9</span>] = &#123; <span class="hljs-number">1</span>,<span class="hljs-number">8</span>,<span class="hljs-number">7</span>,<span class="hljs-number">3</span>,<span class="hljs-number">0</span>,<span class="hljs-number">5</span>,<span class="hljs-number">4</span>,<span class="hljs-number">6</span>,<span class="hljs-number">2</span> &#125;;<span class="hljs-comment">//初始状态</span><br>    <span class="hljs-keyword">int</span> target[<span class="hljs-number">9</span>] = &#123; <span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>,<span class="hljs-number">7</span>,<span class="hljs-number">8</span>,<span class="hljs-number">0</span> &#125;;<span class="hljs-comment">//目标状态</span><br>    <span class="hljs-comment">//int* start;//随机初始状态</span><br>    <span class="hljs-comment">//生成随机初始状态</span><br>    <span class="hljs-comment">//start = randstatus(target);</span><br>    stack&lt;borad*&gt; D_open;<span class="hljs-comment">//DFS的open表，使用栈，深度大的在表头</span><br>    queue&lt;borad*&gt; B_open;<span class="hljs-comment">//BFS的open表，使用队列，深度小的在表头</span><br>    priority_queue&lt;borad*, vector&lt;borad*&gt;, cmp&gt; A_open;<span class="hljs-comment">//A*的open表，使用优先队列，启发函数值小的元素在表头</span><br>    unordered_set&lt;<span class="hljs-keyword">int</span>&gt; close;<span class="hljs-comment">//close表，存放已访问过的状态，元素为状态的int格式</span><br>    <span class="hljs-comment">//例：&#123; 1,2,3,8,0,4,7,6,5 &#125;==》123804765(int)</span><br>    <span class="hljs-comment">//&#123; 0,1,3,8,2,4,7,6,5 &#125;==》13824765(int)</span><br><br><br>    A_open.<span class="hljs-built_in">push</span>(<span class="hljs-keyword">new</span> <span class="hljs-built_in">borad</span>(<span class="hljs-literal">NULL</span>, start, <span class="hljs-number">0</span>, INT_MAX - <span class="hljs-number">1</span>));<br>    borad* temp = A_open.<span class="hljs-built_in">top</span>();<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;初始状态：&quot;</span>);<br>    <span class="hljs-built_in">print</span>(temp-&gt;status);<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;目标状态：&quot;</span>);<br>    <span class="hljs-built_in">print</span>(target);<br><br>    <span class="hljs-keyword">start_t</span> = <span class="hljs-built_in">clock</span>();<br>    <span class="hljs-comment">//--------------------------------------------start-A*-------- Fn=Gn+Hn -----------------------------//</span><br>    <span class="hljs-comment">//初始状态压入队列</span><br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;A* Fn=Gn+Hn：\n&quot;</span>);<br>    <span class="hljs-keyword">while</span> (!A_open.<span class="hljs-built_in">empty</span>()) &#123;<br>        <span class="hljs-comment">//弹出一个状态</span><br>        borad* cur = A_open.<span class="hljs-built_in">top</span>();<br>        A_open.<span class="hljs-built_in">pop</span>();<br>        <span class="hljs-comment">//hn=Fn-depth为与目标状态的曼哈顿距离，为0即到达目标状态</span><br>        <span class="hljs-keyword">if</span> (cur-&gt;Fn - cur-&gt;depth == <span class="hljs-number">0</span>) &#123;<br>            <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;到达目标状态\nclose表大小为%ld\n目标状态深度为%d\n&quot;</span>, close.<span class="hljs-built_in">size</span>(), cur-&gt;depth);<br>            <span class="hljs-comment">//printans(cur);</span><br>            <span class="hljs-keyword">break</span>;<br>        &#125;<br>        <span class="hljs-comment">//存放int格式的状态</span><br>        <span class="hljs-keyword">int</span> intstatus = <span class="hljs-built_in">status2int</span>(cur-&gt;status);<br>        <span class="hljs-comment">//出现重复状态</span><br>        <span class="hljs-keyword">if</span> (close.<span class="hljs-built_in">count</span>(intstatus)) &#123;<br>            <span class="hljs-keyword">continue</span>;<br>        &#125;<br>        <span class="hljs-comment">//加入close表，表示已访问过</span><br>        close.<span class="hljs-built_in">insert</span>(intstatus);<br>        <span class="hljs-comment">//获得0的坐标</span><br>        <span class="hljs-keyword">int</span> zeroindex = <span class="hljs-built_in">getindex</span>(cur-&gt;status, <span class="hljs-number">0</span>);<br>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">4</span>; i++) &#123;<br>            <span class="hljs-comment">//新建节点，复制当前棋盘状态，深度+1</span><br>            borad* temp = <span class="hljs-keyword">new</span> <span class="hljs-built_in">borad</span>(cur, cur-&gt;status, cur-&gt;depth + <span class="hljs-number">1</span>, <span class="hljs-number">0</span>);<br>            <span class="hljs-comment">//0向四个方向移动</span><br>            <span class="hljs-keyword">if</span> (<span class="hljs-built_in">swapnum</span>(zeroindex, zeroindex + go[i], temp-&gt;status)) &#123;<br>                <span class="hljs-comment">//移动成功</span><br>                <span class="hljs-comment">//计算启发函数值，并更新节点</span><br>                temp-&gt;Fn = temp-&gt;depth + <span class="hljs-built_in">hn</span>(temp-&gt;status, target);<br>                <span class="hljs-comment">//加入A_open表</span><br>                A_open.<span class="hljs-built_in">push</span>(temp);<br>            &#125;<br>            <span class="hljs-keyword">else</span> &#123;<br>                <span class="hljs-comment">//移动失败</span><br>                <span class="hljs-built_in"><span class="hljs-keyword">delete</span></span>(temp);<br>            &#125;<br>        &#125;<br>    &#125;<br>    <span class="hljs-comment">//清空close表</span><br>    close.<span class="hljs-built_in">clear</span>();<br>    <span class="hljs-comment">//--------------------------------------------end-A*--------- Fn=Gn+Hn -------------------------//</span><br>    <span class="hljs-keyword">end_t</span> = <span class="hljs-built_in">clock</span>();<br>    <span class="hljs-comment">//清空A_open</span><br>    <span class="hljs-keyword">while</span> (!A_open.<span class="hljs-built_in">empty</span>()) &#123;<br>        A_open.<span class="hljs-built_in">pop</span>();<br>    &#125;<br>    <span class="hljs-keyword">total_t</span> = ((<span class="hljs-keyword">double</span>)<span class="hljs-keyword">end_t</span> - (<span class="hljs-keyword">double</span>)<span class="hljs-keyword">start_t</span>) / CLOCKS_PER_SEC;<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;总时间：%f\n\n\n&quot;</span>, <span class="hljs-keyword">total_t</span>);<br>    <span class="hljs-keyword">start_t</span> = <span class="hljs-built_in">clock</span>();<br>    <span class="hljs-comment">//--------------------------------------------start-BFS------------------------------------------//</span><br>    <span class="hljs-comment">//初始状态压入队列</span><br>    B_open.<span class="hljs-built_in">push</span>(<span class="hljs-keyword">new</span> <span class="hljs-built_in">borad</span>(<span class="hljs-literal">NULL</span>, start, <span class="hljs-number">0</span>, INT_MAX - <span class="hljs-number">1</span>));<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;BFS：\n&quot;</span>);<br>    <span class="hljs-keyword">while</span> (!B_open.<span class="hljs-built_in">empty</span>()) &#123;<br>        <span class="hljs-comment">//弹出一个状态</span><br>        borad* cur = B_open.<span class="hljs-built_in">front</span>();<br>        B_open.<span class="hljs-built_in">pop</span>();<br>        <span class="hljs-comment">//与目标状态的距离，为0即到达目标状态</span><br>        <span class="hljs-keyword">if</span> (<span class="hljs-built_in">hn</span>(cur-&gt;status, target) == <span class="hljs-number">0</span>) &#123;<br>            <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;到达目标状态\nclose表大小为%ld\n目标状态深度为%d\n&quot;</span>, close.<span class="hljs-built_in">size</span>(), cur-&gt;depth);<br>            <span class="hljs-comment">//printans(cur);</span><br>            <span class="hljs-keyword">break</span>;<br>        &#125;<br>        <span class="hljs-comment">//存放int格式的状态</span><br>        <span class="hljs-keyword">int</span> intstatus = <span class="hljs-built_in">status2int</span>(cur-&gt;status);<br>        <span class="hljs-comment">//出现重复状态</span><br>        <span class="hljs-keyword">if</span> (close.<span class="hljs-built_in">count</span>(intstatus)) &#123;<br>            <span class="hljs-keyword">continue</span>;<br>        &#125;<br>        <span class="hljs-comment">//加入close表，表示已访问过</span><br>        close.<span class="hljs-built_in">insert</span>(intstatus);<br><br>        <span class="hljs-comment">//获得0的坐标</span><br>        <span class="hljs-keyword">int</span> zeroindex = <span class="hljs-built_in">getindex</span>(cur-&gt;status, <span class="hljs-number">0</span>);<br>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">4</span>; i++) &#123;<br>            <span class="hljs-comment">//新建节点，复制当前棋盘状态，深度+1</span><br>            borad* temp = <span class="hljs-keyword">new</span> <span class="hljs-built_in">borad</span>(cur, cur-&gt;status, cur-&gt;depth + <span class="hljs-number">1</span>, INT_MAX - <span class="hljs-number">1</span>);<br>            <span class="hljs-comment">//0向四个方向移动</span><br>            <span class="hljs-keyword">if</span> (<span class="hljs-built_in">swapnum</span>(zeroindex, zeroindex + go[i], temp-&gt;status)) &#123;<br>                <span class="hljs-comment">//移动成功</span><br>                B_open.<span class="hljs-built_in">push</span>(temp);<br>            &#125;<br>            <span class="hljs-keyword">else</span> &#123;<br>                <span class="hljs-comment">//移动失败</span><br>                <span class="hljs-built_in"><span class="hljs-keyword">delete</span></span>(temp);<br>            &#125;<br>        &#125;<br>    &#125;<br>    <span class="hljs-comment">//清空close表</span><br>    close.<span class="hljs-built_in">clear</span>();<br>    <span class="hljs-comment">//--------------------------------------------end-BFS------------------------------------------//</span><br>    <span class="hljs-keyword">end_t</span> = <span class="hljs-built_in">clock</span>();<br>    <span class="hljs-keyword">total_t</span> = ((<span class="hljs-keyword">double</span>)<span class="hljs-keyword">end_t</span> - (<span class="hljs-keyword">double</span>)<span class="hljs-keyword">start_t</span>) / CLOCKS_PER_SEC;<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;总时间：%f\n\n\n&quot;</span>, <span class="hljs-keyword">total_t</span>);<br>    <span class="hljs-keyword">start_t</span> = <span class="hljs-built_in">clock</span>();<br>    <span class="hljs-comment">//--------------------------------------------start-DFS------------------------------------------//</span><br>    <span class="hljs-comment">//初始状态压入队列</span><br>    D_open.<span class="hljs-built_in">push</span>(<span class="hljs-keyword">new</span> <span class="hljs-built_in">borad</span>(<span class="hljs-literal">NULL</span>, start, <span class="hljs-number">0</span>, INT_MAX - <span class="hljs-number">1</span>));<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;DFS：\n&quot;</span>);<br>    <span class="hljs-keyword">while</span> (!D_open.<span class="hljs-built_in">empty</span>()) &#123;<br>        <span class="hljs-comment">//弹出一个状态</span><br>        borad* cur = D_open.<span class="hljs-built_in">top</span>();<br>        D_open.<span class="hljs-built_in">pop</span>();<br>        <span class="hljs-comment">//if (cur-&gt;depth == 5) &#123;</span><br>        <span class="hljs-comment">//    break;</span><br>        <span class="hljs-comment">//&#125;</span><br>        <span class="hljs-comment">//与目标状态的距离，为0即到达目标状态</span><br>        <span class="hljs-keyword">if</span> (<span class="hljs-built_in">hn</span>(cur-&gt;status, target) == <span class="hljs-number">0</span>) &#123;<br>            <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;到达目标状态\nclose表大小为%ld\n目标状态深度为%d\n&quot;</span>, close.<span class="hljs-built_in">size</span>(), cur-&gt;depth);<br>            <span class="hljs-comment">//printans(cur);</span><br>            <span class="hljs-keyword">break</span>;<br>        &#125;<br>        <span class="hljs-comment">//存放int格式的状态</span><br>        <span class="hljs-keyword">int</span> intstatus = <span class="hljs-built_in">status2int</span>(cur-&gt;status);<br>        <span class="hljs-comment">//出现重复状态</span><br>        <span class="hljs-keyword">if</span> (close.<span class="hljs-built_in">count</span>(intstatus)) &#123;<br>            <span class="hljs-keyword">continue</span>;<br>        &#125;<br>        <span class="hljs-comment">//加入close表，表示已访问过</span><br>        close.<span class="hljs-built_in">insert</span>(intstatus);<br><br>        <span class="hljs-comment">//获得0的坐标</span><br>        <span class="hljs-keyword">int</span> zeroindex = <span class="hljs-built_in">getindex</span>(cur-&gt;status, <span class="hljs-number">0</span>);<br>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">4</span>; i++) &#123;<br>            <span class="hljs-comment">//新建节点，复制当前棋盘状态，深度+1</span><br>            borad* temp = <span class="hljs-keyword">new</span> <span class="hljs-built_in">borad</span>(cur, cur-&gt;status, cur-&gt;depth + <span class="hljs-number">1</span>, INT_MAX - <span class="hljs-number">1</span>);<br>            <span class="hljs-comment">//0向四个方向移动</span><br>            <span class="hljs-keyword">if</span> (<span class="hljs-built_in">swapnum</span>(zeroindex, zeroindex + go[i], temp-&gt;status)) &#123;<br>                <span class="hljs-comment">//移动成功</span><br>                D_open.<span class="hljs-built_in">push</span>(temp);<br>            &#125;<br>            <span class="hljs-keyword">else</span> &#123;<br>                <span class="hljs-comment">//移动失败</span><br>                <span class="hljs-built_in"><span class="hljs-keyword">delete</span></span>(temp);<br>            &#125;<br>        &#125;<br>    &#125;<br>    <span class="hljs-comment">//--------------------------------------------end-DFS------------------------------------------//</span><br>    <span class="hljs-keyword">end_t</span> = <span class="hljs-built_in">clock</span>();<br>    <span class="hljs-keyword">total_t</span> = ((<span class="hljs-keyword">double</span>)<span class="hljs-keyword">end_t</span> - (<span class="hljs-keyword">double</span>)<span class="hljs-keyword">start_t</span>) / CLOCKS_PER_SEC;<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;总时间：%f\n&quot;</span>, <span class="hljs-keyword">total_t</span>);<br>    <span class="hljs-comment">//delete(start);</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br><br><span class="hljs-comment">//打印棋盘</span><br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-keyword">int</span>* status)</span> </span>&#123;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">9</span>; i++) &#123;<br>        <span class="hljs-keyword">if</span> (i % <span class="hljs-number">3</span> == <span class="hljs-number">0</span>) &#123;<br>            <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;\n&quot;</span>);<br>        &#125;<br>        <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;%d &quot;</span>, status[i]);<br><br>    &#125;<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;\n\n&quot;</span>);<br>&#125;<br><br><span class="hljs-comment">//获得元素在棋盘上的一维坐标</span><br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">getindex</span><span class="hljs-params">(<span class="hljs-keyword">int</span>* status, <span class="hljs-keyword">int</span> num)</span> </span>&#123;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">9</span>; i++) &#123;<br>        <span class="hljs-keyword">if</span> (status[i] == num) &#123;<br>            <span class="hljs-keyword">return</span> i;<br>        &#125;<br>    &#125;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br>&#125;<br><br><span class="hljs-comment">//交换元素</span><br><span class="hljs-function"><span class="hljs-keyword">bool</span> <span class="hljs-title">swapnum</span><span class="hljs-params">(<span class="hljs-keyword">int</span> a, <span class="hljs-keyword">int</span> b, <span class="hljs-keyword">int</span>* status)</span> </span>&#123;<br>    <span class="hljs-keyword">if</span> (b &gt;= <span class="hljs-number">0</span> &amp;&amp; b &lt;= <span class="hljs-number">8</span> &amp;&amp; (a / <span class="hljs-number">3</span> == b / <span class="hljs-number">3</span> || a % <span class="hljs-number">3</span> == b % <span class="hljs-number">3</span>)) &#123;<br>        <span class="hljs-built_in">swap</span>(status[a], status[b]);<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">true</span>;<br>    &#125;<br>    <span class="hljs-keyword">else</span> &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>;<br>    &#125;<br>&#125;<br><br><span class="hljs-comment">//当前状态与目标状态的曼哈顿距离</span><br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">hn</span><span class="hljs-params">(<span class="hljs-keyword">int</span>* status, <span class="hljs-keyword">int</span>* target)</span> </span>&#123;<br>    <span class="hljs-comment">//获得当前状态与目标状态的二维x，y坐标</span><br>    <span class="hljs-keyword">int</span> x, y, xt, yt, it, h = <span class="hljs-number">0</span>;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">9</span>; i++) &#123;<br>        x = i % <span class="hljs-number">3</span>;<br>        y = i / <span class="hljs-number">3</span>;<br>        it = <span class="hljs-built_in">getindex</span>(target, status[i]);<br>        xt = it % <span class="hljs-number">3</span>;<br>        yt = it / <span class="hljs-number">3</span>;<br>        h += <span class="hljs-built_in">abs</span>(x - xt) + <span class="hljs-built_in">abs</span>(y - yt);<br>    &#125;<br>    <span class="hljs-keyword">return</span> h;<br>&#125;<br><br><span class="hljs-comment">//打印解法，回溯</span><br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">printans</span><span class="hljs-params">(borad* cur)</span> </span>&#123;<br>    vector&lt;string&gt; ans;<br>    <span class="hljs-keyword">while</span> (cur) &#123;<br>        ans.<span class="hljs-built_in">push_back</span>(<span class="hljs-built_in">to_string</span>(cur-&gt;status[<span class="hljs-number">0</span>]) + <span class="hljs-built_in">to_string</span>(cur-&gt;status[<span class="hljs-number">1</span>]) + <span class="hljs-built_in">to_string</span>(cur-&gt;status[<span class="hljs-number">2</span>]) + <span class="hljs-string">&quot;\n&quot;</span><br>            + <span class="hljs-built_in">to_string</span>(cur-&gt;status[<span class="hljs-number">3</span>]) + <span class="hljs-built_in">to_string</span>(cur-&gt;status[<span class="hljs-number">4</span>]) + <span class="hljs-built_in">to_string</span>(cur-&gt;status[<span class="hljs-number">5</span>]) + <span class="hljs-string">&quot;\n&quot;</span><br>            + <span class="hljs-built_in">to_string</span>(cur-&gt;status[<span class="hljs-number">6</span>]) + <span class="hljs-built_in">to_string</span>(cur-&gt;status[<span class="hljs-number">7</span>]) + <span class="hljs-built_in">to_string</span>(cur-&gt;status[<span class="hljs-number">8</span>]));<br>        cur = cur-&gt;pre;<br>    &#125;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = ans.<span class="hljs-built_in">size</span>() - <span class="hljs-number">1</span>; i &gt;= <span class="hljs-number">0</span>; i--) &#123;<br>        <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;%s\n ↓\n&quot;</span>, ans[i].<span class="hljs-built_in">c_str</span>());<br>    &#125;<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;END\n\n&quot;</span>);<br>&#125;<br><br><span class="hljs-comment">//棋盘状态转为int格式</span><br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">status2int</span><span class="hljs-params">(<span class="hljs-keyword">int</span>* status)</span> </span>&#123;<br>    <span class="hljs-keyword">int</span> res = <span class="hljs-number">0</span>;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>, j = <span class="hljs-number">8</span>; i &lt; <span class="hljs-number">9</span>; i++, j--) &#123;<br>        res += status[i] * <span class="hljs-built_in">pow</span>(<span class="hljs-number">10</span>, j);<br>    &#125;<br>    <span class="hljs-keyword">return</span> res;<br>&#125;<br><br><span class="hljs-comment">//计算逆序数之和</span><br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">reversesum</span><span class="hljs-params">(<span class="hljs-keyword">int</span>* status)</span> </span>&#123;<br>    <span class="hljs-keyword">int</span> sum = <span class="hljs-number">0</span>;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">9</span>; i++) &#123;<br>        <span class="hljs-keyword">if</span> (status[i] != <span class="hljs-number">0</span>) &#123;<br>            <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> j = <span class="hljs-number">0</span>; j &lt; i; j++) &#123;<br>                <span class="hljs-keyword">if</span> (status[j] &gt; status[i]) &#123;<br>                    sum++;<br>                &#125;<br>            &#125;<br>        &#125;<br>    &#125;<br>    <span class="hljs-keyword">return</span> sum;<br>&#125;<br><br><span class="hljs-comment">//获得随机初始状态</span><br><span class="hljs-function"><span class="hljs-keyword">int</span>* <span class="hljs-title">randstatus</span><span class="hljs-params">(<span class="hljs-keyword">int</span>* target)</span> </span>&#123;<br>    <span class="hljs-keyword">int</span>* start = <span class="hljs-keyword">new</span> <span class="hljs-keyword">int</span>[<span class="hljs-number">9</span>]();<br>    unordered_map&lt;<span class="hljs-keyword">int</span>, <span class="hljs-keyword">int</span>&gt; nums;<span class="hljs-comment">//记录已添加的数</span><br>    <span class="hljs-built_in">srand</span>((<span class="hljs-keyword">int</span>)<span class="hljs-built_in">time</span>(<span class="hljs-number">0</span>));<br>    <span class="hljs-keyword">int</span> element, sum1, sum2;<br>    sum2 = <span class="hljs-built_in">reversesum</span>(target);<br>    <span class="hljs-comment">//根据初始状态与目标状态的逆序数之和（sum1、sum2）是否相等，判断初始状态是否有解，不相等（即无解）则重新生成初始状态</span><br>    <span class="hljs-keyword">do</span> &#123;<br>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">9</span>; i++) &#123;<br>            element = <span class="hljs-built_in">rand</span>() % <span class="hljs-number">9</span>;<br>            <span class="hljs-keyword">while</span> (nums[element]) &#123;<br>                element = <span class="hljs-built_in">rand</span>() % <span class="hljs-number">9</span>;<br>            &#125;<br>            nums[element]++;<br>            start[i] = element;<br>        &#125;<br>        <span class="hljs-comment">//清空记录</span><br>        nums.<span class="hljs-built_in">clear</span>();<br>        <span class="hljs-comment">//计算逆序数之和</span><br>        sum1 = <span class="hljs-built_in">reversesum</span>(start);<br>    &#125; <span class="hljs-keyword">while</span> (sum1 % <span class="hljs-number">2</span> != sum2 % <span class="hljs-number">2</span>);<br>    <span class="hljs-keyword">return</span> start;<br>&#125;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>人工智能</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Texlive+VScode</title>
    <link href="/2021/10/18/latex/"/>
    <url>/2021/10/18/latex/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>搭建Latex环境：Texlive+VScode 相关记录</p><span id="more"></span><h2 id="1-安装-Texlive"><a href="#1-安装-Texlive" class="headerlink" title="1.安装 Texlive"></a>1.安装 Texlive</h2><p>鉴于我校没有（我知道的）可用开源软件镜像站，所以在到清华大学开源软件镜像站的<a href="https://mirrors.tuna.tsinghua.edu.cn/CTAN/systems/texlive/Images/">texlive</a>页下载。</p><p><a href="https://imgtu.com/i/5UYncq"><img src="https://img.enderfga.cn/img/5UYncq.png" alt=""></a></p><p>可能由于更新导致你打开之后的页面与上面的内容不一致，总之下载最新的texlive<em>*</em>.iso，虽然很大但各种宏包齐全，用起来能省去各种麻烦。</p><p>在带宽扩容之后的校园网加持下，不用几年就能下载完这个iso文件了。</p><p>如果你是windows 7甚至xp用户，我建议你把iso文件解压然后进行后续操作。</p><p>如果是windows 10/11，系统自带虚拟光驱，直接双击进入即可。</p><p>（ linux/macOS 我不了解，省略）效果如图：</p><p><a href="https://imgtu.com/i/5UUe9H"><img src="https://img.enderfga.cn/img/5UUe9H.png" alt=""></a></p><p>双击或者右键以管理员身份运行install-tl-advanced.bat，可以点进<strong>Advanced</strong>进入高级安装，点击<strong>Customize</strong>来取消你不需要安装的宏包，比如非中英的语言包，这里我只修改了安装目录，最后开始漫长的等待。</p><p><a href="https://imgtu.com/i/5UaCGQ"><img src="https://img.enderfga.cn/img/5UaCGQ.png" alt=""></a></p><p>（安装TeXworks前端也可以取消掉，毕竟都打算用vscode了，加上前面说的语言包之类的，可以省个1G左右，我想着留条后路就啥都没改，也不缺这点空间）</p><p>（在我的电脑上一共安装了57 min 56 s，教程都快写完了，还没有装好）</p><h2 id="2-安装-VSCode"><a href="#2-安装-VSCode" class="headerlink" title="2. 安装 VSCode"></a>2. 安装 VSCode</h2><p>到<a href="https://code.visualstudio.com/Download">官网</a>根据你的系统选择下载安装即可，这部分应该大多数人都安装过了，没什么需要注意的。</p><p><a href="https://imgtu.com/i/5Ud4hD"><img src="https://img.enderfga.cn/img/5Ud4hD.png" alt=""></a></p><p>安装完成之后可以在应用商店挑选各种提高使用体验的扩展，跟本文相关的主要是<strong>Latex Workshop</strong>。</p><p><a href="https://imgtu.com/i/5U0KJS"><img src="https://img.enderfga.cn/img/5U0KJS.png" alt=""></a></p><p>安装完成之后，可以创建或者打开一个tex文件，此时代码已经被高亮显示了。</p><p><a href="https://imgtu.com/i/5U560K"><img src="https://img.enderfga.cn/img/5U560K.png" alt=""></a></p><p>按下快捷键Ctrl+Alt+B（build latex project），顺利生成，效果不错。</p><p><a href="https://imgtu.com/i/5U5xcn"><img src="https://img.enderfga.cn/img/5U5xcn.png" alt=""></a></p><h2 id="3-配置-VSCode-的-插件"><a href="#3-配置-VSCode-的-插件" class="headerlink" title="3. 配置 VSCode 的 插件"></a>3. 配置 VSCode 的 插件</h2><p>按下F1或者Ctrl＋shift＋P，输入setjson，选择第三个（如图所示）。</p><p><a href="https://imgtu.com/i/5Uo9xA"><img src="https://img.enderfga.cn/img/5Uo9xA.png" alt=""></a></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><code class="hljs json">&quot;latex-workshop.latex.tools&quot;: [<br>        &#123;<br>   <span class="hljs-comment">// 编译工具和命令</span><br>   <span class="hljs-attr">&quot;name&quot;</span>: <span class="hljs-string">&quot;xelatex&quot;</span>,<br>   <span class="hljs-attr">&quot;command&quot;</span>: <span class="hljs-string">&quot;xelatex&quot;</span>,<br>   <span class="hljs-attr">&quot;args&quot;</span>: [<br>   <span class="hljs-string">&quot;-synctex=1&quot;</span>,<br>   <span class="hljs-string">&quot;-interaction=nonstopmode&quot;</span>,<br>   <span class="hljs-string">&quot;-file-line-error&quot;</span>,<br>   <span class="hljs-string">&quot;-pdf&quot;</span>,<br>   <span class="hljs-string">&quot;%DOCFILE%&quot;</span><br>            ]<br>        &#125;,<br>        &#123;<br>   <span class="hljs-attr">&quot;name&quot;</span>: <span class="hljs-string">&quot;pdflatex&quot;</span>,<br>   <span class="hljs-attr">&quot;command&quot;</span>: <span class="hljs-string">&quot;pdflatex&quot;</span>,<br>   <span class="hljs-attr">&quot;args&quot;</span>: [<br>   <span class="hljs-string">&quot;-synctex=1&quot;</span>,<br>   <span class="hljs-string">&quot;-interaction=nonstopmode&quot;</span>,<br>   <span class="hljs-string">&quot;-file-line-error&quot;</span>,<br>   <span class="hljs-string">&quot;%DOCFILE%&quot;</span><br>            ]<br>        &#125;,<br>        &#123;<br>   <span class="hljs-attr">&quot;name&quot;</span>: <span class="hljs-string">&quot;bibtex&quot;</span>,<br>   <span class="hljs-attr">&quot;command&quot;</span>: <span class="hljs-string">&quot;bibtex&quot;</span>,<br>   <span class="hljs-attr">&quot;args&quot;</span>: [<br>   <span class="hljs-string">&quot;%DOCFILE%&quot;</span><br>            ]<br>        &#125;<br>    ],<br>   &quot;latex-workshop.latex.recipes&quot;: [<br>      &#123;<br>   <span class="hljs-attr">&quot;name&quot;</span>: <span class="hljs-string">&quot;xelatex&quot;</span>,<br>   <span class="hljs-attr">&quot;tools&quot;</span>: [<br>   <span class="hljs-string">&quot;xelatex&quot;</span><br>          ],<br>      &#125;,<br>      &#123;<br>   <span class="hljs-attr">&quot;name&quot;</span>: <span class="hljs-string">&quot;pdflatex&quot;</span>,<br>   <span class="hljs-attr">&quot;tools&quot;</span>: [<br>   <span class="hljs-string">&quot;pdflatex&quot;</span><br>          ]<br>      &#125;,<br>      &#123;<br>   <span class="hljs-attr">&quot;name&quot;</span>: <span class="hljs-string">&quot;xelatex-&gt;bibtex-&gt;xelatex-&gt;xelatex&quot;</span>,<br>   <span class="hljs-attr">&quot;tools&quot;</span>: [<br>   <span class="hljs-string">&quot;xelatex&quot;</span>,<br>   <span class="hljs-string">&quot;bibtex&quot;</span>,<br>   <span class="hljs-string">&quot;xelatex&quot;</span>,<br>   <span class="hljs-string">&quot;xelatex&quot;</span><br>          ]<br>      &#125;,<br>      &#123;<br>   <span class="hljs-attr">&quot;name&quot;</span>: <span class="hljs-string">&quot;pdflatex-&gt;bibtex-&gt;pdflatex-&gt;pdflatex&quot;</span>,<br>   <span class="hljs-attr">&quot;tools&quot;</span>: [<br>   <span class="hljs-string">&quot;pdflatex&quot;</span>,<br>   <span class="hljs-string">&quot;bibtex&quot;</span>,<br>   <span class="hljs-string">&quot;pdflatex&quot;</span>,<br>   <span class="hljs-string">&quot;pdflatex&quot;</span><br>          ]<br>      &#125;<br>  ],<br>  &quot;latex-workshop.view.pdf.viewer&quot;: &quot;tab&quot;,<br>&quot;editor.inlineSuggest.enabled&quot;: true,<br>&quot;latex-workshop.latex.autoClean.run&quot;: &quot;onBuilt&quot;,<br>&quot;latex-workshop.latex.autoBuild.run&quot;: &quot;never&quot;,<br></code></pre></td></tr></table></figure><ul><li>Ctrl+Alt+B 是编译</li><li>Ctrl+Alt+V是编译+预览pdf</li></ul><p>我最开始写这些其实是想要把中大的foxitpdf设置成默认的pdf预览软件，不过最终效果并不好，所以作罢。</p><p>（咨询了foxit的技术客服，他们说目前是实现不了的）</p><p>上面这些设置主要是因为默认的编译工具是 latexmk，由于不需要用到 latexmk，因此把其修改为中文环境常用的 xelatex；将 tools 中的 %DOC%替换成%DOCFILE%就可以支持编译中文路径下的文件了。</p><p>还可以研究的设置有很多，什么正向搜索反向搜索之类的，有兴趣的朋友可以自行了解。</p><p><a href="https://ericp.cn/cmd">公式指导手册</a></p><p>如果中文无法显示就加上这一句：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs css">\usepackage<span class="hljs-selector-attr">[UTF8]</span>&#123;ctex&#125;<br></code></pre></td></tr></table></figure><p>Latex的相关公式及使用就不再赘述了。</p><p>由于vscode不一定能成功实现4次编译，故我编写了以下bat文件，可以一次性生成pdf并清除所有过程文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs shell">::======================================<br>:: 四次编译：xe-bib-xe-xe<br>::======================================<br>xelatex report<br>bibtex report<br>xelatex report<br>xelatex report<br>::======================================<br>:: 清除文件以及清除更多文件<br>::======================================<br>:clean<br>echo 删除编译临时文件<br>del /f /q /s *.log *.glo *.ilg *.lof *.ind *.out *.thm *.toc *.lot *.loe *.out.bak *.blg *.synctex.gz *.aux *.bbl *.xdv<br>del /f /q *.idx<br>del /f /s *.dvi *.ps<br>goto end<br><br>::======================================<br>:: 结束符，无任何具体意义<br>::======================================<br>:end<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>写作</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello world!</title>
    <link href="/2021/04/25/Helloworld/"/>
    <url>/2021/04/25/Helloworld/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>My First blog:Something about Enderfga</p><span id="more"></span><h1 id="Think-twice-code-once"><a href="#Think-twice-code-once" class="headerlink" title="Think twice, code once."></a>Think twice, code once.</h1>        <div id="aplayer-aVWkNHKk" class="aplayer aplayer-tag-marker" style="margin-bottom: 20px;">            <pre class="aplayer-lrc-content"></pre>        </div>        <script>          var ap = new APlayer({            element: document.getElementById("aplayer-aVWkNHKk"),            narrow: false,            autoplay: true,            showlrc: false,            music: {              title: "Never Coming Back",              author: "Evan Call",              url: "https://cdn.jsdelivr.net/gh/Enderfga/Enderfga/Backup/music.mp3",              pic: "https://z3.ax1x.com/2021/04/24/cvhTxI.jpg",              lrc: ""            }          });          window.aplayers || (window.aplayers = []);          window.aplayers.push(ap);        </script><p><a href="https://imgtu.com/i/gAAR54"><img src="https://img.enderfga.cn/img/gAAR54.jpg" alt=""></a></p><p>自打成为一个程序猿开始，翻阅博客文章学习就成了我的日常（质量确实有够参差不齐···），CSDN、博客园、简书、知乎、GitHub等我都经常光顾，于是萌生了自己写blog的想法，苦于技术力不足一直搁置至今（现在也不怎么样哈哈哈）。在GZTime的协助下，我自己的小破站终于建成啦~希望我早日产出点技术性文章，现在只能拿来记流水账了······</p><h1 id="Enderfga？"><a href="#Enderfga？" class="headerlink" title="Enderfga？"></a>Enderfga？</h1><p>关于我的id来源其实挺傻的，在成为程序猿之前我是一名资深游戏玩家，我还清楚地记得我接触的第一款网络游戏叫植物大战僵尸OL，然后是洛克王国，卡布西游，奥奇传说···直到六年级那年，我玩了第一款我愿将其称之为“游戏”或者说是“第九艺术”的作品——Minecraft。</p><p><a href="https://imgtu.com/i/gAAhG9"><img src="https://img.enderfga.cn/img/gAAhG9.jpg" alt=""></a></p><p>MC一直陪我走到今天，我对编程的兴趣基本也是萌芽于此。虽然课程里没有涉及Java，但有机会我还是会争取好好自学Java和C#的。至于MC，不管怎么更新换代，mod/红石/命令方块/欺负末影龙/种田养猪都挺吸引我的。</p><figure class="highlight mizar"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs mizar">EnderDragon—Enderman-Enderfga,Doesn&#x27;t <span class="hljs-keyword">that</span> sound cool?<br>&quot;This <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> the <span class="hljs-keyword">end</span>,this <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> even the beginning <span class="hljs-keyword">of</span><br> the <span class="hljs-keyword">end</span>,but,perhaps,the <span class="hljs-keyword">end</span> <span class="hljs-keyword">of</span> the beginning.&quot;<br>                   ——Winston Leonard Spencer Churchill<br></code></pre></td></tr></table></figure><p>这个id不知不觉间居然用了七年了，从个别网站到所有账户统一，我也不舍得换新的了（在互联网上统一貌似不是什么好习惯，还容易查到我年轻的时候留下的黑历史···）只希望外国友人看到这么沙雕的英语名不要笑我哈哈哈。（谷歌娘念的好可爱！）</p><p><a href="https://imgtu.com/i/gAm5Bq"><img src="https://img.enderfga.cn/img/gAm5Bq.jpg" alt=""></a></p><h1 id="ACGN-引きこもり"><a href="#ACGN-引きこもり" class="headerlink" title="ACGN-引きこもり"></a>ACGN-引きこもり</h1><p><img src="https://img.enderfga.cn/img/treasure.jpg" alt=""></p><p><img src="https://img.enderfga.cn/img/image-20230213144536204.png" alt=""></p><div style="text-align:center">宿舍一角</div><h2 id="Animation"><a href="#Animation" class="headerlink" title="Animation"></a>Animation</h2><p>二次元浓度++；</p><p>很庆幸自己的童年有虹猫蓝兔七侠传，洛洛历险记，蓝猫淘气三千问，秦时明月，东方神娃······等等优秀国产作品陪伴（甚至顺便在里面学会了普通话），后来在星空卫视的《动漫先锋》栏目里入坑了日漫：犬夜叉，海贼王，钢之炼金术师，七龙珠，<strong>火影忍者</strong>，<strong>银魂</strong>······没有这几部番，肯定也没有现在时而中二热血，时而沙雕废柴的我。<br>至于B站的入站时间是2015-07-10 17:20:10（这是通过答题的时间，终于不是游客了！）（用时间戳查的，我怎么可能记得这种东西）</p><p><img src="https://img.enderfga.cn/img/gZkFVf.jpg" alt=""></p><p>那个时候特地去看了某科学的超电磁炮，lovelive什么的，四舍五入也算是二刺猿入门了（吧？）</p><p>从零开始的异世界生活，一拳超人，灵能百分百，小林家的龙女仆，干物妹小埋，刺客伍六七，<strong>紫罗兰永恒花园</strong>······这些年看番的频率虽然少了，但那种每周等更新看番的热情已经刻进DNA了。每顿饭的时候刷刷B站的剪辑还能感慨一下“爷青回”，泪目一会。</p><p><img src="https://img.enderfga.cn/img/image-20211208140846878.png" alt=""></p><p>另外我的头像其实是桂小太郎，不过因为版权意识的加深，可能有机会还是得重画一个。</p><p><a href="https://imgtu.com/i/fgyay4"><img src="https://img.enderfga.cn/img/fgyay4.jpg" alt=""></a></p><p>更新：桂先生成功升级了，参考了尼尔机械纪元中9S的装扮，现在科技感满满！</p><p><img src="https://img.enderfga.cn/img/%E7%8C%AB%E7%8C%ABw.jpg" alt=""></p><p>更新的更新：另外一个版本诞生啦！是煤气罐猫猫的拟人版！</p><h2 id="Comic"><a href="#Comic" class="headerlink" title="Comic"></a>Comic</h2><p>回忆了一下，我好像不怎么看漫画。起初看漫画是因为死火海更新太慢了，后来在快看上看了几部，记得名字的有阎王不高兴，哑舍，快把我哥带走，<strong>蝉女</strong>。好看是好看，感觉有点像折中选择，不如动画灵动也不如小说全面。</p><p><img src="https://img.enderfga.cn/img/20180718145340_ZKfkz.thumb.1000_0.jpeg" alt=""></p><p>已经完全想不起来蝉女讲什么了，但画风针不戳。最近听了《<a href="https://bilibili.com/video/BV1WX4y1G7ok">鉴情师</a>》这首歌才想起来的。</p><h2 id="Game"><a href="#Game" class="headerlink" title="Game"></a>Game</h2><p>说到这个我可就不困了（zzzzz….）</p><p><a href="https://imgtu.com/i/gV9KCn"><img src="https://img.enderfga.cn/img/gV9KCn.jpg" alt=""></a></p><div style="text-align:center"><iframe width="100%" height="500" src="https://player.bilibili.com/player.html?aid=5868975&bvid=BV1Ts411k73E&cid=9531080&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe></iframe></div><p><a href="https://imgtu.com/i/gV9eEQ"><img src="https://img.enderfga.cn/img/gV9eEQ.jpg" alt=""></a></p><div style="text-align:center"><iframe width="100%" height="500" src="https://player.bilibili.com/player.html?aid=460307938&bvid=BV1t5411w723&cid=331362469&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe></iframe></div><p><img src="https://img.enderfga.cn/img/image-20211208141258648.png" alt=""></p><div style="text-align:center"><iframe width="100%" height="500" src="https://player.bilibili.com/player.html?aid=12069119&bvid=BV1Wx411q7zb&cid=19911067&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe></div><p>我记得当年进b站答题的时候有一道题让我选出“老头滚动条”的原名，百度的过程中我了解到“上古卷轴5”(老滚)mod的丰富程度堪比MC，中世纪剑与魔法的世界也让我着迷，于是我毅然决然地成为了一名抓根宝（龙裔），但通关的过程中我饱受迷路的困扰：黑灯瞎火的洞窟、乱七八糟的陷阱、不可名状的地图···于是我决定找一款我想怎么走就怎么走的游戏——《Assassin‘s Creed》。一入坑就是10年，我愿时间永远停留在佛罗伦萨塔顶的月圆之夜。库里的全套刺客信条通关了，我又想念剑与魔法的故事了，因为久仰其大名我下载了——《The Witcher 3》。这是一部我最喜欢的游戏，没有之一，这也许就是“第九艺术”吧。三言两语不能表达出其中的波澜壮阔，每一个支线，每一部DLC都值得我一遍又一遍地游玩。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs plain">我吻过凯尔莫罕忽然冷冽而至的白雪，<br>我听过史凯利杰伴着海妖清啸的海风。<br>我仰头饮尽诺威格瑞陈年的矮人烈酒，<br>我策马走遍全威伦最艰险的万水千山。<br>对我而言，家是什么地方。<br>是那抹黑白裙摆的倩影，<br>还是那丁香与醋栗的芬芳。<br></code></pre></td></tr></table></figure><p>给我留下深刻印象的游戏有很多，尼尔里的“世界竟然如此美丽”，2077里的”以我残躯化烈火”，死亡搁浅里的“我在冥滩等你”······希望我的程序猿之路最终可以走到像小岛秀夫那样，拥有自己的工作室，书写自己的艺术。</p><p>（另外上面2B那张图的作者是Wlop，我最喜欢的画师）</p><h2 id="Novel"><a href="#Novel" class="headerlink" title="Novel"></a>Novel</h2><p>我还年轻的时候会看一些天蚕土豆，唐家三少，耳根写的小说······对我的文学水平真是没有半点提升。</p><p>在高二语文老师的耳濡目染下，我一个理工男对文学兴趣盎然。即使很忙，也想抽点时间陶冶情操。</p><div style="text-align:center">落霞与孤鹜齐飞，秋水共长天一色</div><p><a href="https://imgtu.com/i/gV41qf"><img src="https://img.enderfga.cn/img/gV41qf.jpg" alt=""></a></p><p>除了诗与词，还记得名字的书只剩下《巨人的陨落》、《三体》、《外婆的道歉信》、《自由在高处》······</p><p>剩下的各种悬疑侦探小说就不列举了（不过强推一手《神探夏洛克》《POI疑犯追踪》）。</p><h2 id="Music"><a href="#Music" class="headerlink" title="Music"></a>Music</h2><p>我这个人听的歌有亿点点杂，基本歌单里什么都沾一点（二次元&amp;欧美占比较大）</p><p>特地开了一个Music版块都是因为——hanser！</p><div style="text-align:center"><iframe width="100%" height="500" src="https://player.bilibili.com/player.html?aid=4848309&bvid=BV1Cs411i7B1&cid=7870718&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe></div><p>2016-10-01至今，永远单推憨憨！</p><p><a href="https://imgtu.com/i/4jdP0g"><img src="https://img.enderfga.cn/img/4jdP0g.jpg" alt=""></a></p><h1 id="Enderfga。"><a href="#Enderfga。" class="headerlink" title="Enderfga。"></a>Enderfga。</h1><p>不知不觉写了好多废话了······</p><p>总之，大学生活开始了，希望我能当好一个神奇海螺/哆啦安梦。</p><div align="right">----Nothing is true，everything is permitted.</div>]]></content>
    
    
    
    <tags>
      
      <tag>闲谈</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
