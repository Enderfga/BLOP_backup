<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>岭回归，特征工程分析advertising.csv</title>
    <link href="/2021/10/28/ridge/"/>
    <url>/2021/10/28/ridge/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>机器学习第二次作业记录。小组成员：方桂安，刘玥，周敏。</p><span id="more"></span><h1 id="一、数据分析"><a href="#一、数据分析" class="headerlink" title="一、数据分析"></a>一、数据分析</h1><h2 id="1-1-数据缺失检查"><a href="#1-1-数据缺失检查" class="headerlink" title="1.1 数据缺失检查"></a>1.1 数据缺失检查</h2><p>首先，为了我们能正常进行数据分析，我们进行了数据缺失分布情况检查。代码及结果如下：</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211026214742931.png" alt="image-20211026214742931"></p><p>缺失值总数：</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211026214800042.png" alt="image-20211026214800042"></p><p>由上可知，我们的数据中没有缺失值，不需要进行插值处理。</p><h2 id="1-2-销售量与各媒体投入关系分析"><a href="#1-2-销售量与各媒体投入关系分析" class="headerlink" title="1.2 销售量与各媒体投入关系分析"></a>1.2 销售量与各媒体投入关系分析</h2><h3 id="1-2-1-散点图"><a href="#1-2-1-散点图" class="headerlink" title="1.2.1 散点图"></a>1.2.1 散点图</h3><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211026214824135.png" alt="image-20211026214824135"></p><p>以上是销售量与各项媒体投入量的散点图。从上图我们可以看出，sales和TV投入量有明显的正相关关系，随着TV投入增多，sales大体上呈上升趋势。sales和radio投入量也有较弱的正相关趋势，但sales分布在以radio投入量为指标时，分布较零散，相关关系弱于sales与TV投入量。而sales和newspaper的相关性最弱，sales集中分布在newspaper低投入区域内。</p><h3 id="1-2-2-各项数据分析"><a href="#1-2-2-各项数据分析" class="headerlink" title="1.2.2 各项数据分析"></a>1.2.2 各项数据分析</h3><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211026215012236.png" alt="image-20211026215012236"></p><p>由上图可知，TV类广告的平均投入量最大，其投入量的最小值，二分位数，中位数和四分位数，最大值均大于其他类型的广告，说明企业偏向于在TV类广告投入更多资金。</p><h3 id="1-2-3-相关系数"><a href="#1-2-3-相关系数" class="headerlink" title="1.2.3 相关系数"></a>1.2.3 相关系数</h3><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211026215047222.png" alt="image-20211026215047222"></p><p>上图为四个变量的相关系数热力图，由此可以看出，销售量和TV，radio，newspaper的相关性依次减弱。</p><h3 id="1-2-4-散点图矩阵，多变量之间的关系可视化"><a href="#1-2-4-散点图矩阵，多变量之间的关系可视化" class="headerlink" title="1.2.4 散点图矩阵，多变量之间的关系可视化"></a>1.2.4 散点图矩阵，多变量之间的关系可视化</h3><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211026215104208.png" alt="image-20211026215104208"></p><h2 id="1-3-得出结论"><a href="#1-3-得出结论" class="headerlink" title="1.3 得出结论"></a>1.3 得出结论</h2><p>由上面的分析可知，销售量和TV投入量相关性最大，其次是radio，newspaper，这也符合我们目前的社会情况，人们更多的是在电视等电子产品上获取信息。所以，加大上述三种广告方式的投入会对销售量有依次递减的增幅影响。</p><h1 id="二、描述10折交叉验证对数据集的处理"><a href="#二、描述10折交叉验证对数据集的处理" class="headerlink" title="二、描述10折交叉验证对数据集的处理"></a>二、描述10折交叉验证对数据集的处理</h1><h2 id="2-1-引入10折交叉验证的原因"><a href="#2-1-引入10折交叉验证的原因" class="headerlink" title="2.1 引入10折交叉验证的原因"></a>2.1 引入10折交叉验证的原因</h2><p>泛化能力是指模型在训练集上训练后,对新数据进行准确预测的能力。在机器学习的模型选择中，我们要对候选模型的泛化误差进行评估，然后选择泛化误差最小的那个模型。而实际应用中，我们无法直接获得泛化误差，而训练误差又由于过拟合现象的存在而不适合作为标准，所以我们随机将数据集切为三部分：</p><ul><li>训练集：用来训练模型，对应训练误差。</li><li>验证集：用来选择模型，对应测试误差。</li><li>测试集：用来最终对学习方法进行评估，对应泛化误差的近似。</li></ul><p>但是在实际应用中数据往往是不充足的，为了选择泛化能力更好的模型，我们可以对数据集D进行适当的处理，从中产生出训练集S和测试集T。几种常见的做法有：简单交叉验证(holdout cross-validation)、留一交叉验证(leave-one-out cross-validation,LOOCV)、<em>k</em>折交叉验证(<em>k</em>-fold cross-validation)、多重<em>k</em>折交叉验证、分层法(stratification-split cross-validation)、自助法(bootstraps)等。而综合考虑几种方法的特点后，本次我们选择的处理方法是10折交叉验证法。</p><h2 id="2-2-10折交叉验证的基本原理"><a href="#2-2-10折交叉验证的基本原理" class="headerlink" title="2.2 10折交叉验证的基本原理"></a>2.2 10折交叉验证的基本原理</h2><p>10折交叉验证是指将原始数据集随机划分为样本数量近乎相等的10个子集，轮流将其中的9个合并作为训练集，其余1个作为测试集。在每次试验中计算正确率等评价指标，最终通过k次试验后取评价指标的平均值来评估该模型的泛化能力。</p><p>10折交叉验证的基本步骤如下:</p><ol><li>原始数据集划分为10个样本量尽可能均衡的子集；</li><li>使用第1个子集作为测试集，第2～9个子集合并作为训练集；</li><li>使用训练集对模型进行训练,计算多种评价指标在测试集下的结果；</li><li>重复2-3步骤,轮流将第2-10个子集作为测试集；</li><li>计算各评价指标的平均值作为最终结果，最终选出10次测评中平均测试误差最小的模型。</li></ol><p>10折交叉验证的原理示意见下图。</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211026214227086.png" alt="image-20211026214227086"></p><p>由于将数据集D划分为k个子集同样存在多种划分方式，为了减小因样本划分不同而引入的差别，k折交叉验证通常要随机使用不同的划分重复p次。故我们可以采用“10次10折交叉验证”。</p><h2 id="2-3-10折交叉验证函数python代码"><a href="#2-3-10折交叉验证函数python代码" class="headerlink" title="2.3 10折交叉验证函数python代码"></a>2.3 10折交叉验证函数python代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> KFold  <span class="hljs-comment"># 从sklearn导入KFold包</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">Ten_Flod_spilt</span>(<span class="hljs-params">fold,data,label</span>):</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    param fold: 要取第几折的数据。</span><br><span class="hljs-string">    param data: 需要分块的数据</span><br><span class="hljs-string">    param label: 对应的需要分块标签</span><br><span class="hljs-string">    return: 对应折的训练集、测试集和对应的标签</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    split_list = []<br>    kf = KFold(n_splits=<span class="hljs-number">10</span>)<br>    <span class="hljs-keyword">for</span> train, test <span class="hljs-keyword">in</span> kf.split(data):<br>        split_list.append(train.tolist())<br>        split_list.append(test.tolist())<br>    train,test=split_list[<span class="hljs-number">2</span> * fold],split_list[<span class="hljs-number">2</span> * fold + <span class="hljs-number">1</span>]<br>    <span class="hljs-keyword">return</span>  data[train], data[test], label[train], label[test]  <span class="hljs-comment">#已经分好块的数据集</span><br></code></pre></td></tr></table></figure><p>在后续使用中只需循环调用该函数即可达到10折交叉验证的目的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>        x_train, x_test, y_train, y_test = Ten_Flod_spilt(i,X_s,y_s)<br></code></pre></td></tr></table></figure><h1 id="三、描述所使用的线性模型"><a href="#三、描述所使用的线性模型" class="headerlink" title="三、描述所使用的线性模型"></a>三、描述所使用的线性模型</h1><h2 id="3-1-基本形式"><a href="#3-1-基本形式" class="headerlink" title="3.1 基本形式"></a>3.1 基本形式</h2><p>给定由d个属性描述的示例<strong>x</strong>=(x~1~;x~2~;… ; x~d~)，其中x~i~是<strong>x</strong>在第i个属性上的取值，线性回归(linear regression)试图学得一个通过属性的线性组合来进行预测的函数，即</p><script type="math/tex; mode=display">f(\boldsymbol{x})=w_{1} x_{1}+w_{2} x_{2}+\ldots+w_{d} x_{d}+b</script><p>一般用向量形式写成</p><script type="math/tex; mode=display">f(\boldsymbol{x})=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b</script><p>其中<strong>w</strong>=(w~1~;w~2~;… ; w~d~)。<strong>w</strong>和b学得之后，模型就得以确定。</p><p>故本题中的模型应该为</p><script type="math/tex; mode=display">\widehat{Sales}=w_{1} ·TV+w_{2} ·radio+w_{3} ·newspaper+b，使得\widehat{Sales}\cong Sales</script><p>此处有三个属性描述样本，故又称为多元线性回归(multivariate linear regression)。</p><p>其基本形式为</p><script type="math/tex; mode=display">\hat{f}\left(\hat{x}_{N+1}\right)=\hat{x}_{N+1}^{T} \widehat{\omega}^{*}\\其中  \hat{x}_{N+1}=\left(x_{N+1} ; 1\right) \in \mathbb{R}^{n+1}, \widehat{\omega}^{*}=\left(\omega^{*} ; b^{*}\right) \in \mathbb{R}^{n+1}</script><h2 id="3-2-岭回归"><a href="#3-2-岭回归" class="headerlink" title="3.2 岭回归"></a>3.2 岭回归</h2><p>吉洪诺夫正则化以安德烈·尼古拉耶维奇·吉洪诺夫命名，为非适定性问题的正则化中最常见的方法。在统计学中，本方法被称为脊回归或岭回归（ridge regression）；在机器学习领域则称为权重衰减或权值衰减（weight decay）。因为有不同的数学家独立发现此方法，此方法又称做吉洪诺夫－米勒法（Tikhonov–Miller method）、菲利浦斯－图米法（Phillips–Twomey method）、受限线性反演（constrained linear inversion method），或线性正规化（linear regularization）。</p><script type="math/tex; mode=display">min\ L(W)=\frac{1}{2}(XW-y)^T(XW-y)+\frac{1}{2}\alpha||W||^2_2</script><script type="math/tex; mode=display">W=(X^TX+\alpha I)^{-1}X^Ty</script><p>根据4.2、4.3的分析，我们最终决定在最小二乘法的基础上采取L2正则化，即岭回归。相应地，为了使用岭回归和缩减技术，首先需要对特征做标准化处理。因为，我们需要使每个维度特征具有相同的重要性，故采用了z-score标准化。随着模型复杂度的提升，在训练集上的效果就越好，即模型的偏差就越小；但是同时模型的方差就越大。对于岭回归的α而言，随着α的增大，$|X^TX+\alpha I|$就越大，$(X^TX+\alpha I)^{-1}$ 就越小，模型的方差就越小；而α越大使得<strong>W</strong>的估计值更加偏离真实值，模型的偏差就越大。所以岭回归的关键是找到一个合理的α值来平衡模型的方差和偏差。</p><p>本次使用10折交叉验证法来确定α值，每一种训练集和测试集下都会有对应的一个模型及模型评分（如均方误差），进而可以得到一个平均评分。对于α值则选择平均评分最优的α值。</p><h2 id="3-3-特征工程"><a href="#3-3-特征工程" class="headerlink" title="3.3 特征工程"></a>3.3 特征工程</h2><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211027233323219.png" alt="image-20211027233323219"></p><p>如图所示为梯度下降法，最小二乘法和sklearn调用所得结果与真实值的对比折线图。从中可以看出，三种折线都已经接近重合，但又与真实值存在差异。查阅资料后，我们了解了特征工程的相关知识。</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/967090-20170116151505067-1134887580.png" alt="img"></p><p>“数据决定了机器学习的上限，而算法只是尽可能逼近这个上限”，这里的数据指的就是经过特征工程得到的数据。特征工程指的是把原始数据转变为模型的训练数据的过程，它的目的就是获取更好的训练数据特征，使得机器学习模型逼近这个上限。特征工程能使得模型的性能得到提升，有时甚至在简单的模型上也能取得不错的效果。特征工程在机器学习中占有非常重要的作用，一般认为括特征构建、特征提取、特征选择三个部分。特征构建比较麻烦，需要一定的经验。 特征提取与特征选择都是为了从原始特征中找出最有效的特征。它们之间的区别是特征提取强调通过特征转换的方式得到一组具有明显物理或统计意义的特征；而特征选择是从特征集合中挑选一组具有明显物理或统计意义的特征子集。两者都能帮助减少特征的维度、数据冗余，特征提取有时能发现更有意义的特征属性，特征选择的过程经常能表示出每个特征的重要性对于模型构建的重要性。</p><p>本次作业中主要使用了特征构建、特征选择、特征缩放，具体结果将在第五部分讨论。</p><h1 id="四、描述训练模型所使用的算法"><a href="#四、描述训练模型所使用的算法" class="headerlink" title="四、描述训练模型所使用的算法"></a>四、描述训练模型所使用的算法</h1><h2 id="4-1-数据预处理"><a href="#4-1-数据预处理" class="headerlink" title="4.1 数据预处理"></a>4.1 数据预处理</h2><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211027090542621.png" alt="image-20211027090542621"></p><p>本次数据处理使用的是z-score标准化，转换公式为：</p><script type="math/tex; mode=display">z=\frac{x-\mu}{\sigma}</script><p>使用python具体实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fit_transform</span>(<span class="hljs-params">x</span>):</span><br>    x = np.asarray(x)<br>    std_ = np.std(x, axis=<span class="hljs-number">0</span>)  <span class="hljs-comment"># 标准差</span><br>    mean_ = np.mean(x, axis=<span class="hljs-number">0</span>)  <span class="hljs-comment"># 均值</span><br>    <span class="hljs-keyword">return</span> (x - mean_) / std_<br></code></pre></td></tr></table></figure><h2 id="4-2-策略"><a href="#4-2-策略" class="headerlink" title="4.2 策略"></a>4.2 策略</h2><h3 id="4-2-1-经验风险最小化"><a href="#4-2-1-经验风险最小化" class="headerlink" title="4.2.1 经验风险最小化"></a>4.2.1 经验风险最小化</h3><p>均方误差是回归任务中最常用的性能度量，因此我们可试图让均方误差最小化，即</p><script type="math/tex; mode=display">\begin{aligned}\left(w^{*}, b^{*}\right) &=\underset{(w, b)}{\arg \min } \sum_{i=1}^{m}\left(f\left(x_{i}\right)-y_{i}\right)^{2} \\&=\underset{(w, b)}{\arg \min } \sum_{i=1}^{m}\left(y_{i}-w x_{i}-b\right)^{2}\end{aligned}</script><p>均方误差有非常好的几何意义，它对应了常用的欧几里得距离或简称“欧氏距离”(Euclidean distance)。基于均方误差最小化来进行模型求解的方法称为“最小二乘法”(least square method)。具体求解过程在4.3中进行介绍。</p><h3 id="4-2-2-结构风险最小化"><a href="#4-2-2-结构风险最小化" class="headerlink" title="4.2.2 结构风险最小化"></a>4.2.2 结构风险最小化</h3><h4 id="4-2-2-1-正则化"><a href="#4-2-2-1-正则化" class="headerlink" title="4.2.2.1 正则化"></a>4.2.2.1 正则化</h4><p>当模型的复杂度增大时，训练误差会逐渐减小并趋于0；而测试误差会先减小，达到最大值后又增大。当选择的模型复杂度过大时，就会发生过拟合，如下图所示。</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211027095448459.png" alt="image-20211027095448459"></p><p>为了避免因为过拟合问题而导致拟合效果不佳，我们在经验风险上加一个正则化项或罚项，使结构风险最小，这种方法叫做正则化，一般具有如下形式：</p><script type="math/tex; mode=display">\min _{f \in \mathcal{F}} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)+\lambda J(f)</script><h4 id="4-2-2-2-L1与L2正则化"><a href="#4-2-2-2-L1与L2正则化" class="headerlink" title="4.2.2.2 L1与L2正则化"></a>4.2.2.2 L1与L2正则化</h4><p>使用L1范数（也称曼哈顿距离或Taxicab范数，只允许在与空间轴平行行径的距离）又叫<strong>lasso</strong>回归，损失函数变为：</p><script type="math/tex; mode=display">J(\mathbf{W})=\frac{1}{2 n}(\mathbf{X} \mathbf{W}-\mathbf{Y})^{T}(\mathbf{X} \mathbf{W}-\mathbf{Y})+\alpha\|W\|_{1}</script><p>使用L2范数（也称欧几里德距离，是向量到原点的最短距离）又叫<strong>ridge</strong>回归，损失函数变为：</p><script type="math/tex; mode=display">J(\mathbf{W})=\frac{1}{2}(\mathbf{X} \mathbf{W}-\mathbf{Y})^{T}(\mathbf{X} \mathbf{W}-\mathbf{Y})+\frac{1}{2} \alpha\|W\|_{2}^{2}</script><p>L1能使得一些特征的系数变小，甚至还使一些绝对值较小的系数直接变为0，产生稀疏解，起到特征选择的作用，增强模型的泛化能力。</p><p>L2的优点是可以限制|w|的大小，从而使模型更简单，更稳定，即使加入一些干扰样本也不会对模型产生较大的影响，而且还能解决非正定的问题，强制使XTX可逆有解。</p><script type="math/tex; mode=display">\theta=\left(X X^{T}+\alpha I\right)^{-1} X Y</script><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/1335117-20180708193526314-357302334.png" alt="img"></p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/1335117-20180708194712773-1094778410.png" alt="img"></p><p>在上图中，两个坐标分别是要学习到的两个参数ω1和ω2；彩色线是损失函数J的等高线即损失值相等线；方形和圆形就分别是L1和L2所产生的额外误差（约束空间）；最后的目标要是两者最小，即要得到能使两者相加最小的点，也就是图中的黑色交点。在画等差图时，L1的效果就很容易与坐标轴相交了，这就是会产生很多0，即造成参数稀疏的原因。而且同时如果给一个微小的偏移，L2移动不会很大，而L1可能会移动到方形边上产生很多的交点，所以L1比较不稳定。</p><p>L2倾向于使ω的分量取值更均衡，即非零分量个数更稠密，而L1倾向ω的分量取值更稀疏，即非零分量个数更少。所以从图可以看出L1的边缘比较尖锐，与目标函数的等高线相交时，交点会常在那些尖锐的地方，所以很多的参数就是0，即L1能产生稀疏解。所以在调参时如果我们主要的目的只是为了解决过拟合，一般选择L2正则化就够了。但是如果选择L2正则化发现还是过拟合，即预测效果差的时候，就可以考虑L1正则化。另外，如果模型的特征非常多，我们希望一些不重要的特征系数归零，从而让模型系数稀疏化的话，也可以使用L1正则化。</p><p>综合考虑，我们在本次的损失函数中引入的是L2正则化。</p><h2 id="4-3-算法"><a href="#4-3-算法" class="headerlink" title="4.3 算法"></a>4.3 算法</h2><h3 id="4-3-1-最小二乘法"><a href="#4-3-1-最小二乘法" class="headerlink" title="4.3.1 最小二乘法"></a>4.3.1 最小二乘法</h3><h4 id="4-3-1-1-问题分析"><a href="#4-3-1-1-问题分析" class="headerlink" title="4.3.1.1 问题分析"></a>4.3.1.1 问题分析</h4><p>我们的策略是</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211028184420327.png" alt="image-20211028184420327"></p><p>我们进行展开</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211028184438472.png" alt="image-20211028184438472"></p><p>下面，我们进行梯度推导</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211028184450724.png" alt="image-20211028184450724"></p><p>由于$L(W)$是关于$W$的凸函数，所以我们在梯度为零的点，即是我们要求的最优解。</p><script type="math/tex; mode=display">令\frac{\partial L}{\partial W}=0\\得(X^TX+\alpha I)W=X^Ty</script><p>我们要通过此方法求得$W$，需要的条件是$X^TX+\alpha I$可逆，若其可逆，则$W$的解是</p><script type="math/tex; mode=display">W=(X^TX+\alpha I)^{-1}X^Ty</script><p>因为最小二乘法要求$X^TX+\alpha I$必须存在可逆矩阵，在实际问题中可能不满足，于是我们下面采用梯度下降法进行迭代求解。</p><h4 id="4-3-1-2-代码实现"><a href="#4-3-1-2-代码实现" class="headerlink" title="4.3.1.2 代码实现"></a>4.3.1.2 代码实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">lms</span>(<span class="hljs-params">x_train, x_test, y_train, y_test</span>):</span><br>    x_train_=np.c_[np.ones([x_train.shape[<span class="hljs-number">0</span>],<span class="hljs-number">1</span>]),x_train]<br>    x_test_=np.c_[np.ones([x_test.shape[<span class="hljs-number">0</span>],<span class="hljs-number">1</span>]),x_test]<br>    theta_n = np.dot(np.dot(inv(np.dot(x_train_.T, x_train_)+<span class="hljs-number">0.1</span>*np.eye(x_train_.shape[<span class="hljs-number">1</span>])), x_train_.T), y_train)  <span class="hljs-comment"># theta = (X`X)^(-1)X`Y，其中X`表示X的转置，使用L2范数正则化</span><br>    y_pre = np.dot(x_test_, theta_n)<br>    mse = np.average((y_test - y_pre) ** <span class="hljs-number">2</span>)<br>    <span class="hljs-keyword">return</span> theta_n, y_pre, mse<br></code></pre></td></tr></table></figure><h3 id="4-3-2-梯度下降法"><a href="#4-3-2-梯度下降法" class="headerlink" title="4.3.2 梯度下降法"></a>4.3.2 梯度下降法</h3><h4 id="4-3-2-1-问题分析"><a href="#4-3-2-1-问题分析" class="headerlink" title="4.3.2.1 问题分析"></a>4.3.2.1 问题分析</h4><p>首先，我们的目标是下式</p><script type="math/tex; mode=display">令\hat{\omega}=W\\E(\hat{\omega})=\frac{1}{2}(X\hat{\omega}-y)^T(X\hat{\omega}-y)+\frac{1}{2}\alpha||\hat{\omega}||^2_2\quad,\hat{\omega}=\mathop{argmin}_\hat{\omega}\ E(\hat{\omega})</script><p>梯度下降法是一种迭代算法：我们选取适当的初始值$\hat{\omega}^{(0)}$，不断迭代，更新$\hat{\omega}$的值，进行目标函数的极小化，直到收敛。由于负梯度方向是使得函数值下降最快的方向，在迭代的每一步，以负梯度方向更新$\hat{\omega}$的值，从而达到减小函数值的目的。如下图形象化表示：</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211027100605390.png" alt="image-20211027100605390"></p><h4 id="4-3-2-2-核心思想"><a href="#4-3-2-2-核心思想" class="headerlink" title="4.3.2.2 核心思想"></a>4.3.2.2 核心思想</h4><ol><li>$E(\hat{\omega})$是具有一阶连续偏导数的凸函数，其极值点在一阶导数为零的地方取得</li><li>一阶泰勒展开：$E(\hat{\omega})\thickapprox E(\hat{\omega}^{(k)})+\nabla E(\hat{\omega}^{(k)})(\hat{\omega}-\hat{\omega}^{(k)})$，其中，$\nabla E(\hat{\omega}^{(k)})$是$E(\hat{\omega})$在$\hat{\omega}^{(k)}$的梯度：</li></ol><script type="math/tex; mode=display">\nabla E(\hat{\omega}^{(k)})=\frac{\partial E(\hat{\omega})}{\partial \hat{\omega}}|_{\hat{\omega}=\hat{\omega}^{(k)}}</script><ol><li>求取第k+1次迭代值：$\hat{\omega}^{k+1}=\hat{\omega}^{(k)}+\eta_k*(-\nabla E(\hat{\omega}^{(k)}))$，其中$\eta_k$是步长，有我们最初指定。梯度如下（推导在上面部分）：<script type="math/tex; mode=display">\frac{\partial E}{\partial \hat{\omega}}=X^TX\hat{\omega}-X^Ty+\alpha I\hat{\omega}\quad(I是n\times n的单位矩阵)</script></li></ol><h4 id="4-3-2-3-求解步骤"><a href="#4-3-2-3-求解步骤" class="headerlink" title="4.3.2.3 求解步骤"></a>4.3.2.3 求解步骤</h4><p>输入：目标函数$E(\hat{\omega})$，梯度函数$\nabla E(\hat{\omega})$，计算精度ε，步长$\eta_k$；</p><p>输出： $E(\hat{\omega})$的极小点$\hat{\omega}^*$。</p><p>（1）取初始值$\hat{\omega}^{(0)}\in \mathbb{R}^{d+1}$，置k=0；</p><p>（2）计算$E(\hat{\omega}^{(k)})$；</p><p>（3）计算梯度$\nabla E(\hat{\omega}^{(k)})$，当$||\nabla E(\hat{\omega}^{(k)})||&lt;\varepsilon$时，令$\hat{\omega}^*=\hat{\omega}^{(k)}$，</p><p>停止迭代；</p><p>（4）置$\hat{\omega}^{(k+1)}=\hat{\omega}^{(k)}+\eta_k(-\nabla E(\hat{\omega}^{(k)}))$，计算$E(\hat{\omega}^{(k+1)})$，</p><p>当$||E(\hat{\omega}^{(k+1)})-E(\hat{\omega}^{(k)})||&lt;\varepsilon$或$||\hat{\omega}^{(k+1)}-\hat{\omega}^{(k)}||&lt;\varepsilon$时，</p><p>令$\hat{\omega}^*=\hat{\omega}^{(k)}$，停止迭代；</p><p>（5）否则，置k=k+1，转步骤（3）。</p><h4 id="4-3-2-4-代码实现"><a href="#4-3-2-4-代码实现" class="headerlink" title="4.3.2.4 代码实现"></a>4.3.2.4 代码实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">GradientDescent_MultiLine</span>:</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, lr, epochs</span>):</span><br>        self.lr = lr  <span class="hljs-comment"># 学习率，用来控制步长（权重调整幅度）</span><br>        self.epochs = epochs  <span class="hljs-comment"># 循环迭代的次数</span><br>        self.lose = []  <span class="hljs-comment"># 损失值计算（损失函数）：均方误差</span><br><br>    <span class="hljs-string">&#x27;&#x27;&#x27;根据提供的训练数据对模型进行训练&#x27;&#x27;&#x27;</span><br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fit</span>(<span class="hljs-params">self, x, y</span>):</span><br>        x = np.asarray(x)<br>        y = np.asarray(y)<br>        y = np.squeeze(y)  <span class="hljs-comment"># 去掉冗余的维度</span><br><br>        self.w = np.zeros(<span class="hljs-number">1</span> + x.shape[<span class="hljs-number">1</span>])  <span class="hljs-comment"># 初始权重，权重向量初始值为0（或任何其他值），长度比X的特征数量多1（多出来的为截距）</span><br><br>        <span class="hljs-comment"># 开始训练</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.epochs):<br>            y_hat = np.dot(x, self.w[<span class="hljs-number">1</span>:]) + self.w[<span class="hljs-number">0</span>]  <span class="hljs-comment"># 计算预测值</span><br>            error = y - y_hat  <span class="hljs-comment"># 计算真实值与预测值之间的差距</span><br>            self.lose.append(np.<span class="hljs-built_in">sum</span>(error ** <span class="hljs-number">2</span>) / <span class="hljs-number">2</span> + <span class="hljs-number">0.1</span>* np.dot(self.w.T, self.w))  <span class="hljs-comment"># 将损失加入到损失列表中，使用L2范数正则化</span><br>            <span class="hljs-comment">#print(&quot;迭代次数:&#123;0&#125;,进度：&#123;1&#125;%&quot;.format(i + 1, 100.0 * (i + 1) / self.epochs), &quot;  loss:&quot;, np.sum(error ** 2) / 2)</span><br>            <span class="hljs-comment"># j &lt;- j + α * sum((y - y_hat) * x(j))</span><br>            x_=np.c_[np.ones([x.shape[<span class="hljs-number">0</span>],<span class="hljs-number">1</span>]),x]<br>            <span class="hljs-comment">#self.w[0] += self.lr * np.sum(error)</span><br>            <span class="hljs-comment">#self.w[1:] += self.lr * np.dot(x.T, error)</span><br>            I=np.identity(x_.shape[<span class="hljs-number">1</span>])<br>            self.w=self.w-self.lr*(np.dot((np.dot(x_.T, x_)+<span class="hljs-number">0.2</span>*I), self.w)-np.dot(x_.T, y))<br></code></pre></td></tr></table></figure><h1 id="五、分析模型训练结果，包括训练误差和测试误差"><a href="#五、分析模型训练结果，包括训练误差和测试误差" class="headerlink" title="五、分析模型训练结果，包括训练误差和测试误差"></a>五、分析模型训练结果，包括训练误差和测试误差</h1><h2 id="5-1-评估指标计算公式"><a href="#5-1-评估指标计算公式" class="headerlink" title="5.1 评估指标计算公式"></a>5.1 评估指标计算公式</h2><p>训练误差是模型关于训练数据集的平均损失；测试误差是模型关于测试数据集的平均损失。计算公式如下：</p><script type="math/tex; mode=display">R_{e m p}(\hat{f})=\frac{1}{N} \sum_{i=1}^{N} L\left(y, \hat{f}\left(x_{i}\right)\right)</script><script type="math/tex; mode=display">e_{t e s t}=\frac{1}{N^{\prime}} \sum_{i=1}^{N^{\prime}} L\left(y_{i}, \hat{f}\left(x_{i}\right)\right)</script><p>其中N为训练样本容量，<em>N</em>′为测试样本容量。由于我们在线性回归中使用的是平方损失函数，故上述计算结果又叫均方误差 MSE（Mean Squared Error）：</p><script type="math/tex; mode=display">M S E=\frac{1}{m} \sum_{i=1}^{m}\left(y_{\text {test }}^{(i)}-\hat{y}_{\text {test }}^{(i)}\right)^{2}</script><p>但是，MSE 公式有一个问题是会改变量纲。因为公式平方了，我们可以对这个MSE开方，得到第二个评价指标：均方根误差 RMSE（Root Mean Squared Error）：</p><script type="math/tex; mode=display">R M S E = \sqrt{M S E}=\sqrt{\frac{1}{m} \sum_{i=1}^{m}\left(y_{\text {test }}^{(i)}-\hat{y}_{\text {test }}^{(i)}\right)^{2}}</script><p>但是MSE不甚全面，某些情况下决定系数 R2（coefficient of determination）显得尤为有用，它可以看作是MSE的标准化版本，用于更好地解释模型的性能。R2值的定义如下：</p><script type="math/tex; mode=display">R^{2}=1-\frac{\left(\sum_{i=1}^{m}\left(\hat{y}^{(i)}-y^{(i)}\right)^{2}\right) / m}{\left(\sum_{i=1}^{m}\left(y^{(i)}-\bar{y}\right)^{2}\right) / m}=1-\frac{M S E(\hat{y}, y)}{\operatorname{Var}(y)}</script><h2 id="5-2-误差分析思路"><a href="#5-2-误差分析思路" class="headerlink" title="5.2 误差分析思路"></a>5.2 误差分析思路</h2><p>结合前文的推导分析，我们最终采用的是线性最小二乘法与L2正则化，即alpha取值为1.0的<strong>Ridge回归</strong>，并结合特征工程中特征构建（将<strong>TV*radio</strong>，<strong>radio*newspaper</strong>作为新的特征），特征选择（加入新的特征，舍弃相关系数较小的newspaper），特征缩放（将TV，radio，newspaper进行开方、平方、三次方等）的思路进行了14种情况的实验，并得出了每一种情况的MSE，RMSE，R^2^。</p><p>所使用的代码为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">X = np.asarray(data_s.get([<span class="hljs-string">&#x27;TV&#x27;</span>,<span class="hljs-string">&#x27;radio&#x27;</span>,<span class="hljs-string">&#x27;newspaper&#x27;</span>]))<br>y = np.asarray(data_s.get(<span class="hljs-string">&#x27;sales&#x27;</span>))<br>clf.fit(X,y)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;MSE=%f&#x27;</span>%(-<span class="hljs-number">0.1</span>*cross_val_score(clf, X, y, cv=<span class="hljs-number">10</span>, scoring=<span class="hljs-string">&#x27;neg_mean_squared_error&#x27;</span>).<span class="hljs-built_in">sum</span>()))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;RMSE=%f&#x27;</span>%(-<span class="hljs-number">0.1</span>*cross_val_score(clf, X, y, cv=<span class="hljs-number">10</span>, scoring=<span class="hljs-string">&#x27;neg_root_mean_squared_error&#x27;</span>).<span class="hljs-built_in">sum</span>()))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;R2=%f&#x27;</span>%(<span class="hljs-number">10</span>*cross_val_score(clf, X, y, cv=<span class="hljs-number">10</span>, scoring=<span class="hljs-string">&#x27;r2&#x27;</span>).<span class="hljs-built_in">sum</span>())+<span class="hljs-string">&#x27;%&#x27;</span>)<br><span class="hljs-built_in">print</span>(clf.coef_)<br><span class="hljs-built_in">print</span>(clf.intercept_)<br></code></pre></td></tr></table></figure><h2 id="5-3-训练结果"><a href="#5-3-训练结果" class="headerlink" title="5.3 训练结果"></a>5.3 训练结果</h2><p>14种情况的训练误差及测试误差记录在jupyter notebook的ipynb文件中，此处展示其中三种。</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211028143446583.png" alt="image-20211028143446583"></p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211028143509634.png" alt="image-20211028143509634"></p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211028144137987.png" alt="image-20211028144137987"></p><p>上图所示的第13种情况训练所得模型的各项评估指标最优，故将其model文件保存，助教老师可以使用test.ipynb自动载入模型，并计算出测试误差MSE。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> joblib<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> mean_squared_error <br>model = joblib.load(<span class="hljs-string">&#x27;model.pickle&#x27;</span>) <span class="hljs-comment">#载入模型</span><br>data=pd.read_csv(<span class="hljs-string">&#x27;5_test.csv&#x27;</span>) <span class="hljs-comment">#读入数据</span><br>data_s = (data-data.<span class="hljs-built_in">min</span>())/(data.<span class="hljs-built_in">max</span>()-data.<span class="hljs-built_in">min</span>()) <span class="hljs-comment">#归一化</span><br>data_s[<span class="hljs-string">&#x27;TV_min&#x27;</span>] = data_s[<span class="hljs-string">&#x27;TV&#x27;</span>].apply(<span class="hljs-keyword">lambda</span> x:x**<span class="hljs-number">0.2</span>)<br>data_s[<span class="hljs-string">&#x27;TV_radio&#x27;</span>]=data_s[<span class="hljs-string">&#x27;TV&#x27;</span>]*data_s[<span class="hljs-string">&#x27;radio&#x27;</span>]<br>X = np.asarray(data_s.get([<span class="hljs-string">&#x27;TV_radio&#x27;</span>,<span class="hljs-string">&#x27;TV_min&#x27;</span>,<span class="hljs-string">&#x27;radio&#x27;</span>,<span class="hljs-string">&#x27;newspaper&#x27;</span>]))<br>y = np.asarray(data_s.get(<span class="hljs-string">&#x27;sales&#x27;</span>))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;测试误差MSE=%f&#x27;</span>%mean_squared_error(y,model.predict(X)))<br></code></pre></td></tr></table></figure><h1 id="六、总结模型训练过程中的收获"><a href="#六、总结模型训练过程中的收获" class="headerlink" title="六、总结模型训练过程中的收获"></a>六、总结模型训练过程中的收获</h1><h2 id="6-1-学习数据分析处理"><a href="#6-1-学习数据分析处理" class="headerlink" title="6.1 学习数据分析处理"></a>6.1 学习数据分析处理</h2><p>在进行计算之前，我们首先对数据进行了预处理和分析。首先，我们检查了数据是否缺失。然后，我们画出了散点图，散点图矩阵，相关系数热力图等，分析了销售量和各项广告投入量之间的数据关系，以便于对数据的进一步处理。在对数据的处理中，我们首先进行了数据标准化，将不同量级的数据统一转化为同一量级，以保证数据之间的可比性。而后，我们查阅资料，为了获取更好的训练数据特征，了解了特征工程相关内容，再根据之前对数据的分析，我们对数据进行了特征构建、特征选择等，具体结果上面已经展示。</p><h2 id="6-2-加深对十折交叉验证的理解"><a href="#6-2-加深对十折交叉验证的理解" class="headerlink" title="6.2 加深对十折交叉验证的理解"></a>6.2 加深对十折交叉验证的理解</h2><p>在机器学习的模型选择中，我们要对候选模型的泛化误差进行评估，然后选择泛化误差最小的那个模型。而实际应用中，我们无法直接获得泛化误差，而训练误差又由于过拟合现象的存在而不适合作为标准，所以我们随机将数据集切为三部分：训练集，验证集和测试集。在十折交叉验证中，我们通过某种特定的划分，将所有数据划分为十个，并依次选取作为测试集，剩下的作为训练集。在这个过程中，我们加深了对十折交叉验证的理解。</p><h2 id="6-3-对于正则化的理解加深"><a href="#6-3-对于正则化的理解加深" class="headerlink" title="6.3 对于正则化的理解加深"></a>6.3 对于正则化的理解加深</h2><p>正则化的目的：防止过拟合。过拟合指的是给定一堆数据，这堆数据带有噪声，利用模型去拟合这堆数据，可能会把噪声数据也给拟合了，这一方面会造成模型比较复杂，比如，原本一次函数能够拟合的数据，由于数据带有噪声，导致需要用五次函数来拟合；另一方面，同时会导致模型的泛化性能很差，在测试集上的结果准确率非常高，但测试新数据时，因为得到的是过拟合的模型，正确率会很低。</p><p>正则化的本质：约束（限制）要优化的参数。本来<strong>解空间</strong>是全部区域，但通过正则化添加了一些约束，使得解空间变小了，甚至在个别正则化方式下，解变得稀疏了。正如下图所示：</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/1335117-20180708193526314-357302334.png" alt="img"><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/1335117-20180708194712773-1094778410.png" alt="img"></p><p>彩色线就是优化过程中遇到的等高线，一圈代表一个目标函数值，圆心就是样本观测值（假设一个样本），半径就是误差值，受限条件就是黑色边界（就是正则化的部分），二者相交处，才是最优参数。</p><p>可以看到，L1 与L2 的不同就在于L1在和每个坐标轴相交的地方都有“角”出现，而目标函数的测地线除非位置摆得非常好，大部分时候都会在角的地方相交。注意到在角的位置就会产生稀疏性，例如图中的相交点就有w1=0，而更高维的时候除了角点以外，还有很多边的轮廓也是既有很大的概率成为第一次相交的地方，又会产生稀疏性。相比之下，L2就没有这样的性质，因为没有角，所以第一次相交的地方出现在具有稀疏性的位置的概率就变得非常小了。这就从直观上来解释了为什么L1-regularization 能产生稀疏性，而L2-regularization 不行的原因了。</p><h2 id="6-4-关于算法的择优"><a href="#6-4-关于算法的择优" class="headerlink" title="6.4 关于算法的择优"></a>6.4 关于算法的择优</h2><p>最开始我们分析结构风险最小化的策略，最小二乘法可能不可逆，同时为了增加模型的泛化能力，我们在损失函数中加入了惩罚项，由于对L1，L2正则化的分析，我们选择L2正则化，经过推导，发现最小二乘法可以直接得到解析解，解决了W系数矩阵非正定问题。由于梯度下降法是通过迭代逼近结果，所以只能得到近似解，所以我们选择最小二乘法来进行计算。</p>]]></content>
    
    
    
    <tags>
      
      <tag>course</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>逻辑回归模型及lbfgs算法公式推导</title>
    <link href="/2021/10/23/logistic/"/>
    <url>/2021/10/23/logistic/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>机器学习第一次作业记录。小组成员：方桂安，刘玥，周敏。</p><span id="more"></span><h2 id="一、描述逻辑回归模型"><a href="#一、描述逻辑回归模型" class="headerlink" title="一、描述逻辑回归模型"></a>一、描述逻辑回归模型</h2><h3 id="1-1数据"><a href="#1-1数据" class="headerlink" title="1.1数据"></a>1.1数据</h3><script type="math/tex; mode=display">D=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\},x_i\in \mathbb{R}^n,y_i\in\{0,1\}</script><h3 id="1-2模型"><a href="#1-2模型" class="headerlink" title="1.2模型"></a>1.2模型</h3><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211023235517425.png" alt="image-20211023235517425"></p><p>最初模型：</p><script type="math/tex; mode=display">f(x_i)=\omega^Tx_i+b,使得f(x_i)\simeq g(y_i)</script><p>我们的标记变量y的范围是0或1，所以我们需要一个函数能够将上述x的线性组合转化为0或1，最理想的是阶跃函数。</p><script type="math/tex; mode=display">阶跃函数：y=g^{-1}(\omega^Tx+b)= \begin{cases}0, & \omega^Tx+b<0\\0.5, & \omega^Tx+b=0\\1, & \omega^Tx+b>0\end{cases}</script><p>但由于阶跃函数不连续，不满足单调可微的条件。所以我们希望通过一个一定程度上近似阶跃函数的“替代函数”，并且希望它单调可微。由此，我们想到了逻辑斯蒂函数。</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211023235558559.png" alt="image-20211023235558559"></p><p>它的图像如下：</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/1990595-20200922165029694-1712738583.png" alt="img"></p><p>因为Logistic 回归主要用于分类问题，以二分类为例，对于所给数据集假设存在这样的一条直线可以将数据完成线性可分。                <img src="https://gitee.com/sysu_20354027/pic/raw/master/img/tW5koNMJrG193KEtuAH7cQ.png" alt="img"></p><p>当我们要找到分类概率 P(Y=1) 与输入向量 x 的直接关系时，我们引入Sigmoid函数，然后通过比较概率值来判断类别。</p><p>引入sigmoid函数具体实现如下：</p><p>但因为逻辑斯蒂函数的值域在[0,1]之间，无法直接输出0或1。在此基础上，考虑到$\omega^Tx+b$取值是连续的，因此它不能拟合离散变量。可以考虑用它来拟合条件概率$p(y=1|x)$，因为概率的取值也是连续的,我们将逻辑斯蒂函数的输出作为输入x能预测到y为1的概率，并利用对数几率函数，得到下面三个式子。通过此方法，我们将线性模型转换为概率模型。</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211023235625407.png" alt="image-20211023235625407"></p><h3 id="1-3策略"><a href="#1-3策略" class="headerlink" title="1.3策略"></a>1.3策略</h3><p>在策略上，我们采用极大似然法。即选择最优的w，b使得我们输入x得到的正确的y的概率最大，即下式：</p><script type="math/tex; mode=display">(w^*,b^*)=\mathop{argmax}\limits_{(w,b)}\prod_{i=1}^Np(y_i|x_i;\omega,b)</script><p>我们这里做一点变换：</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211023235646944.png" alt="image-20211023235646944"></p><p>因为上式是连乘的函数，我们通过对数似然函数将之转化为求和，即下式：</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211023235715659.png" alt="image-20211023235715659"></p><p>为了方便计算，我们做以下处理</p><script type="math/tex; mode=display">assume \ that\  \hat{\omega}=(\omega;b),\hat{x}=(x;1)</script><p>则上式可化为</p><script type="math/tex; mode=display">\hat{\omega^*}=\mathop{argmin}\limits_{\hat{\omega}}\sum_{i=1}^N(-y_i\hat{\omega }x_i+ln(1+e^{\hat{\omega}^T\hat{x}_i}))</script><p>这是一个凸函数，可用经典的数值优化算法，如梯度下降法、牛顿法求解。</p><p>最终，我们学得的逻辑斯蒂回归模型为</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211023235742051.png" alt="image-20211023235742051"></p><h2 id="二、描述训练模型所使用的算法"><a href="#二、描述训练模型所使用的算法" class="headerlink" title="二、描述训练模型所使用的算法"></a>二、描述训练模型所使用的算法</h2><h3 id="2-1梯度下降法"><a href="#2-1梯度下降法" class="headerlink" title="2.1梯度下降法"></a>2.1梯度下降法</h3><h4 id="2-1-1问题分析"><a href="#2-1-1问题分析" class="headerlink" title="2.1.1问题分析"></a>2.1.1问题分析</h4><p>首先，我们的目标是下式</p><script type="math/tex; mode=display">E(\hat{\omega})=\sum_{i=1}^N(-y_i\hat{\omega}^T\hat{x}_i+ln(1+e^{\hat{\omega}^T\hat{x}_i})),\hat{\omega}^*=\mathop{argmin}_{\hat{\omega}}E(\hat{\omega})</script><p>梯度下降法是一种迭代算法：我们选取适当的初始值$\hat{\omega}^{(0)}$，不断迭代，更新$\hat{\omega}$的值，进行目标函数的极小化，直到收敛。由于负梯度方向是使得函数值下降最快的方向，在迭代的每一步，以负梯度方向更新$\hat{\omega}$的值，从而达到减小函数值的目的。如下图形象化表示：</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211023152139696.png" alt="image-20211023152139696"></p><h4 id="2-1-2核心思想："><a href="#2-1-2核心思想：" class="headerlink" title="2.1.2核心思想："></a>2.1.2核心思想：</h4><ol><li>$E(\hat{\omega})$是具有一阶连续偏导数的凸函数，其极值点在一阶导数为零的地方取得</li><li>一阶泰勒展开：$E(\hat{\omega})\thickapprox E(\hat{\omega}^{(k)})+\nabla E(\hat{\omega}^{(k)})(\hat{\omega}-\hat{\omega}^{(k)})$，其中，$\nabla E(\hat{\omega}^{(k)})$是$E(\hat{\omega})$在$\hat{\omega}^{(k)}$的梯度：</li></ol><script type="math/tex; mode=display">\nabla E(\hat{\omega}^{(k)})=\frac{\partial E(\hat{\omega})}{\partial \hat{\omega}}|_{\hat{\omega}=\hat{\omega}^{(k)}}</script><ol><li>求取第k+1次迭代值：$\hat{\omega}^{k+1}=\hat{\omega}^{(k)}+\eta_k*(-\nabla E(\hat{\omega}^{(k)}))$，其中$\eta_k$是步长，由我们最初指定。梯度推导：</li></ol><script type="math/tex; mode=display">E(\hat{\omega})=\sum_{i=1}^N(-y_i\hat{\omega}^T\hat{x}_i+ln(1+e^{\hat{\omega}^T\hat{x}_i}))\\\nabla E(\hat{\omega}^{(k)})=\sum_{i=1}^N-y_i\hat{x}_i+\frac{1}{1+e^{\hat{\omega}^T\hat{x}_i}}*e^{\hat{\omega}^T\hat{x}_i}*\hat{x}_i\\=-\sum_{i=1}^Nx_i(y_i-\frac{e^{\hat{\omega}^T\hat{x}_i}}{1+e^{\hat{\omega}^T\hat{x}_i}})</script><h4 id="2-1-3伪代码："><a href="#2-1-3伪代码：" class="headerlink" title="2.1.3伪代码："></a>2.1.3伪代码：</h4><p>输入：目标函数$E(\hat{\omega})$，梯度函数$\nabla E(\hat{\omega})$，计算精度ε，步长$\eta_k$；</p><p>输出： $E(\hat{\omega})$的极小点$\hat{\omega}^*$。</p><p>（1）取初始值$\hat{\omega}^{(0)}\in \mathbb{R}^{d+1}$，置k=0；</p><p>（2）计算$E(\hat{\omega}^{(k)})$；</p><p>（3）计算梯度$\nabla E(\hat{\omega}^{(k)})$，当$||\nabla E(\hat{\omega}^{(k)})||&lt;\varepsilon$时，令$\hat{\omega}^*=\hat{\omega}^{(k)}$，</p><p>停止迭代；</p><p>（4）置$\hat{\omega}^{(k+1)}=\hat{\omega}^{(k)}+\eta_k(-\nabla E(\hat{\omega}^{(k)}))$，计算$E(\hat{\omega}^{(k+1)})$，</p><p>当$||E(\hat{\omega}^{(k+1)})-E(\hat{\omega}^{(k)})||&lt;\varepsilon$或$||\hat{\omega}^{(k+1)}-\hat{\omega}^{(k)}||&lt;\varepsilon$时，</p><p>令$\hat{\omega}^*=\hat{\omega}^{(k)}$，停止迭代；</p><p>（5）否则，置k=k+1，转步骤（3）。</p><h4 id="2-1-4分析"><a href="#2-1-4分析" class="headerlink" title="2.1.4分析"></a>2.1.4分析</h4><p>优点：方法简单，易理解</p><p>缺点：迭代次数多，下降速度慢，如下图，我们采用梯度下降法，迭代近50000次才收敛</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211023152205584.png" alt="image-20211023152205584"></p><p>且准确率如下，可以看出准确率不高。</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211023152217061.png" alt="image-20211023152217061"></p><h3 id="2-2牛顿法"><a href="#2-2牛顿法" class="headerlink" title="2.2牛顿法"></a>2.2牛顿法</h3><h4 id="2-2-1核心思想："><a href="#2-2-1核心思想：" class="headerlink" title="2.2.1核心思想："></a>2.2.1核心思想：</h4><p>$E(\hat{\omega})$是具有二阶连续偏导数的函数</p><p>二阶泰勒展开：$E(\hat{\omega})\thickapprox E(\hat{\omega}^{(k)})+\nabla E(\hat{\omega}^{(k)})(\hat{\omega}-\hat{\omega}^{(k)})+\frac{1}{2}(\hat{\omega}-\hat{\omega}^{(k)})^TH(\hat{\omega}^{(k)})(\hat{\omega}-\hat{\omega}^{(k)})$</p><script type="math/tex; mode=display">\nabla E(\hat{\omega})=\frac{\partial E(\hat{\omega})}{\partial \hat{\omega}}|_{(d+1)\times 1},H(\hat{\omega})=\frac{\partial ^2E(\hat{\omega})}{\partial \hat{\omega}_i \partial \hat{\omega}_j}|_{(d+1)\times 1}</script><p>利用二阶泰勒展开$E(\hat{\omega})$取极小点的必要条件$\nabla E(\hat{\omega})=0$，在第k次迭代$\hat{\omega}^{(k)}$，求$\nabla E(\hat{\omega}^{(k)})+H(\hat{\omega}^{(k)})(\hat{\omega}-\hat{\omega}^{(k)})=0$的点，作为第k+1次迭代值$\hat{\omega}^{(k+1)}$</p><h4 id="2-2-2伪代码"><a href="#2-2-2伪代码" class="headerlink" title="2.2.2伪代码"></a>2.2.2伪代码</h4><p>输入：目标函数$E(\hat{\omega})$，梯度函数$\nabla E(\hat{\omega})$，海森矩阵$H(\hat{\omega})$，精度ε；</p><p>输出：$E(\hat{\omega})$的极小点$\hat{\omega}^*$。</p><p>（1）取初始值$\hat{\omega}^{(0)}\in \mathbb{R}^{n+1}$，置k=0；</p><p>（2）计算梯度$\nabla E(\hat{\omega}^{(k)})$；</p><p>（3）当$||E(\hat{\omega}^{(k)})||&lt;\varepsilon$时，令$\hat{\omega}^*=\hat{\omega}^{(k)}$，停止迭代；</p><p>否则，计算海森矩阵$H(\hat{\omega}^{(k)})$ ；</p><p>（4）置$\hat{\omega}^{(k+1)}=\hat{\omega}^{(k)}-(H(\hat{\omega}))^{(-1)}\nabla E(\hat{\omega}^{(k)})$；</p><p>（5）置k=k+1，转步骤（2）。</p><h4 id="2-2-3分析"><a href="#2-2-3分析" class="headerlink" title="2.2.3分析"></a>2.2.3分析</h4><p>牛顿法优点：下降速度快，属于二次收敛</p><p>缺点：海森矩阵计算复杂度高，且要求可逆才能计算，所以我们查阅资料，将采用拟牛顿法。</p><h3 id="2-3-BFGS算法"><a href="#2-3-BFGS算法" class="headerlink" title="2.3 BFGS算法:"></a>2.3 BFGS算法:</h3><p>由于上述牛顿公式中可以看出，我们的海森矩阵不易得到，因此我们有以下迭代公式来逼近海森矩阵：</p><script type="math/tex; mode=display">H_{k+1}=H_k+\frac{y_ky_k^T}{y_k^Ts_k}-\frac{H_ks_ks_k^TH_k^T}{s_k^TH_k^Ts_k}</script><p>但计算量还是很大，矩阵相乘太多。所以我们最终采取$Sherman-Morrison$公式进行变换可得：</p><script type="math/tex; mode=display">H_{k+1}=\left(I-\frac{s_{k} y_{k}^{T}}{y_{k}^{T} s_{k}}\right) H_{k}\left(I-\frac{y_{k} s_{k}^{T}}{y_{k}^{T} s_{k}}\right)+\frac{s_{k} s_{k}^{T}}{y_{k}^{T} s_{k}} \quad(1)</script><p>公式推导如下：</p><script type="math/tex; mode=display">\begin{array}{l}\text { Sherman Morrison 公式: }\\\left(\mathrm{A}+\frac{u u^{T}}{t}\right)^{-1}=A^{-1}-\frac{A^{-1} u u^{T} A^{-1}}{t+u^{T} A^{-1} u}\\\left(\mathrm{H}+\frac{y y^{T}}{y^{T} \mathrm{~s}}-\frac{H s s^{T} \mathrm{H}}{s^{T} H s}\right)^{-1}\\=\left(\mathrm{H}+\frac{y y^{T}}{y^{T} \mathrm{~s}}\right)^{-1}+\left(\mathrm{H}+\frac{y y^{T}}{y^{T} \mathrm{~s}}\right)^{-1} \frac{H s s^{T} H}{s^{T} H^{T} s-s^{T} H\left(\mathrm{H}+\frac{y y^{T}}{y^{T} \mathrm{~S}}\right)^{-1} \mathrm{Hs}}\left(\mathrm{H}+\frac{y y^{T}}{y^{T} \mathrm{~S}}\right)^{-1}\\=\left(H^{-1}-\frac{H^{-1} y y^{T} H^{-1}}{y^{T} S+y^{T} H^{-1} y}\right)+\left(H^{-1}-\frac{H^{-1} y y^{T} H^{-1}}{y^{T} S+y^{T} H^{-1} y}\right) \frac{H s s^{T} H}{s^{T} H S-s^{T} H\left(H^{-1}-\frac{H^{-1} y y^{T}-1}{y^{T} s+y^{T} H^{-1} y}\right) H s}\left(H^{-1}-\frac{H^{-1} y y^{T} H^{-1}}{y^{T} S+y^{T} H^{-1} y}\right)\\=\left(H^{-1}-\frac{H^{-1} y y^{T} H^{-1}}{y^{T} \mathrm{~s}+y^{T} H^{-1} y}\right)+\left(H^{-1}-\frac{H^{-1} y y^{T} H^{-1}}{y^{T} \mathrm{~s}+y^{T} H^{-1} y}\right) \frac{H s s^{T} H}{\frac{s^{T} y y^{T} s}{y^{T} \mathrm{~s}+y^{T} H^{-1} y}}\left(H^{-1}-\frac{H^{-1} y y^{T} H^{-1}}{y^{T} \mathrm{~S}+y^{T} H^{-1} y}\right)\\=\left(H^{-1}-\frac{H^{-1} y y^{T} H^{-1}}{y^{T} \mathrm{~s}+y^{T} H^{-1} y}\right)+\frac{H^{-1} H s s^{T} H H^{-1}}{\frac{s^{T} y y^{T} S}{y^{T} s+y^{T} H^{-1} y}}-\frac{H^{-1} H s s^{T} H}{\frac{s^{T} y y^{T} s}{}}{y^{T} \mathrm{~s}+y^{T} H^{-1} y} H^{-1} \frac{y y^{T}}{y^{T} \mathrm{~s}+y^{T} H^{-1} y} H^{-1}\\-\frac{H^{-1} y y^{T} H^{-1}}{y^{T} \mathrm{~s}+y^{T} H^{-1} y} \frac{H s s^{T} H}{\frac{s^{T} y y^{T} S}{y^{T} \mathrm{~s}+y^{T} H^{-1} y}} H^{-1}\\+H^{-1} \frac{y y^{T}}{y^{T} \mathrm{~s}+y^{T} H^{-1} y} H^{-1} \frac{H s s^{T} H}{\frac{s^{T} y y^{T} S}{y^{T} \mathrm{~s}+y^{T} H^{-1} y}} H^{-1} \frac{y y^{T}}{y^{T} \mathrm{~s}+y^{T} H^{-1} y} H^{-1}\\=\left(H^{-1}-\frac{H^{-1} y y^{T} H^{-1}}{y^{T} \mathrm{~s}+y^{T} H^{-1} y}\right)+\frac{s s^{T}\left(y^{T} \mathrm{~s}+y^{T} H^{-1} y\right)}{s^{T} y y^{T} s}-\frac{s s^{T} y y^{T} H^{-1}}{s^{T} y y^{T} S}-\frac{H^{-1} y y^{T} S S^{T}}{s^{T} y y^{T} S}\\+\frac{H^{-1} y y^{T} S s^{T} y y^{T} H^{-1}}{\left(y^{T} \mathrm{~s}+y^{T} H^{-1} y\right) s^{T} y y^{T} S}\\=\left(H^{-1}-\frac{H^{-1} y y^{T} H^{-1}}{y^{T} \mathrm{~s}+y^{T} H^{-1} y}\right)+\frac{s s^{T}\left(y^{T} \mathrm{~s}+y^{T} H^{-1} y\right)}{\left(s^{T} y\right)^{2}}-\frac{s\left(s^{T} y\right) y^{T} H^{-1}}{\left(s^{T} y\right)^{2}}-\frac{H^{-1} y\left(y^{T} s\right) s^{T}}{\left(s^{T} y\right)^{2}}\\+\frac{H^{-1} y\left(y^{T} S s^{T} y\right) y^{T} H^{-1}}{\left(y^{T} \mathrm{~s}+y^{T} H^{-1} y\right) s^{T} y y^{T} S}\\=\left(H^{-1}-\frac{H^{-1} y y^{T} H^{-1}}{y^{T} \mathrm{~s}+y^{T} H^{-1} y}\right)+\frac{s s^{T}\left(y^{T} \mathrm{~s}+y^{T} H^{-1} y\right)}{\left(s^{T} y\right)^{2}}-\frac{s y^{T} H^{-1}}{s^{T} y}-\frac{H^{-1} y s^{T}}{s^{T} y}+\frac{H^{-1} y y^{T} H^{-1}}{\left(y^{T} \mathrm{~s}+y^{T} H^{-1} y\right)}\\=H^{-1}+\frac{s s^{T}\left(y^{T} \mathrm{~s}+y^{T} H^{-1} y\right)}{\left(s^{T} y\right)^{2}}-\frac{s y^{T} H^{-1}}{s^{T} y}-\frac{H^{-1} y s^{T}}{s^{T} y}\\=H^{-1}+\frac{s s^{T} y^{T} \mathrm{~s}}{\left(s^{T} y\right)^{2}}+\frac{s s^{T} y^{T} H^{-1} y}{\left(s^{T} y\right)^{2}}-\frac{s y^{T} H^{-1}}{s^{T} y}-\frac{H^{-1} y s^{T}}{s^{T} y}\\=H^{-1}\left(I-\frac{y s^{T}}{s^{T} y}\right)-\frac{s y^{T} H^{-1}}{s^{T} y}\left(I-\frac{y s^{T}}{s^{T} y}\right)+\frac{s s^{T}}{s^{T} y}\\=\left(I-\frac{s y^{T}}{s^{T} y}\right) H^{-1}\left(I-\frac{y s^{T}}{s^{T} y}\right)+\frac{s s^{T}}{s^{T} y}\end{array}</script><h3 id="2-4-L-BFGS算法"><a href="#2-4-L-BFGS算法" class="headerlink" title="2.4 L-BFGS算法"></a>2.4 L-BFGS算法</h3><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211023235811630.png" alt="image-20211023235811630"></p><script type="math/tex; mode=display">\begin{array}{c}H_{k+1}=V_{k}^{T} H_{k} V_{k}+\rho_{k} s_{k} s_{k}^{T} \\\end{array}</script><p>给定初始矩阵$H_0=I$，利用上式，可得：</p><script type="math/tex; mode=display">\begin{aligned}H_{1}&=V_{0}^{T} H_{0} V_{0}+\rho_{0} s_{0} s_{0}^{T}\\H_{2} &=V_{1}^{T} H_{1} V_{1}+\rho_{1} s_{1} s_{1}^{T} \\&=V_{1}^{T}\left(V_{0}^{T} H_{0} V_{0}+\rho_{0} s_{0} s_{0}^{T}\right) V_{1}+\rho_{1} s_{1} s_{1}^{T} \\&\left.=V_{1}^{T} V_{0}^{T} H_{0} V_{0} V_{1}+V_{1}^{T} \rho_{0} s_{0} s_{0}^{T}\right) V_{1}+\rho_{1} s_{1} s_{1}^{T} \\& \\\quad H_{k+1} &=\left(V_{k}^{T} V_{k-1}^{T} \ldots V_{1}^{T} V_{0}^{T}\right) H_{0}\left(V_{0} V_{1} \ldots V_{k-1} V_{k}\right) \\&+\left(V_{k}^{T} V_{k-1}^{T} \ldots V_{1}^{T}\right) \rho_{1} s_{1} s_{1}^{T}\left(V_{1} \ldots V_{k-1} V_{k}\right) \\&+\ldots \\&+\left(V_{k}^{T}\right) \rho_{k-1} s_{k-1} s_{k-1}^{T}\left(V_{k}\right) \\&+\rho_{k} s_{k} s_{k}^{T}\end{aligned}</script><p>只保留最近的m步后，上式的迭代公式变为：</p><script type="math/tex; mode=display">\begin{aligned}H_{k+1} &=\left(V_{k}^{T} V_{k-1}^{T} \ldots V_{k-m}^{T}\right) H_{0}\left(V_{k-m} \ldots V_{k-1} V_{k}\right) \\&+\left(V_{k}^{T} V_{k-1}^{T} \ldots V_{k-m+1}^{T}\right) \rho_{k-m} s_{k-m} s_{k-m}^{T}\left(V_{k-m+1} \ldots V_{k-1} V_{k}\right) \\+& \ldots \\&+\left(V_{k}^{T}\right) \rho_{k-1} s_{k-1} s_{k-1}^{T}\left(V_{k}\right) \\&+\rho_{k} s_{k} s_{k}^{T}\end{aligned}</script><p>所求方向为：</p><script type="math/tex; mode=display">\begin{aligned}H_{k} \nabla f &=\left(V_{K-1}^{T} V_{K-2}^{T} \ldots V_{K-m}^{T}\right) H_{0}\left(V_{K-m} V_{K-m+1} \ldots V_{K-1}\right) \nabla f \\&+\left(V_{K-1}^{T} \ldots V_{K-m+1}^{T}\right)\rho_{k-m} s_{k-m} s_{k-m}^T(V_{k-m+1}\dots V_{k-1}V_{k}) \nabla f\\&+\ldots \\&+V_{k-1} \rho_{k-1} s_{k-1}s_{k-1}^TV_k\nabla f \\&+\rho_{k} s_{k}s_{k}^T\nabla f\end{aligned}</script><p>Two-Loop 算法：</p><script type="math/tex; mode=display">\begin{array}{l}q_{k} \leftarrow \nabla f_{k} \\\text { for } i=k-1 \text { to } k-m \text { do } \\\quad \alpha_{i}=\rho_{i} s_{i}^{T} q_{i+1} \\q_{i}=q_{i+1}-\alpha_{i} y_{i} \\\text { end for } \\r_{k-m-1}=H_{0} q_{k-m} \\\text { for } i=k-m, k-m+1 \text { to } k-1 \text { do } \\\quad \beta_{i}=\rho_{i} y_{i}^{T} r_{i-1} \\r_{i}=r_{i-1}+s_{i} \alpha_{i}-\beta_{i} \\\text { end for } \\\text { End, The result is } H_{k+1} \nabla f=r\end{array}</script><p>Two-Loop算法解析—-第一个循环：</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211023235846434.png" alt="image-20211023235846434"></p><p>重写公式：</p><script type="math/tex; mode=display">\begin{aligned}H_{k} \nabla f &=\left(V_{K-1}^{T} V_{K-2}^{T} \ldots V_{K-m}^{T}\right) H_{0}\left(V_{K-m} V_{K-m+1} \ldots V_{K-1}\right) \nabla f \\&+\left(V_{K-1}^{T} \ldots V_{K-m+1}^{T}\right) s_{k-m} \alpha_{k-m} \\&+\ldots \\&+V_{k-1} s_{k-1} \alpha_{k-2} \\&+s_{k-1} \alpha_{k-1}\end{aligned}</script><p>Two-Loop算法解析—-第二个循环：</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211023235910775.png" alt="image-20211023235910775"></p><p>初始:</p><script type="math/tex; mode=display">r_{k-\mathrm{m}}=V_{k-\mathrm{m}} H_{0} V_{k-\mathrm{m}} V_{k-\mathrm{m}+1} \ldots V_{k-1} \nabla \mathrm{f}+\mathrm{s}_{k-\mathrm{m}} \alpha_{k-m}</script><p>得：</p><script type="math/tex; mode=display">\begin{aligned}r_{k-\mathrm{m}+\mathrm{i}} &=V_{k-\mathrm{m}+\mathrm{i}} \ldots V_{k-\mathrm{m}} H_{0} V_{k-\mathrm{m}} \ldots V_{k-\mathrm{m}+\mathrm{i}} \nabla \mathrm{f} \\&+\left(V_{k-\mathrm{m}+\mathrm{i}} \ldots V_{k-\mathrm{m}+1}\right) \mathrm{s}_{k-\mathrm{m}} \alpha_{k-m} \\&+\left(V_{k-\mathrm{m}+\mathrm{i}} \ldots V_{k-\mathrm{m}+2}\right) \mathrm{s}_{k-\mathrm{m}+1} \alpha_{k-m+1}\\&+\dots\\&s_{k-m+1}\alpha_{k-m+i}\end{aligned}</script><p>$r_{k-1}$即是所求的搜索方向d。</p><p>使用LBFGS求解逻辑回归模型代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score<br><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-comment"># 进一步处理数据集和测试集，将输入和输出分割</span><br>train.columns=<span class="hljs-built_in">list</span>([<span class="hljs-string">&#x27;x1&#x27;</span>,<span class="hljs-string">&#x27;x2&#x27;</span>,<span class="hljs-string">&#x27;y&#x27;</span>])<br>test.columns=<span class="hljs-built_in">list</span>([<span class="hljs-string">&#x27;x1&#x27;</span>,<span class="hljs-string">&#x27;x2&#x27;</span>,<span class="hljs-string">&#x27;y&#x27;</span>])<br>X_train = np.asarray(train.get([<span class="hljs-string">&#x27;x1&#x27;</span>, <span class="hljs-string">&#x27;x2&#x27;</span>]))<br>y_train = np.asarray(train.get(<span class="hljs-string">&#x27;y&#x27;</span>))<br>X_test = np.asarray(test.get([<span class="hljs-string">&#x27;x1&#x27;</span>, <span class="hljs-string">&#x27;x2&#x27;</span>]))<br>y_test = np.asarray(test.get(<span class="hljs-string">&#x27;y&#x27;</span>))<br><span class="hljs-comment"># 使用 sklearn 的 LogisticRegression 作为模型</span><br><span class="hljs-comment"># 其中有 penalty，solver，multi_class 几个比较重要的参数，不同的参数有不同的准确率</span><br>model = LogisticRegression(solver=<span class="hljs-string">&#x27;newton-cg&#x27;</span>)<br><span class="hljs-comment"># newton-cg sag lbfgs liblinear</span><br><br><br><span class="hljs-comment"># 对数据进行标准化</span><br>ss = StandardScaler()<br>X_train = ss.fit_transform(X_train) <br>X_test = ss.fit_transform(X_test)<br><span class="hljs-comment"># 拟合</span><br>model.fit(X_train, y_train)<br><br><span class="hljs-comment"># 预测测试集</span><br>predictions = model.predict(X_test)<br><br><span class="hljs-comment"># 打印准确率</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;测试集准确率：&#x27;</span>, accuracy_score(y_test, predictions))<br><br>weights = np.column_stack((model.intercept_, model.coef_)).transpose()<br><span class="hljs-comment">#print(weights)</span><br></code></pre></td></tr></table></figure><h2 id="三、绘制ROC曲线和PR曲线"><a href="#三、绘制ROC曲线和PR曲线" class="headerlink" title="三、绘制ROC曲线和PR曲线"></a>三、绘制ROC曲线和PR曲线</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs markdown">该部分出现的英语缩写：<br>TP: True Positive<br>FP: False Positive<br>FN: False Negative<br>TN: True Negative<br>P: Precision<br>R: Recall<br>TPR: True Positive Rate<br>FPR: False Positive Rate<br></code></pre></td></tr></table></figure><h3 id="3-1-ROC曲线"><a href="#3-1-ROC曲线" class="headerlink" title="3.1 ROC曲线"></a>3.1 ROC曲线</h3><h4 id="3-1-1介绍"><a href="#3-1-1介绍" class="headerlink" title="3.1.1介绍"></a>3.1.1介绍</h4><p>ROC全称是“受试者工作特征”(Receiver Operating Characteristic)曲线，它源于“二战”中用于敌机检测的雷达信号分析技术，二十世纪六七十年代开始被用于一些心理学、医学检测应用中，此后被引入机器学习领域，用来评判分类、检测结果的好坏。因此，ROC曲线是非常重要和常见的统计分析方法。</p><p>为了绘制ROC曲线，我们需要计算出两个重要量的值（</p><p><strong>TPR</strong>、<strong>FPR</strong>），分别以它们为横、纵坐标作图。其中的TP、FP、TN、FN来自于<strong>混淆矩阵</strong>，且TP+FP+TN+FN=样本总数。</p><script type="math/tex; mode=display">TPR=\frac{TP}{TP+FN}</script><script type="math/tex; mode=display">FPR=\frac{FP}{FP + TN}</script><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211023004557339.png" alt="image-20211023004557339"></p><h4 id="3-1-2画图流程"><a href="#3-1-2画图流程" class="headerlink" title="3.1.2画图流程"></a>3.1.2画图流程</h4><ol><li>给定m+个正例和m-个负例，根据学习器预测结果对样例进行排序</li><li>然后把分类阈值设为最大，即把所有样例均预测为反例，此时真正例率和假正例率均为0，在坐标(0,0)处标记一个点</li><li>将分类阈值依次设为每个样例的预测值，即依次将每个样例划分为正例，设前一个标记点坐标为(x,y)，当前若为真正例，则对应标记点的坐标为$\left ( x,y+\frac{1}{m^{+}} \right )$；当前若为假正例，则对应标记点的坐标为$\left ( x+\frac{1}{m^{-}},y \right )$</li><li>最后用线段连接相邻点</li></ol><h4 id="3-1-3-AUC分析"><a href="#3-1-3-AUC分析" class="headerlink" title="3.1.3 AUC分析"></a>3.1.3 AUC分析</h4><p>ROC曲线下方的面积也有着重要意义（英语：Area under the Curve of ROC (AUC ROC)），其意义是：</p><ul><li>因为是在1x1的方格里求面积，AUC必在0~1之间。</li><li>假设阈值以上是正例，以下是反例；</li><li>简单说：<strong>AUC值越大的分类器，正确率越高。</strong></li></ul><p>从AUC判断分类器（预测模型）优劣的标准：</p><ul><li>AUC = 1，是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数预测的场合，不存在完美分类器。</li><li>0.5 &lt; AUC &lt; 1，优于随机猜测。这个分类器（模型）妥善设置阈值的话，能有预测价值。</li><li>AUC = 0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。</li><li>AUC &lt; 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。</li></ul><p>假设ROC曲线由为{ ( x1,y1 ),⋯,( xN′,yN′ ) }的点按需连接而成且有x~1~=0,x~N’~=1，则AUC可估算为：</p><script type="math/tex; mode=display">AUC=\frac{1}{2} \sum_{j=1}^{N{}'-1} \left ( x_{j+1}-x_{j}  \right ) \left ( y_{j+1}+y_{j}  \right )</script><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211023143342594.png" alt="image-20211023143342594"></p><p>如图即为使用本次作业所提供数据绘制的ROC曲线。由于测试样例有限，所以仅能获得有限个（真正例率，假正例率）坐标对，无法产生光滑的ROC曲线；由此计算得到的AUC的值为0.9648，可以得知该模型的性能较优。</p><p>完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">draw_roc</span>(<span class="hljs-params">confidence_scores, data_labels</span>):</span><br>    <span class="hljs-comment">#真正率，假正率</span><br>    fpr, tpr, thresholds = roc_curve(data_labels, confidence_scores)<br>    plt.figure()<br>    plt.grid()<br>    plt.title(<span class="hljs-string">&#x27;ROC Curve&#x27;</span>)<br>    plt.xlabel(<span class="hljs-string">&#x27;FPR&#x27;</span>)<br>    plt.ylabel(<span class="hljs-string">&#x27;TPR&#x27;</span>)<br> <br>    <span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> auc<br>    auc=auc(fpr, tpr) <span class="hljs-comment">#AUC计算</span><br>    plt.plot(fpr,tpr,<span class="hljs-string">&#x27;k--&#x27;</span>, label = <span class="hljs-string">&#x27;roc_curve(AUC=%0.4f)&#x27;</span> % auc)<br>    plt.legend()<br>    plt.show()<br></code></pre></td></tr></table></figure><h3 id="3-2-PR曲线"><a href="#3-2-PR曲线" class="headerlink" title="3.2 PR曲线"></a>3.2 PR曲线</h3><h4 id="3-2-1介绍"><a href="#3-2-1介绍" class="headerlink" title="3.2.1介绍"></a>3.2.1介绍</h4><p>PR曲线全称为查准率-查全率曲线，查准率P与查全率R分别定义为：</p><script type="math/tex; mode=display">P=\frac{TP}{TP+FP}，R=\frac{TP}{TP+FN}</script><p>查准率和查全率是一对矛盾的度量。一般来说，查准率高时，查全率往往偏低；而查全率高时，查准率往往偏低。</p><h4 id="3-2-2画图流程"><a href="#3-2-2画图流程" class="headerlink" title="3.2.2画图流程"></a>3.2.2画图流程</h4><p>绘制PR曲线的流程与ROC曲线类似，我们需要根据学习器的预测结果按正例可能性大小对样例进行排序，再逐个样本的选择阈值，在该样本之前的都属于正例，该样本之后的都属于负例。每一个样本作为划分阈值时，都可以计算对应的precision和recall，那么就可以以此绘制曲线。</p><h4 id="3-2-3-AP分析"><a href="#3-2-3-AP分析" class="headerlink" title="3.2.3 AP分析"></a>3.2.3 AP分析</h4><p>其中平衡点是曲线上“查准率=查全率”时的取值，可用于度量PR曲线有交叉的分类器性能高低。与AUC类似，PR曲线下方面积也有重要意义。PR曲线下的面积称之为AP(Average Precision)，通常来说一个越好的分类器，AP值越高。</p><p>对于连续的PR曲线，有：</p><script type="math/tex; mode=display">AP=\int_{0}^{1} p\left ( r \right ) \mathrm{d}r</script><p>但由于曲线可能出现不可导的部分，故我们常常求其近似值：</p><script type="math/tex; mode=display">p_{\text {interp }}(r)=\max _{\tilde{r} \geq r} p(\tilde{r})</script><p>对于离散的PR曲线，有：</p><script type="math/tex; mode=display">\mathrm{AP}=\sum_{k=1}^{n} p(k) \Delta r(k)</script><p>另外PR曲线平衡点更用常用的是F1度量：</p><script type="math/tex; mode=display">F 1=\frac{2 \times P \times R}{P+R}=\frac{2 \times T P}{\text { 样例总数 }+T P-T N}</script><p>比F1度量更一般的形式是F~β~：</p><script type="math/tex; mode=display">F_{\beta}=\frac{\left(1+\beta^{2}\right) \times P \times R}{\left(\beta^{2} \times P\right)+R}</script><ul><li>β=1：标准F1</li><li>β&gt;1：偏重查全率（逃犯信息检索）</li><li>β&lt;1：偏重查准率（商品推荐系统）</li></ul><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211023150341520.png" alt="image-20211023150341520"></p><p>如图即为使用本次作业所提供数据绘制的PR曲线。在现实任务中，PR曲线是非单调、不平滑的，在很多局部有上下波动；由此计算得到的AP的值为0.9751，可以得知该模型的性能较优。</p><p>完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">draw_pr</span>(<span class="hljs-params">confidence_scores, data_labels</span>):</span><br>    plt.figure()<br>    plt.title(<span class="hljs-string">&#x27;PR Curve&#x27;</span>)<br>    plt.xlabel(<span class="hljs-string">&#x27;Recall&#x27;</span>)<br>    plt.ylabel(<span class="hljs-string">&#x27;Precision&#x27;</span>)<br>    plt.grid()<br> <br>    <span class="hljs-comment">#精确率，召回率，阈值</span><br>    precision,recall,thresholds = precision_recall_curve(data_labels,confidence_scores)<br> <br>    <span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> average_precision_score<br>    AP = average_precision_score(data_labels, confidence_scores) <span class="hljs-comment"># 计算AP</span><br>    plt.plot(recall, precision,<span class="hljs-string">&#x27;k--&#x27;</span>, label = <span class="hljs-string">&#x27;pr_curve(AP=%0.4f)&#x27;</span> % AP)<br>    plt.legend()<br>    plt.show()<br></code></pre></td></tr></table></figure><h2 id="四、总结模型训练过程中的收获"><a href="#四、总结模型训练过程中的收获" class="headerlink" title="四、总结模型训练过程中的收获"></a>四、总结模型训练过程中的收获</h2><h3 id="4-1加深了对逻辑斯蒂回归的理解"><a href="#4-1加深了对逻辑斯蒂回归的理解" class="headerlink" title="4.1加深了对逻辑斯蒂回归的理解"></a>4.1加深了对逻辑斯蒂回归的理解</h3><h4 id="4-1-1简述对模型的理解："><a href="#4-1-1简述对模型的理解：" class="headerlink" title="4.1.1简述对模型的理解："></a>4.1.1简述对模型的理解：</h4><p>因为线性回归模型产生的预测值是一系列实值。为了使得输出的预测结果变成分类所需的0和1，我们需要在线性回归的基础式子外再套一个函数将其输出变成0和1，又要求该函数单调可微，所以我们引入logistic函数，将输出的预测结果成功转为概率值。这样，逻辑斯蒂回归模型被成功应用于解决分类模型。</p><h4 id="4-1-2关于算法的择优："><a href="#4-1-2关于算法的择优：" class="headerlink" title="4.1.2关于算法的择优："></a>4.1.2关于算法的择优：</h4><p>在代码实现过程中，我们最开始使用的是梯度下降法，但是迭代速度较慢，拟合效果不是很好；之后我们选择了牛顿法，但是因为计算海森矩阵的复杂度太高，我们选择用一种拟牛顿法——‘L-BFGS’来逼近海森矩阵，最终达到了我们理想的效果。</p><p>梯度下降法和牛顿法/拟牛顿法相比，两者都是迭代求解，不过梯度下降法是梯度求解，而牛顿法/拟牛顿法是用二阶的海森矩阵的逆矩阵或伪逆矩阵求解。相对而言，使用牛顿法/拟牛顿法收敛更快。</p><h3 id="4-2实现了代码技能的提升"><a href="#4-2实现了代码技能的提升" class="headerlink" title="4.2实现了代码技能的提升"></a>4.2实现了代码技能的提升</h3><p>在代码实现过程中，我们调用了机器学习工具包sklearn中的重要函数——LogisticRegression函数，熟悉了它的常用参数及意义，下面以表格形式列出我们在此次模型训练中使用到的参数。</p><div class="table-container"><table><thead><tr><th>参数</th><th>意义</th><th>备注</th></tr></thead><tbody><tr><td>penalty</td><td>str类型，可选项有{‘L1’,‘L2’}，用来确定惩罚项的规范。‘newton-cg’，‘sag’和‘lbfgs’仅支持‘L2’惩罚项。</td><td>该参数是为了添加惩罚项，避免过拟合，用以提高函数的泛化能力。我们在本次模型训练中使用的是‘L2’。</td></tr><tr><td>solver</td><td>可选的优化算法有{‘newton-cg’，‘lbfgs’,‘liblinear’,‘sag’}</td><td>小数据集中，liblinear是一个好选择，sag和saga对大数据更快； 多分类问题中，除了liblinear其它四种算法都可以使用；newton-cg，lbfgs和sag仅能使用L2惩罚项；  我们经过对比，选择的算法是lbfgs。</td></tr><tr><td>multi_class</td><td>str类型，可选参数有{‘ovr’，‘multinomial’}  如果是二元分类问题则两个选项一样，如果是多元分类则ovr将进行多次二分类，分别为一类别和剩余其它所有类别;  multinomial则分别进行两两分类，需要T(T-1)/2次分类。</td><td>在多分类中，ovr快，精度低; multinomial慢，精度高。</td></tr></tbody></table></div><h3 id="4-3提高了公式推导和文章排版能力"><a href="#4-3提高了公式推导和文章排版能力" class="headerlink" title="4.3提高了公式推导和文章排版能力"></a>4.3提高了公式推导和文章排版能力</h3><p>报告中的所有公式，我们都脚踏实地，一步步手动推导，并学习使用latex将其手动输入并排版。在这个过程中，我们对算法中公式的来源更加清楚，对其原理理解更加深透。这提高了我们的公式推导能力和文章排版能力。</p><h3 id="4-4锻炼了小组合作精神，提高了小组合作能力"><a href="#4-4锻炼了小组合作精神，提高了小组合作能力" class="headerlink" title="4.4锻炼了小组合作精神，提高了小组合作能力"></a>4.4锻炼了小组合作精神，提高了小组合作能力</h3><p>在正式写报告之前，我们对本次作业任务以及对逻辑斯蒂回归模型的理解进行了讨论；然后为了加深彼此对知识的掌握程度，每个人都对代码进行了独立编写，在实现的过程中探讨互助；最后，我们根据彼此的优势项对任务进行了分工合作，齐心协力创作出了这份尽可能完善的报告。</p>]]></content>
    
    
    
    <tags>
      
      <tag>course</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>typora+picgo+gitee高效写作</title>
    <link href="/2021/10/20/picgo/"/>
    <url>/2021/10/20/picgo/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>解决hexo博客图片上传问题</p><span id="more"></span><p>Markdown 是一种轻量级标记语言，它允许人们使用易读易写的纯文本格式编写文档。</p><p>Markdown 语言在 2004 由约翰·格鲁伯（英语：John Gruber）创建。</p><p>Markdown 编写的文档可以导出 HTML 、Word、图像、PDF、Epub 等多种格式的文档。</p><p>Markdown 编写的文档后缀为 <strong>.md</strong>, <strong>.markdown</strong>。</p><hr><p><a href="https://typora.io/">Typora</a> gives you a seamless experience as both a reader and a writer. </p><p>It removes the preview window, mode switcher, syntax symbols of </p><p>markdown source code, and all other unnecessary distractions. Instead, </p><p>it provides a real live preview feature to help you concentrate on the content itself.</p><hr><p>懒得解释了，总之markdown真好用，typora yyds！</p><p>但是一直以来我都觉得写blog是个很痛苦的过程，除了因为我懒，就是每次都得一张一张上传图片，所以迟迟未更新。</p><p>直到我使用了typora+picgo+gitee这一组合。</p><hr><p><a href="https://github.com/PicGo/">Picgo</a>的GitHub页面上包含了<a href="https://github.com/PicGo/PicGo-Core">core</a>，<a href="https://github.com/PicGo/vs-picgo">vscode的扩展版</a>和各种各样<a href="https://github.com/PicGo/Awesome-PicGo">awesome的插件</a>。</p><p>不过我选择的是编译好的<a href="https://github.com/Molunerfinn/PicGo/releases">发行版程序</a>。</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211020213735806.png" alt="image-20211020213735806"></p><p>安装过程中没什么需要注意的，总之就是一路点下去。</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211020213944319.png" alt="image-20211020213944319"></p><p>打开后应该是一个这样的界面，我首先排除GitHub和Imgur了，毕竟不能保证每一位用户都科学上网。</p><p>接下来我根据网上的教程按顺序尝试了SMMS，七牛云，又拍云···</p><p>第一个配置好上传不了，后面俩配置得也很完美，什么域名绑定，cdn加速···结果本地图片显示，上传到博客上就显示不了了，原因不得而知。</p><p>最后我选择了免费，访问速度还快的Gitee。</p><p>首先在插件设置中搜索并下载：</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211020215002022.png" alt="image-20211020215002022"></p><p>如果没有Gitee账户，先进行<a href="https://gitee.com/">注册</a>。</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211020220550100.png" alt="image-20211020220550100"></p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211020220758789.png" alt="image-20211020220758789"></p><p>记好我红框出来的内容，然后到个人设置里生成私人令牌。</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211020221019061.png" alt="image-20211020221019061"></p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211020221038756.png" alt="image-20211020221038756"></p><p>描述由你决定，只需要选择图中两项，然后把令牌<strong>复制</strong>好备用。（关闭之后就不再明文显示了！）</p><p>最后在picgo中填好各项参数：</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211020221334908.png" alt="image-20211020221334908"></p><p>把gitee设置为默认图床之后至此已经可以使用Ctrl+C复制图片，Ctrl＋Shift＋P上传至图床，Ctrl+V粘贴到typora中。</p><p>为了简化操作，可以在typora的偏好设置中进行如下修改：</p><p><img src="https://gitee.com/sysu_20354027/pic/raw/master/img/image-20211020222002514.png" alt="image-20211020222002514"></p><p>（顺便验证一下成功与否）</p><p>大功告成，从此写作可以完全抛弃图片的苦恼，一键上传，so easy~</p><p>另外推荐两个我常常使用的图片<a href="https://bigjpg.com/">无损放大</a>和<a href="https://www.picdiet.com/zh-cn">无损压缩</a>的网站。</p><p>虽然自动化上传之后我就不会去调整图片的大小了······</p>]]></content>
    
    
    
    <tags>
      
      <tag>course</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Texlive+VScode</title>
    <link href="/2021/10/18/latex/"/>
    <url>/2021/10/18/latex/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>搭建Latex环境：Texlive+VScode 相关记录</p><span id="more"></span><h2 id="1-安装-Texlive"><a href="#1-安装-Texlive" class="headerlink" title="1.安装 Texlive"></a>1.安装 Texlive</h2><p>鉴于我校没有（我知道的）可用开源软件镜像站，所以在到清华大学开源软件镜像站的<a href="https://mirrors.tuna.tsinghua.edu.cn/CTAN/systems/texlive/Images/">texlive</a>页下载。</p><p><a href="https://imgtu.com/i/5UYncq"><img src="https://z3.ax1x.com/2021/10/18/5UYncq.png" alt="5UYncq.png"></a></p><p>可能由于更新导致你打开之后的页面与上面的内容不一致，总之下载最新的texlive<em>*</em>.iso，虽然很大但各种宏包齐全，用起来能省去各种麻烦。</p><p>在带宽扩容之后的校园网加持下，不用几年就能下载完这个iso文件了。</p><p>如果你是windows 7甚至xp用户，我建议你把iso文件解压然后进行后续操作。</p><p>如果是windows 10/11，系统自带虚拟光驱，直接双击进入即可。</p><p>（ linux/macOS 我不了解，省略）效果如图：</p><p><a href="https://imgtu.com/i/5UUe9H"><img src="https://z3.ax1x.com/2021/10/18/5UUe9H.png" alt="5UUe9H.png"></a></p><p>双击或者右键以管理员身份运行install-tl-advanced.bat，可以点进<strong>Advanced</strong>进入高级安装，点击<strong>Customize</strong>来取消你不需要安装的宏包，比如非中英的语言包，这里我只修改了安装目录，最后开始漫长的等待。</p><p><a href="https://imgtu.com/i/5UaCGQ"><img src="https://z3.ax1x.com/2021/10/18/5UaCGQ.png" alt="5UaCGQ.png"></a></p><p>（安装TeXworks前端也可以取消掉，毕竟都打算用vscode了，加上前面说的语言包之类的，可以省个1G左右，我想着留条后路就啥都没改，也不缺这点空间）</p><p>（在我的电脑上一共安装了57 min 56 s，教程都快写完了，还没有装好）</p><h2 id="2-安装-VSCode"><a href="#2-安装-VSCode" class="headerlink" title="2. 安装 VSCode"></a>2. 安装 VSCode</h2><p>到<a href="https://code.visualstudio.com/Download">官网</a>根据你的系统选择下载安装即可，这部分应该大多数人都安装过了，没什么需要注意的。</p><p><a href="https://imgtu.com/i/5Ud4hD"><img src="https://z3.ax1x.com/2021/10/18/5Ud4hD.png" alt="5Ud4hD.png"></a></p><p>安装完成之后可以在应用商店挑选各种提高使用体验的扩展，跟本文相关的主要是<strong>Latex Workshop</strong>。</p><p><a href="https://imgtu.com/i/5U0KJS"><img src="https://z3.ax1x.com/2021/10/18/5U0KJS.png" alt="5U0KJS.png"></a></p><p>安装完成之后，可以创建或者打开一个tex文件，此时代码已经被高亮显示了。</p><p><a href="https://imgtu.com/i/5U560K"><img src="https://z3.ax1x.com/2021/10/18/5U560K.png" alt="5U560K.png"></a></p><p>按下快捷键Ctrl+Alt+B（build latex project），顺利生成，效果不错。</p><p><a href="https://imgtu.com/i/5U5xcn"><img src="https://z3.ax1x.com/2021/10/18/5U5xcn.png" alt="5U5xcn.png"></a></p><h2 id="3-配置-VSCode-的-插件"><a href="#3-配置-VSCode-的-插件" class="headerlink" title="3. 配置 VSCode 的 插件"></a>3. 配置 VSCode 的 插件</h2><p>按下F1或者Ctrl＋shift＋P，输入setjson，选择第三个（如图所示）。</p><p><a href="https://imgtu.com/i/5Uo9xA"><img src="https://z3.ax1x.com/2021/10/18/5Uo9xA.png" alt="5Uo9xA.png"></a></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br></pre></td><td class="code"><pre><code class="hljs json">&quot;latex-workshop.view.pdf.viewer&quot;: &quot;tab&quot;,<br>    &quot;latex-workshop.latex.recipes&quot;: [<br>        &#123;<br>            <span class="hljs-attr">&quot;name&quot;</span>: <span class="hljs-string">&quot;xelatex ➞ bibtex ➞ xelatex × 2&quot;</span>,<br>            <span class="hljs-attr">&quot;tools&quot;</span>: [<br>                <span class="hljs-string">&quot;xelatex&quot;</span>,<br>                <span class="hljs-string">&quot;bibtex&quot;</span>,<br>                <span class="hljs-string">&quot;xelatex&quot;</span>,<br>                <span class="hljs-string">&quot;xelatex&quot;</span><br>            ]<br>        &#125;,<br>        &#123;<br>            <span class="hljs-attr">&quot;name&quot;</span>: <span class="hljs-string">&quot;latexmk 🔃&quot;</span>,<br>            <span class="hljs-attr">&quot;tools&quot;</span>: [<br>                <span class="hljs-string">&quot;latexmk&quot;</span><br>            ]<br>        &#125;,<br>        &#123;<br>            <span class="hljs-attr">&quot;name&quot;</span>: <span class="hljs-string">&quot;latexmk (latexmkrc)&quot;</span>,<br>            <span class="hljs-attr">&quot;tools&quot;</span>: [<br>                <span class="hljs-string">&quot;latexmk_rconly&quot;</span><br>            ]<br>        &#125;,<br>        &#123;<br>            <span class="hljs-attr">&quot;name&quot;</span>: <span class="hljs-string">&quot;latexmk (lualatex)&quot;</span>,<br>            <span class="hljs-attr">&quot;tools&quot;</span>: [<br>                <span class="hljs-string">&quot;lualatexmk&quot;</span><br>            ]<br>        &#125;,<br>        &#123;<br>            <span class="hljs-attr">&quot;name&quot;</span>: <span class="hljs-string">&quot;pdflatex ➞ bibtex ➞ pdflatex × 2&quot;</span>,<br>            <span class="hljs-attr">&quot;tools&quot;</span>: [<br>                <span class="hljs-string">&quot;pdflatex&quot;</span>,<br>                <span class="hljs-string">&quot;bibtex&quot;</span>,<br>                <span class="hljs-string">&quot;pdflatex&quot;</span>,<br>                <span class="hljs-string">&quot;pdflatex&quot;</span><br>            ]<br>        &#125;,<br>        &#123;<br>            <span class="hljs-attr">&quot;name&quot;</span>: <span class="hljs-string">&quot;Compile Rnw files&quot;</span>,<br>            <span class="hljs-attr">&quot;tools&quot;</span>: [<br>                <span class="hljs-string">&quot;rnw2tex&quot;</span>,<br>                <span class="hljs-string">&quot;latexmk&quot;</span><br>            ]<br>        &#125;,<br>        &#123;<br>            <span class="hljs-attr">&quot;name&quot;</span>: <span class="hljs-string">&quot;Compile Jnw files&quot;</span>,<br>            <span class="hljs-attr">&quot;tools&quot;</span>: [<br>                <span class="hljs-string">&quot;jnw2tex&quot;</span>,<br>                <span class="hljs-string">&quot;latexmk&quot;</span><br>            ]<br>        &#125;,<br>        &#123;<br>            <span class="hljs-attr">&quot;name&quot;</span>: <span class="hljs-string">&quot;tectonic&quot;</span>,<br>            <span class="hljs-attr">&quot;tools&quot;</span>: [<br>                <span class="hljs-string">&quot;tectonic&quot;</span><br>            ]<br>        &#125;<br>    ],<br>    &quot;latex-workshop.latex.tools&quot;: [<br>        &#123;<br>            <span class="hljs-attr">&quot;name&quot;</span>: <span class="hljs-string">&quot;xelatex&quot;</span>,<br>            <span class="hljs-attr">&quot;command&quot;</span>: <span class="hljs-string">&quot;xelatex&quot;</span>,<br>            <span class="hljs-attr">&quot;args&quot;</span>: [<br>                <span class="hljs-string">&quot;-synctex=1&quot;</span>,<br>                <span class="hljs-string">&quot;-interaction=nonstopmode&quot;</span>,<br>                <span class="hljs-string">&quot;-file-line-error&quot;</span>,<br>                <span class="hljs-string">&quot;--shell-escape&quot;</span>,<br>                <span class="hljs-string">&quot;%DOC%&quot;</span><br>            ],<br>            <span class="hljs-attr">&quot;env&quot;</span>: &#123;&#125;<br>        &#125;,<br>        &#123;<br>            <span class="hljs-attr">&quot;name&quot;</span>: <span class="hljs-string">&quot;latexmk&quot;</span>,<br>            <span class="hljs-attr">&quot;command&quot;</span>: <span class="hljs-string">&quot;latexmk&quot;</span>,<br>            <span class="hljs-attr">&quot;args&quot;</span>: [<br>                <span class="hljs-string">&quot;-synctex=1&quot;</span>,<br>                <span class="hljs-string">&quot;-interaction=nonstopmode&quot;</span>,<br>                <span class="hljs-string">&quot;-file-line-error&quot;</span>,<br>                <span class="hljs-string">&quot;-pdf&quot;</span>,<br>                <span class="hljs-string">&quot;-outdir=%OUTDIR%&quot;</span>,<br>                <span class="hljs-string">&quot;%DOC%&quot;</span><br>            ],<br>            <span class="hljs-attr">&quot;env&quot;</span>: &#123;&#125;<br>        &#125;,<br>        &#123;<br>            <span class="hljs-attr">&quot;name&quot;</span>: <span class="hljs-string">&quot;lualatexmk&quot;</span>,<br>            <span class="hljs-attr">&quot;command&quot;</span>: <span class="hljs-string">&quot;latexmk&quot;</span>,<br>            <span class="hljs-attr">&quot;args&quot;</span>: [<br>                <span class="hljs-string">&quot;-synctex=1&quot;</span>,<br>                <span class="hljs-string">&quot;-interaction=nonstopmode&quot;</span>,<br>                <span class="hljs-string">&quot;-file-line-error&quot;</span>,<br>                <span class="hljs-string">&quot;-lualatex&quot;</span>,<br>                <span class="hljs-string">&quot;-outdir=%OUTDIR%&quot;</span>,<br>                <span class="hljs-string">&quot;%DOC%&quot;</span><br>            ],<br>            <span class="hljs-attr">&quot;env&quot;</span>: &#123;&#125;<br>        &#125;,<br>        &#123;<br>            <span class="hljs-attr">&quot;name&quot;</span>: <span class="hljs-string">&quot;latexmk_rconly&quot;</span>,<br>            <span class="hljs-attr">&quot;command&quot;</span>: <span class="hljs-string">&quot;latexmk&quot;</span>,<br>            <span class="hljs-attr">&quot;args&quot;</span>: [<br>                <span class="hljs-string">&quot;%DOC%&quot;</span><br>            ],<br>            <span class="hljs-attr">&quot;env&quot;</span>: &#123;&#125;<br>        &#125;,<br>        &#123;<br>            <span class="hljs-attr">&quot;name&quot;</span>: <span class="hljs-string">&quot;pdflatex&quot;</span>,<br>            <span class="hljs-attr">&quot;command&quot;</span>: <span class="hljs-string">&quot;pdflatex&quot;</span>,<br>            <span class="hljs-attr">&quot;args&quot;</span>: [<br>                <span class="hljs-string">&quot;-synctex=1&quot;</span>,<br>                <span class="hljs-string">&quot;-interaction=nonstopmode&quot;</span>,<br>                <span class="hljs-string">&quot;-file-line-error&quot;</span>,<br>                <span class="hljs-string">&quot;%DOC%&quot;</span><br>            ],<br>            <span class="hljs-attr">&quot;env&quot;</span>: &#123;&#125;<br>        &#125;,<br>        &#123;<br>            <span class="hljs-attr">&quot;name&quot;</span>: <span class="hljs-string">&quot;bibtex&quot;</span>,<br>            <span class="hljs-attr">&quot;command&quot;</span>: <span class="hljs-string">&quot;bibtex&quot;</span>,<br>            <span class="hljs-attr">&quot;args&quot;</span>: [<br>                <span class="hljs-string">&quot;%DOCFILE%&quot;</span><br>            ],<br>            <span class="hljs-attr">&quot;env&quot;</span>: &#123;&#125;<br>        &#125;,<br>        &#123;<br>            <span class="hljs-attr">&quot;name&quot;</span>: <span class="hljs-string">&quot;rnw2tex&quot;</span>,<br>            <span class="hljs-attr">&quot;command&quot;</span>: <span class="hljs-string">&quot;Rscript&quot;</span>,<br>            <span class="hljs-attr">&quot;args&quot;</span>: [<br>                <span class="hljs-string">&quot;-e&quot;</span>,<br>                <span class="hljs-string">&quot;knitr::opts_knit$set(concordance = TRUE); knitr::knit(&#x27;%DOCFILE_EXT%&#x27;)&quot;</span><br>            ],<br>            <span class="hljs-attr">&quot;env&quot;</span>: &#123;&#125;<br>        &#125;,<br>        &#123;<br>            <span class="hljs-attr">&quot;name&quot;</span>: <span class="hljs-string">&quot;jnw2tex&quot;</span>,<br>            <span class="hljs-attr">&quot;command&quot;</span>: <span class="hljs-string">&quot;julia&quot;</span>,<br>            <span class="hljs-attr">&quot;args&quot;</span>: [<br>                <span class="hljs-string">&quot;-e&quot;</span>,<br>                <span class="hljs-string">&quot;using Weave; weave(\&quot;%DOC_EXT%\&quot;, doctype=\&quot;tex\&quot;)&quot;</span><br>            ],<br>            <span class="hljs-attr">&quot;env&quot;</span>: &#123;&#125;<br>        &#125;,<br>        &#123;<br>            <span class="hljs-attr">&quot;name&quot;</span>: <span class="hljs-string">&quot;jnw2texmintex&quot;</span>,<br>            <span class="hljs-attr">&quot;command&quot;</span>: <span class="hljs-string">&quot;julia&quot;</span>,<br>            <span class="hljs-attr">&quot;args&quot;</span>: [<br>                <span class="hljs-string">&quot;-e&quot;</span>,<br>                <span class="hljs-string">&quot;using Weave; weave(\&quot;%DOC_EXT%\&quot;, doctype=\&quot;texminted\&quot;)&quot;</span><br>            ],<br>            <span class="hljs-attr">&quot;env&quot;</span>: &#123;&#125;<br>        &#125;,<br>        &#123;<br>            <span class="hljs-attr">&quot;name&quot;</span>: <span class="hljs-string">&quot;tectonic&quot;</span>,<br>            <span class="hljs-attr">&quot;command&quot;</span>: <span class="hljs-string">&quot;tectonic&quot;</span>,<br>            <span class="hljs-attr">&quot;args&quot;</span>: [<br>                <span class="hljs-string">&quot;--synctex&quot;</span>,<br>                <span class="hljs-string">&quot;--keep-logs&quot;</span>,<br>                <span class="hljs-string">&quot;%DOC%.tex&quot;</span><br>            ],<br>            <span class="hljs-attr">&quot;env&quot;</span>: &#123;&#125;<br>        &#125;<br>    ],<br>    &quot;latex-workshop.latex.recipe.default&quot;: &quot;first&quot;<br></code></pre></td></tr></table></figure><ul><li>Ctrl+Alt+B 是编译</li><li>Ctrl+Alt+V是编译+预览pdf</li></ul><p>我最开始写这些其实是想要把中大的foxitpdf设置成默认的pdf预览软件，不过最终效果并不好，所以作罢。</p><p>上面这些设置主要是因为默认的编译工具是 latexmk，由于不需要用到 latexmk，因此把其修改为中文环境常用的 xelatex；将 tools 中的 %DOC%替换成%DOCFILE%就可以支持编译中文路径下的文件了。</p><p>若要自动处理bib参考文献，在开头加上：</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">%!<span class="hljs-keyword">BIB </span>program = <span class="hljs-keyword">bibtex </span> <br></code></pre></td></tr></table></figure><p>若要使用pdflatex编译（纯英文文档），在开头加上：</p><figure class="highlight delphi"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs delphi">%!TEX <span class="hljs-keyword">program</span> = pdflatex<br></code></pre></td></tr></table></figure><p>还可以研究的设置有很多，什么正向搜索反向搜索之类的，有兴趣的朋友可以自行了解。</p><p><a href="https://ericp.cn/cmd">公式指导手册</a></p><p>如果中文无法显示就加上这一句：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs css">\usepackage<span class="hljs-selector-attr">[UTF8]</span>&#123;ctex&#125;<br></code></pre></td></tr></table></figure><p>Latex的相关公式及使用就不再赘述了。</p>]]></content>
    
    
    
    <tags>
      
      <tag>course</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello world!</title>
    <link href="/2021/04/25/Helloworld/"/>
    <url>/2021/04/25/Helloworld/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>My First blog:Something about Enderfga</p><span id="more"></span><h1 id="Think-twice-code-once"><a href="#Think-twice-code-once" class="headerlink" title="Think twice, code once."></a>Think twice, code once.</h1>        <div id="aplayer-JnmEHFak" class="aplayer aplayer-tag-marker" style="margin-bottom: 20px;">            <pre class="aplayer-lrc-content"></pre>        </div>        <script>          var ap = new APlayer({            element: document.getElementById("aplayer-JnmEHFak"),            narrow: false,            autoplay: true,            showlrc: false,            music: {              title: "Never Coming Back",              author: "Evan Call",              url: "https://cdn.jsdelivr.net/gh/Enderfga/Enderfga/Backup/music.mp3",              pic: "https://z3.ax1x.com/2021/04/24/cvhTxI.jpg",              lrc: ""            }          });          window.aplayers || (window.aplayers = []);          window.aplayers.push(ap);        </script><p><a href="https://imgtu.com/i/gAAR54"><img src="https://z3.ax1x.com/2021/04/30/gAAR54.jpg" alt="gAAR54.jpg"></a></p><p>自打成为一个程序猿开始，翻阅博客文章学习就成了我的日常（质量确实有够参差不齐···），CSDN、博客园、简书、知乎、GitHub等我都经常光顾，于是萌生了自己写blog的想法，苦于技术力不足一直搁置至今（现在也不怎么样哈哈哈）。在GZTime的协助下，我自己的小破站终于建成啦~希望我早日产出点技术性文章，现在只能拿来记流水账了······</p><h1 id="Enderfga？"><a href="#Enderfga？" class="headerlink" title="Enderfga？"></a>Enderfga？</h1><p>关于我的id来源其实挺傻的，在成为程序猿之前我是一名资深游戏玩家，我还清楚地记得我接触的第一款网络游戏叫植物大战僵尸OL，然后是洛克王国，卡布西游，奥奇传说···直到六年级那年，我玩了第一款我愿将其称之为“游戏”或者说是“第九艺术”的作品——Minecraft。</p><p><a href="https://imgtu.com/i/gAAhG9"><img src="https://z3.ax1x.com/2021/04/30/gAAhG9.jpg" alt="gAAhG9.jpg"></a></p><p>MC一直陪我走到今天，我对编程的兴趣基本也是萌芽于此。虽然课程里没有涉及Java，但有机会我还是会争取好好自学Java和C#的。至于MC，不管怎么更新换代，mod/红石/命令方块/欺负末影龙/种田养猪都挺吸引我的。</p><figure class="highlight mizar"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs mizar">EnderDragon—Enderman-Enderfga,Doesn&#x27;t <span class="hljs-keyword">that</span> sound cool?<br>&quot;This <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> the <span class="hljs-keyword">end</span>,this <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> even the beginning <span class="hljs-keyword">of</span><br> the <span class="hljs-keyword">end</span>,but,perhaps,the <span class="hljs-keyword">end</span> <span class="hljs-keyword">of</span> the beginning.&quot;<br>                   ——Winston Leonard Spencer Churchill<br></code></pre></td></tr></table></figure><p>这个id不知不觉间居然用了七年了，从个别网站到所有账户统一，我也不舍得换新的了（在互联网上统一貌似不是什么好习惯，还容易查到我年轻的时候留下的黑历史···）只希望外国友人看到这么沙雕的英语名不要笑我哈哈哈。（谷歌娘念的好可爱！）</p><p><a href="https://imgtu.com/i/gAm5Bq"><img src="https://z3.ax1x.com/2021/04/30/gAm5Bq.jpg" alt="gAm5Bq.jpg"></a></p><h1 id="ACGN-引きこもり"><a href="#ACGN-引きこもり" class="headerlink" title="ACGN-引きこもり"></a>ACGN-引きこもり</h1><p><a href="https://imgtu.com/i/fgsIRU"><img src="https://z3.ax1x.com/2021/08/15/fgsIRU.jpg" alt="fgsIRU.jpg"></a></p><div style="text-align:center">宿舍一角</div><h2 id="Animation"><a href="#Animation" class="headerlink" title="Animation"></a>Animation</h2><p>二次元浓度++；</p><p>很庆幸自己的童年有虹猫蓝兔七侠传，洛洛历险记，蓝猫淘气三千问，秦时明月，东方神娃······等等优秀国产作品陪伴（甚至顺便在里面学会了普通话），后来在星空卫视的《动漫先锋》栏目里入坑了日漫：犬夜叉，海贼王，钢之炼金术师，七龙珠，<strong>火影忍者</strong>，<strong>银魂</strong>······没有这几部番，肯定也没有现在时而中二热血，时而沙雕废柴的我。<br>至于B站的入站时间是2015-07-10 17:20:10（这是通过答题的时间，终于不是游客了！）（用时间戳查的，我怎么可能记得这种东西）</p><p><a href="https://imgtu.com/i/gZkFVf"><img src="https://z3.ax1x.com/2021/05/01/gZkFVf.jpg" alt="gZkFVf.jpg"></a></p><p>那个时候特地去看了某科学的超电磁炮，lovelive什么的，四舍五入也算是二刺猿入门了（吧？）</p><p>从零开始的异世界生活，一拳超人，灵能百分百，小林家的龙女仆，干物妹小埋，刺客伍六七，<strong>紫罗兰永恒花园</strong>······这些年看番的频率虽然少了，但那种每周等更新看番的热情已经刻进DNA了。每顿饭的时候刷刷B站的剪辑还能感慨一下“爷青回”，泪目一会。</p><p><a href="https://imgtu.com/i/gAYknO"><img src="https://z3.ax1x.com/2021/04/30/gAYknO.jpg" alt="gAYknO.jpg"></a></p><p>另外我的头像其实是桂小太郎，不过因为版权意识的加深，可能有机会还是得重画一个。</p><p><a href="https://imgtu.com/i/fgyay4"><img src="https://z3.ax1x.com/2021/08/15/fgyay4.jpg" alt="fgyay4.jpg"></a></p><p>更新：桂先生成功升级了，参考了尼尔机械纪元中9S的装扮，现在科技感满满！</p><h2 id="Comic"><a href="#Comic" class="headerlink" title="Comic"></a>Comic</h2><p>回忆了一下，我好像不怎么看漫画。起初看漫画是因为死火海更新太慢了，后来在快看上看了几部，记得名字的有阎王不高兴，哑舍，快把我哥带走，<strong>蝉女</strong>。好看是好看，感觉有点像折中选择，不如动画灵动也不如小说全面。</p><p><a href="https://imgtu.com/i/gANmlt"><img src="https://z3.ax1x.com/2021/04/30/gANmlt.jpg" alt="gANmlt.jpg"></a></p><p>已经完全想不起来蝉女讲什么了，但画风针不戳。最近听了《<a href="https://bilibili.com/video/BV1WX4y1G7ok">鉴情师</a>》这首歌才想起来的。</p><h2 id="Game"><a href="#Game" class="headerlink" title="Game"></a>Game</h2><p>说到这个我可就不困了（zzzzz….）</p><p><a href="https://imgtu.com/i/gV9KCn"><img src="https://z3.ax1x.com/2021/05/01/gV9KCn.jpg" alt="gV9KCn.jpg"></a></p><div style="text-align:center"><iframe width="100%" height="500" src="https://player.bilibili.com/player.html?aid=5868975&bvid=BV1Ts411k73E&cid=9531080&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe></iframe></div><p><a href="https://imgtu.com/i/gV9eEQ"><img src="https://z3.ax1x.com/2021/05/01/gV9eEQ.jpg" alt="gV9eEQ.jpg"></a></p><div style="text-align:center"><iframe width="100%" height="500" src="https://player.bilibili.com/player.html?aid=460307938&bvid=BV1t5411w723&cid=331362469&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe></iframe></div><p><a href="https://imgtu.com/i/gVyodA"><img src="https://z3.ax1x.com/2021/05/01/gVyodA.jpg" alt="gVyodA.jpg"></a></p><div style="text-align:center"><iframe width="100%" height="500" src="https://player.bilibili.com/player.html?aid=12069119&bvid=BV1Wx411q7zb&cid=19911067&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe></div><p>我记得当年进b站答题的时候有一道题让我选出“老头滚动条”的原名，百度的过程中我了解到“上古卷轴5”(老滚)mod的丰富程度堪比MC，中世纪剑与魔法的世界也让我着迷，于是我毅然决然地成为了一名抓根宝（龙裔），但通关的过程中我饱受迷路的困扰：黑灯瞎火的洞窟、乱七八糟的陷阱、不可名状的地图···于是我决定找一款我想怎么走就怎么走的游戏——《Assassin‘s Creed》。一入坑就是10年，我愿时间永远停留在佛罗伦萨塔顶的月圆之夜。库里的全套刺客信条通关了，我又想念剑与魔法的故事了，因为久仰其大名我下载了——《The Witcher 3》。这是一部我最喜欢的游戏，没有之一，这也许就是“第九艺术”吧。三言两语不能表达出其中的波澜壮阔，每一个支线，每一部DLC都值得我一遍又一遍地游玩。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs plain">我吻过凯尔莫罕忽然冷冽而至的白雪，<br>我听过史凯利杰伴着海妖清啸的海风。<br>我仰头饮尽诺威格瑞陈年的矮人烈酒，<br>我策马走遍全威伦最艰险的万水千山。<br>对我而言，家是什么地方。<br>是那抹黑白裙摆的倩影，<br>还是那丁香与醋栗的芬芳。<br></code></pre></td></tr></table></figure><p>给我留下深刻印象的游戏有很多，尼尔里的“世界竟然如此美丽”，2077里的”以我残躯化烈火”，死亡搁浅里的“我在冥滩等你”······希望我的程序猿之路最终可以走到像小岛秀夫那样，拥有自己的工作室，书写自己的艺术。</p><p>（另外上面2B那张图的作者是Wlop，我最喜欢的画师）</p><h2 id="Novel"><a href="#Novel" class="headerlink" title="Novel"></a>Novel</h2><p>我还年轻的时候会看一些天蚕土豆，唐家三少，耳根写的小说······对我的文学水平真是没有半点提升。</p><p>在高二语文老师的耳濡目染下，我一个理工男对文学兴趣盎然。即使很忙，也想抽点时间陶冶情操。</p><div style="text-align:center">落霞与孤鹜齐飞，秋水共长天一色</div><p><a href="https://imgtu.com/i/gV41qf"><img src="https://z3.ax1x.com/2021/05/01/gV41qf.jpg" alt="gV41qf.jpg"></a></p><p>除了诗与词，还记得名字的书只剩下《巨人的陨落》、《三体》、《外婆的道歉信》、《自由在高处》······</p><p>剩下的各种悬疑侦探小说就不列举了（不过强推一手《神探夏洛克》《POI疑犯追踪》）。</p><h2 id="Music"><a href="#Music" class="headerlink" title="Music"></a>Music</h2><p>我这个人听的歌有亿点点杂，基本歌单里什么都沾一点（二次元&amp;欧美占比较大）</p><p>特地开了一个Music版块都是因为——hanser！</p><div style="text-align:center"><iframe width="100%" height="500" src="https://player.bilibili.com/player.html?aid=4848309&bvid=BV1Cs411i7B1&cid=7870718&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe></div><p>2016-10-01至今，永远单推憨憨！</p><p><a href="https://imgtu.com/i/4jdP0g"><img src="https://z3.ax1x.com/2021/10/05/4jdP0g.jpg" alt="4jdP0g.jpg"></a></p><h1 id="Enderfga。"><a href="#Enderfga。" class="headerlink" title="Enderfga。"></a>Enderfga。</h1><p>不知不觉写了好多废话了······</p><p>总之，大学生活开始了，希望我能当好一个神奇海螺/哆啦安梦。</p><div align="right">----Nothing is true，everything is permitted.</div>]]></content>
    
    
    
    <tags>
      
      <tag>self</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
